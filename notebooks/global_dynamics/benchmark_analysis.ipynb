{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8dc17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://193.171.177.138:9999/blazegraph/namespace/ito/sparql'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPARQL endpoint hosting previous version of ITO.owl\n",
    "import yaml\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "endpoint = config['blazegraph']['endpoint']\n",
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4389151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import types\n",
    "import datetime\n",
    "import decimal\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pylab as py\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "if not os.path.exists('artefacts'):\n",
    "    os.makedirs('artefacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e29c8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefixes = \"\"\"\n",
    "prefix owl: <http://www.w3.org/2002/07/owl#>\n",
    "prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "prefix ito: <https://identifiers.org/ito:>\n",
    "prefix edam: <http://edamontology.org/>\n",
    "prefix obo: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def query(query, return_format = JSON):\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.method = 'POST'\n",
    "    sparql.setReturnFormat(return_format)\n",
    "    sparql.setQuery(prefixes + query)\n",
    "    results = sparql.query().convert()\n",
    "    return results\n",
    "\n",
    "\n",
    "def query_df(query, numeric_cols = []):\n",
    "    # Run SPARQL query, and convert results to Pandas dataframe\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.method = 'POST'\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setQuery(prefixes + query)\n",
    "    results = sparql.query()\n",
    "    processed_results = json.load(results.response)\n",
    "    cols = processed_results['head']['vars']\n",
    "\n",
    "    out = []\n",
    "    for row in processed_results['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        out.append(item)\n",
    "        \n",
    "    df = pd.DataFrame(out, columns=cols)\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tasks_stat_dict(df):\n",
    "    from collections import defaultdict\n",
    "    tasks_stat = defaultdict(int)\n",
    "    for idx, row in df.iterrows():\n",
    "        for task in row[\"tasks\"]:\n",
    "            tasks_stat[task] += 1\n",
    "    return dict(sorted(tasks_stat.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def get_variant_to_parent():\n",
    "    \"\"\"\n",
    "    filename = './datasets.json.gz'\n",
    "    if not os.path.exists(filename):\n",
    "        url = 'https://paperswithcode.com/media/about/datasets.json.gz'\n",
    "        myfile = requests.get(url)\n",
    "        with open(filename, 'wb') as o:\n",
    "            o.write(myfile.content)\n",
    "        with gzip.open(filename, 'rb') as f_in:\n",
    "            with open('./data/datasets.json', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    \"\"\"\n",
    "\n",
    "    with open('./data/datasets.json') as f:\n",
    "        datasets_ = json.load(f)\n",
    "        \n",
    "    variant_to_parent = list()\n",
    "    variant_to_meta = dict()\n",
    "    # unprioritize independency\n",
    "\n",
    "    for dataset in datasets_:\n",
    "        paper = dataset.get(\"paper\", None)\n",
    "        variant_to_meta[dataset[\"name\"]] = (dataset[\"url\"], dataset[\"homepage\"], paper[\"title\"] if paper else None, paper[\"url\"] if paper else None)\n",
    "    for dataset in datasets_:\n",
    "        ## create variant_to_parent mapping \n",
    "        paper = dataset.get(\"paper\", None)\n",
    "        for variant in dataset[\"variants\"]:\n",
    "            if dataset[\"name\"] != variant:\n",
    "                variant_to_parent.append([variant, dataset[\"name\"]])\n",
    "                variant_to_meta[dataset[\"name\"]] = (dataset[\"url\"], dataset[\"homepage\"], paper[\"title\"] if paper else None, paper[\"url\"] if paper else None)\n",
    "    variant_to_parent = pd.DataFrame(variant_to_parent, columns=[\"variant\", \"parent\"])\n",
    "    return variant_to_parent, variant_to_meta\n",
    "\n",
    "def extract_df(endpoint, prefix, root, sort_by):\n",
    "    df = query_df(\"\"\"\n",
    "    SELECT * WHERE {\n",
    "        ?benchmark rdfs:subClassOf+ <\"\"\" + root + \"\"\"> .\n",
    "        ?benchmark rdfs:subClassOf <https://identifiers.org/ito:Benchmarking> .\n",
    "        ?benchmark rdfs:label ?benchmark_label .\n",
    "        ?result rdf:type ?benchmark .\n",
    "        ?result ito:has_input ?dataset .\n",
    "        OPTIONAL{\n",
    "            ?result obo:date ?date .\n",
    "            ?result rdfs:seeAlso ?paper .\n",
    "            ?paper a edam:data_0971 .\n",
    "            ?paper rdfs:label ?paper_label .\n",
    "        }\n",
    "    }\n",
    "    \"\"\")\n",
    "    df[\"benchmark_label\"] = df[\"benchmark_label\"].str.replace(\" benchmarking\", \"\")\n",
    "    df[[\"benchmark_label\", \"task_label\"]] = df[\"benchmark_label\"].str.split(' - ', 1, expand=True)\n",
    "    df[\"date\"] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "    df = df.sort_values(by=\"date\")\n",
    "\n",
    "    # df[\"benchmark_parent_label\"] = df[\"benchmark_label\"].apply(lambda x: variant_to_parent.get(x, None))\n",
    "\n",
    "    variant_to_parent, variant_to_meta = get_variant_to_parent()\n",
    "\n",
    "    df = df.merge(variant_to_parent, left_on=\"benchmark_label\", right_on=\"variant\", how=\"left\")\n",
    "    df = df.rename(columns={\"parent\": \"benchmark_parent_label\"}).drop(columns=\"variant\")\n",
    "    df[\"benchmark_parent_label\"] = df[\"benchmark_parent_label\"].fillna(df[\"benchmark_label\"])\n",
    "    df = df.dropna(subset=[\"benchmark_parent_label\"])\n",
    "    df[\"url\"] = df[\"benchmark_parent_label\"].apply(lambda x: variant_to_meta[x][0] if x in variant_to_meta else None)\n",
    "    df[\"homepage\"] = df[\"benchmark_parent_label\"].apply(lambda x: variant_to_meta[x][1] if x in variant_to_meta else None)\n",
    "    df[\"paper_name\"] = df[\"benchmark_parent_label\"].apply(lambda x: variant_to_meta[x][2] if x in variant_to_meta else None)\n",
    "    df[\"paper_url\"] = df[\"benchmark_parent_label\"].apply(lambda x: variant_to_meta[x][3] if x in variant_to_meta else None)\n",
    "    return df\n",
    "\n",
    "def aggregate_metrics(df, prefix, root, sort_by):\n",
    "    def agg(examples):\n",
    "        examples = examples.sort_values(by=\"date\")\n",
    "        first_date = examples.iloc[0][\"date\"]\n",
    "        tasks = set(examples[\"task_label\"])\n",
    "        count = examples.groupby([\"benchmark_label\"])[\"benchmark_label\"].count()\n",
    "        \n",
    "        papers = examples[[\"benchmark_label\", \"paper_label\"]]\n",
    "        papers = papers.dropna().drop_duplicates()\n",
    "        count_paper = papers.groupby([\"benchmark_label\"])[\"paper_label\"].count()\n",
    "        return pd.Series({\n",
    "            \"first_date\": first_date, \n",
    "            \"n_variant\": len(set(examples[\"benchmark_label\"])), \n",
    "            \"n_row_sum\": count.sum(), \n",
    "            \"n_row_min\": count.min(), \n",
    "            \"n_row_max\": count.max(), \n",
    "            \"n_row_mean\": count.mean(), \n",
    "            \"n_unique_paper_sum\": len(set(papers[\"paper_label\"])), \n",
    "            \"n_paper_min\": count_paper.min(), \n",
    "            \"n_paper_max\": count_paper.max(), \n",
    "            \"n_paper_mean\": count_paper.mean(), \n",
    "            \"n_tasks\": len(tasks), \n",
    "            \"tasks\": tasks})\n",
    "\n",
    "    df = df.groupby([\"benchmark_parent_label\", \"url\"])[[\"benchmark_parent_label\", \"result\", \"date\", \"benchmark_label\", \"paper_label\", \"task_label\"]].apply(agg).reset_index()\n",
    "    df = df.sort_values(by=sort_by, ascending=False)\n",
    "    df = df[df[sort_by] > 0]\n",
    "    df.to_csv(f\"artefacts/{prefix}_all_unfiltered.csv\", index=None)\n",
    "    return df\n",
    "\n",
    "def split_popular_unpopular(df, sort_by):\n",
    "    cumsum = df.sort_values(by=[sort_by]).reset_index(drop=True)\n",
    "    cumsum[\"cumsum\"] = cumsum[sort_by].cumsum()\n",
    "    max_ = cumsum[\"cumsum\"].max()\n",
    "    half = max_ / 2.\n",
    "\n",
    "    foo = 0\n",
    "    for ix, row in cumsum.iterrows():\n",
    "        if row[\"cumsum\"] > half:\n",
    "            corr_ix = ix\n",
    "            break\n",
    "\n",
    "    print(f\"{corr_ix * 100 / len(cumsum)} % of smallest datasets have the same amount of papers as {100 - (corr_ix * 100 / len(cumsum))} % of biggest datasets\")\n",
    "    print(f\"Length popular: {len(cumsum) - corr_ix}\")\n",
    "    popular = cumsum.iloc[corr_ix:].sort_values(by=sort_by, ascending=False)\n",
    "    unpopular = cumsum.iloc[:corr_ix].sample(len(popular), random_state=1).sort_values(by=sort_by, ascending=False)\n",
    "    \n",
    "    popular_task_types = pd.DataFrame(tasks_stat_dict(popular).items(), columns=[\"task_type\", \"n\"])\n",
    "    unpopular_task_types = pd.DataFrame(tasks_stat_dict(unpopular).items(), columns=[\"task_type\", \"n\"])\n",
    "    \n",
    "    return popular, unpopular, popular_task_types, unpopular_task_types\n",
    "\n",
    "# plot and wirte all datasets\n",
    "def plot_and_write_all(df, prefix, sort_by, plot=True):\n",
    "    label_x = \"Number of papers utilizing dataset\"\n",
    "    label_y = \"Number of datasets\"\n",
    "\n",
    "    g = sns.displot(df[sort_by], color='g', kind=\"hist\", kde=True, aspect=2, bins=12)\n",
    "    g.set_axis_labels(label_x, label_y)\n",
    "    plt.savefig(f\"artefacts/{prefix}.png\")\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    g = sns.displot(df[sort_by], color='g', kind=\"hist\", kde=True, aspect=2, log_scale=True, bins=12)\n",
    "    g.set_axis_labels(label_x, label_y)\n",
    "    plt.savefig(f\"artefacts/{prefix}_log.png\")\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    popular, unpopular, popular_task_types, unpopular_task_types = split_popular_unpopular(df, sort_by)\n",
    "\n",
    "    if plot:\n",
    "        display(df.describe())\n",
    "        display(popular[[\"n_tasks\", \"n_variant\"]].describe())\n",
    "        display(unpopular[[\"n_tasks\", \"n_variant\"]].describe())\n",
    "        display(popular[[\"benchmark_parent_label\", sort_by, \"n_variant\", \"n_tasks\", \"tasks\", \"url\"]].head(5).style.set_caption(\"Popular benchmarks\"))\n",
    "        display(unpopular[[\"benchmark_parent_label\", sort_by, \"n_variant\", \"n_tasks\", \"tasks\", \"url\"]].head(5).style.set_caption(\"Unpopular benchmarks\"))\n",
    "        display(unpopular_task_types.head(5).style.set_caption(\"Task types in unpopular\"))\n",
    "        display(popular_task_types.head(5).style.set_caption(\"Task types in popular\"))\n",
    "    popular.to_csv(f\"artefacts/{prefix}_popular.csv\", index=None)\n",
    "    unpopular.to_csv(f\"artefacts/{prefix}_unpopular.csv\", index=None)\n",
    "    popular_task_types.to_csv(f\"artefacts/{prefix}_popular_task_types.csv\", index=None)\n",
    "    unpopular_task_types.to_csv(f\"artefacts/{prefix}_unpopular_task_types.csv\", index=None)\n",
    "\n",
    "\n",
    "# plot and write only starting form 2018\n",
    "def plot_and_write_starting_year(df, prefix, sort_by, first_year, plot=True):\n",
    "    # filter year\n",
    "    if first_year > -1:\n",
    "        filtered_df = df[df['first_date'].dt.year == first_year]\n",
    "\n",
    "    label_x = \"Number of papers utilizing dataset\"\n",
    "    label_y = \"Number of datasets\"\n",
    "    g = sns.displot(filtered_df[sort_by], color='g', kind=\"hist\", kde=True, aspect=2, bins=12)\n",
    "    g.set_axis_labels(label_x, label_y)\n",
    "    plt.savefig(f\"artefacts/{prefix}_2018.png\")\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    popular, unpopular, popular_task_types, unpopular_task_types = split_popular_unpopular(filtered_df, sort_by)\n",
    "\n",
    "    if plot:\n",
    "        display(filtered_df.describe())\n",
    "        display(popular[[\"n_tasks\", \"n_variant\"]].describe())\n",
    "        display(unpopular[[\"n_tasks\", \"n_variant\"]].describe())\n",
    "        display(popular[[\"benchmark_parent_label\", sort_by, \"n_variant\", \"n_tasks\", \"tasks\", \"url\"]].head(5).style.set_caption(\"Popular benchmarks\"))\n",
    "        display(unpopular[[\"benchmark_parent_label\", sort_by, \"n_variant\", \"n_tasks\", \"tasks\", \"url\"]].head(5).style.set_caption(\"Unpopular benchmarks\"))\n",
    "        display(unpopular_task_types.head(5).style.set_caption(\"Task types in unpopular\"))\n",
    "        display(popular_task_types.head(5).style.set_caption(\"Task types in popular\"))\n",
    "    \n",
    "    popular.to_csv(f\"artefacts/{prefix}_popular_2018.csv\", index=None)\n",
    "    unpopular.to_csv(f\"artefacts/{prefix}_unpopular_2018.csv\", index=None)\n",
    "    popular_task_types.to_csv(f\"artefacts/{prefix}_popular_task_types_2018.csv\", index=None)\n",
    "    unpopular_task_types.to_csv(f\"artefacts/{prefix}_unpopular_task_types_2018.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac4d0b",
   "metadata": {},
   "source": [
    "## Plots for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aab7d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural_language_processing\n",
      "ALL\n",
      "90.31719532554257 % of smallest datasets have the same amount of papers as 9.68280467445743 % of biggest datasets\n",
      "Length popular: 58\n",
      "START 2018\n",
      "78.125 % of smallest datasets have the same amount of papers as 21.875 % of biggest datasets\n",
      "Length popular: 14\n",
      "\n",
      "vision_process\n",
      "ALL\n",
      "94.76744186046511 % of smallest datasets have the same amount of papers as 5.232558139534888 % of biggest datasets\n",
      "Length popular: 54\n",
      "START 2018\n",
      "79.23076923076923 % of smallest datasets have the same amount of papers as 20.769230769230774 % of biggest datasets\n",
      "Length popular: 27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tlc, root in [(\"natural_language_processing\", \"https://identifiers.org/ito:ITO_00141\"), (\"vision_process\", \"https://identifiers.org/ito:ITO_00101\")]:\n",
    "    print(tlc)\n",
    "    sort_by = \"n_unique_paper_sum\"\n",
    "    df = extract_df(endpoint, tlc, root, sort_by)\n",
    "    df = aggregate_metrics(df, tlc, root, sort_by)\n",
    "    print(\"ALL\")\n",
    "    plot_and_write_all(df, tlc, sort_by, plot=False)\n",
    "    print(\"START 2018\")\n",
    "    plot_and_write_starting_year(df, tlc, sort_by, 2018, plot=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac3579",
   "metadata": {},
   "source": [
    "# Incompletness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16719937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmark</th>\n",
       "      <th>benchmark_label</th>\n",
       "      <th>result</th>\n",
       "      <th>dataset</th>\n",
       "      <th>date</th>\n",
       "      <th>paper</th>\n",
       "      <th>paper_label</th>\n",
       "      <th>task_label</th>\n",
       "      <th>benchmark_parent_label</th>\n",
       "      <th>url</th>\n",
       "      <th>homepage</th>\n",
       "      <th>paper_name</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://identifiers.org/ito:ITO_53501</td>\n",
       "      <td>Penn Treebank</td>\n",
       "      <td>https://identifiers.org/ito:ITO_iqwDxdruDLC3K2ewW</td>\n",
       "      <td>https://identifiers.org/ito:ITO_04494</td>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>https://identifiers.org/ito:ITO_53505</td>\n",
       "      <td>Corpus-Based Induction of Syntactic Structure:...</td>\n",
       "      <td>Unsupervised dependency parsing</td>\n",
       "      <td>Penn Treebank</td>\n",
       "      <td>https://paperswithcode.com/dataset/penn-treebank</td>\n",
       "      <td>https://catalog.ldc.upenn.edu/docs/LDC95T7/cl9...</td>\n",
       "      <td>Building a Large Annotated Corpus of English: ...</td>\n",
       "      <td>http://dl.acm.org/citation.cfm?id=972470.972475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://identifiers.org/ito:ITO_56230</td>\n",
       "      <td>Penn Treebank</td>\n",
       "      <td>https://identifiers.org/ito:ITO_izjN0bn2si9MTQk8H</td>\n",
       "      <td>https://identifiers.org/ito:ITO_04494</td>\n",
       "      <td>2006-06-01</td>\n",
       "      <td>https://identifiers.org/ito:ITO_34554</td>\n",
       "      <td>Effective Self-Training for Parsing</td>\n",
       "      <td>Constituency parsing</td>\n",
       "      <td>Penn Treebank</td>\n",
       "      <td>https://paperswithcode.com/dataset/penn-treebank</td>\n",
       "      <td>https://catalog.ldc.upenn.edu/docs/LDC95T7/cl9...</td>\n",
       "      <td>Building a Large Annotated Corpus of English: ...</td>\n",
       "      <td>http://dl.acm.org/citation.cfm?id=972470.972475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://identifiers.org/ito:ITO_45414</td>\n",
       "      <td>Linux IRC (Ch2 Elsner)</td>\n",
       "      <td>https://identifiers.org/ito:ITO_iwcpMJE5wRevyLqJV</td>\n",
       "      <td>https://identifiers.org/ito:ITO_21330</td>\n",
       "      <td>2008-06-01</td>\n",
       "      <td>https://identifiers.org/ito:ITO_45413</td>\n",
       "      <td>You Talking to Me? A Corpus and Algorithm for ...</td>\n",
       "      <td>Conversation disentanglement</td>\n",
       "      <td>irc-disentanglement</td>\n",
       "      <td>https://paperswithcode.com/dataset/irc-disenta...</td>\n",
       "      <td>https://github.com/jkkummerfeld/irc-disentangl...</td>\n",
       "      <td>A Large-Scale Corpus for Conversation Disentan...</td>\n",
       "      <td>https://paperswithcode.com/paper/analyzing-ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://identifiers.org/ito:ITO_45412</td>\n",
       "      <td>Linux IRC (Ch2 Kummerfeld)</td>\n",
       "      <td>https://identifiers.org/ito:ITO_iPxkvmyiiIdT4pNeV</td>\n",
       "      <td>https://identifiers.org/ito:ITO_21324</td>\n",
       "      <td>2008-06-01</td>\n",
       "      <td>https://identifiers.org/ito:ITO_45413</td>\n",
       "      <td>You Talking to Me? A Corpus and Algorithm for ...</td>\n",
       "      <td>Conversation disentanglement</td>\n",
       "      <td>irc-disentanglement</td>\n",
       "      <td>https://paperswithcode.com/dataset/irc-disenta...</td>\n",
       "      <td>https://github.com/jkkummerfeld/irc-disentangl...</td>\n",
       "      <td>A Large-Scale Corpus for Conversation Disentan...</td>\n",
       "      <td>https://paperswithcode.com/paper/analyzing-ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://identifiers.org/ito:ITO_45415</td>\n",
       "      <td>irc-disentanglement</td>\n",
       "      <td>https://identifiers.org/ito:ITO_iujJfxeLdxFdVFNpL</td>\n",
       "      <td>https://identifiers.org/ito:ITO_21309</td>\n",
       "      <td>2008-06-01</td>\n",
       "      <td>https://identifiers.org/ito:ITO_45413</td>\n",
       "      <td>You Talking to Me? A Corpus and Algorithm for ...</td>\n",
       "      <td>Conversation disentanglement</td>\n",
       "      <td>irc-disentanglement</td>\n",
       "      <td>https://paperswithcode.com/dataset/irc-disenta...</td>\n",
       "      <td>https://github.com/jkkummerfeld/irc-disentangl...</td>\n",
       "      <td>A Large-Scale Corpus for Conversation Disentan...</td>\n",
       "      <td>https://paperswithcode.com/paper/analyzing-ass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               benchmark             benchmark_label  \\\n",
       "0  https://identifiers.org/ito:ITO_53501               Penn Treebank   \n",
       "1  https://identifiers.org/ito:ITO_56230               Penn Treebank   \n",
       "2  https://identifiers.org/ito:ITO_45414      Linux IRC (Ch2 Elsner)   \n",
       "3  https://identifiers.org/ito:ITO_45412  Linux IRC (Ch2 Kummerfeld)   \n",
       "4  https://identifiers.org/ito:ITO_45415         irc-disentanglement   \n",
       "\n",
       "                                              result  \\\n",
       "0  https://identifiers.org/ito:ITO_iqwDxdruDLC3K2ewW   \n",
       "1  https://identifiers.org/ito:ITO_izjN0bn2si9MTQk8H   \n",
       "2  https://identifiers.org/ito:ITO_iwcpMJE5wRevyLqJV   \n",
       "3  https://identifiers.org/ito:ITO_iPxkvmyiiIdT4pNeV   \n",
       "4  https://identifiers.org/ito:ITO_iujJfxeLdxFdVFNpL   \n",
       "\n",
       "                                 dataset       date  \\\n",
       "0  https://identifiers.org/ito:ITO_04494 2004-07-01   \n",
       "1  https://identifiers.org/ito:ITO_04494 2006-06-01   \n",
       "2  https://identifiers.org/ito:ITO_21330 2008-06-01   \n",
       "3  https://identifiers.org/ito:ITO_21324 2008-06-01   \n",
       "4  https://identifiers.org/ito:ITO_21309 2008-06-01   \n",
       "\n",
       "                                   paper  \\\n",
       "0  https://identifiers.org/ito:ITO_53505   \n",
       "1  https://identifiers.org/ito:ITO_34554   \n",
       "2  https://identifiers.org/ito:ITO_45413   \n",
       "3  https://identifiers.org/ito:ITO_45413   \n",
       "4  https://identifiers.org/ito:ITO_45413   \n",
       "\n",
       "                                         paper_label  \\\n",
       "0  Corpus-Based Induction of Syntactic Structure:...   \n",
       "1                Effective Self-Training for Parsing   \n",
       "2  You Talking to Me? A Corpus and Algorithm for ...   \n",
       "3  You Talking to Me? A Corpus and Algorithm for ...   \n",
       "4  You Talking to Me? A Corpus and Algorithm for ...   \n",
       "\n",
       "                        task_label benchmark_parent_label  \\\n",
       "0  Unsupervised dependency parsing          Penn Treebank   \n",
       "1             Constituency parsing          Penn Treebank   \n",
       "2     Conversation disentanglement    irc-disentanglement   \n",
       "3     Conversation disentanglement    irc-disentanglement   \n",
       "4     Conversation disentanglement    irc-disentanglement   \n",
       "\n",
       "                                                 url  \\\n",
       "0   https://paperswithcode.com/dataset/penn-treebank   \n",
       "1   https://paperswithcode.com/dataset/penn-treebank   \n",
       "2  https://paperswithcode.com/dataset/irc-disenta...   \n",
       "3  https://paperswithcode.com/dataset/irc-disenta...   \n",
       "4  https://paperswithcode.com/dataset/irc-disenta...   \n",
       "\n",
       "                                            homepage  \\\n",
       "0  https://catalog.ldc.upenn.edu/docs/LDC95T7/cl9...   \n",
       "1  https://catalog.ldc.upenn.edu/docs/LDC95T7/cl9...   \n",
       "2  https://github.com/jkkummerfeld/irc-disentangl...   \n",
       "3  https://github.com/jkkummerfeld/irc-disentangl...   \n",
       "4  https://github.com/jkkummerfeld/irc-disentangl...   \n",
       "\n",
       "                                          paper_name  \\\n",
       "0  Building a Large Annotated Corpus of English: ...   \n",
       "1  Building a Large Annotated Corpus of English: ...   \n",
       "2  A Large-Scale Corpus for Conversation Disentan...   \n",
       "3  A Large-Scale Corpus for Conversation Disentan...   \n",
       "4  A Large-Scale Corpus for Conversation Disentan...   \n",
       "\n",
       "                                           paper_url  \n",
       "0    http://dl.acm.org/citation.cfm?id=972470.972475  \n",
       "1    http://dl.acm.org/citation.cfm?id=972470.972475  \n",
       "2  https://paperswithcode.com/paper/analyzing-ass...  \n",
       "3  https://paperswithcode.com/paper/analyzing-ass...  \n",
       "4  https://paperswithcode.com/paper/analyzing-ass...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = extract_df(endpoint, \"natural_language_processing\", \"https://identifiers.org/ito:ITO_00141\", \"n_unique_paper_sum\")\n",
    "df2 = extract_df(endpoint, \"vision_process\", \"https://identifiers.org/ito:ITO_00101\", \"n_unique_paper_sum\")\n",
    "#df.groupby([\"benchmark_label\", \"benchmark_parent_label\",\"url\", \"task_label\"])\n",
    "df = pd.concat([df1, df2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bb1685f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmark_label</th>\n",
       "      <th>benchmark_parent_label</th>\n",
       "      <th>url</th>\n",
       "      <th>homepage</th>\n",
       "      <th>paper_name</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7126</th>\n",
       "      <td>VIDIT’20 validation set</td>\n",
       "      <td>VIDIT’20 validation set</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>n-MNIST</td>\n",
       "      <td>n-MNIST</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6113</th>\n",
       "      <td>CAT 256x256</td>\n",
       "      <td>CAT 256x256</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8056</th>\n",
       "      <td>FashionIQ</td>\n",
       "      <td>Fashion IQ</td>\n",
       "      <td>https://paperswithcode.com/dataset/fashion-iq</td>\n",
       "      <td>https://github.com/XiaoxiaoGuo/fashion-iq</td>\n",
       "      <td>Fashion IQ: A New Dataset Towards Retrieving I...</td>\n",
       "      <td>https://paperswithcode.com/paper/the-fashion-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5723</th>\n",
       "      <td>YouTube-VOS 2018 val</td>\n",
       "      <td>YouTube-VOS 2018 val</td>\n",
       "      <td>https://paperswithcode.com/dataset/youtube-vos</td>\n",
       "      <td>https://youtube-vos.org/</td>\n",
       "      <td>YouTube-VOS: A Large-Scale Video Object Segmen...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.03327.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5063</th>\n",
       "      <td>SAT-4</td>\n",
       "      <td>SAT-4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>Oxf5k</td>\n",
       "      <td>Oxf5k</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Persona-Chat</td>\n",
       "      <td>Persona-Chat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7576</th>\n",
       "      <td>FC100 5-way (1-shot)</td>\n",
       "      <td>FC100</td>\n",
       "      <td>https://paperswithcode.com/dataset/fc100</td>\n",
       "      <td>https://github.com/ElementAI/TADAM</td>\n",
       "      <td>TADAM: Task dependent adaptive metric for impr...</td>\n",
       "      <td>https://paperswithcode.com/paper/tadam-task-de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>TabFact</td>\n",
       "      <td>TabFact</td>\n",
       "      <td>https://paperswithcode.com/dataset/tabfact</td>\n",
       "      <td>https://tabfact.github.io/</td>\n",
       "      <td>TabFact: A Large-scale Dataset for Table-based...</td>\n",
       "      <td>https://paperswithcode.com/paper/tabfact-a-lar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              benchmark_label   benchmark_parent_label  \\\n",
       "7126  VIDIT’20 validation set  VIDIT’20 validation set   \n",
       "7764                  n-MNIST                  n-MNIST   \n",
       "6113              CAT 256x256              CAT 256x256   \n",
       "8056                FashionIQ               Fashion IQ   \n",
       "5723     YouTube-VOS 2018 val     YouTube-VOS 2018 val   \n",
       "5063                    SAT-4                    SAT-4   \n",
       "5332                    Oxf5k                    Oxf5k   \n",
       "71               Persona-Chat             Persona-Chat   \n",
       "7576     FC100 5-way (1-shot)                    FC100   \n",
       "2474                  TabFact                  TabFact   \n",
       "\n",
       "                                                 url  \\\n",
       "7126                                                   \n",
       "7764                                                   \n",
       "6113                                                   \n",
       "8056   https://paperswithcode.com/dataset/fashion-iq   \n",
       "5723  https://paperswithcode.com/dataset/youtube-vos   \n",
       "5063                                                   \n",
       "5332                                                   \n",
       "71                                                     \n",
       "7576        https://paperswithcode.com/dataset/fc100   \n",
       "2474      https://paperswithcode.com/dataset/tabfact   \n",
       "\n",
       "                                       homepage  \\\n",
       "7126                                              \n",
       "7764                                              \n",
       "6113                                              \n",
       "8056  https://github.com/XiaoxiaoGuo/fashion-iq   \n",
       "5723                   https://youtube-vos.org/   \n",
       "5063                                              \n",
       "5332                                              \n",
       "71                                                \n",
       "7576         https://github.com/ElementAI/TADAM   \n",
       "2474                 https://tabfact.github.io/   \n",
       "\n",
       "                                             paper_name  \\\n",
       "7126                                                      \n",
       "7764                                                      \n",
       "6113                                                      \n",
       "8056  Fashion IQ: A New Dataset Towards Retrieving I...   \n",
       "5723  YouTube-VOS: A Large-Scale Video Object Segmen...   \n",
       "5063                                                      \n",
       "5332                                                      \n",
       "71                                                        \n",
       "7576  TADAM: Task dependent adaptive metric for impr...   \n",
       "2474  TabFact: A Large-scale Dataset for Table-based...   \n",
       "\n",
       "                                              paper_url  \n",
       "7126                                                     \n",
       "7764                                                     \n",
       "6113                                                     \n",
       "8056  https://paperswithcode.com/paper/the-fashion-i...  \n",
       "5723               https://arxiv.org/pdf/1809.03327.pdf  \n",
       "5063                                                     \n",
       "5332                                                     \n",
       "71                                                       \n",
       "7576  https://paperswithcode.com/paper/tadam-task-de...  \n",
       "2474  https://paperswithcode.com/paper/tabfact-a-lar...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = df.dropna(subset=[\"paper_label\"])\n",
    "benchmarks = benchmarks[[\"date\", \"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\", \"paper_label\"]].drop_duplicates()\n",
    "benchmarks = benchmarks[benchmarks[\"date\"] <= datetime.datetime(2021, 12, 31)]\n",
    "benchmarks = benchmarks[[\"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\", \"paper_label\"]]\n",
    "benchmarks = benchmarks.fillna(\"\")\n",
    "benchmarks = benchmarks.groupby([\"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\"], as_index=False, dropna=False).filter(lambda x: len(x) >= 3).reset_index(drop=True)\n",
    "random_benchmarks = benchmarks[[\"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\"]].drop_duplicates().sample(10, random_state=1)\n",
    "random_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606a044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural_language_processing\n",
      "1077\n",
      "vision_process\n",
      "1837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmark_parent_label</th>\n",
       "      <th>url</th>\n",
       "      <th>homepage</th>\n",
       "      <th>paper_name</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>benchmark_label</th>\n",
       "      <th>tasks</th>\n",
       "      <th>n_unique_papers</th>\n",
       "      <th>n_unique_sota_papers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Billion Word Benchmark</td>\n",
       "      <td>https://paperswithcode.com/dataset/billion-wor...</td>\n",
       "      <td>https://code.google.com/archive/p/1-billion-wo...</td>\n",
       "      <td>One Billion Word Benchmark for Measuring Progr...</td>\n",
       "      <td>https://paperswithcode.com/paper/one-billion-w...</td>\n",
       "      <td>One Billion Word</td>\n",
       "      <td>Text generation, Language modelling</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAS-VSR-W1k (LRW-1000)</td>\n",
       "      <td>https://paperswithcode.com/dataset/lrw-1000</td>\n",
       "      <td>https://vipl.ict.ac.cn/en/view_database.php?id=13</td>\n",
       "      <td>LRW-1000: A Naturally-Distributed Large-Scale ...</td>\n",
       "      <td>https://paperswithcode.com/paper/lrw-1000-a-na...</td>\n",
       "      <td>CAS-VSR-W1k (LRW-1000)</td>\n",
       "      <td>Lip reading</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLEVR</td>\n",
       "      <td>https://paperswithcode.com/dataset/clevr</td>\n",
       "      <td>https://cs.stanford.edu/people/jcjohns/clevr/</td>\n",
       "      <td>CLEVR: A Diagnostic Dataset for Compositional ...</td>\n",
       "      <td>https://paperswithcode.com/paper/clevr-a-diagn...</td>\n",
       "      <td>CLEVR</td>\n",
       "      <td>Image question answering</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HellaSwag</td>\n",
       "      <td>https://paperswithcode.com/dataset/hellaswag</td>\n",
       "      <td>https://rowanzellers.com/hellaswag/</td>\n",
       "      <td>HellaSwag: Can a Machine Really Finish Your Se...</td>\n",
       "      <td>https://paperswithcode.com/paper/hellaswag-can...</td>\n",
       "      <td>HellaSwag</td>\n",
       "      <td>Sentence Completion</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NNE</td>\n",
       "      <td>https://paperswithcode.com/dataset/nne</td>\n",
       "      <td></td>\n",
       "      <td>NNE: A Dataset for Nested Named Entity Recogni...</td>\n",
       "      <td>https://paperswithcode.com/paper/nne-a-dataset...</td>\n",
       "      <td>NNE</td>\n",
       "      <td>Nested named entity recognition</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NewsQA</td>\n",
       "      <td>https://paperswithcode.com/dataset/newsqa</td>\n",
       "      <td>https://www.microsoft.com/en-us/research/proje...</td>\n",
       "      <td>NewsQA: A Machine Comprehension Dataset</td>\n",
       "      <td>https://paperswithcode.com/paper/newsqa-a-mach...</td>\n",
       "      <td>NewsQA</td>\n",
       "      <td>Question answering</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sepehr_RumTel01</td>\n",
       "      <td>https://paperswithcode.com/dataset/sepehr-rumt...</td>\n",
       "      <td></td>\n",
       "      <td>A Speech Act Classifier for Persian Texts and ...</td>\n",
       "      <td>https://paperswithcode.com/paper/a-speech-act-...</td>\n",
       "      <td>Sepehr_RumTel01</td>\n",
       "      <td>Rumor Detection</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Spoken-SQuAD</td>\n",
       "      <td>https://paperswithcode.com/dataset/spoken-squad</td>\n",
       "      <td>https://github.com/chiahsuan156/Spoken-SQuAD</td>\n",
       "      <td>Spoken SQuAD: A Study of Mitigating the Impact...</td>\n",
       "      <td>https://paperswithcode.com/paper/spoken-squad-...</td>\n",
       "      <td>Spoken-SQuAD</td>\n",
       "      <td>Spoken language understanding</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WebQuestionsSP</td>\n",
       "      <td>https://paperswithcode.com/dataset/webquestionssp</td>\n",
       "      <td>https://www.microsoft.com/en-us/download/detai...</td>\n",
       "      <td>The Value of Semantic Parse Labeling for Knowl...</td>\n",
       "      <td>https://paperswithcode.com/paper/the-value-of-...</td>\n",
       "      <td>WebQuestionsSP</td>\n",
       "      <td>Knowledge base question answering, Semantic pa...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WikiHow</td>\n",
       "      <td>https://paperswithcode.com/dataset/wikihow</td>\n",
       "      <td>https://github.com/mahnazkoupaee/WikiHow-Dataset</td>\n",
       "      <td>WikiHow: A Large Scale Text Summarization Dataset</td>\n",
       "      <td>https://paperswithcode.com/paper/wikihow-a-lar...</td>\n",
       "      <td>WikiHow</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ActivityNet Captions</td>\n",
       "      <td>https://paperswithcode.com/dataset/activitynet...</td>\n",
       "      <td>https://cs.stanford.edu/people/ranjaykrishna/d...</td>\n",
       "      <td>Dense-Captioning Events in Videos</td>\n",
       "      <td>https://paperswithcode.com/paper/dense-caption...</td>\n",
       "      <td>ActivityNet Captions</td>\n",
       "      <td>Video captioning, Temporal action proposal gen...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AdobeVFR syn</td>\n",
       "      <td>https://paperswithcode.com/dataset/adobevfr</td>\n",
       "      <td></td>\n",
       "      <td>DeepFont: Identify Your Font from An Image</td>\n",
       "      <td>https://paperswithcode.com/paper/deepfont-iden...</td>\n",
       "      <td>AdobeVFR syn</td>\n",
       "      <td>Font Recognition</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Birdsnap</td>\n",
       "      <td>https://paperswithcode.com/dataset/birdsnap</td>\n",
       "      <td>http://thomasberg.org/</td>\n",
       "      <td>Birdsnap: Large-scale Fine-grained Visual Cate...</td>\n",
       "      <td>https://paperswithcode.com/paper/birdsnap-larg...</td>\n",
       "      <td>Birdsnap</td>\n",
       "      <td>Fine-grained image classification</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CHASE_DB1</td>\n",
       "      <td>https://paperswithcode.com/dataset/chase-db1</td>\n",
       "      <td>https://blogs.kingston.ac.uk/retinal/chasedb1/</td>\n",
       "      <td>An Ensemble Classification-Based Approach Appl...</td>\n",
       "      <td>https://doi.org/10.1109/TBME.2012.2205687</td>\n",
       "      <td>CHASE_DB1</td>\n",
       "      <td>Retinal vessel segmentation, Medical image seg...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrowdHuman</td>\n",
       "      <td>https://paperswithcode.com/dataset/crowdhuman</td>\n",
       "      <td>http://www.crowdhuman.org/</td>\n",
       "      <td>CrowdHuman: A Benchmark for Detecting Human in...</td>\n",
       "      <td>https://paperswithcode.com/paper/crowdhuman-a-...</td>\n",
       "      <td>CrowdHuman (full body)</td>\n",
       "      <td>Object detection</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DeepFashion</td>\n",
       "      <td>https://paperswithcode.com/dataset/deepfashion</td>\n",
       "      <td>https://liuziwei7.github.io/projects/DeepFashi...</td>\n",
       "      <td>DeepFashion: Powering Robust Clothes Recogniti...</td>\n",
       "      <td>https://paperswithcode.com/paper/deepfashion-p...</td>\n",
       "      <td>DeepFashion, Deep-Fashion</td>\n",
       "      <td>Consumer-to-shop - Image retrieval, Image retr...</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Google Refexp</td>\n",
       "      <td>https://paperswithcode.com/dataset/google-refexp</td>\n",
       "      <td>https://github.com/mjhucla/Google_Refexp_toolbox</td>\n",
       "      <td>Generation and Comprehension of Unambiguous Ob...</td>\n",
       "      <td>https://paperswithcode.com/paper/generation-an...</td>\n",
       "      <td>RefCOCOg-test, RefCOCOg-val</td>\n",
       "      <td>Referring expression segmentation</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MTL-AQA</td>\n",
       "      <td>https://paperswithcode.com/dataset/mtl-aqa</td>\n",
       "      <td>https://github.com/ParitoshParmar/MTL-AQA</td>\n",
       "      <td>What and How Well You Performed? A Multitask L...</td>\n",
       "      <td>https://paperswithcode.com/paper/what-and-how-...</td>\n",
       "      <td>MTL-AQA</td>\n",
       "      <td>Action quality assessment, Action recognition</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PACS</td>\n",
       "      <td>https://paperswithcode.com/dataset/pacs</td>\n",
       "      <td>https://domaingeneralization.github.io/#data</td>\n",
       "      <td>Deeper, Broader and Artier Domain Generalization</td>\n",
       "      <td>https://paperswithcode.com/paper/deeper-broade...</td>\n",
       "      <td>PACS</td>\n",
       "      <td>Domain generalization</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UCF-Crime</td>\n",
       "      <td>https://paperswithcode.com/dataset/ucf-crime</td>\n",
       "      <td>https://webpages.uncc.edu/cchen62/dataset.html</td>\n",
       "      <td>Real-world Anomaly Detection in Surveillance V...</td>\n",
       "      <td>https://paperswithcode.com/paper/real-world-an...</td>\n",
       "      <td>UCF-Crime</td>\n",
       "      <td>Anomaly detection in surveillance videos</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    benchmark_parent_label                                                url  \\\n",
       "0   Billion Word Benchmark  https://paperswithcode.com/dataset/billion-wor...   \n",
       "1   CAS-VSR-W1k (LRW-1000)        https://paperswithcode.com/dataset/lrw-1000   \n",
       "2                    CLEVR           https://paperswithcode.com/dataset/clevr   \n",
       "3                HellaSwag       https://paperswithcode.com/dataset/hellaswag   \n",
       "4                      NNE             https://paperswithcode.com/dataset/nne   \n",
       "5                   NewsQA          https://paperswithcode.com/dataset/newsqa   \n",
       "6          Sepehr_RumTel01  https://paperswithcode.com/dataset/sepehr-rumt...   \n",
       "7             Spoken-SQuAD    https://paperswithcode.com/dataset/spoken-squad   \n",
       "8           WebQuestionsSP  https://paperswithcode.com/dataset/webquestionssp   \n",
       "9                  WikiHow         https://paperswithcode.com/dataset/wikihow   \n",
       "10    ActivityNet Captions  https://paperswithcode.com/dataset/activitynet...   \n",
       "11            AdobeVFR syn        https://paperswithcode.com/dataset/adobevfr   \n",
       "12                Birdsnap        https://paperswithcode.com/dataset/birdsnap   \n",
       "13               CHASE_DB1       https://paperswithcode.com/dataset/chase-db1   \n",
       "14              CrowdHuman      https://paperswithcode.com/dataset/crowdhuman   \n",
       "15             DeepFashion     https://paperswithcode.com/dataset/deepfashion   \n",
       "16           Google Refexp   https://paperswithcode.com/dataset/google-refexp   \n",
       "17                 MTL-AQA         https://paperswithcode.com/dataset/mtl-aqa   \n",
       "18                    PACS            https://paperswithcode.com/dataset/pacs   \n",
       "19               UCF-Crime       https://paperswithcode.com/dataset/ucf-crime   \n",
       "\n",
       "                                             homepage  \\\n",
       "0   https://code.google.com/archive/p/1-billion-wo...   \n",
       "1   https://vipl.ict.ac.cn/en/view_database.php?id=13   \n",
       "2       https://cs.stanford.edu/people/jcjohns/clevr/   \n",
       "3                 https://rowanzellers.com/hellaswag/   \n",
       "4                                                       \n",
       "5   https://www.microsoft.com/en-us/research/proje...   \n",
       "6                                                       \n",
       "7        https://github.com/chiahsuan156/Spoken-SQuAD   \n",
       "8   https://www.microsoft.com/en-us/download/detai...   \n",
       "9    https://github.com/mahnazkoupaee/WikiHow-Dataset   \n",
       "10  https://cs.stanford.edu/people/ranjaykrishna/d...   \n",
       "11                                                      \n",
       "12                             http://thomasberg.org/   \n",
       "13     https://blogs.kingston.ac.uk/retinal/chasedb1/   \n",
       "14                         http://www.crowdhuman.org/   \n",
       "15  https://liuziwei7.github.io/projects/DeepFashi...   \n",
       "16   https://github.com/mjhucla/Google_Refexp_toolbox   \n",
       "17          https://github.com/ParitoshParmar/MTL-AQA   \n",
       "18       https://domaingeneralization.github.io/#data   \n",
       "19     https://webpages.uncc.edu/cchen62/dataset.html   \n",
       "\n",
       "                                           paper_name  \\\n",
       "0   One Billion Word Benchmark for Measuring Progr...   \n",
       "1   LRW-1000: A Naturally-Distributed Large-Scale ...   \n",
       "2   CLEVR: A Diagnostic Dataset for Compositional ...   \n",
       "3   HellaSwag: Can a Machine Really Finish Your Se...   \n",
       "4   NNE: A Dataset for Nested Named Entity Recogni...   \n",
       "5             NewsQA: A Machine Comprehension Dataset   \n",
       "6   A Speech Act Classifier for Persian Texts and ...   \n",
       "7   Spoken SQuAD: A Study of Mitigating the Impact...   \n",
       "8   The Value of Semantic Parse Labeling for Knowl...   \n",
       "9   WikiHow: A Large Scale Text Summarization Dataset   \n",
       "10                  Dense-Captioning Events in Videos   \n",
       "11         DeepFont: Identify Your Font from An Image   \n",
       "12  Birdsnap: Large-scale Fine-grained Visual Cate...   \n",
       "13  An Ensemble Classification-Based Approach Appl...   \n",
       "14  CrowdHuman: A Benchmark for Detecting Human in...   \n",
       "15  DeepFashion: Powering Robust Clothes Recogniti...   \n",
       "16  Generation and Comprehension of Unambiguous Ob...   \n",
       "17  What and How Well You Performed? A Multitask L...   \n",
       "18   Deeper, Broader and Artier Domain Generalization   \n",
       "19  Real-world Anomaly Detection in Surveillance V...   \n",
       "\n",
       "                                            paper_url  \\\n",
       "0   https://paperswithcode.com/paper/one-billion-w...   \n",
       "1   https://paperswithcode.com/paper/lrw-1000-a-na...   \n",
       "2   https://paperswithcode.com/paper/clevr-a-diagn...   \n",
       "3   https://paperswithcode.com/paper/hellaswag-can...   \n",
       "4   https://paperswithcode.com/paper/nne-a-dataset...   \n",
       "5   https://paperswithcode.com/paper/newsqa-a-mach...   \n",
       "6   https://paperswithcode.com/paper/a-speech-act-...   \n",
       "7   https://paperswithcode.com/paper/spoken-squad-...   \n",
       "8   https://paperswithcode.com/paper/the-value-of-...   \n",
       "9   https://paperswithcode.com/paper/wikihow-a-lar...   \n",
       "10  https://paperswithcode.com/paper/dense-caption...   \n",
       "11  https://paperswithcode.com/paper/deepfont-iden...   \n",
       "12  https://paperswithcode.com/paper/birdsnap-larg...   \n",
       "13          https://doi.org/10.1109/TBME.2012.2205687   \n",
       "14  https://paperswithcode.com/paper/crowdhuman-a-...   \n",
       "15  https://paperswithcode.com/paper/deepfashion-p...   \n",
       "16  https://paperswithcode.com/paper/generation-an...   \n",
       "17  https://paperswithcode.com/paper/what-and-how-...   \n",
       "18  https://paperswithcode.com/paper/deeper-broade...   \n",
       "19  https://paperswithcode.com/paper/real-world-an...   \n",
       "\n",
       "                benchmark_label  \\\n",
       "0              One Billion Word   \n",
       "1        CAS-VSR-W1k (LRW-1000)   \n",
       "2                         CLEVR   \n",
       "3                     HellaSwag   \n",
       "4                           NNE   \n",
       "5                        NewsQA   \n",
       "6               Sepehr_RumTel01   \n",
       "7                  Spoken-SQuAD   \n",
       "8                WebQuestionsSP   \n",
       "9                       WikiHow   \n",
       "10         ActivityNet Captions   \n",
       "11                 AdobeVFR syn   \n",
       "12                     Birdsnap   \n",
       "13                    CHASE_DB1   \n",
       "14       CrowdHuman (full body)   \n",
       "15    DeepFashion, Deep-Fashion   \n",
       "16  RefCOCOg-test, RefCOCOg-val   \n",
       "17                      MTL-AQA   \n",
       "18                         PACS   \n",
       "19                    UCF-Crime   \n",
       "\n",
       "                                                tasks  n_unique_papers  \\\n",
       "0                 Text generation, Language modelling               16   \n",
       "1                                         Lip reading                7   \n",
       "2                            Image question answering               14   \n",
       "3                                 Sentence Completion                3   \n",
       "4                     Nested named entity recognition                5   \n",
       "5                                  Question answering                5   \n",
       "6                                     Rumor Detection                4   \n",
       "7                       Spoken language understanding                3   \n",
       "8   Knowledge base question answering, Semantic pa...                6   \n",
       "9                                  Text summarization                3   \n",
       "10  Video captioning, Temporal action proposal gen...               12   \n",
       "11                                   Font Recognition                3   \n",
       "12                  Fine-grained image classification                5   \n",
       "13  Retinal vessel segmentation, Medical image seg...               11   \n",
       "14                                   Object detection                7   \n",
       "15  Consumer-to-shop - Image retrieval, Image retr...               18   \n",
       "16                  Referring expression segmentation                2   \n",
       "17      Action quality assessment, Action recognition                4   \n",
       "18                              Domain generalization               56   \n",
       "19           Anomaly detection in surveillance videos                9   \n",
       "\n",
       "    n_unique_sota_papers  \n",
       "0                      8  \n",
       "1                      4  \n",
       "2                      4  \n",
       "3                      2  \n",
       "4                      3  \n",
       "5                      5  \n",
       "6                      3  \n",
       "7                      3  \n",
       "8                      4  \n",
       "9                      3  \n",
       "10                    10  \n",
       "11                     1  \n",
       "12                     3  \n",
       "13                    10  \n",
       "14                     2  \n",
       "15                    12  \n",
       "16                     2  \n",
       "17                     4  \n",
       "18                     9  \n",
       "19                     4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Der Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "\n",
    "def select_benchmarks(df, tlc, selection):\n",
    "\n",
    "    import random\n",
    "    benchmarks = df.dropna(subset=[\"paper_label\", \"paper_url\"])\n",
    "    benchmarks = benchmarks[[\"date\", \"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\", \"paper_label\"]].drop_duplicates()\n",
    "    benchmarks = benchmarks[benchmarks[\"date\"] <= datetime.datetime(2021, 12, 31)]\n",
    "    benchmarks = benchmarks[[\"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\", \"paper_label\"]]\n",
    "    benchmarks = benchmarks.fillna(\"\")\n",
    "\n",
    "    selection_ = benchmarks.groupby([\"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\"], as_index=False, dropna=False).filter(lambda x: len(x) >= 3).reset_index(drop=True)[[\"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\"]].drop_duplicates()\n",
    "    # 0 and 4\n",
    "    # selected_benchmarks = selection.sample(10, random_state=0)\n",
    "    selected_benchmarks = selection_[selection_[\"benchmark_parent_label\"].isin(selection)]\n",
    "    selected_benchmarks = benchmarks.merge(selected_benchmarks, how=\"inner\", on=[\"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\"])\n",
    "\n",
    "    if tlc == \"natural_language_processing\":\n",
    "        sota_papers = pd.read_csv(\"data/sota_papers_ITO_00141.csv\")\n",
    "    else:\n",
    "        sota_papers = pd.read_csv(\"data/sota_papers_ITO_00101.csv\")\n",
    "    sota_papers = sota_papers.rename(columns={\"dataset_label\": \"benchmark_label\"})[[\"benchmark_label\", \"paper_label\"]]\n",
    "    variant_to_parent, variant_to_meta = get_variant_to_parent()\n",
    "    sota_papers = sota_papers.merge(variant_to_parent, left_on=\"benchmark_label\", right_on=\"variant\", how=\"left\")\n",
    "    sota_papers = sota_papers.rename(columns={\"parent\": \"benchmark_parent_label\"}).drop(columns=\"variant\")\n",
    "    sota_papers[\"benchmark_parent_label\"] = sota_papers[\"benchmark_parent_label\"].fillna(sota_papers[\"benchmark_label\"])\n",
    "    sota_papers = sota_papers.dropna(subset=[\"benchmark_parent_label\"])\n",
    "    sota_papers[\"sota\"] = 1\n",
    "    selected_benchmarks = selected_benchmarks.merge(sota_papers, on=[\"benchmark_parent_label\", \"benchmark_label\", \"paper_label\"], how=\"left\")\n",
    "    def agg(ex):\n",
    "        return pd.Series({\"benchmark_label\": \", \".join(set(ex[\"benchmark_label\"].to_list())), \"task_label\": \", \".join(set(ex[\"task_label\"].to_list())), \"paper_label\": len(set(ex[\"paper_label\"].to_list())), \"sota\": int(ex[[\"paper_label\", \"sota\"]].drop_duplicates(subset=\"paper_label\")[\"sota\"].sum())})\n",
    "    selected_benchmarks = selected_benchmarks.groupby([\"benchmark_parent_label\", \"url\", \"homepage\", \"paper_name\", \"paper_url\"], dropna=False)[[\"benchmark_label\", \"task_label\", \"paper_label\", \"sota\"]].apply(agg).reset_index()\n",
    "    selected_benchmarks.rename(columns={\"task_label\": \"tasks\", \"paper_label\": \"n_unique_papers\", \"sota\": \"n_unique_sota_papers\"}, inplace=True)\n",
    "    selected_benchmarks = selected_benchmarks.sort_values(by=\"benchmark_parent_label\")\n",
    "    return selected_benchmarks\n",
    "\n",
    "\n",
    "    selected_benchmarks = benchmarks.merge(random_benchmarks, how=\"inner\", on=[\"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\"])\n",
    "\n",
    "    selected_benchmarks = selected_benchmarks.set_index([\"benchmark_label\", \"benchmark_parent_label\", \"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\"])\n",
    "    selected_benchmarks[\"n_benchmarking_papers\"] = selected_benchmarks.groupby([\"benchmark_label\", \"benchmark_parent_label\", \"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\"], dropna=False).size()\n",
    "    # selected_benchmarks = selected_benchmarks.reset_index().set_index([\"benchmark_label\", \"benchmark_parent_label\", \"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\", \"n_papers\"])\n",
    "    def agg(ex):\n",
    "        return set(ex.to_list())\n",
    "    selected_benchmarks = selected_benchmarks.groupby([\"benchmark_label\", \"benchmark_parent_label\",\"url\", \"homepage\", \"paper_name\", \"paper_url\", \"task_label\", \"n_benchmarking_papers\"], dropna=False)[\"paper_label\"].apply(agg).reset_index()\n",
    "    selected_benchmarks.rename(columns={\"paper_label\": \"benchmarking_paper_label\"}, inplace=True)\n",
    "\n",
    "    return selected_benchmarks\n",
    "\n",
    "selected_benchmarks = []\n",
    "for tlc, root in [(\"natural_language_processing\", \"https://identifiers.org/ito:ITO_00141\"), (\"vision_process\", \"https://identifiers.org/ito:ITO_00101\")]:\n",
    "    \n",
    "    if tlc == \"natural_language_processing\":\n",
    "        selection = [\"CAS-VSR-W1k (LRW-1000)\", \"CLEVR\", \"Spoken-SQuAD\", \"NNE\", \"NewsQA\", \"HellaSwag\", \"Sepehr_RumTel01\", \"Billion Word Benchmark\", \"WebQuestionsSP\", \"WikiHow\"]\n",
    "    else:\n",
    "        selection = [\"ActivityNet Captions\", \"AdobeVFR syn\", \"Birdsnap\", \"PACS\", \"CHASE_DB1\", \"CrowdHuman\", \"DeepFashion\", \"Google Refexp\", \"MTL-AQA\", \"UCF-Crime\"]\n",
    "\n",
    "    \n",
    "    print(tlc)\n",
    "    sort_by = \"n_unique_paper_sum\"\n",
    "    df = extract_df(endpoint, tlc, root, sort_by)\n",
    "    print(len(df[\"benchmark_parent_label\"].unique()))\n",
    "    df[[\"benchmark_parent_label\"]].drop_duplicates().to_csv(\"benchmark_parent_label\" + \"_\" + tlc + \".csv\")\n",
    "    selected_benchmarks.append(select_benchmarks(df, tlc, selection))\n",
    "selected_benchmarks = pd.concat(selected_benchmarks, ignore_index=True)\n",
    "\n",
    "\n",
    "display(selected_benchmarks)\n",
    "selected_benchmarks.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "01bda4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sheet_name = \"Tabellenblatt1\"\n",
    "url = f\"https://docs.google.com/spreadsheets/d/18sBVxSW4fFe1ZCjvo_7b6RpUSJHCrNZJNX3trBJi-jo/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "df = pd.read_csv(url).drop(columns=[\"Unnamed: 12\", \"Unnamed: 13\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7263a901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "n_samples = math.ceil(((math.pow(1.96,2)*0.25)/(math.pow(0.05,2)))/(1+((math.pow(1.96,2)*0.25)/((math.pow(0.05,2))*df[\"gs_citations\"].sum()))))\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5242532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "distro = df[[\"benchmark_parent_label\", \"gs_citations\", \"gsid\"]].copy()\n",
    "citations = df[\"gs_citations\"].to_numpy()\n",
    "distro[\"n_sample\"] = (citations / citations.sum() * n_samples).round()\n",
    "distro[\"n_sample_100\"] = (citations / citations.sum() * 100).round()\n",
    "\n",
    "samples = {}\n",
    "for idx, row in distro.iterrows():\n",
    "    n = row[\"gs_citations\"]\n",
    "    samples[row[\"benchmark_parent_label\"]] = np.random.choice([i for i in range(1, n+1)], size=int(row[\"n_sample\"]), replace=False)\n",
    "\n",
    "\"\"\"\n",
    "for key in samples:\n",
    "    samples[key] = [(math.ceil(i/10.),i - (math.ceil(i/10.) - 1) * 10) for i in samples[key]]\n",
    "\"\"\"\n",
    "    \n",
    "sample_df = pd.DataFrame([(key, val) for key in samples for val in samples[key]], columns=[\"benchmark_parent_label\", \"idx\"])\n",
    "sample_df = sample_df.sort_values(by=[\"benchmark_parent_label\", \"idx\"])\n",
    "\n",
    "sample_df = sample_df.merge(distro[[\"benchmark_parent_label\", \"gsid\", \"n_sample\", \"n_sample_100\"]], how=\"left\", on=\"benchmark_parent_label\")\n",
    "sample_df[\"gsid\"] = \"http://scholar.google.com/scholar?lr=lang_en&hl=en&as_sdt=2005&as_yhi=2021&cites=\" + sample_df[\"gsid\"].astype(str) + \"&num=1&start=\" + sample_df[\"idx\"].astype(str)\n",
    "\n",
    "def agg(ex):\n",
    "    ex[\"100\"] = 0\n",
    "    assert((ex[\"n_sample_100\"] == ex[\"n_sample_100\"].iloc[0]).all())\n",
    "    ex[\"100\"].iloc[0:int(ex[\"n_sample_100\"].iloc[0])] = 1\n",
    "    return ex\n",
    "sample_df = sample_df.groupby([\"benchmark_parent_label\"], as_index=False).apply(agg).reset_index()\n",
    "\n",
    "sample_df[[\"benchmark_parent_label\", \"idx\", \"gsid\", \"100\"]].to_csv(\"sample.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1839f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Ungefähr 1 190 Ergebnisse (0,02 Sek.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import time\n",
    " \n",
    "cite = \"1033880884200484288\"\n",
    " \n",
    "# Making a GET request\n",
    "r = requests.get(f'https://scholar.google.com/scholar?cites={cite}&as_yhi=2021&lr=lang_en')\n",
    " \n",
    "# check status code for response received\n",
    "# success code - 200\n",
    "print(r.status_code)\n",
    " \n",
    "# Parsing the HTML\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "number = soup.find(\"div\", id=\"gs_ab_md\").find(\"div\", class_=\"gs_ab_mdw\").text\n",
    "print(number)\n",
    "number = number[:number.find(\"Ergebnis\")]\n",
    "number = number.replace(\"Ungefähr\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\" \", \"\").replace(u'\\xa0', '')\n",
    "number = int(number)\n",
    "n_pages = math.ceil(number/10)\n",
    "n_pages\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "84e8978afb7f63c6ee27450fa6ee2e656ae864757fb7210e0f2225ddad31e255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
