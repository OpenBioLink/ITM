l1,l2,l3,task,dataset,date,model,result,metric,polarity
Biomedical natural language processing,Biomedical relation extraction,Biomedical relation extraction,Biomedical relation extraction,CMeIE,2021-06,RoBERTa-wwm-ext-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',55.9,Micro F1,pos
Biomedical natural language processing,Biomedical relation extraction,Biomedical relation extraction,Biomedical relation extraction,DDI extraction 2013 corpus,2019-06,NCBI_BERT(large) (P) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',79.9,F1,pos
Biomedical natural language processing,Biomedical relation extraction,Biomedical relation extraction,Biomedical relation extraction,DDI extraction 2013 corpus,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',83.35,F1,pos
Biomedical natural language processing,Clinical assertion status detection,Clinical assertion status detection,Clinical assertion status detection,2010 i2b2/VA,2020-12,BiLSTM (SparkNLP) model in \'Improving Clinical Document Understanding on COVID-19 Research with Spark NLP\',0.939,Micro F1,pos
Biomedical natural language processing,Clinical concept extraction,Clinical concept extraction,Clinical concept extraction,2010 i2b2/VA,2011-05,deBruijn et al. (System 1.1) model in \'Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010\',85.23,Exact Span F1,pos
Biomedical natural language processing,Clinical concept extraction,Clinical concept extraction,Clinical concept extraction,2010 i2b2/VA,2019-02,BERTlarge (MIMIC) model in \'Enhancing Clinical Concept Extraction with Contextual Embeddings\',90.25,Exact Span F1,pos
Biomedical natural language processing,Multi-label classification of biomedical texts,Multi-label classification of biomedical texts,Multi-label classification of biomedical texts,MIMIC-III,2020-07,Convolutional Neural Network with per-label Attention model in \'Predicting Multiple ICD-10 Codes from Brazilian-Portuguese Clinical Notes\',0.537,Micro F1,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,AS,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.7,F1,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,AS,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.6,Precision,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,AS,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.8,Recall,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,MSRA,2015-09,Pre-trained+bigram+ LSTM+CRF model in \'Long Short-Term Memory Neural Networks for Chinese Word Segmentation\',97.4,F1,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,PKU,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.7,F1,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,PKU,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',97.1,Precision,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,PKU,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.4,Recall,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,MSR,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',98.3,F1,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,MSR,2019-11,ZEN (Init with Chinese BERT) model in \'ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations\',98.35,F1,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,MSR,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',98.2,Precision,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,MSR,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',98.3,Recall,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,CITYU,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',97.9,F1,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,CITYU,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',97.9,Precision,pos
Chinese language processing,Chinese word segmentation,Chinese word segmentation,Chinese word segmentation,CITYU,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',98.0,Recall,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC German-to-English,2015-11,Monolingual training data model in \'Improving Neural Machine Translation Models with Monolingual Data\',76.9,F1 score,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC German-to-English,2018-11,Multilingual Sentence Embeddings model in \'Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings\',95.58,F1 score,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC German-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',96.19,F1 score,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC Chinese-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',92.27,F1 score,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC Russian-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',93.3,F1 score,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC French-to-English,2015-11,Monolingual training data model in \'Improving Neural Machine Translation Models with Monolingual Data\',75.8,F1 score,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC French-to-English,2018-11,Multilingual Sentence Embeddings model in \'Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings\',92.89,F1 score,pos
Cross-lingual natural language processing,Cross-lingual bitext mining,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC French-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',93.91,F1 score,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-German,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',81.2,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-German,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',84.78,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-German,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',96.95,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Japanese,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',67.63,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Japanese,2019-09,"MultiFiT, pseudo model in \'MultiFiT: Efficient Multi-lingual Language Model Fine-tuning\'",69.57,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Spanish,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',72.5,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Spanish,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',77.33,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Spanish,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',96.8,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 English-to-German,2013-12,biCVM+ model in \'Multilingual Distributed Representations without Word Alignment\',86.2,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 English-to-German,2014-04,Bi+ model in \'Multilingual Models for Compositional Distributed Semantics\',88.1,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 English-to-German,2014-12,Biinclusion (Euro500kReuters) model in \'Leveraging Monolingual Data for Crosslingual Compositional Word Representations\',92.7,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Russian,2018-05,BiLSTM (UN) model in \'A Corpus for Multilingual Document Classification in Eight Languages\',61.42,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Russian,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',67.78,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Russian,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',89.7,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Italian,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',69.38,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Italian,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',69.43,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Italian,2019-09,"MultiFiT, pseudo model in \'MultiFiT: Efficient Multi-lingual Language Model Fine-tuning\'",76.02,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-French,2018-05,BiLSTM (UN) model in \'A Corpus for Multilingual Document Classification in Eight Languages\',74.52,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-French,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',77.95,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-French,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',96.05,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Chinese,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',74.73,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Chinese,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',93.32,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot German-to-French,2018-05,BiLSTM (Europarl) model in \'A Corpus for Multilingual Document Classification in Eight Languages\',75.45,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 German-to-English,2013-12,biCVM+ model in \'Multilingual Distributed Representations without Word Alignment\',76.9,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 German-to-English,2014-04,Bi+ model in \'Multilingual Models for Compositional Distributed Semantics\',79.2,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 German-to-English,2014-12,Biinclusion (Euro500kReuters) model in \'Leveraging Monolingual Data for Crosslingual Compositional Word Representations\',84.4,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,News Classification,News Classification,N15News,2021-08,"Multimodal(ViT+BERT, Input: Image + Body) model in \'N24News: A New Dataset for Multimodal News Classification\'",0.9249,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,News Classification,News Classification,BBC Hindi News Article Classification,2022-01,xlmindic-base-uniscript model in \'Does Transliteration Help Multilingual Language Modeling?\',79.14,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,News Classification,News Classification,Soham News Article Classification,2020-11,"IndicBERT Base model in \'IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages\'",78.45,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual document classification,News Classification,News Classification,Soham News Article Classification,2022-01,xlmindic-base-uniscript model in \'Does Transliteration Help Multilingual Language Modeling?\',93.89,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI,2020-10,Decoupled model in \'Rethinking embedding coupling in pre-trained language models\',71.3,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',83.7,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2017-05,X-BiLSTM model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',68.7,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',74.3,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',85.2,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-French,2017-05,X-BiLSTM model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',67.7,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-French,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',84.7,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2017-05,X-CBOW model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',61.0,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',70.5,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual natural language inference,Cross-lingual natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',84.2,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,NoDaLiDa Norwegian Bokmål,2020-07,UniTrans model in \'UniTrans: Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data\',81.17,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,MSRA,2019-11,Meta-Cross model in \'Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources\',77.89,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,NER,2020-10,Coupled model in \'Rethinking embedding coupling in pre-trained language models\',69.2,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL German,2018-10,MAN-MoE+CharCNN+UMWE model in \'Multi-Source Cross-Lingual Model Transfer: Learning What to Share\',56.0,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL German,2019-04,"mBERT model in \'Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\'",69.56,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL German,2019-11,Meta-Cross model in \'Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources\',73.16,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL German,2020-04,SMTS Multi sim model in \'Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language\',75.33,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Spanish,2018-10,MAN-MoE+CharCNN+UMWE model in \'Multi-Source Cross-Lingual Model Transfer: Learning What to Share\',73.5,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Spanish,2019-04,"mBERT model in \'Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\'",74.96,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Spanish,2019-11,Meta-Cross model in \'Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources\',76.75,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Spanish,2020-04,SMTS Multi sim model in \'Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language\',78.0,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Spanish,2020-07,UniTrans model in \'UniTrans: Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data\',79.31,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Dutch,2018-10,MAN-MoE+CharCNN+UMWE model in \'Multi-Source Cross-Lingual Model Transfer: Learning What to Share\',72.4,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Dutch,2019-04,"mBERT model in \'Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\'",77.57,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,CoNLL Dutch,2019-11,Zero shot mBERT 3 model in \'Towards Lingua Franca Named Entity Recognition with BERT\',83.35,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,WikiAnn NER,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',67.7,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual NER,Cross-lingual NER,Europeana French,2019-11,Meta-Cross model in \'Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources\',55.3,F1,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual transfer,Cross-lingual transfer,XCOPA,2020-04,MAD-X Base model in \'MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\',60.94,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual transfer,Cross-lingual transfer,Cross-lingual transfer,XCOPA,2020-05,RoBERTa Large (translate test) model in \'XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning\',76.05,Accuracy,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,MaRVL,2021-09,xUNITER model in \'Visually Grounded Reasoning across Languages and Cultures\',56.1,Accuracy (%),pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-05,X-STILTs model in \'English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too\',73.5,Avg,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-07,T-ULRv2 + StableTune model in \'InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training\',80.7,Avg,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-12,ERNIE-M model in \'ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora\',80.9,Avg,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2021-06,Turing ULR v5 model in \'XLM-E: Cross-lingual Language Model Pre-training via ELECTRA\',84.5,Avg,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-05,X-STILTs model in \'English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too\',-83.9,Sentence-pair Classification,neg
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-05,X-STILTs model in \'English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too\',69.4,Structured prediction,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-07,T-ULRv2 + StableTune model in \'InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training\',75.4,Structured prediction,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-12,ERNIE-M model in \'ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora\',75.6,Structured prediction,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2021-06,Turing ULR v5 model in \'XLM-E: Cross-lingual Language Model Pre-training via ELECTRA\',81.7,Structured prediction,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-05,X-STILTs model in \'English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too\',67.2,Question answering,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-07,T-ULRv2 + StableTune model in \'InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training\',72.9,Question answering,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-10,mT5 model in \'mT5: A massively multilingual pre-trained text-to-text transformer\',73.6,Question answering,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2021-06,Turing ULR v5 model in \'XLM-E: Cross-lingual Language Model Pre-training via ELECTRA\',76.3,Question answering,pos
Cross-lingual natural language processing,Cross-lingual transfer,Zero-shot cross-lingual transfer,Zero-shot cross-lingual transfer,XTREME,2020-05,X-STILTs model in \'English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too\',-76.5,Sentence Retrieval,neg
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,Linux IRC (Ch2 Kummerfeld),2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',59.7,1-1,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,Linux IRC (Ch2 Kummerfeld),2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',80.8,Local,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,Linux IRC (Ch2 Kummerfeld),2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',63.0,Shen F-1,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,Linux IRC (Ch2 Elsner),2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',53.1,1-1,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,Linux IRC (Ch2 Elsner),2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',81.9,Local,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,Linux IRC (Ch2 Elsner),2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',55.1,Shen F-1,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',82.1,VI,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2018-10,FF ensemble: Vote model in \'A Large-Scale Corpus for Conversation Disentanglement\',91.5,VI,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2020-04,BERT + BiLSTM model in \'Pre-Trained and Attention-Based Neural Networks for Building Noetic Task-Oriented Dialogue Systems\',93.3,VI,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',51.4,1-1,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2018-10,FF ensemble: Vote model in \'A Large-Scale Corpus for Conversation Disentanglement\',76.0,1-1,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',12.1,P,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2018-10,FF ensemble: Intersect model in \'A Large-Scale Corpus for Conversation Disentanglement\',67.0,P,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',21.5,R,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2018-10,FF ensemble: Vote model in \'A Large-Scale Corpus for Conversation Disentanglement\',39.7,R,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2020-04,BERT + BiLSTM model in \'Pre-Trained and Attention-Based Neural Networks for Building Noetic Task-Oriented Dialogue Systems\',49.6,R,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2008-06,Linear model in \'You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement\',15.5,F,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2018-10,FF ensemble: Vote model in \'A Large-Scale Corpus for Conversation Disentanglement\',38.0,F,pos
Dialog process,Conversation disentanglement,Conversation disentanglement,Conversation disentanglement,irc-disentanglement,2020-04,BERT + BiLSTM model in \'Pre-Trained and Attention-Based Neural Networks for Building Noetic Task-Oriented Dialogue Systems\',46.8,F,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,Switchboard corpus,2016-03,CNN[[Lee and Dernoncourt2016]] model in \'Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks\',73.1,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,Switchboard corpus,2017-09,Bi-LSTM-CRF model in \'Dialogue Act Sequence Labeling using Hierarchical encoder with CRF\',79.2,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,Switchboard corpus,2017-11,CRF-ASN model in \'Dialogue Act Recognition via CRF-Attentive Structured Network\',81.3,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,Switchboard corpus,2018-10,DAH-CRF  model in \'A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification\',82.3,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,Switchboard corpus,2019-04,Bi-RNN + Self-Attention + Context model in \'Dialogue Act Classification with Context-Aware Self-Attention\',82.9,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,Switchboard corpus,2020-02,HGRU + Beam Search + Guided attention model in \'Guiding attention in Sequence-to-sequence models for Dialogue Act prediction\',85.0,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,ICSI Meeting Recorder Dialog Act (MRDA) corpus,2017-09,Bi-LSTM-CRF model in \'Dialogue Act Sequence Labeling using Hierarchical encoder with CRF\',90.9,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,ICSI Meeting Recorder Dialog Act (MRDA) corpus,2017-11,CRF-ASN model in \'Dialogue Act Recognition via CRF-Attentive Structured Network\',91.7,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,ICSI Meeting Recorder Dialog Act (MRDA) corpus,2020-09,Pretrained Hierarchical Transformer model in \'Hierarchical Pre-training for Sequence Labelling in Spoken Dialog\',92.4,Accuracy,pos
Dialog process,Dialog act classification,Dialog act classification,Dialog act classification,Switchboard dialogue act corpus,2018-04,Probabilistic-LSTM model in \'Probabilistic Word Association for Dialogue Act Classification with Recurrent Neural Networks\',75.48,Accuracy (%),pos
Dialog process,Dialog generation,Conversational Response Generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.1403,BLEU,pos
Dialog process,Dialog generation,Conversational Response Generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',-17.48,PPL,neg
Dialog process,Dialog generation,Conversational Response Generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.0614,Distinct-1,pos
Dialog process,Dialog generation,Conversational Response Generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.343,Distinct-2,pos
Dialog process,Dialog generation,Conversational Response Generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo+da} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',5012.0,Greedy Embedding,pos
Dialog process,Dialog generation,Conversational Response Generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.5617,Average Embedding,pos
Dialog process,Dialog generation,Conversational Response Generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo+da} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.5722,bertscore,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Amazon-5,2017-01,mm model in \'Adversarial Learning for Neural Dialogue Generation\',5.0,1 in 10 R-at-2,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',12.17,BLEU,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Two-in-one model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',-10.49,PPL,neg
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',60.9,Success,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.51,Specificity,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.973,Slot Accuracy,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.6,Joint SA,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',75.1,Inform,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',90.8,Inform_mct,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',74.4,Success_mct,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.58,Sensibleness,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.55,SSA,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,WikiText-103,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',-32.48,Perplexity,neg
Dialog process,Dialog generation,Dialog generation,Dialog generation,Reddit (multi-ref),2019-02,SpaceFusion model in \'Jointly Optimizing Diversity and Relevance in Neural Response Generation\',2.53,interest (human),pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Reddit (multi-ref),2019-02,SpaceFusion model in \'Jointly Optimizing Diversity and Relevance in Neural Response Generation\',2.72,relevance (human),pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Twitter Dialogue (Tense),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',34.48,Accuracy,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Twitter Dialogue (Noun),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',4.63,F1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Twitter Dialogue (Noun),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',4.82,Precision,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Twitter Dialogue (Noun),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',5.22,Recall,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Persona-Chat,2014-09,Seq2Seq + Attention model in \'Neural Machine Translation by Jointly Learning to Align and Translate\',16.18,Avg F1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Persona-Chat,2019-01,TransferTransfo model in \'TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\',19.09,Avg F1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Persona-Chat,2020-04,P^2 Bot model in \'You Impress Me: Dialogue Generation via Mutual Persona Perception\',19.77,Avg F1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',14.7,BLEU-1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',14.79,ROUGE-L,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',19.09,CIDr,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',6.39,METEOR,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Tense),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',29.01,Accuracy,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Entity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',3.72,F1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Entity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',4.91,Precision,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Entity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',3.36,Recall,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Activity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',11.43,F1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Activity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',16.84,Precision,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Activity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',9.72,Recall,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',9.01,F1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',12.56,Rouge-L,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',15.37,ROUGE-1,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',7.55,Meteor,pos
Dialog process,Dialog generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Cmd),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',95.04,Accuracy,pos
Dialog process,Dialog generation,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",1.99,BLEU,pos
Dialog process,Dialog generation,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.0056,Dis-1,pos
Dialog process,Dialog generation,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.0431,Dis-2,pos
Dialog process,Dialog generation,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.125,Dis-3,pos
Dialog process,Dialog generation,Multi-modal Dialogue Generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.2215,Dis-4,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',84.4,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',88.1,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-10,StateNet model in \'Towards Universal Dialogue State Tracking\',88.9,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2019-10,BERT-based tracker model in \'A Simple but Effective BERT Model for Dialog State Tracking on Resource-Limited Systems\',90.5,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2020-11,Seq2Seq-DU-w/oSchema model in \'A Sequence-to-Sequence Approach to Dialogue State Tracking\',91.2,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2021-10,AG-DST model in \'Amendable Generation for Dialogue State Tracking\',91.37,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',96.5,Request,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',97.1,Request,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-12,GCE model in \'Toward Scalable Neural Dialogue State Tracking Model\',97.4,Request,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2019-10,BERT-based tracker model in \'A Simple but Effective BERT Model for Dialog State Tracking on Resource-Limited Systems\',97.6,Request,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,CoSQL,2021-09,T5-3B + PICARD model in \'PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models\',54.6,question match accuracy,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,CoSQL,2022-05,RASAT+PICARD model in \'RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL\',55.7,question match accuracy,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,CoSQL,2021-09,T5-3B + PICARD model in \'PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models\',23.7,interaction match accuracy,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,CoSQL,2022-05,RASAT+PICARD model in \'RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL\',26.5,interaction match accuracy,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',73.4,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',74.5,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2018-10,StateNet model in \'Towards Universal Dialogue State Tracking\',75.5,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2020-11,Seq2Seq-DU-w/oSchema model in \'A Sequence-to-Sequence Approach to Dialogue State Tracking\',85.0,Joint,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',96.5,Request,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',97.5,Request,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',90.0,Area,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',84.0,Food,pos
Dialog process,Dialog state tracking,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',94.0,Price,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Crackling Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.93,Accuracy,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Crackling Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.93,F1 Score,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,VOXLINGUA107,2020-11,Noisy model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-7.1,Average,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,VOXLINGUA107,2020-11,Noisy model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-12.3,0..5sec,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,VOXLINGUA107,2020-11,Noisy model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-6.1,5..20sec,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,VoxForge Commonwealth,2019-10,2D ConvNet(MixUp=YES) model in \'Spoken Language Identification using ConvNets\',95.4,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (White Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.91,Accuracy,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (White Noise),2021-10,CRNN model in \'Is Attention always needed? A Case Study on Language Identification from Speech\',0.912,Accuracy,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (White Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.91,F1 Score,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (No Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.96,Accuracy,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (No Noise),2021-10,CRNN model in \'Is Attention always needed? A Case Study on Language Identification from Speech\',0.967,Accuracy,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (No Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.96,F1 Score,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,VoxForge European,2019-10,2D ConvNet(MixUp=YES) model in \'Spoken Language Identification using ConvNets\',96.3,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,CNN-LDE model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-4.0,Average,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,CNN-LDE model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-8.25,3 sec,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,Kaldi i-vector model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',11.93,10 sec,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,Kaldi i-vector model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',4.52,30 sec,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.041,PC,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.022,EC,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.058,EO,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.056,PO,neg
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,Untranscribed mixed-speech dataset,2015-09,SVM model in \'Automatic Dialect Detection in Arabic Broadcast Speech\',45.2,ACC,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,Untranscribed mixed-speech dataset,2015-09,SVM model in \'Automatic Dialect Detection in Arabic Broadcast Speech\',44.8,PRC,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,Untranscribed mixed-speech dataset,2015-09,Naive Bayes model in \'Automatic Dialect Detection in Arabic Broadcast Speech\',50.2,RCL,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Background Music),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.89,Accuracy,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Background Music),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.89,F1 Score,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language identification,IndicTTS,2021-10,CRNN model in \'Is Attention always needed? A Case Study on Language Identification from Speech\',0.987,Classification Accuracy,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Spoken-SQuAD,2018-04,Baseline model in \'Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension\',58.71,F1 score,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Spoken-SQuAD,2019-04,QANet + GAN model in \'Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation\',63.11,F1 score,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Spoken-SQuAD,2019-10,SpeechBERT model in \'SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering\',71.75,F1 score,pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartLights,2018-10,Snips model in \'Spoken Language Understanding on the Edge\',84.2,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartLights,2020-12,AT-AT model in \'Exploring Transfer Learning For End-to-End Spoken Language Understanding\',84.9,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartLights,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',88.0,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Timers and Such,2021-04,Baseline model in \'Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers\',81.6,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Timers and Such,2021-06,"wav2vec 2.0 \""960 Hr\"" model in \'SpeechBrain: A General-Purpose Speech Toolkit\'",94.0,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Timers and Such,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',95.4,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2018-10,Snips model in \'Spoken Language Understanding on the Edge\',68.7,Accuracy-EN (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',80.4,Accuracy-EN (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2018-10,Snips model in \'Spoken Language Understanding on the Edge\',75.1,Accuracy-FR (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',78.3,Accuracy-FR (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Fluent Speech Commands,2019-04,Pooling classifier pre-trained using force-aligned phoneme and word labels on LibriSpeech model in \'Speech Model Pre-training for End-to-End Spoken Language Understanding\',98.8,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Fluent Speech Commands,2020-08,Reptile model in \'Improving End-to-End Speech-to-Intent Classification with Reptile\',99.2,Accuracy (%),pos
Dialog process,Dialog understanding,Spoken language understanding,Spoken language understanding,Fluent Speech Commands,2020-10,textual-kd-slu model in \'Two-stage Textual Knowledge Distillation for End-to-End Spoken Language Understanding\',99.7,Accuracy (%),pos
Dialog process,Empathetic Response Generation,Empathetic Response Generation,Empathetic Response Generation,EmpatheticDialogues,2022-04,Emotion-aware transformer encoder (Transformer-XL) model in \'Emotion-Aware Transformer Encoder for Empathetic Dialogue Generation\',0.225,BLEU,pos
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.1,2020-05,SimpleTOD model in \'A Simple Language Model for Task-Oriented Dialogue\',15.2,BLEU,pos
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.1,2020-09,LABES-S2S model in \'A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning\',18.3,BLEU,pos
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.1,2021-11,GALAXY model in \'GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection\',20.01,BLEU,pos
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.1,2020-05,SimpleTOD model in \'A Simple Language Model for Task-Oriented Dialogue\',-85.0,MultiWOZ (Inform),neg
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.1,2020-09,LABES-S2S model in \'A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning\',-78.1,MultiWOZ (Inform),neg
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.1,2020-05,SimpleTOD model in \'A Simple Language Model for Task-Oriented Dialogue\',-70.5,MultiWOZ (Success),neg
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.1,2020-09,LABES-S2S model in \'A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning\',-67.1,MultiWOZ (Success),neg
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.0,2019-11,DAMD model in \'Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\',18.6,BLEU,pos
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.0,2021-03,Noisy Channel Model model in \'Pretraining the Noisy Channel Model for Task-Oriented Dialogue\',20.6,BLEU,pos
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.0,2019-11,DAMD model in \'Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\',-76.3,MultiWOZ (Inform),neg
Dialog process,End-to-end dialogue modelling,End-to-end dialogue modelling,End-to-end dialogue modelling,MULTIWOZ 2.0,2019-11,DAMD model in \'Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\',-60.4,MultiWOZ (Success),neg
Dialog process,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',0.1831,BLEU,pos
Dialog process,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',95.5,Embedding Average,pos
Dialog process,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',62.5,Greedy Matching,pos
Dialog process,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',97.4,Vector Extrema,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.7441,Diversity,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',4.15,Overall Human Rating,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.8017,Coherent,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.7518,Error Recovery,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',0.939,Consistent,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.7678,Topic Depth,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.7878,Likeable,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.8285,Understanding,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.8,Flexible,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.7881,Informative,pos
Dialog process,Interactive evaluation of dialog,Interactive evaluation of dialog,Interactive evaluation of dialog,DSTC9 Track 3 - Task 2,2021-05,PLATO-2 model in \'A Unified Pre-training Framework for Conversational AI\',2.7949,Inquisitive,pos
Dialog process,Multi-agent Integration,Multi-agent Integration,Multi-agent Integration,BBAI Dataset,2022-03,MARS Encoder model in \'One Agent To Rule Them All: Towards Multi-agent Conversational AI\',83.55,P-at-1,pos
Dialog process,Multimodal GIF Dialog,Multimodal GIF Dialog,Multimodal GIF Dialog,GIF Reply Dataset,2021-09,Pepe the King Prawn model in \'An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog\',0.8145,nDCG-at-10,pos
Dialog process,Open-domain dialog,Dialogue Evaluation,Dialogue Evaluation,USR-TopicalChat,2020-05,USR model in \'USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation\',0.422,Pearson Correlation,pos
Dialog process,Open-domain dialog,Dialogue Evaluation,Dialogue Evaluation,USR-TopicalChat,2021-12,MDD-Eval model in \'MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation\',0.4575,Pearson Correlation,pos
Dialog process,Open-domain dialog,Dialogue Evaluation,Dialogue Evaluation,USR-TopicalChat,2020-05,USR model in \'USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation\',0.4192,Spearman Correlation,pos
Dialog process,Open-domain dialog,Dialogue Evaluation,Dialogue Evaluation,USR-TopicalChat,2021-12,MDD-Eval model in \'MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation\',0.5109,Spearman Correlation,pos
Dialog process,Open-domain dialog,Dialogue Evaluation,Dialogue Evaluation,USR-PersonaChat,2020-05,USR - DR (x = c) model in \'USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation\',0.6087,Pearson Correlation,pos
Dialog process,Open-domain dialog,Dialogue Evaluation,Dialogue Evaluation,USR-PersonaChat,2020-05,USR - DR (x = c) model in \'USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation\',0.4814,Spearman Correlation,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',13.53,F1,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",18.9,F1,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',12.4,ROUGE-L,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",16.76,ROUGE-L,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",60.1,R-Prec,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",79.98,Recall@5,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",12.98,KILT-F1,pos
Dialog process,Open-domain dialog,Open-domain dialog,Open-domain dialog,KILT: Wizard of Wikipedia,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",11.39,KILT-RL,pos
Dialog process,Personality Recognition in Conversation,Personality Recognition in Conversation,Personality Recognition in Conversation,CPED,2022-05,BERT$_{ssenet}^{c}$ model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',74.08,Macro-F1,pos
Dialog process,Personality Recognition in Conversation,Personality Recognition in Conversation,Personality Recognition in Conversation,CPED,2022-05,BERT$_{ssenet}^{c}$ model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',67.25,Accuracy (%),pos
Dialog process,Personality Recognition in Conversation,Personality Recognition in Conversation,Personality Recognition in Conversation,CPED,2022-05,BERT$^{c}$ model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',55.29,Accuracy of Neurotism,pos
Dialog process,Personality Recognition in Conversation,Personality Recognition in Conversation,Personality Recognition in Conversation,CPED,2022-05,BERT$_{ssenet}^{c}$ model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',78.21,Accuracy of Extraversion,pos
Dialog process,Personality Recognition in Conversation,Personality Recognition in Conversation,Personality Recognition in Conversation,CPED,2022-05,BERT$^{s}$ model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',57.93,Accuracy of Openness,pos
Dialog process,Personality Recognition in Conversation,Personality Recognition in Conversation,Personality Recognition in Conversation,CPED,2022-05,BERT$_{ssenet}^{c}$ model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',85.89,Accuracy of Agreeableness,pos
Dialog process,Personality Recognition in Conversation,Personality Recognition in Conversation,Personality Recognition in Conversation,CPED,2022-05,BERT$^{s}$ model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',63.6,Accuracy of Conscientiousness,pos
Dialog process,Problem-Solving Deliberation,Problem-Solving Deliberation,Problem-Solving Deliberation,DeliData,2021-08,Bag-Of-Annotations + Sol. Tracker model in \'DeliData: A dataset for deliberation in multi-party problem solving\',0.62,AUC,pos
Dialog process,Response Generation,Response Generation,Response Generation,ArgSciChat,2022-02,"LED(Q,F) model in \'ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers\'",8.53,Mover,pos
Dialog process,Response Generation,Response Generation,Response Generation,ArgSciChat,2022-02,"LED(Q,F) model in \'ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers\'",19.54,Message-F1,pos
Dialog process,Response Generation,Response Generation,Response Generation,ArgSciChat,2022-02,"LED(Q,F) model in \'ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers\'",86.64,BScore,pos
Dialog process,Task-oriented dialog systems,SSTOD,SSTOD,SSD_NAME,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',57.73,Dialogue Success Rate,pos
Dialog process,Task-oriented dialog systems,SSTOD,SSTOD,SSD_NAME,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',84.96,Joint Acc,pos
Dialog process,Task-oriented dialog systems,SSTOD,SSTOD,SSD_NAME,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',93.12,Slot Acc,pos
Dialog process,Task-oriented dialog systems,SSTOD,SSTOD,automata,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',45.8,Dialogue Success Rate,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2017-05,KV Retrieval Net model in \'Key-Value Retrieval Networks for Task-Oriented Dialogue\',48.0,Entity F1,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2018-06,DSR model in \'Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation\',51.9,Entity F1,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2019-01,GLMP model in \'Global-to-local Memory Pointer Networks for Task-Oriented Dialogue\',59.97,Entity F1,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-04,DF-Net model in \'Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog\',62.5,Entity F1,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-10,COMET model in \'Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems\',63.6,Entity F1,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2022-01,T5-3b(UnifiedSKG) model in \'UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\',70.07,Entity F1,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2017-05,KV Retrieval Net model in \'Key-Value Retrieval Networks for Task-Oriented Dialogue\',13.2,BLEU,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2019-01,GLMP model in \'Global-to-local Memory Pointer Networks for Task-Oriented Dialogue\',14.79,BLEU,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-04,DF-Net model in \'Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog\',15.2,BLEU,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-10,COMET model in \'Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems\',17.3,BLEU,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,SGD,2021-02,"T5 model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.331,METEOR,pos
Dialog process,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,Kvret,2020-04,DF-Net model in \'Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog\',62.7,Entity F1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Wizard of Wikipedia,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',18.6,F1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Wizard of Wikipedia,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',2.2,BLEU-4,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Wizard of Wikipedia,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',17.4,ROUGE-L,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,BlendedSkillTalk,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',17.8,F1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,BlendedSkillTalk,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',1.0,BLEU-4,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,BlendedSkillTalk,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',19.3,ROUGE-L,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,ConvAI2,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',18.4,F1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,ConvAI2,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',1.1,BLEU-4,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,ConvAI2,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',22.6,ROUGE-L,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v1.0 test-std,2021-04,5xFGA + LS*+ model in \'Ensemble of MRR and NDCG models for Visual Dialog\',0.7124,MRR,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v1.0 test-std,2021-04,5xFGA + LS*+ model in \'Ensemble of MRR and NDCG models for Visual Dialog\',-2.96,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v1.0 test-std,2021-04,Two-Step model in \'Ensemble of MRR and NDCG models for Visual Dialog\',72.16,NDCG,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v1.0 test-std,2021-04,5xFGA + LS*+ model in \'Ensemble of MRR and NDCG models for Visual Dialog\',58.28,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v1.0 test-std,2021-04,5xFGA + LS*+ model in \'Ensemble of MRR and NDCG models for Visual Dialog\',94.45,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v1.0 test-std,2021-04,5xFGA + LS*+ model in \'Ensemble of MRR and NDCG models for Visual Dialog\',87.55,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2016-11,MN-QIH-D model in \'Visual Dialog\',40.98,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2017-04,NMN model in \'Learning to Reason: End-to-End Module Networks for Visual Question Answering\',44.15,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-09,CorefNMN (ResNet-152) model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',47.55,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',49.03,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-02,HACAN model in \'Making History Matter: History-Advantage Sequence Training for Visual Dialog\',50.88,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-04,5xFGA (F-RCNNx101) model in \'Factor Graph Attention\',55.65,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2021-04,2 Step: Factor Graph Attention + VD-Bert model in \'Ensemble of MRR and NDCG models for Visual Dialog\',58.3,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2016-11,MN-QIH-D model in \'Visual Dialog\',83.3,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2017-04,NMN model in \'Learning to Reason: End-to-End Module Networks for Visual Question Answering\',86.88,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-09,CorefNMN (ResNet-152) model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',88.8,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',89.83,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-04,5xFGA (F-RCNNx101) model in \'Factor Graph Attention\',94.05,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2016-11,MN-QIH-D model in \'Visual Dialog\',72.45,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2017-04,NMN model in \'Learning to Reason: End-to-End Module Networks for Visual Question Answering\',76.88,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-09,CorefNMN (ResNet-152) model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',78.1,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',80.4,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-02,HACAN model in \'Making History Matter: History-Advantage Sequence Training for Visual Dialog\',80.63,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-04,5xFGA (F-RCNNx101) model in \'Factor Graph Attention\',86.73,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2016-11,MN-QIH-D model in \'Visual Dialog\',47.5,NDCG (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2017-04,NMN model in \'Learning to Reason: End-to-End Module Networks for Visual Question Answering\',58.1,NDCG (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-11,Ensemble + Finetune model in \'Efficient Attention Mechanism for Visual Dialog that can Handle All the Interactions between Multiple Inputs\',74.88,NDCG (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2016-11,MN-QIH-D model in \'Visual Dialog\',55.5,MRR (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2017-04,NMN model in \'Learning to Reason: End-to-End Module Networks for Visual Question Answering\',58.8,MRR (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-09,CorefNMN (ResNet-152) model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',61.5,MRR (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',63.03,MRR (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-02,HACAN model in \'Making History Matter: History-Advantage Sequence Training for Visual Dialog\',64.22,MRR (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-04,5xFGA (F-RCNNx101) model in \'Factor Graph Attention\',69.3,MRR (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2021-04,2 Step: Factor Graph Attention + VD-Bert model in \'Ensemble of MRR and NDCG models for Visual Dialog\',69.92,MRR (x 100),pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2016-11,MN-QIH-D model in \'Visual Dialog\',-5.92,Mean,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2017-04,NMN model in \'Learning to Reason: End-to-End Module Networks for Visual Question Answering\',-4.4,Mean,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',-4.18,Mean,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-02,Synergistic model in \'Image-Question-Answer Synergistic Network for Visual Dialog\',-4.17,Mean,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,Visual Dialog v1.0 test-std,2019-04,5xFGA (F-RCNNx101) model in \'Factor Graph Attention\',-3.14,Mean,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,EmpatheticDialogues,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',19.2,F1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,EmpatheticDialogues,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',1.5,BLEU-4,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,EmpatheticDialogues,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',24.5,ROUGE-L,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-05,HieCoAtt-QI model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',57.88,MRR,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-06,HCIAE-NP-ATT model in \'Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\',62.22,MRR,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-11,CoAtt model in \'Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning\',63.98,MRR,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2018-09,CorefNMN (ResNet-152) model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',64.1,MRR,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-02,DAN model in \'Dual Attention Networks for Visual Reference Resolution in Visual Dialog\',66.38,MRR,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-04,9xFGA (VGG) model in \'Factor Graph Attention\',68.92,MRR,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-05,HieCoAtt-QI model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',-5.84,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-11,MN-QIH-D model in \'Visual Dialog\',-5.46,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-06,HCIAE-NP-ATT model in \'Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\',-4.81,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-11,CoAtt model in \'Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning\',-4.47,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2018-09,CorefNMN (ResNet-152) model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',-4.45,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',-3.93,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-04,9xFGA (VGG) model in \'Factor Graph Attention\',-3.39,Mean Rank,neg
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-05,HieCoAtt-QI model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',43.51,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-11,MN-QIH-D model in \'Visual Dialog\',45.55,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-06,HCIAE-NP-ATT model in \'Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\',48.48,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-09,AMEM model in \'Visual Reference Resolution using Attention Memory for Visual Dialog\',48.53,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-11,CoAtt model in \'Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning\',50.29,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2018-09,CorefNMN (ResNet-152) model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',50.92,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',52.71,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-02,HACAN model in \'Making History Matter: History-Advantage Sequence Training for Visual Dialog\',54.76,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-04,9xFGA (VGG) model in \'Factor Graph Attention\',55.16,R-at-1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-05,HieCoAtt-QI model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',83.96,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-11,MN-QIH-D model in \'Visual Dialog\',85.37,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-06,HCIAE-NP-ATT model in \'Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\',87.59,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-11,CoAtt model in \'Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning\',88.81,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',90.73,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-04,9xFGA (VGG) model in \'Factor Graph Attention\',92.95,R-at-10,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-05,HieCoAtt-QI model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',74.49,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2016-11,MN-QIH-D model in \'Visual Dialog\',76.22,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-06,HCIAE-NP-ATT model in \'Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\',78.75,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2017-11,CoAtt model in \'Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning\',80.71,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2018-12,RVA model in \'Recursive Visual Attention in Visual Dialog\',82.97,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-02,HACAN model in \'Making History Matter: History-Advantage Sequence Training for Visual Dialog\',83.03,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,VisDial v0.9 val,2019-04,9xFGA (VGG) model in \'Factor Graph Attention\',86.26,R-at-5,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Image-Chat,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',13.1,F1,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Image-Chat,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',40.0,BLEU-4,pos
Dialog process,Visual dialog,Visual dialog,Visual dialog,Image-Chat,2020-10,Multi-Modal BlenderBot model in \'Multi-Modal Open-Domain Dialogue\',18.0,ROUGE-L,pos
Natural Language Transduction,Lip reading,Lip password classification,Lip password classification,MIRACL-VC1,2020-12,AuthNet model in \'AuthNet: A Deep Learning based Authentication Mechanism using Temporal Facial Feature Movements\',0.981,2-Class Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,Lip Reading in the Wild,2017-03,3D Conv + ResNet-34 + Bi-LSTM model in \'Combining Residual Networks with LSTMs for Lipreading\',83.0,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,Lip Reading in the Wild,2018-02,3D Conv + ResNet-34 + Bi-GRU model in \'End-to-end Audiovisual Speech Recognition\',83.39,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,Lip Reading in the Wild,2020-01,3D Conv + ResNet-18 + MS-TCN model in \'Lipreading using Temporal Convolutional Networks\',85.3,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,Lip Reading in the Wild,2020-07,3D Conv + ResNet-18 + MS-TCN + KD (Ensemble) model in \'Towards Practical Lipreading with Distilled and Efficient Models\',88.5,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,GRID corpus (mixed-speech),2016-11,WAS model in \'Lip Reading Sentences in the Wild\',-3.0,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,GRID corpus (mixed-speech),2018-03,LCANet model in \'LCANet: End-to-End Lipreading with Cascaded Attention-CTC\',-2.9,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,GRID corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',14.08,WER,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,LRS3-TED,2018-07,CTC-V2P model in \'Large-Scale Visual Speech Recognition\',-55.1,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,LRS3-TED,2019-11,RNN-T model in \'Recurrent Neural Network Transducer for Audio-Visual Speech Recognition\',-33.6,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,LRS3-TED,2022-01,AV-HuBERT Large model in \'Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction\',-26.9,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,CAS-VSR-W1k (LRW-1000),2018-10,3D Conv + ResNet-34 + Bi-GRU model in \'LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild\',38.19,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,CAS-VSR-W1k (LRW-1000),2020-01,3D Conv + ResNet-18 + MS-TCN model in \'Lipreading using Temporal Convolutional Networks\',41.4,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,CAS-VSR-W1k (LRW-1000),2020-03,3D Conv + ResNet-18 + Bi-GRU (Face Cutout) model in \'Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition\',45.24,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,CAS-VSR-W1k (LRW-1000),2020-11,3D-ResNet + Bi-GRU + MixUp + Label Smooth + Cosine LR (Word Boundary) model in \'Learn an Effective Lip Reading Model without Pains\',55.7,Top-1 Accuracy,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,LRS2,2018-09,TM-seq2seq + extLM model in \'Deep Audio-Visual Speech Recognition\',-48.3,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,LRS2,2021-02,Hybrid CTC / Attention model in \'End-to-end Audio-visual Speech Recognition with Conformers\',-39.1,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,LRS2,2021-10,VTP (more data) model in \'Sub-word Level Lip Reading With Visual Attention\',-22.6,Word Error Rate (WER),neg
Natural Language Transduction,Lip reading,Lip reading,Lip reading,CMLR,2019-08,WAS model in \'A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading\',38.93,CER,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,LRW,2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',34.2,WER,pos
Natural Language Transduction,Lip reading,Lip reading,Lip reading,TCD-TIMIT corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',31.26,WER,pos
Natural language analysis,Document analysis,Document analysis,Document AI,EPHOIE,2022-04,LayoutLMv3 model in \'LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking\',99.21,Average F1,pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,MusicBrainz20K,2018-10,FAMER-SplitMerge model in \'Scalable Matching and Clustering of Entities with FAMER\',0.88,F1,pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,MusicBrainz20K,2021-09,ALMSER-GB model in \'Graph-boosted Active Learning for Multi-Source Entity Resolution\',0.951,F1,pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,Abt-Buy,2018-05,DeepMatcher - Hybrid model in \'Deep Learning for Entity Matching: A Design Space Exploration\',62.8,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,Abt-Buy,2020-04,Ditto model in \'Deep Entity Matching with Pre-Trained Language Models\',89.33,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,Abt-Buy,2022-02,RoBERTa-SupCon model in \'Supervised Contrastive Learning for Product Matching\',94.29,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Watches-xlarge,2020-04,Ditto model in \'Deep Entity Matching with Pre-Trained Language Models\',96.53,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Watches-xlarge,2021-06,JointBERT model in \'Dual-Objective Fine-Tuning of BERT for Entity Matching\',97.09,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Computers-xlarge,2020-04,Ditto model in \'Deep Entity Matching with Pre-Trained Language Models\',95.45,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Computers-xlarge,2020-08,BERT model in \'Intermediate Training of BERT for Product Matching\',97.37,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Computers-xlarge,2021-06,JointBERT model in \'Dual-Objective Fine-Tuning of BERT for Entity Matching\',97.49,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Computers-xlarge,2022-02,RoBERTa-SupCon model in \'Supervised Contrastive Learning for Product Matching\',98.33,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,Amazon-Google,2018-05,DeepMatcher - Hybrid model in \'Deep Learning for Entity Matching: A Design Space Exploration\',69.3,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,Amazon-Google,2020-04,Ditto model in \'Deep Entity Matching with Pre-Trained Language Models\',75.58,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,Amazon-Google,2020-10,Random Forest model in \'Profiling Entity Matching Benchmark Tasks\',79.0,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,Amazon-Google,2022-02,RoBERTa-SupCon model in \'Supervised Contrastive Learning for Product Matching\',79.28,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Watches-small,2020-04,Ditto model in \'Deep Entity Matching with Pre-Trained Language Models\',85.12,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Watches-small,2022-06,HG model in \'Entity Resolution with Hierarchical Graph Attention Networks\',94.0,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Computers-small,2020-04,Ditto model in \'Deep Entity Matching with Pre-Trained Language Models\',80.76,F1 (%),pos
Natural language analysis,Entity Resolution,Entity Resolution,Entity Resolution,WDC Computers-small,2020-08,BERT model in \'Intermediate Training of BERT for Product Matching\',96.53,F1 (%),pos
Natural language analysis,Information extraction,Clinical concept extraction,Clinical concept extraction,2010 i2b2/VA,2011-05,deBruijn et al. (System 1.1) model in \'Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010\',85.23,Exact Span F1,pos
Natural language analysis,Information extraction,Clinical concept extraction,Clinical concept extraction,2010 i2b2/VA,2019-02,BERTlarge (MIMIC) model in \'Enhancing Clinical Concept Extraction with Contextual Embeddings\',90.25,Exact Span F1,pos
Natural language analysis,Information extraction,Drug–drug interaction extraction,Drug–drug interaction extraction,DDI extraction 2013 corpus,2018-05,MOL+CNN model in \'Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information\',72.55,Micro F1,pos
Natural language analysis,Information extraction,Drug–drug interaction extraction,Drug–drug interaction extraction,DDI extraction 2013 corpus,2020-10,DESC+MOL+SciBERT model in \'Using Drug Descriptions and Molecular Structures for Drug-Drug Interaction Extraction from Literature\',84.08,Micro F1,pos
Natural language analysis,Information extraction,Drug–drug interaction extraction,Drug–drug interaction extraction,DDI extraction 2013 corpus,2017-10,Hierarchy Bi-LSTMs +Att.+SDP model in \'Drug–drug interaction extraction via hierarchical RNNs on sequence and shortest dependency paths\',0.729,F1,pos
Natural language analysis,Information extraction,Drug–drug interaction extraction,Drug–drug interaction extraction,DDI extraction 2013 corpus,2020-10,DESC+MOL+SciBERT model in \'Using Drug Descriptions and Molecular Structures for Drug-Drug Interaction Extraction from Literature\',0.8408,F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2018-08,"SciIE model in \'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction\'",64.2,Entity F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',65.2,Entity F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2019-09,SpERT model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',70.33,Entity F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2021-09,SpERT.PL (SciBERT) model in \'Joint Entity and Relation Extraction from Scientific Documents: Role of Linguistic Information and Entity Types\',70.53,Entity F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2018-08,"SciIE model in \'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction\'",39.3,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',41.6,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2019-09,SpERT (with overlap) model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',50.84,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',53.2,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2020-10,Ours: cross-sentence model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',36.7,RE+ Micro F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',38.4,RE+ Micro F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SciERC,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',41.6,RE+ Micro F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,ACE 2005,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',62.2,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,DocRED,2021-02,JEREX model in \'An End-to-end Model for Entity-level Relation Extraction using Multi-instance Learning\',40.38,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,DocRED,2021-10,REBEL model in \'REBEL: Relation Extraction By End-to-end Language generation\',47.1,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SemEval 2022 Task 12: Symlink - Linking Mathematical Symbols to their Descriptions,2022-03,SciBERT (mean pooling / no preprocessing) model in \'AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities\',32.28,Relation F1,pos
Natural language analysis,Information extraction,Entity extraction,Joint entity and relation extraction,SemEval 2022 Task 12: Symlink - Linking Mathematical Symbols to their Descriptions,2022-03,SciBERT (mean pooling / no preprocessing) model in \'AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities\',41.21,Entity F1 (partial),pos
Natural language analysis,Information extraction,Entity extraction,Role-filler Entity Extraction,MUC-4,2021-09,TempGen model in \'Document-level Entity-based Extraction as Template Generation\',57.76,Avg. F1,pos
Natural language analysis,Information extraction,Keyphrase extraction,Keyphrase extraction,KPTimes,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',83.4,Recall,pos
Natural language analysis,Information extraction,Keyphrase extraction,Keyphrase extraction,KPTimes,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',10.9,F1@10,pos
Natural language analysis,Information extraction,Keyphrase extraction,Keyphrase extraction,KP20k,2021-05,Wiki+RoBERTa model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',73.0,Recall,pos
Natural language analysis,Information extraction,Keyphrase extraction,Keyphrase extraction,KP20k,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',19.7,F1@10,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval2017,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',54.0,F1 score,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval2017,2021-06,"Phraseformer(BERT, ExEm(ft)) model in \'Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding\'",67.13,F1 score,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval2017,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',54.4,Recall-at-10,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval2017,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',53.6,Precision@10,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,Inspec,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',58.9,F1 score,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,Inspec,2021-06,"Phraseformer(BERT, ExEm(ft)) model in \'Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding\'",69.87,F1 score,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,Inspec,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',57.2,Precision@10,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,Inspec,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',60.7,Recall @ 10,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval 2010 Task 8,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',37.5,F1 score,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval 2010 Task 8,2021-06,"Phraseformer(BERT, ExEm(ft)) model in \'Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding\'",48.65,F1 score,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval 2010 Task 8,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',34.3,Recall-at-10,pos
Natural language analysis,Information extraction,Keyword extraction,Keyword extraction,SemEval 2010 Task 8,2021-04,FRAKE model in \'FRAKE: Fusional Real-time Automatic Keyword Extraction\',41.5,Precision@10,pos
Natural language analysis,Information extraction,Language identification,Language identification,OpenSubtitles,2021-02,Apple bi-LSTM model in \'A reproduction of Apple\'s bi-directional LSTM models for language identification in short strings\',91.37,Accuracy,pos
Natural language analysis,Information extraction,Language identification,Language identification,nordic_langid,2020-12,FastText model in \'Discriminating Between Similar Nordic Languages\',0.9711,Accuracy,pos
Natural language analysis,Information extraction,Language identification,Language identification,Universal Dependencies,2021-02,Apple bi-LSTM model in \'A reproduction of Apple\'s bi-directional LSTM models for language identification in short strings\',86.93,Accuracy,pos
Natural language analysis,Information extraction,Language identification,Native language identification,italki NLI,2017-09,Tubasfs model in \'Fewer features perform well at Native Language Identification task\',0.5807,Average F1,pos
Natural language analysis,Information extraction,Low resource named entity recognition,Low resource named entity recognition,CONLL 2003 Dutch,2019-11,Zero-Resource Transfer From CoNLL-2003 English dataset. model in \'Zero-Resource Cross-Lingual Named Entity Recognition\',74.61,F1 score,pos
Natural language analysis,Information extraction,Low resource named entity recognition,Low resource named entity recognition,CONLL 2003 German,2019-11,Zero-Resource Transfer From CoNLL-2003 English dataset. model in \'Zero-Resource Cross-Lingual Named Entity Recognition\',65.24,F1 score,pos
Natural language analysis,Information extraction,Low resource named entity recognition,Low resource named entity recognition,Conll 2003 Spanish,2019-11,Zero-Resource Cross-lingual Transfer From CoNLL-2003 English dataset. model in \'Zero-Resource Cross-Lingual Named Entity Recognition\',75.93,F1 score,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,OntoNotes 4,2018-05,Lattice model in \'Chinese NER Using Lattice LSTM\',73.88,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,OntoNotes 4,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',80.62,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,OntoNotes 4,2019-10,BERT-MRC model in \'A Unified MRC Framework for Named Entity Recognition\',82.11,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,OntoNotes 4,2019-11,BERT-MRC+DSC model in \'Dice Loss for Data-imbalanced NLP Tasks\',84.47,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,OntoNotes 4,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',81.87,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,OntoNotes 4,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',81.4,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,SighanNER,2018-10,BiLSTM+CRF+adversarial+self-attention model in \'Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism\',90.64,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA Dev,2019-04,ERNIE model in \'ERNIE: Enhanced Representation through Knowledge Integration\',95.0,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA Dev,2019-07,ERNIE 2.0 Large model in \'ERNIE 2.0: A Continual Pre-training Framework for Language Understanding\',96.3,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Resume NER,2018-05,Lattice model in \'Chinese NER Using Lattice LSTM\',94.46,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Resume NER,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.54,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Resume NER,2020-01,FGN model in \'FGN: Fusion Glyph Network for Chinese Named Entity Recognition\',96.79,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Resume NER,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.62,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Resume NER,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',96.48,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2018-05,Lattice model in \'Chinese NER Using Lattice LSTM\',58.79,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',67.6,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2020-01,FGN model in \'FGN: Fusion Glyph Network for Chinese Named Entity Recognition\',71.25,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',67.68,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',67.71,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2019-04,CAN-NER Model model in \'CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition\',55.38,Accuracy-NE,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2019-04,CAN-NER Model model in \'CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition\',59.31,Overall,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,Weibo NER,2019-04,CAN-NER Model model in \'CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition\',62.98,Accuracy-NM,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,OntoNotes 5.0,2019-09,DGLSTM-CRF model in \'Dependency-Guided LSTM-CRF for Named Entity Recognition\',79.92,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA,2018-05,Lattice model in \'Chinese NER Using Lattice LSTM\',93.18,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',95.54,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA,2019-10,BERT-MRC model in \'A Unified MRC Framework for Named Entity Recognition\',95.75,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA,2019-11,BERT-MRC+DSC model in \'Dice Loss for Data-imbalanced NLP Tasks\',96.72,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',95.57,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Chinese named entity recognition,MSRA,2019-01,Glyce + BERT model in \'Glyce: Glyph-vectors for Chinese Character Representations\',95.51,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Cross-domain named entity recognition,CoNLL04,2020-02,BiLSTM w/ MTL and MoEE model in \'Zero-Resource Cross-Domain Named Entity Recognition\',70.04,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTRA),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',33.84,10 way 1~2 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTRA),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',40.43,5 way 1~2 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTRA),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',53.7,5 way 5~10 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTRA),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',47.49,10 way 5~10 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTER),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',48.35,10 way 1~2 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTER),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',55.95,5 way 1~2 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTER),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',61.83,5 way 5~10 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,Few-NERD (INTER),2021-09,CONTaiNER model in \'CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning\',57.12,10 way 5~10 shot,pos
Natural language analysis,Information extraction,Named entity recognition,Few-shot NER,XGLUE,2022-04,mGPT model in \'mGPT: Few-Shot Learners Go Multilingual\',0.85,Avg F1,pos
Natural language analysis,Information extraction,Named entity recognition,Medical named entity recognition,ShARe/CLEF eHealth corpus,2015-07,BANNER + DNORM model in \'Challenges in clinical natural language processing for automated disorder normalization\',0.753,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Medical named entity recognition,ShARe/CLEF eHealth corpus,2019-06,NCBI_BERT(base) (P+M) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',0.792,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Medical named entity recognition,ShARe/CLEF eHealth corpus,2021-06,BioELECTRA model in \'BioELECTRA:Pretrained Biomedical text Encoder using Discriminators\',0.8371,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Medical named entity recognition,ShARe/CLEF eHealth corpus,2015-07,BANNER + DNORM model in \'Challenges in clinical natural language processing for automated disorder normalization\',0.797,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Medical named entity recognition,ShARe/CLEF eHealth corpus,2015-07,BANNER + DNORM model in \'Challenges in clinical natural language processing for automated disorder normalization\',0.713,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,HiNER-collapsed,2022-04,cfilt/HiNER-collapsed-xlm-roberta-large model in \'HiNER: A Large Hindi Named Entity Recognition Dataset\',92.22,F1-score (Weighted),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WLPC,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',79.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Code-Switching English-Spanish NER,2019-09,HME (word + BPE + char) model in \'Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition\',69.17,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,French Treebank,2019-11,CamemBERT (subword masking) model in \'CamemBERT: a Tasty French Language Model\',87.93,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,French Treebank,2019-11,CamemBERT (subword masking) model in \'CamemBERT: a Tasty French Language Model\',88.35,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,French Treebank,2019-11,CamemBERT (subword masking) model in \'CamemBERT: a Tasty French Language Model\',87.46,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (German),2019-08,"Straková et al., 2019 model in \'Neural Architectures for Nested NER through Linearization\'",85.1,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (German),2020-05,Biaffine-NER model in \'Named Entity Recognition as Dependency Parsing\',86.4,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (German),2020-06,Cross-sentence context (CMV) model in \'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT\',87.31,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (German),2020-10,ACE + document-context model in \'Automated Concatenation of Embeddings for Structured Prediction\',88.38,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',110000000.0,Params,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',92.9,ENG,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,PIXEL model in \'Language Modelling with Pixels\',47.7,AMH,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',83.5,IBO,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',86.6,HAU,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',72.0,KIN,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',78.4,LUG,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',73.2,LUO,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',87.0,PCM,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',83.3,SWA,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',62.2,WOL,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,MasakhaNER,2022-07,BERT model in \'Language Modelling with Pixels\',73.8,YOR,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,DWIE,2020-09,Joint+RelProp model in \'DWIE: an entity-centric dataset for multi-task document-level information extraction\',74.8,F1-Hard,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,DWIE,2021-07,KB-both model in \'Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution\',75.0,F1-Hard,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Few-NERD (SUP),2021-05,BERT-Tagger model in \'Few-NERD: A Few-Shot Named Entity Recognition Dataset\',65.56,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Few-NERD (SUP),2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',71.2,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Few-NERD (SUP),2021-05,BERT-Tagger model in \'Few-NERD: A Few-Shot Named Entity Recognition Dataset\',68.78,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Few-NERD (SUP),2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',70.6,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Few-NERD (SUP),2022-03,Locate and Label model in \'Parallel Instance Query Network for Named Entity Recognition\',70.87,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Few-NERD (SUP),2021-05,BERT-Tagger model in \'Few-NERD: A Few-Shot Named Entity Recognition Dataset\',67.13,F1-Measure,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Few-NERD (SUP),2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',70.9,F1-Measure,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2002 (Spanish),2019-08,"Straková et al., 2019 model in \'Neural Architectures for Nested NER through Linearization\'",88.8,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2002 (Spanish),2020-05,Biaffine-NER model in \'Named Entity Recognition as Dependency Parsing\',90.3,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2002 (Spanish),2020-10,ACE + document-context model in \'Automated Concatenation of Embeddings for Structured Prediction\',95.9,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2016,2016-12,CambridgeLTL model in \'Bidirectional LSTM for Named Entity Recognition in Twitter Messages\',52.41,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2016,2020-10,SA-NER model in \'Named Entity Recognition for Social Media Texts with Semantic Augmentation\',55.01,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2016,2021-05,CL-KL model in \'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning\',58.98,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2016,2022-05,HGN model in \'Hero-Gang Neural Model For Named Entity Recognition\',59.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CMeEE,2021-06,MacBERT-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',62.4,Micro F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,LeNER-Br,2018-09,LSTM-CRF model in \'LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text\',0.8661,Micro F1 (Exact Span),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,LeNER-Br,2018-09,LSTM-CRF model in \'LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text\',0.9253,Micro F1 (Tokens),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR-disease,2019-06,NCBI_BERT(base) (P) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',86.6,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR-disease,2020-10,BioMegatron model in \'BioMegatron: Larger Biomedical Domain Language Model\',88.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2020,2020-10,Baseline model in \'WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols\',65.73,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2020,2020-11,mgsohrab model in \'mgsohrab at WNUT 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols\',76.6,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2020,2020-10,Baseline model in \'WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols\',70.06,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2020,2020-10,Baseline model in \'WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols\',61.91,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SciERC,2018-08,"SCIIE model in \'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction\'",64.2,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SciERC,2018-10,BERT Base model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',65.24,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SciERC,2019-03,SciBERT (SciVocab) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',67.57,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SciERC,2019-09,SpERT model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',70.33,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2004,2018-10,Neural segmental hypergraphs model in \'Neural Segmental Hypergraphs for Overlapping Mention Recognition\',75.1,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2004,2019-06,MGNER model in \'Multi-Grained Named Entity Recognition\',79.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2004,2019-08,seq2seq+BERT+Flair model in \'Neural Architectures for Nested NER through Linearization\',84.4,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2004,2019-09,Second-best learning and decoding model in \'Nested Named Entity Recognition via Second-best Sequence Learning and Decoding\',85.82,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2004,2020-05,Biaffine-NER model in \'Named Entity Recognition as Dependency Parsing\',86.7,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2004,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',90.3,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,GENIA,2018-06,Neural layered model model in \'A Neural Layered Model for Nested Named Entity Recognition\',74.7,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,GENIA,2019-06,Anchor-Region Networks model in \'Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks\',74.8,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,GENIA,2019-08,seq2seq+BERT+Flair model in \'Neural Architectures for Nested NER through Linearization\',78.31,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,GENIA,2020-05,Biaffine-NER model in \'Named Entity Recognition as Dependency Parsing\',80.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,i2b2 De-identification Dataset,2020-05,BiLSTM with ELMo model in \'MASK: A flexible framework to facilitate de-identification of clinical texts\',0.97,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,i2b2 De-identification Dataset,2020-05,BiLSTM with ELMo model in \'MASK: A flexible framework to facilitate de-identification of clinical texts\',96.0,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,DaNE,2018-02,DaLUKE v. 0.0.5 model in \'007: Democratically Finding The Cause of Packet Drops\',82.9,Micro-average F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,DaNE,2021-07,DaCy-large model in \'DaCy: A Unified Framework for Danish NLP\',84.39,Micro-average F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SoSciSoCi,2020-03,Bi-LSTM-CRF (SSC->GSC) model in \'Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach\',0.82,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SoSciSoCi,2020-03,Bi-LSTM-CRF (SSC->GSC) model in \'Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach\',0.83,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SoSciSoCi,2020-03,Bi-LSTM-CRF (SSC->GSC) model in \'Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach\',0.82,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR-chemical,2019-04,Att-BiLSTM-CRF model in \'An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition\',92.57,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR-chemical,2019-06,NCBI_BERT(base) (P) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',93.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR-chemical,2020-11,Spark NLP model in \'Biomedical Named Entity Recognition at Scale\',94.88,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR,2018-09,CollaboNet model in \'CollaboNet: collaboration of deep neural networks for biomedical named entity recognition\',87.12,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR,2019-03,SciBERT (SciVocab) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',88.94,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR,2019-08,BioFLAIR model in \'BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical Sequence Labeling Tasks\',89.42,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR,2020-11,Spark NLP model in \'Biomedical Named Entity Recognition at Scale\',89.73,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR,2021-04,ELECTRAMed model in \'ELECTRAMed: a new pre-trained language representation model for biomedical NLP\',90.03,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC5CDR,2021-05,CL-L2 model in \'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning\',90.99,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WetLab,2019-04,BiLSTM-CRF with ELMo model in \'Using Similarity Measures to Select Pretraining Data for NER\',79.62,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2002 (Dutch),2019-08,"Straková et al., 2019 model in \'Neural Architectures for Nested NER through Linearization\'",92.7,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2002 (Dutch),2020-05,Biaffine-NER model in \'Named Entity Recognition as Dependency Parsing\',93.7,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2002 (Dutch),2020-10,ACE + document-context model in \'Automated Concatenation of Embeddings for Structured Prediction\',95.7,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,NCBI-disease,2018-10,BERT Base model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',86.37,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,NCBI-disease,2019-01,BioBERT model in \'BioBERT: a pre-trained biomedical language representation model for biomedical text mining\',89.71,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,NCBI-disease,2020-11,Spark NLP model in \'Biomedical Named Entity Recognition at Scale\',90.48,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SLUE,2021-11,"W2V2-L-LL60K (pipeline approach, uses LM) model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\'",69.6,F1 (%),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,SLUE,2021-11,"W2V2-L-LL60K (pipeline approach, uses LM) model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\'",82.2,label-F1 (%),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Species800,2020-11,BLSTM-CNN-Char (SparkNLP) model in \'Biomedical Named Entity Recognition at Scale\',80.91,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Species800,2022-07,BertForTokenClassification (Spark NLP) model in \'Accurate clinical and biomedical Named entity recognition at scale\',82.59,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2005,2018-06,Neural layered model model in \'A Neural Layered Model for Nested Named Entity Recognition\',72.2,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2005,2018-10,Neural segmental hypergraphs model in \'Neural Segmental Hypergraphs for Overlapping Mention Recognition\',74.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2005,2019-06,Merge and Label model in \'Merge and Label: A novel neural network architecture for nested NER\',82.4,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2005,2019-08,seq2seq+BERT+Flair model in \'Neural Architectures for Nested NER through Linearization\',84.33,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2005,2019-09,Second-best learning and decoding model in \'Nested Named Entity Recognition via Second-best Sequence Learning and Decoding\',84.34,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2005,2019-10,BERT-MRC model in \'A Unified MRC Framework for Named Entity Recognition\',86.88,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,ACE 2005,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',90.9,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,LINNAEUS,2019-08,BioFLAIR model in \'BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical Sequence Labeling Tasks\',87.02,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BioNLP13-CG,2020-11,BLSTM-CNN-Char (SparkNLP) model in \'Biomedical Named Entity Recognition at Scale\',85.58,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BioNLP13-CG,2022-07,BertForTokenClassification (Spark NLP) model in \'Accurate clinical and biomedical Named entity recognition at scale\',87.83,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2000,2018-05,SWEM-CRF model in \'Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms\',90.34,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,IECSIL FIRE-2018 Shared Task,2021-02,XLM-RoBERTa model in \'Analysis Of Contextual and Non-Contextual Word Embedding Models For Hindi NER With Web Application For Data Collection\',90.8419,Average F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,JNLPBA,2019-01,BioBERT model in \'BioBERT: a pre-trained biomedical language representation model for biomedical text mining\',77.59,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,JNLPBA,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',79.1,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,JNLPBA,2020-11,Spark NLP model in \'Biomedical Named Entity Recognition at Scale\',81.29,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,JNLPBA,2021-04,KeBioLM model in \'Improving Biomedical Pretrained Language Models with Knowledge\',82.0,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,"NEMO-Corpus (token,test)",2020-07,LSTM-CharLSTM-CRF token-multi model in \'Neural Modeling for Named Entities and Morphology (NEMO^2)\',77.75,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,"NEMO-Corpus (token,test)",2021-04,AlephBERT-base model in \'AlephBERT:A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With\',84.91,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,OntoNotes 5.0,2022-05,HGN model in \'Hero-Gang Neural Model For Named Entity Recognition\',90.92,Average F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC7 NLM-Chem,2021-11,PubMedBERT+MLP+CRF model in \'Chemical detection and indexing in PubMed full text articles using deep learning and rule-based methods\',0.8454,F1-score (strict),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC7 NLM-Chem,2022-07,PubMedBERT+MLP+CRF model in \'Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics\',0.8731,F1-score (strict),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,HiNER-original,2022-04,cfilt/HiNER-original-xlm-roberta-large model in \'HiNER: A Large Hindi Named Entity Recognition Dataset\',88.78,F1-score (Weighted),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2017-09,SpinningBytes model in \'Transfer Learning and Sentence Level Features for Named Entity Recognition on Tweets\',40.78,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2019-06,Aguilar et al. model in \'Modeling Noisiness to Recognize Named Entities using Multitask Neural Networks on Social Media\',45.55,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2019-09,CrossWeigh + Pooled Flair model in \'CrossWeigh: Training Named Entity Tagger from Imperfect Annotations\',50.03,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2019-12,Truecase model in \'Robust Named Entity Recognition with Truecasing Pretraining\',52.3,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',56.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2021-04,TNER -xlm-r-large model in \'T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition\',58.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2021-05,CL-KL model in \'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning\',60.45,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2019-08,Cross-BiLSTM-CNN model in \'Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER\',58.28,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2019-08,Cross-BiLSTM-CNN model in \'Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER\',33.92,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2017-09,SpinningBytes model in \'Transfer Learning and Sentence Level Features for Named Entity Recognition on Tweets\',39.33,F1 (surface form),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,WNUT 2017,2019-06,Aguilar et al. model in \'Modeling Noisiness to Recognize Named Entities using Multitask Neural Networks on Social Media\',40.24,F1 (surface form),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,AnatEM,2022-07,BertForTokenClassification (Spark NLP) model in \'Accurate clinical and biomedical Named entity recognition at scale\',91.65,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,"NEMO-Corpus (morph,test)",2020-07,LSTM-CharCNN-CRF morph hybrid model in \'Neural Modeling for Named Entities and Morphology (NEMO^2)\',77.11,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,"NEMO-Corpus (morph,test)",2021-04,AlephBERT-base Pipeline model in \'AlephBERT:A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With\',80.15,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Adverse Drug Events (ADE) Corpus,2022-01,Spark NLP model in \'Mining Adverse Drug Reactions from Unstructured Mediums at Scale\',91.75,NER Macro F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Species-800,2019-08,BioFLAIR model in \'BioFLAIR: Pretrained Pooled Contextualized Embeddings for Biomedical Sequence Labeling Tasks\',82.44,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Species-800,2020-11,Spark NLP model in \'Biomedical Named Entity Recognition at Scale\',82.59,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (English),2015-11,Bi-LSTM-CNN model in \'Named Entity Recognition with Bidirectional LSTM-CNNs\',91.62,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (English),2017-04,TagLM model in \'Semi-supervised sequence tagging with bidirectional language models\',91.93,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (English),2018-02,BiLSTM-CRF+ELMo model in \'Deep contextualized word representations\',92.22,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (English),2018-08,Flair embeddings model in \'Contextual String Embeddings for Sequence Labeling\',93.09,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (English),2019-03,CNN Large + fine-tune model in \'Cloze-driven Pretraining of Self-attention Networks\',93.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (English),2020-06,Cross-sentence context (First) model in \'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT\',93.74,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (English),2020-10,ACE + document-context model in \'Automated Concatenation of Embeddings for Structured Prediction\',94.6,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2014-01,"Joint Model model in \'A Joint Model for Entity Analysis: Coreference, Typing, and Linking\'",84.04,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2015-11,Chiu and Nichols (2016) model in \'Named Entity Recognition with Bidirectional LSTM-CNNs\',86.19,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2017-02,BiLSTM-CRF model in \'Fast and Accurate Entity Recognition with Iterated Dilated Convolutions\',86.99,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2018-06,Bi-LSTM-CRF + Lexical Features model in \'Robust Lexical Features for Improved Neural Network Named-Entity Recognition\',87.95,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2018-09,CVT + Multi-Task + Large model in \'Semi-Supervised Sequence Modeling with Cross-View Training\',88.81,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2019-07,HSCRF + softdict model in \'Towards Improving Neural Named Entity Recognition with Gazetteers\',89.94,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2019-10,BERT-MRC model in \'A Unified MRC Framework for Named Entity Recognition\',91.11,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2019-11,BERT-MRC+DSC model in \'Dice Loss for Data-imbalanced NLP Tasks\',92.07,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2019-08,Att-BiLSTM-CNN model in \'Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER\',88.71,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',92.0,Precision,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2019-08,Att-BiLSTM-CNN model in \'Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER\',88.11,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,Ontonotes v5 (English),2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',91.7,Recall,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC2GM,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',84.52,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC2GM,2020-11,Spark NLP model in \'Biomedical Named Entity Recognition at Scale\',88.75,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (German) Revised,2018-08,Flair model in \'Contextual String Embeddings for Sequence Labeling\',88.3,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (German) Revised,2020-10,ACE + document-context model in \'Automated Concatenation of Embeddings for Structured Prediction\',91.7,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL 2003 (German) Revised,2020-11,FLERT XLM-R model in \'FLERT: Document-Level Features for Named Entity Recognition\',92.23,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC4CHEMD,2019-04,Att-BiLSTM-CRF model in \'An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition\',91.14,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC4CHEMD,2020-11,BLSTM-CNN-Char (SparkNLP) model in \'Biomedical Named Entity Recognition at Scale\',93.72,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,BC4CHEMD,2022-07,BertForTokenClassification (Spark NLP) model in \'Accurate clinical and biomedical Named entity recognition at scale\',94.39,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL++,2016-03,BiLSTM-CNN-CRF model in \'End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF\',91.87,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL++,2018-02,BiLSTM-CRF+ELMo model in \'Deep contextualized word representations\',93.42,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL++,2019-09,CrossWeigh + Pooled Flair model in \'CrossWeigh: Training Named Entity Tagger from Imperfect Annotations\',94.28,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition,CoNLL++,2021-04,Noise-robust Co-regularization + LUKE model in \'Learning from Noisy Labels for Entity-Centric Information Extraction\',95.88,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition in Vietnamese,PhoNER COVID19,2021-04,PhoBERT model in \'COVID-19 Named Entity Recognition for Vietnamese\',94.5,F1 (%),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition in Vietnamese,PhoNER COVID19,2022-06,ViHealthBERT model in \'ViHealthBERT: Pre-trained Language Models for Vietnamese in Health Text Mining\',96.7,F1 (%),pos
Natural language analysis,Information extraction,Named entity recognition,Named entity recognition in Vietnamese,VLSP-2016,2018-02,Bi-LSTM-CNN-CRF model in \'A Deep Neural Network Model for the Task of Named Entity Recognition\',94.43,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,NNE,2018-10,Neural Segmental Hypergraphs model in \'Neural Segmental Hypergraphs for Overlapping Mention Recognition\',91.4,Micro F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,NNE,2019-09,Second-best learning and decoding model in \'Nested Named Entity Recognition via Second-best Sequence Learning and Decoding\',93.19,Micro F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,NNE,2020-07,Pyramid model in \'Pyramid: A Layered Model for Nested Named Entity Recognition\',94.68,Micro F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,GENIA,2019-08,seq2seq+BERT+Flair model in \'Neural Architectures for Nested NER through Linearization\',78.31,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,GENIA,2020-07,Pyramid + BERT model in \'Pyramid: A Layered Model for Nested Named Entity Recognition\',79.19,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,GENIA,2021-05,Locate and Label model in \'Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition\',80.54,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,GENIA,2021-10,Triaffine + BioBERT model in \'Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition\',81.23,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,GENIA,2021-12,W2NER model in \'Unified Named Entity Recognition as Word-Word Relation Classification\',81.39,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,GENIA,2022-03,PIQN model in \'Parallel Instance Query Network for Named Entity Recognition\',81.77,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,TAC-KBP 2017,2021-10,Triaffine + ALBERT model in \'Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition\',87.27,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2004,2019-06,MGNER model in \'Multi-Grained Named Entity Recognition\',79.5,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2004,2019-08,seq2seq+BERT+Flair model in \'Neural Architectures for Nested NER through Linearization\',84.4,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2004,2019-09,Second-best learning and decoding + BERT + Flair model in \'Nested Named Entity Recognition via Second-best Sequence Learning and Decoding\',85.82,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2004,2020-12,PO-TreeCRFs model in \'Nested Named Entity Recognition with Partially-Observed TreeCRFs\',86.6,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2004,2021-05,Locate and Label model in \'Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition\',87.41,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2004,2021-10,Triaffine + ALBERT model in \'Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition\',88.56,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,Chilean Waiting List,2022-04,Multiple Single-entity NER (MSEN) [Word embeddings + Character embeddings + Flair embeddings] model in \'Automatic Extraction of Nested Entities in Clinical Referrals in Spanish\',80.27,NER Micro F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2005,2018-10,neural transition-based model model in \'A Neural Transition-based Model for Nested Mention Recognition\',73.0,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2005,2019-06,Merge and Label model in \'Merge and Label: A novel neural network architecture for nested NER\',82.4,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2005,2019-08,seq2seq+BERT+Flair model in \'Neural Architectures for Nested NER through Linearization\',84.33,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2005,2019-09,Second-best learning and decoding + BERT + Flair model in \'Nested Named Entity Recognition via Second-best Sequence Learning and Decoding\',84.34,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2005,2020-12,PO-TreeCRFs model in \'Nested Named Entity Recognition with Partially-Observed TreeCRFs\',85.4,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2005,2021-05,Sequence-to-Set model in \'A Sequence-to-Set Network for Nested Named Entity Recognition\',87.05,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Nested named entity recognition,ACE 2005,2021-10,Triaffine + ALBERT model in \'Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition\',88.83,F1,pos
Natural language analysis,Information extraction,Named entity recognition,Scientific concept extraction,STM-corpus,2020-01,SciBERT (active learning) model in \'Domain-independent Extraction of Scientific Concepts from Research Articles\',66.4,Exact Span F1,pos
Natural language analysis,Information extraction,Nested mention recognition,Nested mention recognition,ACE 2004,2018-10,Neural transition-based model model in \'A Neural Transition-based Model for Nested Mention Recognition\',73.1,F1,pos
Natural language analysis,Information extraction,Nested mention recognition,Nested mention recognition,ACE 2004,2019-10,BERT-MRC model in \'A Unified MRC Framework for Named Entity Recognition\',85.98,F1,pos
Natural language analysis,Information extraction,Open information extraction,Event extraction,GENIA,2020-06,DeepEventMine model in \'DeepEventMine: end-to-end neural nested event extraction from biomedical texts\',63.96,F1,pos
Natural language analysis,Information extraction,Open information extraction,Event extraction,Epigenetics and Post-translational Modifications 2011 (EPI),2020-06,DeepEventMine model in \'DeepEventMine: end-to-end neural nested event extraction from biomedical texts\',65.57,F1,pos
Natural language analysis,Information extraction,Open information extraction,Event extraction,GENIA 2013,2020-06,DeepEventMine model in \'DeepEventMine: end-to-end neural nested event extraction from biomedical texts\',56.72,F1,pos
Natural language analysis,Information extraction,Open information extraction,Event extraction,Pathway Curation 2013 (PC),2020-06,DeepEventMine model in \'DeepEventMine: end-to-end neural nested event extraction from biomedical texts\',57.72,F1,pos
Natural language analysis,Information extraction,Open information extraction,Event extraction,Multi-Level Event Extraction (MLEE),2020-06,DeepEventMine model in \'DeepEventMine: end-to-end neural nested event extraction from biomedical texts\',61.87,F1,pos
Natural language analysis,Information extraction,Open information extraction,Event extraction,Infectious Diseases 2011 (ID),2020-06,DeepEventMine model in \'DeepEventMine: end-to-end neural nested event extraction from biomedical texts\',61.62,F1,pos
Natural language analysis,Information extraction,Open information extraction,Event extraction,Cancer Genetics 2013 (CG),2020-06,DeepEventMine model in \'DeepEventMine: end-to-end neural nested event extraction from biomedical texts\',61.74,F1,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,BenchIE,2021-09,ClausIE model in \'BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation\',0.34,F1,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,BenchIE,2021-09,ClausIE model in \'BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation\',0.5,Precision,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,BenchIE,2021-09,MinIE model in \'BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation\',0.28,Recall,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,CaRB OIE benchmark (Greek Use-case),2021-03,PENELOPIE Greek OIE model in \'PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation\',0.255,F1,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,Penn Treebank,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',81.5,AUC,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,Penn Treebank,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',88.5,F1,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,OIE2016,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',58.6,AUC,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,OIE2016,2019-01,SpanOIE model in \'Span Model for Open Information Extraction on Accurate Corpus\',68.65,F1,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,OIE2016,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',72.6,F1,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,NYT,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',72.5,AUC,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,NYT,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',85.5,F1,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,Web,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',82.4,AUC,pos
Natural language analysis,Information extraction,Open information extraction,Open information extraction,Web,2021-09,DeepEx (zero-shot) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',91.2,F1,pos
Natural language analysis,Information extraction,Participant intervention comparison outcome extraction,Participant intervention comparison outcome extraction,EBM-NLP,2018-06,"bi-LSTM model in \'A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature\'",66.3,F1,pos
Natural language analysis,Information extraction,Participant intervention comparison outcome extraction,Participant intervention comparison outcome extraction,EBM-NLP,2019-03,SciBERT (SciVocab) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',71.18,F1,pos
Natural language analysis,Information extraction,Participant intervention comparison outcome extraction,Participant intervention comparison outcome extraction,EBM-NLP,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',73.38,F1,pos
Natural language analysis,Information extraction,Relation extraction,4-ary Relation Extraction,SciREX,2021-09,TempGen model in \'Document-level Entity-based Extraction as Template Generation\',3.55,Avg. F1,pos
Natural language analysis,Information extraction,Relation extraction,Binary Relation Extraction,SciREX,2021-09,TempGen model in \'Document-level Entity-based Extraction as Template Generation\',14.47,Avg. F1,pos
Natural language analysis,Information extraction,Relation extraction,Biomedical relation extraction,CMeIE,2021-06,RoBERTa-wwm-ext-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',55.9,Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Biomedical relation extraction,DDI extraction 2013 corpus,2019-06,NCBI_BERT(large) (P) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',79.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Biomedical relation extraction,DDI extraction 2013 corpus,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',83.35,F1,pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2020-12,SimpleRE model in \'An Embarrassingly Simple Model for Dialogue Relation Extraction\',66.7,F1 (v2),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2021-05,Dual model in \'Semantic Representation for Dialogue Modeling\',67.1,F1 (v2),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2021-06,SocAoG model in \'SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues\',69.1,F1 (v2),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2021-09,TUCORE-GCN_RoBERTa model in \'Graph Based Network with Contextualized Representations of Turns in Dialogue\',73.1,F1 (v2),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2020-12,SimpleRE model in \'An Embarrassingly Simple Model for Dialogue Relation Extraction\',63.3,F1c (v2),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2021-06,SocAoG model in \'SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues\',66.5,F1c (v2),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2020-04,BERTS model in \'Dialogue-Based Relation Extraction\',61.2,F1 (v1),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2020-12,SimpleRE model in \'An Embarrassingly Simple Model for Dialogue Relation Extraction\',66.3,F1 (v1),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2021-05,Dual model in \'Semantic Representation for Dialogue Modeling\',67.3,F1 (v1),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2020-04,BERTS model in \'Dialogue-Based Relation Extraction\',55.4,F1c (v1),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2020-12,GDPNet model in \'GDPNet: Refining Latent Multi-View Graph for Relation Extraction\',60.1,F1c (v1),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2021-05,Dual model in \'Semantic Representation for Dialogue Modeling\',61.4,F1c (v1),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DialogRE,2020-12,SimpleRE model in \'An Embarrassingly Simple Model for Dialogue Relation Extraction\',65.2,F1 (Chinese),pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DDRel,2020-12,BERT model in \'DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues\',39.73,Pair-level 13-class Acc,pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DDRel,2020-12,BERT model in \'DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues\',58.13,Pair-level 4-class Acc,pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DDRel,2020-12,BERT model in \'DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues\',42.33,Pair-level 6-class Acc,pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DDRel,2020-12,BERT model in \'DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues\',39.4,Session-level 13-class Acc,pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DDRel,2020-12,BERT model in \'DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues\',47.1,Session-level 4-class Acc,pos
Natural language analysis,Information extraction,Relation extraction,Dialog relation extraction,DDRel,2020-12,BERT model in \'DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues\',41.87,Session-level 6-class Acc,pos
Natural language analysis,Information extraction,Relation extraction,DrugProt,DrugProt,2021-10,R-BERT-CNN model in \'R-BERT-CNN: Drug-target interactions extraction from biomedical literature\',55.67,F1 (micro),pos
Natural language analysis,Information extraction,Relation extraction,Few-Shot Relation Classification,FREDo (cross-domain),2022-05,DL-MNAV+SIE+SBN model in \'Few-Shot Document-Level Relation Extraction\',2.85,F1 (1-Doc),pos
Natural language analysis,Information extraction,Relation extraction,Few-Shot Relation Classification,FREDo (cross-domain),2022-05,DL-MNAV+SIE+SBN model in \'Few-Shot Document-Level Relation Extraction\',3.72,F1 (3-Doc),pos
Natural language analysis,Information extraction,Relation extraction,Few-Shot Relation Classification,DocRED,2022-05,DL-MNAV model in \'Few-Shot Document-Level Relation Extraction\',7.05,F1 (1-Doc),pos
Natural language analysis,Information extraction,Relation extraction,Few-Shot Relation Classification,DocRED,2022-05,DL-MNAV model in \'Few-Shot Document-Level Relation Extraction\',8.42,F1 (3-Doc),pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2018-08,"SciIE model in \'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction\'",64.2,Entity F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',65.2,Entity F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2019-09,SpERT model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',70.33,Entity F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2021-09,SpERT.PL (SciBERT) model in \'Joint Entity and Relation Extraction from Scientific Documents: Role of Linguistic Information and Entity Types\',70.53,Entity F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2018-08,"SciIE model in \'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction\'",39.3,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',41.6,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2019-09,SpERT (with overlap) model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',50.84,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',53.2,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2020-10,Ours: cross-sentence model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',36.7,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',38.4,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SciERC,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',41.6,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,ACE 2005,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',62.2,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,DocRED,2021-02,JEREX model in \'An End-to-end Model for Entity-level Relation Extraction using Multi-instance Learning\',40.38,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,DocRED,2021-10,REBEL model in \'REBEL: Relation Extraction By End-to-end Language generation\',47.1,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SemEval 2022 Task 12: Symlink - Linking Mathematical Symbols to their Descriptions,2022-03,SciBERT (mean pooling / no preprocessing) model in \'AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities\',32.28,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Joint entity and relation extraction,SemEval 2022 Task 12: Symlink - Linking Mathematical Symbols to their Descriptions,2022-03,SciBERT (mean pooling / no preprocessing) model in \'AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities\',41.21,Entity F1 (partial),pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,Discovery Dataset,2019-03,BERT model in \'Mining Discourse Markers for Unsupervised Sentence Representation Learning\',20.6,1:1 Accuracy,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,AbstRCT - Neoplasm,2021-02,ResAttArg model in \'Multi-Task Attentive Residual Networks for Argument Mining\',70.92,Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,MATRES,2022-04,SCS-EERE model in \'Selecting Optimal Context Sentences for Event-Event Relation Extraction\',0.834,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,FewRel,2021-09,DeepEx (zero-shot top-10) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',92.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,DRI Corpus,2021-02,ResAttArg model in \'Multi-Task Attentive Residual Networks for Argument Mining\',37.72,Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,SemEval 2010 Task 8,2012-07,MVRNN model in \'Semantic Compositionality through Recursive Matrix-Vector Spaces\',82.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,SemEval 2010 Task 8,2015-06,depLCNN + NS  model in \'Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling\',85.6,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,SemEval 2010 Task 8,2016-01,DRNNs model in \'Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation\',86.1,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,SemEval 2010 Task 8,2016-08,BRCNN model in \'Bidirectional Recurrent Convolutional Neural Network for Relation Classification\',86.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,TACRED,2021-01,TANL model in \'Structured Prediction as Translation between Augmented Natural Languages\',71.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,TACRED,2021-09,DeepEx (zero-shot top-10) model in \'Zero-Shot Information Extraction as a Unified Text-to-Triple Translation\',76.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation classification,CDCP,2021-02,ResAttArg model in \'Multi-Task Attentive Residual Networks for Argument Mining\',42.95,Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,PGR,2021-12,Spark NLP model in \'Deeper Clinical Document Understanding Using Relation Extraction\',87.9,Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SKE,2021-05,ReRe (exact) model in \'Revisiting the Negative Data of Distantly Supervised Relation Extraction\',87.21,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DWIE,2020-09,Joint+AttProp model in \'DWIE: an entity-centric dataset for multi-task document-level information extraction\',50.4,F1-Hard,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DWIE,2021-07,KB-both model in \'Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution\',52.1,F1-Hard,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,2012 i2b2 Temporal Relations,2021-12,Spark NLP model in \'Deeper Clinical Document Understanding Using Relation Extraction\',73.6,Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT10-HRL,2018-11,HRL Takanobu et al. (2019) model in \'A Hierarchical Framework for Relation Extraction with Reinforcement Learning\',64.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT10-HRL,2019-09,CasRel (exact) model in \'A Novel Cascade Binary Tagging Framework for Relational Triple Extraction\',70.11,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT10-HRL,2020-10,TPLinker Wang et al. (2020)* model in \'TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking\',72.45,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT10-HRL,2021-05,ReRe model in \'Revisiting the Negative Data of Distantly Supervised Relation Extraction\',73.95,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WNUT 2020,2020-10,Baseline model in \'WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols\',72.5,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WNUT 2020,2020-10,Baseline model in \'WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols\',80.1,Precision,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WNUT 2020,2020-10,Baseline model in \'WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols\',66.21,Recall,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2018-04,multi-head model in \'Joint entity recognition and relation extraction as a multi-head selection problem\',74.58,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2018-08,multi-head + AT model in \'Adversarial training for multi-context joint entity and relation extraction\',75.52,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2019-05,Relation-Metric model in \'Neural Metric Learning for Fast End-to-End Relation Extraction\',77.19,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2019-09,SpERT (without overlap) model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',79.24,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2020-02,Deeper model in \'Deeper Task-Specificity Improves Joint Entity and Relation Extraction\',83.74,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2021-12,Spark NLP model in \'Deeper Clinical Document Understanding Using Relation Extraction\',90.0,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2020-10,Table-Sequence model in \'Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\',80.1,RE Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2021-12,Spark NLP model in \'Deeper Clinical Document Understanding Using Relation Extraction\',90.0,RE Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2018-04,multi-head model in \'Joint entity recognition and relation extraction as a multi-head selection problem\',86.4,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2018-08,multi-head + AT model in \'Adversarial training for multi-context joint entity and relation extraction\',86.73,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2019-05,Relation-Metric model in \'Neural Metric Learning for Fast End-to-End Relation Extraction\',87.02,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2019-09,SpERT (with overlap) model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',89.28,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2020-02,Deeper model in \'Deeper Task-Specificity Improves Joint Entity and Relation Extraction\',89.48,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2020-10,Table-Sequence model in \'Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\',89.7,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2021-09,SpERT.PL (with overlap and BioBERT) model in \'Joint Entity and Relation Extraction from Scientific Documents: Role of Linguistic Information and Entity Types\',91.17,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Adverse Drug Events (ADE) Corpus,2021-12,Spark NLP model in \'Deeper Clinical Document Understanding Using Relation Extraction\',91.75,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,2018 n2c2 (Track 2) - Adverse Drug Events and Medication Extraction,2021-12,Spark NLP model in \'Deeper Clinical Document Understanding Using Relation Extraction\',96.7,Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Re-TACRED,2017-09,PA-LSTM model in \'Position-aware Attention and Supervised Data Improve Slot Filling\',79.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Re-TACRED,2018-09,C-GCN model in \'Graph Convolution over Pruned Dependency Trees Improves Relation Extraction\',80.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Re-TACRED,2019-07,SpanBERT model in \'SpanBERT: Improving Pre-training by Representing and Predicting Spans\',85.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Re-TACRED,2021-02,RoBERTa-large-typed-marker model in \'An Improved Baseline for Sentence-level Relation Extraction\',91.1,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Re-TACRED,2021-07,EXOBRAIN model in \'Improving Sentence-Level Relation Extraction through Curriculum Learning\',91.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ADE Corpus,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',83.2,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ADE Corpus,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',91.3,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2012-07,MIML-RE model in \'Multi-instance Multi-label Learning for Relation Extraction\',60.7,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2016-08,PCNN+ATT model in \'Neural Relation Extraction with Selective Attention over Instances\',69.4,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2018-12,RESIDE model in \'RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information\',73.6,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2019-03,HRERE model in \'Connecting Language and Knowledge with Heterogeneous Representations for Neural Relation Extraction\',84.9,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2020-09,RECON model in \'RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Network\',87.5,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2021-06,KGPOOL model in \'KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction\',92.3,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2016-08,PCNN+ATT model in \'Neural Relation Extraction with Selective Attention over Instances\',51.8,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2018-12,RESIDE model in \'RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information\',59.5,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2019-03,HRERE model in \'Connecting Language and Knowledge with Heterogeneous Representations for Neural Relation Extraction\',72.8,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2020-09,RECON model in \'RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Network\',74.1,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT Corpus,2021-06,KGPOOL model in \'KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction\',86.7,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CDR,2020-05,LSR w/o MDP Nodes model in \'Reasoning with Latent Structure Refinement for Document-Level Relation Extraction\',64.8,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CDR,2020-10,SciBERT-ATLOPBASE model in \'Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling\',69.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CDR,2021-06,DocuNet-SciBERTbase model in \'Document-level Relation Extraction as Semantic Segmentation\',76.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CDR,2021-09,SAISORE+CR+ET-SciBERT model in \'SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction\',79.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT-single,2017-06,NovelTagging model in \'Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\',49.5,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT-single,2019-07,PA-LSTM model in \'Joint extraction of entities and overlapping relations using position-attentive sequence labeling\',53.8,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT-single,2019-09,ETL-Span model in \'Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy\',59.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Dataset: Relationship extraction for knowledge graph creation from biomedical literature (Gene-Disease relationships),2022-01,DistilBERT model in \'Relationship extraction for knowledge graph creation from biomedical literature\',91.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2014-08,CNN model in \'Relation Classification via Convolutional Deep Neural Network\',82.7,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2015-04,CR-CNN model in \'Classifying Relations by Ranking with Convolutional Neural Networks\',84.1,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2016-08,Att-Pooling-CNN model in \'Relation Classification via Multi-Level Attention CNNs\',88.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2019-02,Entity-Aware BERT model in \'Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers\',89.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2019-05,R-BERT model in \'Enriching Pre-trained Language Model with Entity Information for Relation Classification\',89.25,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2019-06,BERTEM+MTB model in \'Matching the Blanks: Distributional Similarity for Relation Learning\',89.5,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2019-11,EPGNN model in \'Improving Relation Classification by Entity Pair Graph\',90.2,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2019-12,Skeleton-Aware BERT model in \'Enhancing Relation Extraction Using Syntactic Indicators and Sentential Contexts\',90.36,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2020-04,REDN model in \'Downstream Model Design of Pre-trained Language Model for Relation Extraction Task\',91.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval-2010 Task 8,2020-10,QA model in \'Relation Classification as Two-way Span-Prediction\',91.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WebNLG,2017-06,NovelTagging model in \'Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\',28.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WebNLG,2018-07,CopyRE MultiDecoder model in \'Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism\',37.1,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WebNLG,2019-09,HBT (CasRel) model in \'A Novel Cascade Binary Tagging Framework for Relational Triple Extraction\',91.8,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WebNLG,2020-10,TPLinker model in \'TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking\',91.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WebNLG,2020-11,SPN model in \'Joint Entity and Relation Extraction with Set Prediction Networks\',93.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WebNLG,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',93.6,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WebNLG,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',98.0,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ChemProt,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',77.24,Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ChemProt,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',79.98,Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ChemProt,2019-01,BioBERT model in \'BioBERT: a pre-trained biomedical language representation model for biomedical text mining\',76.46,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ChemProt,2019-03,SciBert (Finetune) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',83.64,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ChemProt,2021-05,SciFive-Large model in \'SciFive: a text-to-text transformer model for biomedical literature\',88.95,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT11-HRL,2016-01,SPTree model in \'End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures\',0.531,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT11-HRL,2018-11,HRL model in \'A Hierarchical Framework for Relation Extraction with Reinforcement Learning\',0.538,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT11-HRL,2019-09,CasRel model in \'A Novel Cascade Binary Tagging Framework for Relational Triple Extraction\',0.539,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT11-HRL,2020-10,TPLinker model in \'TPLinker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking\',0.5567,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT11-HRL,2021-05,RERE model in \'Revisiting the Negative Data of Distantly Supervised Relation Extraction\',0.5623,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2017-09,PA-LSTM model in \'Position-aware Attention and Supervised Data Improve Slot Filling\',65.1,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2018-09,C-GCN + PA-LSTM model in \'Graph Convolution over Pruned Dependency Trees Improves Relation Extraction\',68.2,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2019-05,R-BERT model in \'Enriching Pre-trained Language Model with Entity Information for Relation Classification\',69.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2019-06,BERTEM+MTB model in \'Matching the Blanks: Distributional Similarity for Relation Learning\',71.5,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2019-11,KEPLER model in \'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\',71.7,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2020-02,K-ADAPTER (F+L) model in \'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\',72.04,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2020-09,"DeNERT-KG model in \'DeNERT-KG: Named Entity and Relation Extraction Model Using DQN, Knowledge Graph, and BERT\'",72.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2020-10,Relation Reduction model in \'Relation Classification as Two-way Span-Prediction\',74.8,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2021-05,RECENT+SpanBERT model in \'Relation Classification with Entity Type Restriction\',75.2,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2022-06,RE-MC model in \'Enhancing Targeted Minority Class Prediction in Sentence-Level Relation Extraction\',75.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2019-06,BERTEM+MTB model in \'Matching the Blanks: Distributional Similarity for Relation Learning\',64.8,F1 (10% Few-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2021-09,NLI_DeBERTa model in \'Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction\',67.9,F1 (10% Few-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2020-02,K-ADAPTER (F+L) model in \'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\',45.1,F1 (5% Few-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2020-10,LUKE model in \'LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\',51.6,F1 (5% Few-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2021-09,NLI_DeBERTa model in \'Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction\',69.0,F1 (5% Few-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2019-06,BERTEM+MTB model in \'Matching the Blanks: Distributional Similarity for Relation Learning\',43.4,F1 (1% Few-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2021-09,NLI_DeBERTa model in \'Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction\',63.7,F1 (1% Few-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,TACRED,2021-09,NLI_DeBERTa model in \'Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction\',62.8,F1 (Zero-Shot),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Wikipedia-Wikidata relations,2017-09,ContextAtt model in \'Context-Aware Representations for Knowledge Base Relation Extraction\',-0.159,Error rate,neg
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WLPC,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',64.1,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,WLPC,2019-11,SpanRel model in \'Generalizing Natural Language Analysis through Span-relation Representations\',65.5,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GAD,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',82.34,Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GAD,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',84.9,Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GAD,2021-04,KeBioLM model in \'Improving Biomedical Pretrained Language Models with Knowledge\',84.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GAD,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',84.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,MUC6,2018-10,iDepNN model in \'Neural Relation Extraction Within and Across Sentence Boundaries\',0.94,Average F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SciERC,2019-03,SciBERT (SciVocab) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',74.64,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SciERC,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',38.4,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SciERC,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',66.8,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,Dataset: Relationship extraction for knowledge graph creation from biomedical literature (Gene-Disease relationships) n,2022-01,DistilBERT model in \'Relationship extraction for knowledge graph creation from biomedical literature\',89.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2014-06,Joint w/ Global model in \'Incremental Joint Extraction of Entity Mentions and Relations\',45.3,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2016-01,SPTree model in \'End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures\',48.4,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2019-05,Multi-turn QA model in \'Entity-Relation Extraction as Multi-Turn Question Answering\',49.4,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',62.2,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',62.5,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',66.5,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2014-06,Joint w/ Global model in \'Incremental Joint Extraction of Entity Mentions and Relations\',48.3,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2017-07,Attention model in \'Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees\',49.3,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',59.7,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',66.1,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',69.7,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2014-06,Joint w/ Global model in \'Incremental Joint Extraction of Entity Mentions and Relations\',79.7,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2016-01,SPTree model in \'End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures\',81.8,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',87.4,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',90.3,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2004,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',90.4,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval 2018 Task 10,2018-04,"Gradient boosting with co-occurrence count features and JoBimText features model in \'BomJi at SemEval-2018 Task 10: Combining Vector-, Pattern- and Graph-based Information to Identify Discriminative Attributes\'",0.73,F1-Score,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,SemEval 2018 Task 10,2018-06,SVM with GloVe model in \'SUNNYNLP at SemEval-2018 Task 10: A Support-Vector-Machine-Based Method for Detecting Semantic Difference using Taxonomy and Word Embedding Features\',0.76,F1-Score,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GDA,2020-05,LSR w/o MDP Nodes model in \'Reasoning with Latent Structure Refinement for Document-Level Relation Extraction\',82.2,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GDA,2020-10,SciBERT-ATLOPBASE model in \'Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling\',83.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GDA,2021-06,DocuNet-SciBERTbase model in \'Document-level Relation Extraction as Semantic Segmentation\',85.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,GDA,2021-09,SAISORE+CR+ET-SciBERT model in \'SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction\',87.1,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,2010 i2b2/VA,2021-12,Spark NLP model in \'Deeper Clinical Document Understanding Using Relation Extraction\',69.1,Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DuIE,2020-08,BiTT model in \'BiTT: Bidirectional Tree Tagging for Joint Extraction of Overlapping Entities and Relations\',76.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2021-06,HySPA (ours) w/ RoBERTa model in \'HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction\',68.2,Relation F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2014-06,Joint w/ Global model in \'Incremental Joint Extraction of Entity Mentions and Relations\',49.5,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2016-01,SPTree model in \'End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures\',55.6,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2017-09,Global model in \'End-to-End Neural Relation Extraction with Global Optimization\',57.5,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2018-10,MRT model in \'Extracting Entities and Relations with Joint Minimum Risk Training\',59.6,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2019-05,Multi-turn QA model in \'Entity-Relation Extraction as Multi-Turn Question Answering\',60.2,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2020-07,MRC4ERE++ model in \'Asking Effective and Diverse Questions: A Machine Reading Comprehension based Framework for Joint Entity-Relation Extraction\',62.1,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',67.0,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',71.1,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2014-06,Joint w/ Global model in \'Incremental Joint Extraction of Entity Mentions and Relations\',52.1,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2017-07,Attention model in \'Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees\',55.9,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2018-11,Hierarchical Multi-task model in \'A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks\',62.7,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',63.2,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2019-09,"DYGIE++ model in \'Entity, Relation, and Event Extraction with Contextualized Span Representations\'",63.4,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',69.4,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',73.0,RE Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2014-06,Joint w/ Global model in \'Incremental Joint Extraction of Entity Mentions and Relations\',80.8,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2016-01,SPTree model in \'End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures\',83.4,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2017-09,Global model in \'End-to-End Neural Relation Extraction with Global Optimization\',83.6,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2018-11,Hierarchical Multi-task model in \'A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks\',87.5,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2019-04,DyGIE model in \'A General Framework for Information Extraction using Dynamic Span Graphs\',88.4,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2019-09,"DYGIE++ model in \'Entity, Relation, and Event Extraction with Contextualized Span Representations\'",88.6,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2020-10,Ours: cross-sentence ALB model in \'A Frustratingly Easy Approach for Entity and Relation Extraction\',90.9,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2021-09,PL-Marker model in \'Packed Levitated Marker for Entity and Relation Extraction\',91.1,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2015-05,FCM model in \'Improved Relation Extraction with Feature-Rich Compositional Embedding Models\',58.2,Relation classification F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2015-06,CNN model in \'Relation Extraction: Perspective from Convolutional Neural Networks\',61.3,Relation classification F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2015-11,RNN+CNN model in \'Combining Neural Networks and Log-linear Models to Improve Relation Extraction\',67.7,Relation classification F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2019-11,Dual Pointer Network model in \'Relation Extraction among Multiple Entities Using a Dual Pointer Network with a Multi-Head Attention Mechanism\',80.5,Relation classification F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,ACE 2005,2021-03,Dual Pointer Network(multi-head) model in \'Dual Pointer Network for Fast Extraction of Multiple Relations in a Sentence\',80.8,Relation classification F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,JNLPBA,2019-03,SciBERT (SciVocab) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',76.09,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT21,2021-05,ReRe model in \'Revisiting the Negative Data of Distantly Supervised Relation Extraction\',59.62,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2019-06,BiLSTM model in \'DocRED: A Large-Scale Document-Level Relation Extraction Dataset\',51.06,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2019-09,Two-Step+BERT-base model in \'Fine-tune Bert for DocRED with Two-step Process\',53.92,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-03,HIN-BERT-base model in \'HIN: Hierarchical Inference Network for Document-Level Relation Extraction\',55.6,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-04,CorefRoBERTa-large model in \'Coreferential Reasoning Learning for Language Representation\',60.25,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-08,E2GRE-RoBERTa-large model in \'Entity and Evidence Guided Relation Extraction for DocRED\',62.5,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-09,GAIN-BERT-large model in \'Double Graph Based Reasoning for Document-level Relation Extraction\',62.76,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-10,ATLOP-RoBERTa-large model in \'Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling\',63.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2021-02,SSAN-RoBERTa-large+Adaptation model in \'Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction\',65.92,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2022-03,KD-Rb-l model in \'Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation\',67.28,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2019-06,BiLSTM model in \'DocRED: A Large-Scale Document-Level Relation Extraction Dataset\',44.73,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2019-09,BERT-base model in \'Fine-tune Bert for DocRED with Two-step Process\',56.17,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-04,CorefRoBERTa-large model in \'Coreferential Reasoning Learning for Language Representation\',57.9,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-08,E2GRE-RoBERTa-large model in \'Entity and Evidence Guided Relation Extraction for DocRED\',60.3,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-09,GAIN-BERT-large model in \'Double Graph Based Reasoning for Document-level Relation Extraction\',60.31,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2020-10,ATLOP-RoBERTa-large model in \'Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling\',61.39,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2021-02,SSAN-RoBERTa-large+Adaptation model in \'Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction\',63.78,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DocRED,2022-03,KD-Rb-l model in \'Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation\',65.24,Ign F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,FewRel,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',88.32,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,FewRel,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',88.49,Precision,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,FewRel,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',88.44,Recall,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DDI,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',82.36,Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DDI,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',83.35,Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DDI,2021-04,KeBioLM model in \'Improving Biomedical Pretrained Language Models with Knowledge\',81.9,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,DDI,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',83.35,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT29,2018-11,HRLRE model in \'A Hierarchical Framework for Relation Extraction with Reinforcement Learning\',64.3,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT29,2019-11,WDec model in \'Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction\',71.6,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT24,2018-11,HRLRE model in \'A Hierarchical Framework for Relation Extraction with Reinforcement Learning\',77.6,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT24,2019-11,WDec model in \'Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction\',84.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2018-04,multi-head model in \'Joint entity recognition and relation extraction as a multi-head selection problem\',83.9,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2018-12,Biaffine attention model in \'End-to-end neural relation extraction using deep biaffine attention\',86.2,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2019-09,SpERT model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',86.25,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2020-02,Deeper model in \'Deeper Task-Specificity Improves Joint Entity and Relation Extraction\',87.0,NER Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2014-10,Table Representation model in \'Modeling Joint Entity and Relation Extraction with Table Representation\',61.0,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2017-09,Global model in \'End-to-End Neural Relation Extraction with Global Optimization\',67.8,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2019-05,Multi-turn QA model in \'Entity-Relation Extraction as Multi-Turn Question Answering\',68.9,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2019-09,SpERT model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',71.47,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2020-10,Table-Sequence model in \'Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\',73.6,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2021-10,REBEL model in \'REBEL: Relation Extraction By End-to-end Language generation\',75.4,RE+ Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2014-10,Table Representation model in \'Modeling Joint Entity and Relation Extraction with Table Representation\',80.7,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2017-09,Global model in \'End-to-End Neural Relation Extraction with Global Optimization\',85.6,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2019-05,Multi-turn QA model in \'Entity-Relation Extraction as Multi-Turn Question Answering\',87.8,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2019-09,SpERT model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',88.94,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2020-02,Deeper model in \'Deeper Task-Specificity Improves Joint Entity and Relation Extraction\',89.78,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2020-10,TablERT model in \'Named Entity Recognition and Relation Extraction using Enhanced Table Filling by Contextualized Representations\',90.2,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2021-01,TriMF model in \'A Trigger-Sense Memory Flow Framework for Joint Entity and Relation Extraction\',90.3,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2018-04,multi-head model in \'Joint entity recognition and relation extraction as a multi-head selection problem\',62.04,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2018-12,Biaffine attention model in \'End-to-end neural relation extraction using deep biaffine attention\',64.4,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2019-09,SpERT model in \'Span-based Joint Entity and Relation Extraction with Transformer Pre-training\',72.87,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2020-10,Table-Sequence model in \'Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\',75.4,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,CoNLL04,2021-10,REBEL model in \'REBEL: Relation Extraction By End-to-end Language generation\',76.65,RE+ Macro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2017-06,NovelTagging model in \'Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme\',42.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2018-07,CopyRE MultiDecoder model in \'Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism\',58.7,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2019-09,HBT(CasRel) model in \'A Novel Cascade Binary Tagging Framework for Relational Triple Extraction\',89.6,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2020-10,PCNN+RL+HME model in \'RH-Net: Improving Neural Relation Extraction via Reinforcement Learning and Hierarchical Relational Searching\',90.0,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2020-11,SPN model in \'Joint Entity and Relation Extraction with Set Prediction Networks\',92.5,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2021-10,REBEL model in \'REBEL: Relation Extraction By End-to-end Language generation\',93.4,F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2021-08,PFN model in \'A Partition Filter Network for Joint Entity and Relation Extraction\',95.8,NER Micro F1,pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2021-06,DIRECT model in \'Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning\',90.2,F1 (strict),pos
Natural language analysis,Information extraction,Relation extraction,Relation extraction,NYT,2021-10,REBEL model in \'REBEL: Relation Extraction By End-to-end Language generation\',92.0,F1 (strict),pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,NYT,2020-12,DocDS model in \'From Bag of Sentences to Document: Distantly Supervised Relation Extraction via Machine Reading Comprehension\',0.595,PR AUC,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,NYT,2020-12,DocDS model in \'From Bag of Sentences to Document: Distantly Supervised Relation Extraction via Machine Reading Comprehension\',0.939,P@100,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,NYT,2020-12,DocDS model in \'From Bag of Sentences to Document: Distantly Supervised Relation Extraction via Machine Reading Comprehension\',0.889,P@200,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,NYT,2020-12,DocDS model in \'From Bag of Sentences to Document: Distantly Supervised Relation Extraction via Machine Reading Comprehension\',0.873,P@300,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2018-08,BGRU-SET model in \'Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning\',0.39,AUC,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2021-02,REDSandT model in \'Improving Distantly-Supervised Relation Extraction through BERT-based Label & Instance Embeddings\',0.424,AUC,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2021-05,CGRE model in \'Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs\',0.52,AUC,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2015-09,PCNN model in \'Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks\',61.3,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2018-04,BGWA model in \'Improving Distantly Supervised Relation Extraction using Word and Entity Based Attention\',70.9,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2021-05,CGRE model in \'Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs\',84.5,P-at-10%,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2015-09,PCNN model in \'Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks\',46.5,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2018-04,BGWA model in \'Improving Distantly Supervised Relation Extraction using Word and Entity Based Attention\',52.4,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2021-05,CGRE model in \'Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs\',71.5,P-at-30%,pos
Natural language analysis,Information extraction,Relation extraction,Relationship extraction using distant supervision,New York Times Corpus,2018-08,BGRU-SET model in \'Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning\',0.39,Average Precision,pos
Natural language analysis,Information extraction,Relation extraction,Zero-shot Relation Triplet Extraction,Wiki-ZSL,2020-10,TableSequence model in \'Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\',6.4,Avg. F1,pos
Natural language analysis,Information extraction,Relation extraction,Zero-shot Relation Triplet Extraction,Wiki-ZSL,2022-03,RelationPrompt model in \'RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction\',31.19,Avg. F1,pos
Natural language analysis,Information extraction,Relation extraction,Zero-shot Relation Triplet Extraction,FewRel,2020-10,TableSequence model in \'Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders\',6.37,Avg. F1,pos
Natural language analysis,Information extraction,Relation extraction,Zero-shot Relation Triplet Extraction,FewRel,2022-03,RelationPrompt model in \'RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction\',24.61,Avg. F1,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",8.8,Macro F1,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',19.7,Macro F1,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",7.5,Micro F1,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',25.8,Micro F1,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",9.5,Macro Precision,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',20.2,Macro Precision,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",8.6,Macro Recall,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',20.6,Macro Recall,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",6.8,Micro Precision,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',27.4,Micro Precision,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",8.4,Micro Recall,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',24.4,Micro Recall,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',21.1,Macro F1,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',28.7,Micro F1,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',24.0,Macro Precision,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',21.8,Macro Recall,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',37.4,Micro Precision,pos
Natural language analysis,Information extraction,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',23.2,Micro Recall,pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',90.43,Type,pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',96.37,Strict Detection (Pr.),pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',96.37,Strict Detection (Re.),pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',96.37,Strict Detection (F1),pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,B2B model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',100.0,Relaxed Detection (Pr.),pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',100.0,Relaxed Detection (Re.),pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',100.0,Relaxed Detection (F1),pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal information extraction,TempEval-3,2013-06,ClearTK model in \'ClearTK-TimeML: A minimalist approach to TempEval 2013\',30.98,Temporal awareness,pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal information extraction,TempEval-3,2019-06,Ning et al. model in \'A Structured Learning Approach to Temporal Relation Extraction\',67.2,Temporal awareness,pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal information extraction,TimeBank,2014-01,CAEVO model in \'Dense Event Ordering with a Multi-Pass Architecture\',0.507,F1 score,pos
Natural language analysis,Information extraction,Temporal information extraction,Temporal information extraction,TimeBank,2016-12,Catena model in \'CATENA: CAusal and TEmporal relation extraction from NAtural language texts\',0.511,F1 score,pos
Natural language analysis,Information extraction,Term Extraction,Term Extraction,AWARE,2021-11,Baseline model in \'AWARE: Aspect-Based Sentiment Analysis Dataset of Apps Reviews for Requirements Elicitation\',0.82,F1-Score,pos
Natural language analysis,Information extraction,Term Extraction,Term Extraction,SemEval 2014 Task 4 Laptop,2019-07,Seq2Seq4ATE model in \'Exploring Sequence-to-Sequence Learning in Aspect Term Extraction\',0.8031,F1-Score,pos
Natural language analysis,Information extraction,Timex normalization,Timex normalization,TimeBank,2012-05,TIMEN model in \'TIMEN: An Open Temporal Expression Normalisation Resource\',0.89,F1-Score,pos
Natural language analysis,Information extraction,Timex normalization,Timex normalization,PNT,2015-09,HeidelTime model in \'A Baseline Temporal Tagger for all Languages\',0.74,F1-Score,pos
Natural language analysis,Information extraction,Timex normalization,Timex normalization,PNT,2018-01,Laparra et al. model in \'From Characters to Time Intervals: New Paradigms for Evaluation and Neural Parsing of Time Normalizations\',0.764,F1-Score,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2017-04,FNRM-RankProb_Embed model in \'Neural Ranking Models with Weak Supervision\',0.2837,MAP,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2018-10,SNRM-PRF model in \'From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing\',0.2971,MAP,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2018-12,Anserini BM25+RM3 model in \'The Neural Hype and Comparisons Against Weak Baselines\',0.302,MAP,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2019-03,BERT FT(Microblog) model in \'Simple Applications of BERT for Ad Hoc Document Retrieval\',0.3278,MAP,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2020-03,monoT5-3B (zero-shot) model in \'Document Ranking with a Pretrained Sequence-to-Sequence Model\',0.3876,MAP,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2017-11,DRMM model in \'A Deep Relevance Matching Model for Ad-hoc Retrieval\',0.431,nDCG-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2018-09,POSIT-DRMM-MV model in \'Deep Relevance Ranking Using Enhanced Document-Query Interactions\',0.464,nDCG-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2019-04,CEDR-KNRM model in \'CEDR: Contextualized Embeddings for Document Ranking\',0.5381,nDCG-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2020-03,monoT5-3B (zero-shot) model in \'Document Ranking with a Pretrained Sequence-to-Sequence Model\',0.6091,nDCG-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2017-11,DRMM model in \'A Deep Relevance Matching Model for Ad-hoc Retrieval\',0.382,P-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2018-09,POSIT-DRMM-MV model in \'Deep Relevance Ranking Using Enhanced Document-Query Interactions\',0.389,P-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2018-10,NPRF-DRMM model in \'NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval\',0.4064,P-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2019-03,BERT FT(Microblog) model in \'Simple Applications of BERT for Ad Hoc Document Retrieval\',0.4287,P-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2019-04,CEDR-KNRM model in \'CEDR: Contextualized Embeddings for Document Ranking\',0.4667,P-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Ad-hoc information retrieval,TREC Robust04,2020-03,monoT5-3B (zero-shot) model in \'Document Ranking with a Pretrained Sequence-to-Sequence Model\',0.5165,P-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Document ranking,DaReCzech,2021-12,Query-doc RobeCzech (Roberta-base) model in \'Siamese BERT-based Model for Web Search Relevance Ranking Evaluated on a New Czech Dataset\',46.73,P-at-10,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Document ranking,ClueWeb09-B,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',31.1,nDCG-at-20,pos
Natural language analysis,Information retrieval,Ad-hoc information retrieval,Document ranking,ClueWeb09-B,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',20.28,ERR-at-20,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.501,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.534,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2019-08,BERT model in \'An Effective Domain Adaptive Post-Training Method for BERT in Response Selection\',0.639,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2020-04,SA-BERT+BERT-FP model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.715,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.309,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.347,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2019-08,BERT model in \'An Effective Domain Adaptive Post-Training Method for BERT in Response Selection\',0.453,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2020-04,SA-BERT+BERT-FP model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.555,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.487,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.511,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2019-08,BERT model in \'An Effective Domain Adaptive Post-Training Method for BERT in Response Selection\',0.625,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2020-04,SA-BERT+BERT-FP model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.701,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.702,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.281,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.308,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2019-08,BERT model in \'An Effective Domain Adaptive Post-Training Method for BERT in Response Selection\',0.404,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2020-04,SA-BERT+BERT-FP model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.497,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.442,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.457,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2019-08,BERT model in \'An Effective Domain Adaptive Post-Training Method for BERT in Response Selection\',0.606,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2020-04,SA-BERT+BERT-FP model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.685,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.708,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.723,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.751,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2019-08,BERT model in \'An Effective Domain Adaptive Post-Training Method for BERT in Response Selection\',0.875,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS,2020-04,SA-BERT+BERT-FP model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.931,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS Ranking Test,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.765,NDCG@5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,RRS Ranking Test,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.679,NDCG@3,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Advising Corpus,2019-01,CtxDec & -Rev model in \'Sequential Attention-based Network for Noetic End-to-End Response Selection\',31.0,R-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Advising Corpus,2019-01,CtxDec & -Rev model in \'Sequential Attention-based Network for Noetic End-to-End Response Selection\',78.8,R-at-10,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Advising Corpus,2019-01,CtxDec & -Rev model in \'Sequential Attention-based Network for Noetic End-to-End Response Selection\',97.8,R@50,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,DSTC7 Ubuntu,2018-12,Sequential Inference Models model in \'Building Sequential Inference Models for End-to-End Response Selection\',60.8,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,DSTC7 Ubuntu,2019-01,Sequential Attention-based Network model in \'Sequential Attention-based Network for Noetic End-to-End Response Selection\',64.5,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,DSTC7 Ubuntu,2019-04,Bi-encoder (v2) model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',70.9,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,DSTC7 Ubuntu,2019-11,Multi-context ConveRT model in \'ConveRT: Efficient and Accurate Conversational Representations from Transformers\',71.2,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,PolyAI AmazonQA,2019-04,PolyAI Encoder model in \'A Repository of Conversational Datasets\',71.3,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,PolyAI AmazonQA,2019-11,ConveRT model in \'ConveRT: Efficient and Accurate Conversational Representations from Transformers\',84.3,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-06,Dual-LSTM model in \'The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems\',0.604,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-10,Dual-BiLSTM model in \'Improved Deep Learning Baselines for Ubuntu Corpus Dialogs\',0.63,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-11,Multi-View model in \'Multi-view Response Selection for Human-Computer Conversation\',0.662,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.726,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.752,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.767,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-01,ESIM model in \'Sequential Attention-based Network for Noetic End-to-End Response Selection\',0.796,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.882,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2020-09,BERT-SL model in \'Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues\',0.884,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.911,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-06,Dual-LSTM model in \'The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems\',0.745,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-10,Dual-BiLSTM model in \'Improved Deep Learning Baselines for Ubuntu Corpus Dialogs\',0.78,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-11,Multi-View model in \'Multi-view Response Selection for Human-Computer Conversation\',0.801,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.822,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.868,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.874,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-01,ESIM model in \'Sequential Attention-based Network for Noetic End-to-End Response Selection\',0.894,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.949,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.962,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-06,Dual-LSTM model in \'The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems\',0.926,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-10,Dual-BiLSTM model in \'Improved Deep Learning Baselines for Ubuntu Corpus Dialogs\',0.944,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-11,Multi-View model in \'Multi-view Response Selection for Human-Computer Conversation\',0.951,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.96,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.962,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.969,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-01,ESIM model in \'Sequential Attention-based Network for Noetic End-to-End Response Selection\',0.975,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.99,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.994,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-06,Dual-LSTM model in \'The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems\',0.878,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2015-10,Dual-BiLSTM model in \'Improved Deep Learning Baselines for Ubuntu Corpus Dialogs\',0.895,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-11,Multi-View model in \'Multi-view Response Selection for Human-Computer Conversation\',0.908,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.926,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.938,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-01,IMN model in \'Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.946,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2019-07,IoI-local model in \'One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues\',0.947,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.965,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,"Ubuntu Dialogue (v1, Ranking)",2020-09,BERT-SL model in \'Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues\',0.975,R2-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,PolyAI Reddit,2018-02,ELMO model in \'Deep contextualized word representations\',19.3,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,PolyAI Reddit,2018-03,USE model in \'Universal Sentence Encoder\',47.7,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,PolyAI Reddit,2019-04,PolyAI Encoder model in \'A Repository of Conversational Datasets\',61.3,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,PolyAI Reddit,2019-11,Multi-context ConveRT model in \'ConveRT: Efficient and Accurate Conversational Representations from Transformers\',71.8,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,PolyAI OpenSubtitles,2019-04,PolyAI Encoder model in \'A Repository of Conversational Datasets\',30.6,1-of-100 Accuracy,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.569,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.599,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.601,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-01,IMN model in \'Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.615,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.65,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.659,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-09,UMS_BERT+ model in \'Do Response Selection Models Really Know What\'s Next? Utterance Manipulation Strategies for Multi-turn Response Selection\',0.664,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-12,SA-BERT+HCL model in \'Dialogue Response Selection with Hierarchical Curriculum Learning\',0.681,MRR,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.397,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.421,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.427,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-01,IMN model in \'Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.433,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.475,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.496,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-09,UMS_BERT+ model in \'Do Response Selection Models Really Know What\'s Next? Utterance Manipulation Strategies for Multi-turn Response Selection\',0.499,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-12,SA-BERT+HCL model in \'Dialogue Response Selection with Hierarchical Curriculum Learning\',0.514,P-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.529,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.551,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-01,IMN model in \'Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.57,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.608,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.619,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-09,UMS_BERT+ model in \'Do Response Selection Models Really Know What\'s Next? Utterance Manipulation Strategies for Multi-turn Response Selection\',0.625,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-12,SA-BERT+HCL model in \'Dialogue Response Selection with Hierarchical Curriculum Learning\',0.639,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.644,MAP,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.233,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.243,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-07,DAM model in \'Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network\',0.254,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-01,IMN model in \'Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.262,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.299,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.313,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-09,UMS_BERT+ model in \'Do Response Selection Models Really Know What\'s Next? Utterance Manipulation Strategies for Multi-turn Response Selection\',0.318,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-12,SA-BERT+HCL model in \'Dialogue Response Selection with Hierarchical Curriculum Learning\',0.33,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.396,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.421,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-01,IMN model in \'Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.452,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.494,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-12,SA-BERT+HCL model in \'Dialogue Response Selection with Hierarchical Curriculum Learning\',0.531,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.542,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.724,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2018-06,DUA model in \'Modeling Multi-turn Conversation with Deep Utterance Aggregation\',0.78,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-01,IMN model in \'Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.789,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-04,Poly-encoder model in \'Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\',0.822,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2019-08,BERT model in \'An Effective Domain Adaptive Post-Training Method for BERT in Response Selection\',0.828,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.847,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2020-09,UMS_BERT+ model in \'Do Response Selection Models Really Know What\'s Next? Utterance Manipulation Strategies for Multi-turn Response Selection\',0.858,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,Douban,2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.87,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.453,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2018-02,DCM model in \'007: Democratically Finding The Cause of Packet Drops\',0.685,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.704,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2020-09,BERT-SL model in \'Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues\',0.776,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.87,R10-at-1,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.654,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2018-02,DCM model in \'007: Democratically Finding The Cause of Packet Drops\',0.864,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.879,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2020-09,BERT-SL model in \'Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues\',0.919,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2021-05,BERT-FP model in \'Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\',0.956,R10-at-2,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2016-12,SMN model in \'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\',0.886,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2018-02,DCM model in \'007: Democratically Finding The Cause of Packet Drops\',0.982,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2020-04,SA-BERT model in \'Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots\',0.985,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2020-09,BERT-SL model in \'Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues\',0.991,R10-at-5,pos
Natural language analysis,Information retrieval,Conversational response selection,Conversational response selection,E-commerce,2020-12,SA-BERT+HCL model in \'Dialogue Response Selection with Hierarchical Curriculum Learning\',0.993,R10-at-5,pos
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,TREC-PM,2018-11,hpipubcommon model in \'HPI-DHC at TREC 2018 Precision Medicine Track\',0.5605,infNDCG,pos
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,MS MARCO,2022-05,ConAE-128 model in \'Dimension Reduction for Efficient Dense Retrieval via Conditional Autoencoder\',-0.3245,Time (ms),neg
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,CQADupStack,2021-04,BM25+CE model in \'BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\',0.37,mAP@100,pos
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,CQADupStack,2022-02,SGPT-CE-6.1B model in \'SGPT: GPT Sentence Embeddings for Semantic Search\',0.42,mAP@100,pos
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,BSARD,2021-08,Two-tower Bi-Encoder (RoBERTa) model in \'A Statutory Article Retrieval Dataset in French\',74.78,Recall-at-100,pos
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,BSARD,2021-08,Siamese Bi-Encoder (RoBERTa) model in \'A Statutory Article Retrieval Dataset in French\',78.38,Recall@200,pos
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,BSARD,2021-08,Siamese Bi-Encoder (RoBERTa) model in \'A Statutory Article Retrieval Dataset in French\',83.77,Recall@500,pos
Natural language analysis,Information retrieval,Information retrieval,Information retrieval,Ohsumed,2021-03,BERT+CONCEPT FILTER model in \'Semantic Enrichment of Pretrained Embedding Output for Unsupervised IR\',0.25,NDCG,pos
Natural language analysis,Information retrieval,Math Information Retrieval,Math Information Retrieval,ARQMath2 - Task 1,2022-03,Approach0+ColBERT (fusion) model in \'Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval\',0.215,MAP,pos
Natural language analysis,Information retrieval,Math Information Retrieval,Math Information Retrieval,ARQMath2 - Task 1,2022-03,Approach0+ColBERT (fusion) model in \'Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval\',0.447,NDCG,pos
Natural language analysis,Information retrieval,Math Information Retrieval,Math Information Retrieval,ARQMath2 - Task 1,2022-03,Approach0+ColBERT (reranking) model in \'Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval\',0.276,P-at-10,pos
Natural language analysis,Information retrieval,Math Information Retrieval,Math Information Retrieval,ARQMath2 - Task 1,2022-03,Approach0+ColBERT (fusion) model in \'Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval\',0.202,bpref,pos
Natural language analysis,Information retrieval,Passage Ranking,Passage Ranking,MS MARCO,2022-01,Fine-tuned SOTA model in \'Text and Code Embeddings by Contrastive Pre-Training\',44.3,MRR@10,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,MSMARCO (BEIR),2021-04,ColBERT model in \'BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\',0.425,nDCG-at-10,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2022-03,DAR model in \'Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation\',42.92,MRR,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2020-04,DPR model in \'Dense Passage Retrieval for Open-Domain Question Answering\',79.4,Precision@20,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2020-07,ANCE model in \'Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\',81.9,Precision@20,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2020-10,RocketQA model in \'RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\',82.7,Precision@20,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2021-07,DPR-PAQ model in \'Domain-matched Pre-training Tasks for Dense Retrieval\',84.68,Precision@20,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2021-09,DPR+ELECTRA-large-extreader-reranker model in \'R2-D2: A Modular Baseline for Open-Domain Question Answering\',85.26,Precision@20,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2020-04,DPR model in \'Dense Passage Retrieval for Open-Domain Question Answering\',86.0,Precision@100,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2020-07,ANCE model in \'Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\',87.5,Precision@100,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2020-10,RocketQA model in \'RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\',88.5,Precision@100,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,Natural Questions,2021-07,DPR-PAQ model in \'Domain-matched Pre-training Tasks for Dense Retrieval\',89.22,Precision@100,pos
Natural language analysis,Information retrieval,Passage Retrieval,Passage Retrieval,EntityQuestions,2021-10,TOME-2 model in \'Mention Memory: incorporating textual knowledge into Transformers through entity mention attention\',0.838,Recall-at-20,pos
Natural language analysis,Information retrieval,Passage re-ranking,Passage re-ranking,TREC-PM,2019-04,BERT + Doc2query model in \'Document Expansion by Query Prediction\',36.5,mAP,pos
Natural language analysis,Information retrieval,Passage re-ranking,Passage re-ranking,MS MARCO,2019-01,BERT + Small Training model in \'Passage Re-ranking with BERT\',0.359,MRR,pos
Natural language analysis,Information retrieval,Passage re-ranking,Passage re-ranking,MS MARCO,2019-04,BERT + Doc2query model in \'Document Expansion by Query Prediction\',0.368,MRR,pos
Natural language analysis,Information retrieval,Phrase ranking,Phrase ranking,KPTimes,2021-05,Wiki+RoBERTa  model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',99.0,P@5K,pos
Natural language analysis,Information retrieval,Phrase ranking,Phrase ranking,KPTimes,2021-05,Wiki+RoBERTa  model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',96.5,P@50K,pos
Natural language analysis,Information retrieval,Phrase ranking,Phrase ranking,KP20k,2021-05,Wiki+RoBERTa model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',100.0,P@5K,pos
Natural language analysis,Information retrieval,Phrase ranking,Phrase ranking,KP20k,2021-05,Wiki+RoBERTa model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',98.5,P@50K,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",8.8,Macro F1,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',19.7,Macro F1,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",7.5,Micro F1,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',25.8,Micro F1,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",9.5,Macro Precision,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',20.2,Macro Precision,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",8.6,Macro Recall,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',20.6,Macro Recall,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",6.8,Micro Precision,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',27.4,Micro Precision,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2019-06,"TDMS-IE model in \'Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction\'",8.4,Micro Recall,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,"NLP-TDMS (Exp, arXiv only)",2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',24.4,Micro Recall,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',21.1,Macro F1,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',28.7,Micro F1,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',24.0,Macro Precision,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',21.8,Macro Recall,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',37.4,Micro Precision,pos
Natural language analysis,Information retrieval,Scientific results extraction,Scientific results extraction,PWC Leaderboards (restricted),2020-04,AxCell model in \'AxCell: Automatic Extraction of Results from Machine Learning Papers\',23.2,Micro Recall,pos
Natural language analysis,Information retrieval,Semantic retrieval,Semantic retrieval,Contract Discovery,2019-11,Human baseline model in \'Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines\',0.84,Soft-F1,pos
Natural language analysis,Morphological analysis,Lexical normalization,Lexical normalization,LexNorm,2013-10,unLOL model in \'A Log-Linear Model for Unsupervised Text Normalization\',82.06,Accuracy,pos
Natural language analysis,Morphological analysis,Lexical normalization,Lexical normalization,LexNorm,2015-07,Syllable based model in \'Tweet Normalization with Syllables\',86.08,Accuracy,pos
Natural language analysis,Morphological analysis,Lexical normalization,Lexical normalization,LexNorm,2017-10,MoNoise model in \'MoNoise: Modeling Noise Using a Modular Normalization System\',87.63,Accuracy,pos
Natural language analysis,Morphological analysis,Thai word segmentation,Thai word segmentation,BEST-2010,2019-11,AttaCut-SC model in \'AttaCut: A Fast and Accurate Neural Thai Word Segmenter\',0.9839,F1-Score,pos
Natural language analysis,Morphological analysis,Thai word segmentation,Thai word segmentation,BEST-2010,2020-05,ThaiLMCut model in \'ThaiLMCut: Unsupervised Pretraining for Thai Word Segmentation\',0.9878,F1-Score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Automatic Misogynistic Identification,2018-12,Logistic Regression model in \'Hateminers : Detecting Hate speech against Women\',0.704,Accuracy,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Automatic Misogynistic Identification,2020-04,mBert model in \'Deep Learning Models for Multilingual Hate Speech Detection\',0.832,Accuracy,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,AbusEval,2020-10,HateBERT model in \'HateBERT: Retraining BERT for Abusive Language Detection in English\',0.742,Macro F1,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,"Waseem et al., 2018",2021-06,"Mozafari et al., 2019 model in \'AAA: Fair Evaluation for Abuse Detection Systems Wanted\'",50.94,AAA,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,"Waseem et al., 2018",2021-06,"Mozafari et al., 2019 model in \'AAA: Fair Evaluation for Abuse Detection Systems Wanted\'",84.42,F1 (micro),pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,ToLD-Br,2020-10,Multilingual BERT model in \'Toxic Language Detection in Social Media for Brazilian Portuguese: New Dataset and Multilingual Analysis\',0.75,F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,HatEval,2020-10,HateBERT model in \'HateBERT: Retraining BERT for Abusive Language Detection in English\',0.494,Macro F1,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Ethos MultiLabel,2020-06,MLARAM model in \'ETHOS: an Online Hate Speech Detection Dataset\',0.2948,Hamming Loss,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,SHAJ,2021-07,Baseline BERT (task A) model in \'Detecting Abusive Albanian\',0.77,F1,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,OffensEval 2019,2020-10,HateBERT model in \'HateBERT: Retraining BERT for Abusive Language Detection in English\',0.805,Macro F1,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,HateXplain,2020-12,BERT-HateXplain [Attn] model in \'HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection\',0.698,Accuracy,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,HateXplain,2020-12,BERT-HateXplain [LIME] model in \'HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection\',0.687,Macro F1,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,HateXplain,2020-12,BERT-HateXplain [LIME] model in \'HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection\',0.851,AUROC,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,bajer_danish_misogyny,2021-08,AOM mBERT model in \'Annotating Online Misogyny\',0.8549,F1,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Ethos Binary,2020-06,BERT model in \'ETHOS: an Online Hate Speech Detection Dataset\',0.7883,F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Ethos Binary,2021-06,BiLSTM + static BE model in \'Hate speech detection using static BERT embeddings\',0.7971,F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Ethos Binary,2020-06,BiLSTM+Attention+FT model in \'ETHOS: an Online Hate Speech Detection Dataset\',0.7734,Classification Accuracy,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Ethos Binary,2021-06,BiLSTM + static BE model in \'Hate speech detection using static BERT embeddings\',0.8015,Classification Accuracy,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Ethos Binary,2020-06,BERT model in \'ETHOS: an Online Hate Speech Detection Dataset\',79.17,Precision,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,Hostility Detection Dataset in Hindi,2021-01,Auxiliary IndicBert model in \'Hostility Detection in Hindi leveraging Pre-Trained Language Models\',0.5725,F1 score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hate speech detection,DKhate,2019-08,Baseline model in \'Offensive Language and Hate Speech Detection for Danish\',0.7,F1,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hope speech detection,KanHope,2021-08,Dual-Channel mBERT model in \'Hope Speech detection in under-resourced Kannada language\',0.65,F1-score (Weighted),pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hope speech detection,HopeEDI,2020-12,"Decision Tree Classifier model in \'HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion\'",0.9,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hope speech detection,HopeEDI,2021-04,RoBERTa model in \'TeamUNCC@LT-EDI-EACL2021: Hope Speech Detection using Transfer Learning with Transformers\',0.93,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hope speech detection for Malayalam,HopeEDI,2020-12,"Decision Tree Classifier model in \'HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion\'",0.73,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hope speech detection for Malayalam,HopeEDI,2021-04,XLM-RoBERTa model in \'TeamUNCC@LT-EDI-EACL2021: Hope Speech Detection using Transfer Learning with Transformers\',0.87,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hope speech detection for Tamil,HopeEDI,2020-12,"Logistic Regression model in \'HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion\'",0.56,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Abuse detection,Hope speech detection for Tamil,HopeEDI,2021-04,XLM-RoBERTa model in \'TeamUNCC@LT-EDI-EACL2021: Hope Speech Detection using Transfer Learning with Transformers\',0.6,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Bias detection,Bias detection,PlantVillage_8px,2022-06,RandomForest_default_hyperparameters model in \'Uncovering bias in the PlantVillage dataset\',49.0,Accuracy (%),pos
Natural language analysis,Pragmatics analysis,Bias detection,Bias detection,StereoSet,2020-04,GPT-2 (small) model in \'StereoSet: Measuring stereotypical bias in pretrained language models\',72.97,ICAT Score,pos
Natural language analysis,Pragmatics analysis,Bias detection,Bias detection,Wiki Neutrality Corpus,2020-02,RoBERTa+ALBERT model in \'Towards Detection of Subjective Bias using Contextualized Word Embeddings\',70.4,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,The ARRAU Corpus,2019-11,dali-full-anaphora model in \'A Cluster Ranking Model for Full Anaphora Resolution\',77.9,Avg F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoGUM,2021-06,SpanBERT model in \'OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres\',64.6,Avg F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,CoNLL 2012,2017-07,e2e-coref + ELMo model in \'End-to-end Neural Coreference Resolution\',70.4,Avg F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,CoNLL 2012,2018-04,c2f-coref + ELMo model in \'Higher-order Coreference Resolution with Coarse-to-fine Inference\',73.0,Avg F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,CoNLL 2012,2019-07,EE + BERT-large model in \'Coreference Resolution with Entity Equalization\',76.61,Avg F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,CoNLL 2012,2019-08,c2f-coref + BERT-large model in \'BERT for Coreference Resolution: Baselines and Analysis\',76.9,Avg F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,CoNLL 2012,2020-07,CorefQA + SpanBERT-large model in \'CorefQA: Coreference Resolution as Query-based Span Prediction\',83.1,Avg F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,Winograd Schema Challenge,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',93.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,Winograd Schema Challenge,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',95.9,Accuracy,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,WSC,2022-04,PaLM 540B model in \'PaLM: Scaling Language Modeling with Pathways\',89.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,STM-coref,2021-01,BFCR + SpanBERT + Transfer Learning model in \'Coreference Resolution in Research Papers from Multiple Domains\',61.4,CoNLL F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2016-04,Global model in \'Learning Global Features for Coreference Resolution\',64.21,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2016-06,NN Cluster Ranker model in \'Improving Coreference Resolution by Learning Entity-Level Distributed Representations\',65.29,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2016-09,Reward Rescaling model in \'Deep Reinforcement Learning for Mention-Ranking Coreference Models\',65.73,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2017-07,e2e-coref model in \'End-to-end Neural Coreference Resolution\',67.2,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2018-02,e2e-coref + ELMo model in \'Deep contextualized word representations\',70.4,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2018-04,c2f-coref model in \'Higher-order Coreference Resolution with Coarse-to-fine Inference\',73.0,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2019-07,SpanBERT model in \'SpanBERT: Improving Pre-training by Representing and Predicting Spans\',79.6,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,OntoNotes,2021-09,wl-coref + RoBERTa model in \'Word-Level Coreference Resolution\',81.0,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,DWIE,2020-09,Joint model in \'DWIE: an entity-centric dataset for multi-task document-level information extraction\',91.6,Avg. F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2020-05,PeTra model in \'PeTra: A Sparsely Supervised Memory Model for People Tracking\',85.3,F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2019-06,Full Ensemble model in \'Gendered Pronoun Resolution using BERT and an extractive question answering formulation\',90.2,Overall F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2019-08,ProBERT model in \'Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling\',92.5,Overall F1,pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2019-06,Full Ensemble model in \'Gendered Pronoun Resolution using BERT and an extractive question answering formulation\',90.9,Masculine F1 (M),pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2019-08,ProBERT model in \'Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling\',94.0,Masculine F1 (M),pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2019-06,Full Ensemble model in \'Gendered Pronoun Resolution using BERT and an extractive question answering formulation\',89.5,Feminine F1 (F),pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2019-08,ProBERT model in \'Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling\',91.1,Feminine F1 (F),pos
Natural language analysis,Pragmatics analysis,Coreference resolution,Coreference resolution,GAP,2019-06,Full Ensemble model in \'Gendered Pronoun Resolution using BERT and an extractive question answering formulation\',0.98,Bias (F/M),pos
Natural language analysis,Pragmatics analysis,Counter-speech detection,Counter-speech detection,Youtube counterspeech dataset,2018-08,XGBoost model in \'Thou shalt not hate: Countering Online Hate Speech\',0.715,F1 score,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Causal emotion entailment,RECCON,2020-12,RoBERTa Large model in \'Recognizing Emotion Cause in Conversations\',77.06,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Causal emotion entailment,RECCON,2020-12,RoBERTa Large model in \'Recognizing Emotion Cause in Conversations\',66.23,Pos. F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Causal emotion entailment,RECCON,2020-12,RoBERTa Base model in \'Recognizing Emotion Cause in Conversations\',88.74,Neg. F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,EEG Emotion Recognition,SEED-IV,2019-07,SVM model in \'EEG-Based Emotion Recognition Using Regularized Graph Neural Networks\',56.61,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion cause extraction,ECE,2017-08,ConvMS-Memnet model in \'A Question Answering Approach to Emotion Cause Extraction\',69.55,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion cause extraction,ECE,2019-06,RTHN model in \'RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction\',76.77,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,RAVDESS,2021-06,ERANN-0-4 model in \'ERANNs: Efficient Residual Audio Neural Networks for Audio Pattern Recognition\',74.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,RAVDESS,2021-11,Logistic Regression on posteriors of the CNN-14&biLSTM-GuidedST model in \'Multimodal Emotion Recognition on RAVDESS Dataset Using Transfer Learning\',80.08,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,RAVDESS,2021-12,LogisticRegression on posteriors of xlsr-Wav2Vec2.0&bi-LSTM+Attention model in \'A proposal for Multimodal Emotion Recognition using aural transformers and Action Units on RAVDESS dataset\',86.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,SEED,2021-01,4D-aNN model in \'4D Attention-based Neural Network for EEG Emotion Recognition\',96.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,SEED-IV,2019-07,RGNN model in \'EEG-Based Emotion Recognition Using Regularized Graph Neural Networks\',79.37,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,MPED,2019-05,BiHDM model in \'A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition\',40.34,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,MSP-Podcast,2022-03,w2v2-L-robust-12 model in \'Dawn of the transformer era in speech emotion recognition: closing the valence gap\',0.638,Concordance correlation coefficient (CCC),pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,Emomusic,2021-07,Jukebox (Pre-training: CALM) model in \'Codified audio language modeling learns useful representations for music information retrieval\',72.1,EmoA,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition,Emomusic,2021-07,Jukebox (Pre-training: CALM) model in \'Codified audio language modeling learns useful representations for music information retrieval\',61.7,EmoV,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in context,EMOTIC,2019-08,CAER-Net (Adaptive Fusion) model in \'Context-Aware Emotion Recognition Networks\',20.84,mAP,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in context,EMOTIC,2020-03,EmotiCon (Depth-based) model in \'EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege\'s Principle\',35.48,mAP,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoWoz,2021-09,COSMIC model in \'EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems\',56.3,Macro F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoWoz,2021-09,ContextBERT model in \'EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems\',79.7,Weighted F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',57.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',59.54,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2021-06,DialogueCRN model in \'DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations\',60.73,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2022-03,MM-DFN model in \'MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations\',62.49,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2022-06,M2FNet model in \'M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation\',67.85,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',56.44,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',57.03,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2019-07,ConGCN model in \'Modeling both context- and speaker-sensitive dependence for emotion detection in multi-speaker conversations\',57.4,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',58.1,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2019-09,KET model in \'Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations\',58.18,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2020-03,BERT+MTL model in \'Multi-Task Learning with Auxiliary Speaker Identification for Conversational Emotion Recognition\',61.9,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2020-10,COSMIC model in \'COSMIC: COmmonSense knowledge for eMotion Identification in Conversations\',65.21,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2021-06,TODKAT model in \'Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection\',65.47,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2021-08,CoMPM model in \'CoMPM: Context Modeling with Speaker\'s Pre-trained Memory Tracking for Emotion Recognition in Conversation\',66.52,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,MELD,2022-06,M2FNet model in \'M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation\',66.71,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoryNLP,2021-06,TODKAT model in \'Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection\',42.38,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoryNLP,2021-12,CoG-BART model in \'Contrast and Generation Make BART a Good Dialogue Emotion Recognizer\',42.58,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoryNLP,2019-09,KET model in \'Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations\',34.39,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoryNLP,2020-03,BERT+MTL model in \'Multi-Task Learning with Auxiliary Speaker Identification for Conversational Emotion Recognition\',35.92,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoryNLP,2020-10,COSMIC model in \'COSMIC: COmmonSense knowledge for eMotion Identification in Conversations\',38.11,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoryNLP,2021-05,DAG-ERC model in \'Directed Acyclic Graph Network for Conversational Emotion Recognition\',39.02,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmoryNLP,2021-09,TUCORE-GCN_RoBERTa model in \'Graph Based Network with Contextualized Representations of Turns in Dialogue\',39.24,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2014-08,TextCNN model in \'Convolutional Neural Networks for Sentence Classification\',48.9,Accuracy of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2015-01,TextRCNN model in \'Recurrent Convolutional Neural Networks for Text Classification\',49.13,Accuracy of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2017-07,bcLSTM model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',49.65,Accuracy of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2020-12,DialogXL model in \'DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition\',51.24,Accuracy of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2022-05,BERT+AVG+MLP model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',51.5,Accuracy of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2014-08,TextCNN model in \'Convolutional Neural Networks for Sentence Classification\',34.37,Macro-F1 of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2015-01,TextRCNN model in \'Recurrent Convolutional Neural Networks for Text Classification\',37.95,Macro-F1 of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2017-07,bcLSTM model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',45.4,Macro-F1 of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2020-12,DialogXL model in \'DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition\',46.96,Macro-F1 of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,CPED,2022-05,BERT+AVG+MLP model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',48.02,Macro-F1 of Sentiment,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',-0.189,MAE (Valence),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-10,ICON model in \'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection\',-0.181,MAE (Valence),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',-0.168,MAE (Valence),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',-0.157,MAE (Valence),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',-0.213,MAE (Arousal),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-10,ICON model in \'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection\',-0.19,MAE (Arousal),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',-0.165,MAE (Arousal),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',-0.161,MAE (Arousal),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2020-09,Pretrained Hierarchical Transformer model in \'Hierarchical Pre-training for Sequence Labelling in Spoken Dialog\',-0.16,MAE (Arousal),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2021-06,DialogueCRN model in \'DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations\',-0.152,MAE (Arousal),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',-0.19,MAE (Expectancy),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-10,ICON model in \'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection\',-0.185,MAE (Expectancy),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',-0.175,MAE (Expectancy),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',-0.168,MAE (Expectancy),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2020-09,Pretrained Hierarchical Transformer model in \'Hierarchical Pre-training for Sequence Labelling in Spoken Dialog\',-0.16,MAE (Expectancy),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',-8.67,MAE (Power),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-10,ICON model in \'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection\',-8.45,MAE (Power),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',-7.9,MAE (Power),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,SEMAINE,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',-7.68,MAE (Power),neg
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,DailyDialog,2020-10,COSMIC model in \'COSMIC: COmmonSense knowledge for eMotion Identification in Conversations\',51.05,Macro F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,DailyDialog,2021-08,CoMPM model in \'CoMPM: Context Modeling with Speaker\'s Pre-trained Memory Tracking for Emotion Recognition in Conversation\',53.15,Macro F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,DailyDialog,2019-09,KET model in \'Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations\',53.37,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,DailyDialog,2020-07,CESTa model in \'Contextualized Emotion Recognition in Conversation as Sequence Tagging\',63.12,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,DailyDialog,2021-12,S+PAGE model in \'S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation\',64.07,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,DailyDialog,2021-06,TODKAT model in \'Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection\',52.56,Weighted F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,DailyDialog,2021-12,CoG-BART model in \'Contrast and Generation Make BART a Good Dialogue Emotion Recognizer\',54.71,Weighted F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',56.32,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2018-06,CMN model in \'Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos\',56.56,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2018-10,ICON model in \'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection\',59.09,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',63.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',65.25,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2020-05,BiERU-lc model in \'BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis\',66.11,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2020-12,SumAggGIN model in \'Summarize before Aggregate: A Global-to-local Heterogeneous Graph Inference Network for Conversational Emotion Recognition\',66.76,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2022-03,MM-DFN model in \'MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations\',68.21,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2022-06,M2FNet model in \'M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation\',69.69,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2021-06,TODKAT model in \'Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection\',61.11,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2021-12,CoG-BART model in \'Contrast and Generation Make BART a Good Dialogue Emotion Recognizer\',64.1,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',54.84,Macro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2018-10,ICON model in \'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection\',56.52,Macro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',60.66,Macro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',63.43,Macro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2021-06,DialogueCRN model in \'DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations\',66.38,Macro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2017-07,bc-LSTM+Att model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',56.19,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2018-10,ICON model in \'ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection\',58.54,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2018-11,DialogueRNN model in \'DialogueRNN: An Attentive RNN for Emotion Detection in Conversations\',62.75,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2019-08,DialogueGCN model in \'DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation\',64.18,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2020-05,BiERU-lc model in \'BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis\',64.65,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2020-07,CESTa model in \'Contextualized Emotion Recognition in Conversation as Sequence Tagging\',67.1,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2021-05,DAG-ERC model in \'Directed Acyclic Graph Network for Conversational Emotion Recognition\',68.03,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2021-08,EmoBERTa model in \'EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa\',68.57,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2021-12,DAG-ERC+HCL model in \'Hybrid Curriculum Learning for Emotion Recognition in Conversation\',68.73,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,IEMOCAP,2022-03,EmoCaps model in \'EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition\',71.77,Weighted-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EC,2019-03,HRLCE + BERT model in \'ANA at SemEval-2019 Task 3: Contextual Emotion detection in Conversations through hierarchical LSTMs and BERT\',0.7709,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EC,2019-04,NELEC model in \'NELEC at SemEval-2019 Task 3: Think Twice Before Going Deep\',0.7765,Micro-F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmotionPush,2020-02,HiTransformer-s model in \'Hierarchical Transformer Network for Utterance-level Emotion Recognition\',86.92,Weighted Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion recognition in conversation,EmotionPush,2020-02,HiTransformer-s model in \'Hierarchical Transformer Network for Utterance-level Emotion Recognition\',63.03,Unweighted Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion-cause pair extraction,ECPE-FanSplit,2020-07,RANKCP model in \'Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction\',69.15,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion-cause pair extraction,ECPE,2019-06,Inter-EC model in \'Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts\',61.28,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion-cause pair extraction,ECPE,2020-02,E2EECPE model in \'End-to-end Emotion-Cause Pair Extraction via Learning to Link\',62.8,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Emotion-cause pair extraction,ECPE,2020-07,RANKCP model in \'Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction\',73.6,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Facial Emotion Recognition,RAVDESS,2021-11,Guided-ST and bi-LSTM with attention model in \'Multimodal Emotion Recognition on RAVDESS Dataset Using Transfer Learning\',57.08,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Facial Emotion Recognition,RAVDESS,2021-12,bi-LSTM+Attention model in \'A proposal for Multimodal Emotion Recognition using aural transformers and Action Units on RAVDESS dataset\',62.13,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Facial Emotion Recognition,RAVDESS,2022-01,"Intermediate-Transformer-Fusion, visual branch only model in \'Self-attention fusion for audiovisual emotion recognition with incomplete data\'",74.92,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Multimodal emotion recognition,Expressive hands and faces dataset (EHF).,2018-04,SMPLify-X model in \'Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning\',52.9,v2v error,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Multimodal emotion recognition,IEMOCAP,2018-06,CHFusion (A+T+V) model in \'Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling\',0.768,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Multimodal emotion recognition,IEMOCAP,2021-01,PATHOSnet v2 (English) model in \'Combining deep and unsupervised features for multilingual speech emotion recognition\',0.78,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Multimodal emotion recognition,IEMOCAP,2022-05,COGMEN model in \'COGMEN: COntextualized GNN based Multimodal Emotion recognitioN\',0.845,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Multimodal emotion recognition,IEMOCAP,2017-07,bc-LSTM  model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',0.741,UA,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Multimodal emotion recognition,IEMOCAP,2018-06,CHFusion (A+T+V) model in \'Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling\',0.765,UA,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Multimodal emotion recognition,IEMOCAP,2021-01,PATHOSnet v2 (English) model in \'Combining deep and unsupervised features for multilingual speech emotion recognition\',0.804,Weighted Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Recognizing emotion cause in conversations,EmoCause,2021-09,Human model in \'Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes\',41.3,Top-1 Recall,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Recognizing emotion cause in conversations,EmoCause,2021-09,Human model in \'Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes\',81.1,Top-3 Recall,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Recognizing emotion cause in conversations,EmoCause,2021-09,Human model in \'Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes\',95.0,Top-5 Recall,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Recognizing emotion cause in conversations,RECCON,2020-12,SpanBERT model in \'Recognizing Emotion Cause in Conversations\',75.71,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Recognizing emotion cause in conversations,RECCON,2020-12,SpanBERT model in \'Recognizing Emotion Cause in Conversations\',34.64,Exact Span F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Recognizing emotion cause in conversations,RECCON,2020-12,SpanBERT model in \'Recognizing Emotion Cause in Conversations\',60.0,F1(Pos),pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Recognizing emotion cause in conversations,RECCON,2020-12,SpanBERT model in \'Recognizing Emotion Cause in Conversations\',86.02,F1(Neg),pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,ShEMO,2022-04,CNN model in \'Emotion Recognition In Persian Speech Using Deep Neural Networks\',65.2,Unweighted Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,MSP-Podcast (Valence),2021-02,preCPC model in \'Contrastive Unsupervised Learning for Speech Emotion Recognition\',0.377,CCC,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',82.99,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',0.82,Precision,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',0.82,Recall,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',0.82,F1 Score,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,MSP-Podcast (Dominance),2021-02,preCPC model in \'Contrastive Unsupervised Learning for Speech Emotion Recognition\',0.639,CCC,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,MSP-Podcast (Activation),2021-02,preCPC model in \'Contrastive Unsupervised Learning for Speech Emotion Recognition\',0.706,CCC,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,IEMOCAP,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression) model in \'Multimodal Speech Emotion Recognition and Ambiguity Resolution\',0.718,F1,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,IEMOCAP,2018-02,CNN+LSTM model in \'CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation\',0.65,UA,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,IEMOCAP,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression) model in \'Multimodal Speech Emotion Recognition and Ambiguity Resolution\',0.701,UA,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,IEMOCAP,2020-10,SYSCOMB: BLSTMATT with CSA model in \'Empirical Interpretation of Speech Emotion Perception with Attention Based Model for Speech Emotion Recognition\',0.74,UA,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,IEMOCAP,2021-09,SER with MTL model in \'Speech Emotion Recognition with Multi-Task Learning\',0.78,UA,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,IEMOCAP,2019-11,LSTM+FC model in \'Speech Emotion Recognition Using Speech Feature and Word Embedding\',0.755,WAP,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,IEMOCAP,2020-10,DANN model in \'Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition\',0.827,WAP,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,CREMA-D,2020-01,GRU model in \'Visually Guided Self Supervised Learning of Speech Representations\',55.01,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,CREMA-D,2020-02,ResNet-18 + PyNADA model in \'Non-linear Neurons with Human-like Apical Dendrite Activations\',65.15,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,CREMA-D,2021-03,ResNet-18 + SPEL model in \'Self-paced ensemble learning for speech and audio classification\',68.12,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,CREMA-D,2022-03,SepTr model in \'SepTr: Separable Transformer for Audio Spectrogram Processing\',70.47,Accuracy,pos
Natural language analysis,Pragmatics analysis,Emotion recognition,Speech emotion recognition,CREMA-D,2022-05,SepTr + LeRaC model in \'LeRaC: Learning Rate Curriculum\',70.95,Accuracy,pos
Natural language analysis,Pragmatics analysis,Event detection,Twitter event detection,Events2012 - Oct 11 to Oct 17,2019-06,SEDTWik model in \'SEDTWik: Segmentation-based Event Detection from Tweets Using Wikipedia\',88.12,Precision,pos
Natural language analysis,Pragmatics analysis,Event detection,Twitter event detection,Events2012 - Oct 11 to Oct 17,2019-06,SEDTWik model in \'SEDTWik: Segmentation-based Event Detection from Tweets Using Wikipedia\',14.1,Duplicate Event Rate (DERate),pos
Natural language analysis,Pragmatics analysis,Event detection,Twitter event detection,Events2012 - Oct 11 to Oct 17,2019-06,SEDTWik model in \'SEDTWik: Segmentation-based Event Detection from Tweets Using Wikipedia\',79.0,Number of Events,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2020-09,RAG model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',86.31,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",89.55,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2020-09,RAG model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',61.94,R-Prec,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",88.92,R-Prec,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2020-09,RAG model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',75.55,Recall@5,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",92.52,Recall@5,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2020-09,RAG model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',53.45,KILT-AC,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,KILT: FEVER,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",78.53,KILT-AC,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,FEVER,2019-07,GEAR model in \'GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification\',71.6,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,FEVER,2019-10,KGAT model in \'Fine-grained Fact Verification with Kernel Graph Attention Network\',74.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,FEVER,2020-05,RAG model in \'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\',89.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,FEVER,2019-07,GEAR model in \'GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification\',67.1,FEVER,pos
Natural language analysis,Pragmatics analysis,Fact verification,Fact verification,FEVER,2019-10,KGAT model in \'Fine-grained Fact Verification with Kernel Graph Attention Network\',70.4,FEVER,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,Social media,2021-01,TextRNN model in \'Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News Detection in English\',92.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,LIAR,2017-05,"Hybrid CNNs (Text + All) model in \'\""Liar, Liar Pants on Fire\"": A New Benchmark Dataset for Fake News Detection\'",0.274,Test Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,LIAR,2017-05,"Hybrid CNNs (Text + Speaker) model in \'\""Liar, Liar Pants on Fire\"": A New Benchmark Dataset for Fake News Detection\'",0.277,Validation Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,Hostility Detection Dataset in Hindi,2021-01,Auxiliary IndicBert model in \'Hostility Detection in Hindi leveraging Pre-Trained Language Models\',0.7741,F1 score,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,Grover-Mega,2019-05,Grover-Mega model in \'Defending Against Neural Fake News\',92.0,Unpaired Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,Grover-Mega,2021-01,Text-Transformers + Five-fold five model cross-validation +Pseudo Label Algorithm model in \'Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News Detection in English\',98.5,Unpaired Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,COVID-19 Fake News Dataset,2021-01,Ensemble Model + Heuristic Post-Processing model in \'A Heuristic-driven Ensemble Framework for COVID-19 Fake News Detection\',0.9883,F1,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,MediaEval2016,2022-05,SEMI-FND model in \'SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection\',85.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,Weibo NER,2022-05,SEMI-FND model in \'SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection\',86.83,Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017) model in \'A simple but tough-to-beat baseline for the Fake News Challenge stance detection task\'",81.72,Weighted Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-12,"Bhatt et al. model in \'On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification\'",83.08,Weighted Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2021-06,"Sepúlveda-Torres R., Vicente M., Saquete E., Lloret E., Palomar M. (2021) model in \'Exploring Summarization to Enhance Headline Stance Detection\'",90.73,Weighted Accuracy,pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017) model in \'A simple but tough-to-beat baseline for the Fake News Challenge stance detection task\'",97.9,Per-class Accuracy (Unrelated),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-12,"Bhatt et al. model in \'On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification\'",98.04,Per-class Accuracy (Unrelated),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2021-06,"Sepúlveda-Torres R., Vicente M., Saquete E., Lloret E., Palomar M. (2021) model in \'Exploring Summarization to Enhance Headline Stance Detection\'",99.36,Per-class Accuracy (Unrelated),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017) model in \'A simple but tough-to-beat baseline for the Fake News Challenge stance detection task\'",44.04,Per-class Accuracy (Agree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-12,"Baseline based on word2vec + hand-crafted features (Bhatt et al., 2017) model in \'On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification\'",50.7,Per-class Accuracy (Agree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2018-11,"Bi-LSTM (max-pooling, attention) model in \'Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News\'",51.34,Per-class Accuracy (Agree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2021-06,"Sepúlveda-Torres R., Vicente M., Saquete E., Lloret E., Palomar M. (2021) model in \'Exploring Summarization to Enhance Headline Stance Detection\'",75.03,Per-class Accuracy (Agree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017) model in \'A simple but tough-to-beat baseline for the Fake News Challenge stance detection task\'",6.6,Per-class Accuracy (Disagree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-12,"Baseline based on word2vec + hand-crafted features (Bhatt et al., 2017) model in \'On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification\'",9.61,Per-class Accuracy (Disagree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2018-11,"Bi-LSTM (max-pooling, attention) model in \'Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News\'",10.33,Per-class Accuracy (Disagree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2021-06,"Sepúlveda-Torres R., Vicente M., Saquete E., Lloret E., Palomar M. (2021) model in \'Exploring Summarization to Enhance Headline Stance Detection\'",63.41,Per-class Accuracy (Disagree),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017) model in \'A simple but tough-to-beat baseline for the Fake News Challenge stance detection task\'",81.38,Per-class Accuracy (Discuss),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2017-12,"Bhatt et al. model in \'On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification\'",85.68,Per-class Accuracy (Discuss),pos
Natural language analysis,Pragmatics analysis,Fake news detection,Fake news detection,FNC-1,2021-06,"Sepúlveda-Torres R., Vicente M., Saquete E., Lloret E., Palomar M. (2021) model in \'Exploring Summarization to Enhance Headline Stance Detection\'",85.97,Per-class Accuracy (Discuss),pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Automatic Misogynistic Identification,2018-12,Logistic Regression model in \'Hateminers : Detecting Hate speech against Women\',0.704,Accuracy,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Automatic Misogynistic Identification,2020-04,mBert model in \'Deep Learning Models for Multilingual Hate Speech Detection\',0.832,Accuracy,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,AbusEval,2020-10,HateBERT model in \'HateBERT: Retraining BERT for Abusive Language Detection in English\',0.742,Macro F1,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,"Waseem et al., 2018",2021-06,"Mozafari et al., 2019 model in \'AAA: Fair Evaluation for Abuse Detection Systems Wanted\'",50.94,AAA,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,"Waseem et al., 2018",2021-06,"Mozafari et al., 2019 model in \'AAA: Fair Evaluation for Abuse Detection Systems Wanted\'",84.42,F1 (micro),pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,ToLD-Br,2020-10,Multilingual BERT model in \'Toxic Language Detection in Social Media for Brazilian Portuguese: New Dataset and Multilingual Analysis\',0.75,F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,HatEval,2020-10,HateBERT model in \'HateBERT: Retraining BERT for Abusive Language Detection in English\',0.494,Macro F1,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Ethos MultiLabel,2020-06,MLARAM model in \'ETHOS: an Online Hate Speech Detection Dataset\',0.2948,Hamming Loss,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,SHAJ,2021-07,Baseline BERT (task A) model in \'Detecting Abusive Albanian\',0.77,F1,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,OffensEval 2019,2020-10,HateBERT model in \'HateBERT: Retraining BERT for Abusive Language Detection in English\',0.805,Macro F1,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,HateXplain,2020-12,BERT-HateXplain [Attn] model in \'HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection\',0.698,Accuracy,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,HateXplain,2020-12,BERT-HateXplain [LIME] model in \'HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection\',0.687,Macro F1,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,HateXplain,2020-12,BERT-HateXplain [LIME] model in \'HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection\',0.851,AUROC,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,bajer_danish_misogyny,2021-08,AOM mBERT model in \'Annotating Online Misogyny\',0.8549,F1,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Ethos Binary,2020-06,BERT model in \'ETHOS: an Online Hate Speech Detection Dataset\',0.7883,F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Ethos Binary,2021-06,BiLSTM + static BE model in \'Hate speech detection using static BERT embeddings\',0.7971,F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Ethos Binary,2020-06,BiLSTM+Attention+FT model in \'ETHOS: an Online Hate Speech Detection Dataset\',0.7734,Classification Accuracy,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Ethos Binary,2021-06,BiLSTM + static BE model in \'Hate speech detection using static BERT embeddings\',0.8015,Classification Accuracy,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Ethos Binary,2020-06,BERT model in \'ETHOS: an Online Hate Speech Detection Dataset\',79.17,Precision,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,Hostility Detection Dataset in Hindi,2021-01,Auxiliary IndicBert model in \'Hostility Detection in Hindi leveraging Pre-Trained Language Models\',0.5725,F1 score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hate speech detection,DKhate,2019-08,Baseline model in \'Offensive Language and Hate Speech Detection for Danish\',0.7,F1,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hope speech detection,KanHope,2021-08,Dual-Channel mBERT model in \'Hope Speech detection in under-resourced Kannada language\',0.65,F1-score (Weighted),pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hope speech detection,HopeEDI,2020-12,"Decision Tree Classifier model in \'HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion\'",0.9,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hope speech detection,HopeEDI,2021-04,RoBERTa model in \'TeamUNCC@LT-EDI-EACL2021: Hope Speech Detection using Transfer Learning with Transformers\',0.93,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hope speech detection for Malayalam,HopeEDI,2020-12,"Decision Tree Classifier model in \'HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion\'",0.73,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hope speech detection for Malayalam,HopeEDI,2021-04,XLM-RoBERTa model in \'TeamUNCC@LT-EDI-EACL2021: Hope Speech Detection using Transfer Learning with Transformers\',0.87,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hope speech detection for Tamil,HopeEDI,2020-12,"Logistic Regression model in \'HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion\'",0.56,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Hate speech detection,Hope speech detection for Tamil,HopeEDI,2021-04,XLM-RoBERTa model in \'TeamUNCC@LT-EDI-EACL2021: Hope Speech Detection using Transfer Learning with Transformers\',0.6,Weighted Average F1-score,pos
Natural language analysis,Pragmatics analysis,Humor detection,Humor detection,200k Short Texts for Humor Detection,2016-03,XGBoost model in \'XGBoost: A Scalable Tree Boosting System\',0.813,F1-score,pos
Natural language analysis,Pragmatics analysis,Humor detection,Humor detection,200k Short Texts for Humor Detection,2019-06,XLNet Large Cased model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',0.92,F1-score,pos
Natural language analysis,Pragmatics analysis,Humor detection,Humor detection,200k Short Texts for Humor Detection,2020-04,ColBERT model model in \'ColBERT: Using BERT Sentence Embedding for Humor Detection\',0.982,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent classification,Intent classification,SLURP,2020-11,Multi-SLURP model in \'SLURP: A Spoken Language Understanding Resource Package\',78.33,Accuracy (%),pos
Natural language analysis,Pragmatics analysis,Intent classification,Intent classification,SLURP,2021-11,"Partially Fine-tuned HuBERT model in \'A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding\'",87.51,Accuracy (%),pos
Natural language analysis,Pragmatics analysis,Intent classification,Intent classification,KUAKE-QIC,2021-06,RoBERTa-wwm-ext-base model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',85.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent classification,Intent classification,ORCAS-I,2022-05,BERT (query + URL) model in \'ORCAS-I: Queries Annotated with Intent using Weak Supervision\',0.774,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent classification,Intent classification,ORCAS-I,2022-05,BERT (query + URL) model in \'ORCAS-I: Queries Annotated with Intent using Weak Supervision\',0.789,Precision,pos
Natural language analysis,Pragmatics analysis,Intent classification,Intent classification,ORCAS-I,2022-05,BERT (query + URL) model in \'ORCAS-I: Queries Annotated with Intent using Weak Supervision\',0.764,Recall,pos
Natural language analysis,Pragmatics analysis,Intent classification,Intent classification,MASSIVE,2022-04,mT5 Base (encoder-only) model in \'MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\',86.1,Intent Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,ASOS.com user intent,2019-12,plain-LSTM model in \'“Where is My Parcel?” Fast and Efficient Classifiers to Detect User Intent in Natural Language\',0.887,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,MixATIS,2021-06,GL-GIN model in \'GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling\',76.3,Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,MixSNIPS,2021-06,GL-GIN model in \'GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling\',95.6,Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,ATIS,2016-09,Attention Encoder-Decoder NN model in \'Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling\',98.43,Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,ATIS,2018-12,Bi-model with decoder model in \'A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling\',98.99,Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,ATIS,2016-09,Attention Encoder-Decoder NN model in \'Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling\',95.87,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,ATIS,2018-12,Bi-model with decoder model in \'A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling\',96.89,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,SNIPS,2018-06,Slot-Gated BLSTM with Attension model in \'Slot-Gated Modeling for Joint Slot Filling and Intent Prediction\',88.8,Slot F1 Score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,SNIPS,2018-12,Capsule-NLU model in \'Joint Slot Filling and Intent Detection via Capsule Neural Networks\',91.8,Slot F1 Score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,SNIPS,2019-06,SF-ID model in \'A Novel Bi-directional Interrelated Model for Joint Intent Detection and Slot Filling\',92.23,Slot F1 Score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,SNIPS,2019-09,Stack-Propagation (+BERT) model in \'A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding\',97.0,Slot F1 Score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,SNIPS,2018-06,Slot-Gated BLSTM with Attension model in \'Slot-Gated Modeling for Joint Slot Filling and Intent Prediction\',97.0,Intent Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,SNIPS,2018-12,Capsule-NLU model in \'Joint Slot Filling and Intent Detection via Capsule Neural Networks\',97.7,Intent Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Intent detection,SNIPS,2019-09,Stack-Propagation (+BERT) model in \'A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding\',99.0,Intent Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,StackOverFlow(75%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',85.99,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,StackOverFlow(75%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',82.78,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,SNIPS (50% known),2019-06,LMCL model in \'Deep Unknown Intent Detection with Margin Loss\',0.841,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,SNIPS (25% known),2019-06,LMCL model in \'Deep Unknown Intent Detection with Margin Loss\',0.792,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,StackOverFlow(25%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',80.83,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,StackOverFlow(25%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',86.72,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,BANKING77 (25%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',71.62,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,BANKING77 (25%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',78.85,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,BANKING-77 (50% known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',80.9,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,BANKING-77 (50% known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',78.86,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,OOS(75%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',88.53,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,OOS(75%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',86.32,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,BANKING-77 (75% known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',85.96,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,BANKING-77 (75% known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',81.08,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,SNIPS (75% known),2019-06,LMCL model in \'Deep Unknown Intent Detection with Margin Loss\',0.788,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,ATIS (50% known),2019-06,LMCL model in \'Deep Unknown Intent Detection with Margin Loss\',0.396,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,ATIS (25% known),2019-06,LMCL model in \'Deep Unknown Intent Detection with Margin Loss\',0.696,F1,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,StackOverFlow(50%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',85.83,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,StackOverFlow(50%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',86.4,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,OOS(25%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',77.19,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,OOS(25%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',87.59,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,OOS(50%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',85.05,F1-score,pos
Natural language analysis,Pragmatics analysis,Intent detection,Open Intent Detection,OOS(50%known),2020-12,ADB model in \'Deep Open Intent Classification with Adaptive Decision Boundary\',86.54,1:1 Accuracy,pos
Natural language analysis,Pragmatics analysis,Language acquisition,Language acquisition,SLAM 2018,2018-06,Context Based Model model in \'Context Based Approach for Second Language Acquisition\',0.821,AUC,pos
Natural language analysis,Pragmatics analysis,Negation detection,Negation scope resolution,_sem 2012 Shared Task: Sherlock Dataset,2019-11,NegBERT model in \'NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution\',92.36,F1,pos
Natural language analysis,Pragmatics analysis,Negation detection,Negation scope resolution,BioScope : Full Papers,2019-11,NegBERT model in \'NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution\',91.24,F1,pos
Natural language analysis,Pragmatics analysis,Negation detection,Negation scope resolution,BioScope : Full Papers,2020-01,XLNet model in \'Resolving the Scope of Speculation and Negation using Transformer-Based Architectures\',94.4,F1,pos
Natural language analysis,Pragmatics analysis,Negation detection,Negation scope resolution,BioScope : Abstracts,2019-11,NegBERT model in \'NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution\',95.68,F1,pos
Natural language analysis,Pragmatics analysis,Negation detection,Negation scope resolution,BioScope : Abstracts,2020-01,XLNet model in \'Resolving the Scope of Speculation and Negation using Transformer-Based Architectures\',95.74,F1,pos
Natural language analysis,Pragmatics analysis,Negation detection,Negation scope resolution,SFU Review Corpus,2019-11,NegBERT model in \'NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution\',90.95,F1,pos
Natural language analysis,Pragmatics analysis,Negation detection,Negation scope resolution,SFU Review Corpus,2020-01,XLNet model in \'Resolving the Scope of Speculation and Negation using Transformer-Based Architectures\',91.25,F1,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,MRPC,2020-09,PAR BERT Base model in \'Pay Attention when Required\',89.2,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,MSRP,2013-10,"FEAT2, TFKLD, SVM, Fine-grained features model in \'Discriminative Improvements to Distributional Sentence Similarity\'",80.41,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,MSRP,2013-10,"FEAT2, TFKLD, SVM, Fine-grained features model in \'Discriminative Improvements to Distributional Sentence Similarity\'",85.96,F1,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2017-02,BiMPM model in \'Bilateral Multi-Perspective Matching for Natural Language Sentences\',88.17,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2017-04,pt-DecAtt model in \'Neural Paraphrase Identification of Questions with Noisy Pretraining\',88.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2017-09,DIIN model in \'Natural Language Inference over Interaction Space\',89.06,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2018-07,MwAN  model in \'Multiway Attention Networks for Modeling Sentence Pairs\',89.12,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2018-10,Snorkel MeTaL(ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',89.9,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2019-06,XLNet-Large (ensemble) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',90.3,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',90.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2020-12,RealFormer model in \'RealFormer: Transformer Likes Residual Attention\',91.34,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2021-06,Charformer-Tall model in \'Charformer: Fast Character Transformers via Gradient-based Subword Tokenization\',91.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2022-02,"data2vec model in \'data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language\'",92.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2018-10,Snorkel MeTaL(ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',73.1,F1,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2019-06,XLNet-Large (ensemble) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',74.2,F1,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',74.4,F1,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2020-12,RealFormer model in \'RealFormer: Transformer Likes Residual Attention\',88.28,F1,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2021-04,EFL model in \'Entailment as Few-Shot Learner\',89.2,F1,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2020-12,BERT-Base model in \'Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\',8030.0,Structure Aware Intrinsic Dimension,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,Quora Question Pairs,2020-12,BERT-Base model in \'Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\',9295.0,Direct Intrinsic Dimension,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,2017_test set,2018-06,"CNN model in \'Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering\'",50.0,10 fold Cross validation,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,WikiHop,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',90.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,PIT,2021-04,TSDAE model in \'TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning\',69.2,AP,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,TURL,2021-04,TSDAE model in \'TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning\',76.8,AP,pos
Natural language analysis,Pragmatics analysis,Paraphrase identification,Paraphrase identification,AP,2021-06,RoBETRa base model in \'Improving Paraphrase Detection with the Adversarial Paraphrasing Task\',0.525,MCC,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,WITS,2022-03,"BART model in \'When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues\'",36.88,R1,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,SARC (all-bal),2017-04,Bag-of-Bigrams model in \'A Large Self-Annotated Corpus for Sarcasm\',75.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,SARC (all-bal),2018-05,CASCADE model in \'CASCADE: Contextual Sarcasm Detection in Online Discussion Forums\',77.0,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,SARC (pol-bal),2017-04,Bag-of-Bigrams model in \'A Large Self-Annotated Corpus for Sarcasm\',76.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,MUStARD++,2022-06,MUStARD++ model in \'A Multimodal Corpus for Emotion Recognition in Sarcasm\',70.2,F1,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,MUStARD++,2022-06,MUStARD++ model in \'A Multimodal Corpus for Emotion Recognition in Sarcasm\',70.2,Precision,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,MUStARD++,2022-06,MUStARD++ model in \'A Multimodal Corpus for Emotion Recognition in Sarcasm\',70.2,Recall,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,SARC (pol-unbal),2017-04,Bag-of-Words model in \'A Large Self-Annotated Corpus for Sarcasm\',27.0,Avg F1,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,FigLang 2020 Twitter Dataset,2020-06,RoBERTa_large (Context-Response) model in \'Sarcasm Detection using Context Separators in Online Discourse\',0.772,F1,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,iSarcasm,2022-04,RoBERTa + Mutation Data Augmentation model in \'UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection Using Generative-based and Mutation-based Data Augmentation\',0.414,F1-Score,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,SCv1,2017-08,"DeepMoji model in \'Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\'",0.69,F1,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,FigLang 2020 Reddit Dataset,2020-06,RoBERTa_large - (Separated Context-Response) model in \'Sarcasm Detection using Context Separators in Online Discourse\',0.716,F1,pos
Natural language analysis,Pragmatics analysis,Sarcasm detection,Sarcasm detection,FigLang 2020 Reddit Dataset,2020-07,BERT+Aspect-based approaches model in \'Applying Transformers and Aspect-based Sentiment Analysis approaches on Sarcasm Detection\',0.737,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,SemEval 2014 Task 4 Sub Task 2,2022-04,pxp model in \'Latent Aspect Detection from Online Unsolicited Customer Reviews\',0.66,MRR,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,SemEval 2014 Task 4 Sub Task 2,2022-04,pxp model in \'Latent Aspect Detection from Online Unsolicited Customer Reviews\',0.66,NDCG,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,SemEval 2014 Task 4 Sub Task 2,2022-04,pxp model in \'Latent Aspect Detection from Online Unsolicited Customer Reviews\',0.82,Hit-at-5,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,SemEval 2014 Task 4 Sub Task 2,2022-04,pxp model in \'Latent Aspect Detection from Online Unsolicited Customer Reviews\',0.72,Average Recall,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,Citysearch,2020-04,CAt model in \'Embarrassingly Simple Unsupervised Aspect Extraction\',86.4,F-measure (%),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,AWARE,2021-11,Baseline model in \'AWARE: Aspect-Based Sentiment Analysis Dataset of Apps Reviews for Requirements Elicitation\',0.32,F1-score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,SemEval 2014 Task 4 Subtask 3,2019-03,BERT-pair-NLI-B model in \'Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\',92.18,F1 score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,SemEval 2014 Task 4 Subtask 3,2019-03,BERT-pair-NLI-B model in \'Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\',93.57,Precision,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Detection,SemEval 2014 Task 4 Subtask 3,2019-03,BERT-pair-NLI-B model in \'Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\',90.83,Recall,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect Category Polarity,AWARE,2021-11,Baseline model in \'AWARE: Aspect-Based Sentiment Analysis Dataset of Apps Reviews for Requirements Elicitation\',67.0,Accuracy (%),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Russian),2020-04,M-BERT+Word+Char model in \'Structure-Level Knowledge Distillation For Multilingual Sequence Labeling\',71.8,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Russian),2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',79.4,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2016 Task 5 Sub Task 1 Slot 2,2018-05,DE-CNN model in \'Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\',74.37,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Turkish),2020-04,M-BERT+Word+Char model in \'Structure-Level Knowledge Distillation For Multilingual Sequence Labeling\',59.3,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Turkish),2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',81.9,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1,2020-04,M-BERT+Flair+Word+Char model in \'Structure-Level Knowledge Distillation For Multilingual Sequence Labeling\',72.8,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1,2020-07,Wei et al. (2020) model in \'Don\'t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction\',77.7,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',81.3,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2015 Task 12,2018-05,DE-CNN model in \'Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\',68.28,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2015 Task 12,2020-07,Wei et al. (2020) model in \'Don\'t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction\',72.7,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2015 Task 12,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',80.3,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Spanish),2020-04,M-BERT+Flair+Word+Char model in \'Structure-Level Knowledge Distillation For Multilingual Sequence Labeling\',74.3,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Spanish),2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',79.9,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 1,2018-05,DE-CNN model in \'Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\',81.59,Laptop (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Dutch),2020-04,M-BERT+Flair+Word+Char model in \'Structure-Level Knowledge Distillation For Multilingual Sequence Labeling\',72.9,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval-2016 Task 5 Subtask 1 (Dutch),2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',80.5,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,BAT model in \'Adversarial Training for Aspect-Based Sentiment Analysis with BERT\',85.57,Laptop (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 2,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',87.4,Laptop (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,BAT model in \'Adversarial Training for Aspect-Based Sentiment Analysis with BERT\',83.54,Mean F1 (Laptop + Restaurant),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 2,2020-10,PH-SUM model in \'Improving BERT Performance for Aspect-Based Sentiment Analysis\',84.215,Mean F1 (Laptop + Restaurant),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 2,2018-05,DE-CNN model in \'Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction\',85.2,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 2,2020-07,Wei et al. (2020) model in \'Don\'t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction\',87.1,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect extraction,SemEval 2014 Task 4 Sub Task 2,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',92.0,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,Res14,2020-10,GTS-BERT model in \'Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction\',70.2,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,ASTE-Data-V2,2020-10,GTS model in \'Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction\',68.17,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,ASTE-Data-V2,2021-07,SPAN-ASTE model in \'Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction\',71.85,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,ASTE-Data-V2,2021-08,GAS model in \'Towards Generative Aspect-Based Sentiment Analysis\',72.16,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,SemEval,2019-11,"KHW model in \'Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis\'",50.9,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,SemEval,2020-10,JET model in \'Position-Aware Tagging for Aspect Sentiment Triplet Extraction\',62.4,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,SemEval,2021-01,Dual-MRC model in \'A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis\',70.32,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect sentiment triplet extraction,SemEval,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',72.46,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2019-06,SPAN-BERT model in \'Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification\',65.74,Avg F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2020-07,RACL-BERT model in \'Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis\',68.29,Avg F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2021-01,Dual-MRC model in \'A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis\',68.99,Avg F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',69.18,Avg F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2019-06,SPAN-BERT model in \'Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification\',73.68,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2020-07,RACL-BERT model in \'Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis\',75.42,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2021-01,Dual-MRC model in \'A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis\',75.95,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2019-06,IMN-BERT model in \'An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis\',61.73,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2020-07,RACL-BERT model in \'Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis\',63.4,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2021-01,Dual-MRC model in \'A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis\',65.94,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',67.37,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2019-06,SPAN-BERT model in \'Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification\',62.29,Restaurant 2015 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2020-07,RACL-BERT model in \'Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis\',66.05,Restaurant 2015 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect term extraction and sentiment classification,SemEval,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',66.61,Restaurant 2015 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-Category-Opinion-Sentiment Quadruple Extraction,Restaurant-ACOS,2021-08,Extract-Classify-ACOS model in \'Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions\',44.61,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-Category-Opinion-Sentiment Quadruple Extraction,Laptop-ACOS,2021-08,Extract-Classify-ACOS model in \'Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions\',35.8,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-Sentiment-Opinion Triplet Extraction,Res14,2021-03,PBF model in \'A More Fine-Grained Aspect-Sentiment-Opinion Triplet Extraction Task\',69.2,F1 score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-Sentiment-Opinion Triplet Extraction,Res14,2020-10,GTS model in \'Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction\',67.3,Precision,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 1,2016-03,RNCRF model in \'Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis\',78.42,Laptop (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 1,2019-04,BERT-PT model in \'BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis\',84.26,Laptop (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 1,2016-03,RNCRF model in \'Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis\',69.74,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 1,2019-04,BERT-PT model in \'BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis\',77.97,Restaurant (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2015 Task 12,2019-03,HAABSA model in \'A Hybrid Approach for Aspect-Based Sentiment Analysis Using a Lexicalized Domain Ontology and Attentional Neural Models\',80.6,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2015 Task 12,2020-04,HAABSA++ model in \'A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention\',81.7,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Subtask 4,2014-08,ATLX model in \'SemEval-2014 Task 4: Aspect Based Sentiment Analysis\',82.62,Accuracy (3-way),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Subtask 4,2019-03,BERT-pair-QA-B model in \'Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\',89.9,Accuracy (3-way),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Subtask 4,2019-03,BERT-pair-QA-B model in \'Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\',85.9,Accuracy (4-way),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Subtask 4,2019-03,BERT-pair-QA-B model in \'Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\',95.6,Binary Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Lap14,2022-04,HGCN+BERT model in \'Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks\',81.49,Acc,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2015-12,TD-LSTM model in \'Effective LSTMs for Target-Dependent Sentiment Classification\',71.88,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2016-05,MemNet model in \'Aspect Level Sentiment Classification with Deep Memory Network\',76.58,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2017-09,RAM model in \'Recurrent Attention Network on Memory for Aspect Sentiment Analysis\',77.36,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-02,LCR-Rot model in \'Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention\',78.29,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-04,SA-LSTM-P model in \'Learning Latent Opinions for Aspect-Level Sentiment Classification\',78.35,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-05,TNet-LF model in \'Transformation Networks for Target-Oriented Sentiment Classification\',78.4,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-10,HAPN model in \'Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment Analysis\',79.75,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-02,BERT-SPC model in \'Attentional Encoder Network for Targeted Sentiment Classification\',81.73,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-06,SDGCN-BERT model in \'Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect-level Sentiment Classification\',82.46,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-08,BERT-ADA model in \'Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification\',84.06,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-12,LCF-ATEPC model in \'A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction\',86.24,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2021-10,LSA+DeBERTa-V3-Large model in \'Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable Sentiment Dependency Learning\',88.64,Mean Acc (Restaurant + Laptop),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2015-12,TD-LSTM model in \'Effective LSTMs for Target-Dependent Sentiment Classification\',75.63,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2016-05,MemNet model in \'Aspect Level Sentiment Classification with Deep Memory Network\',80.95,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-02,LCR-Rot model in \'Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention\',81.34,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-04,SA-LSTM-P model in \'Learning Latent Opinions for Aspect-Level Sentiment Classification\',81.6,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-10,HAPN model in \'Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment Analysis\',82.23,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-02,BERT-SPC model in \'Attentional Encoder Network for Targeted Sentiment Classification\',84.46,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-04,BERT-PT model in \'BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis\',84.95,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-08,BERT-ADA model in \'Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification\',87.89,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-12,LCF-ATEPC model in \'A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction\',90.18,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2021-10,LSA+DeBERTa-V3-Large model in \'Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable Sentiment Dependency Learning\',91.07,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2015-12,TD-LSTM model in \'Effective LSTMs for Target-Dependent Sentiment Classification\',68.13,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2016-05,MemNet model in \'Aspect Level Sentiment Classification with Deep Memory Network\',72.21,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2017-09,RAM model in \'Recurrent Attention Network on Memory for Aspect Sentiment Analysis\',74.49,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-02,LCR-Rot model in \'Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention\',75.24,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-05,TNet model in \'Transformation Networks for Target-Oriented Sentiment Classification\',76.01,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2018-10,HAPN model in \'Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment Analysis\',77.27,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-02,AEN-BERT model in \'Attentional Encoder Network for Targeted Sentiment Classification\',79.93,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-06,SDGCN-BERT model in \'Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect-level Sentiment Classification\',81.35,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2019-12,LCF-ATEPC model in \'A Multi-task Learning Model for Chinese-oriented Aspect Polarity Classification and Aspect Term Extraction\',82.29,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2021-04,RoBERTa+MLP model in \'Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa\',83.78,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval 2014 Task 4 Sub Task 2,2021-10,LSA+DeBERTa-V3-Large model in \'Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable Sentiment Dependency Learning\',86.21,Laptop (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,MAMS,2019-11,CapsNet-BERT model in \'A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis\',83.391,Acc,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,MAMS,2020-02,RGAT+ model in \'Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network\',84.52,Acc,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,MAMS,2020-02,RGAT+ model in \'Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network\',83.74,Macro-F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Rest15,2022-04,HGCN+BERT model in \'Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks\',85.61,Acc,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Rest14,2022-04,HGCN+BERT model in \'Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks\',87.41,Acc,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Sentihood,2016-10,LSTM-LOC model in \'SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods\',69.3,Aspect,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Sentihood,2018-04,Liu et al. model in \'Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-based Sentiment Analysis\',78.5,Aspect,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Sentihood,2019-03,BERT-pair-QA-B model in \'Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence\',87.9,Aspect,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Sentihood,2016-10,LSTM-LOC model in \'SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods\',-81.9,Sentiment,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,Rest16,2022-04,HGCN+BERT model in \'Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks\',93.02,Acc,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval-2016 Task 5 Subtask 1,2019-03,HAABSA model in \'A Hybrid Approach for Aspect-Based Sentiment Analysis Using a Lexicalized Domain Ontology and Attentional Neural Models\',88.0,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-based sentiment analysis,SemEval-2016 Task 5 Subtask 1,2020-11,BERT-IL Finetuned model in \'Does BERT Understand Sentiment? Leveraging Comparisons Between Contextual and Non-Contextual Embeddings to Improve Aspect-Based Sentiment Models\',88.7,Restaurant (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2019-06,IOG model in \'Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling\',80.02,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,LOTN model in \'Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction\',82.21,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-10,ONG model in \'Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning\',82.33,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2021-01,Dual-MRC model in \'A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis\',83.73,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',85.38,Restaurant 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2019-06,IOG model in \'Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling\',71.35,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,LOTN model in \'Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction\',72.02,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-10,ONG model in \'Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning\',75.77,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2021-01,Dual-MRC model in \'A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis\',79.9,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',80.55,Laptop 2014 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2019-06,IOG model in \'Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling\',73.25,Restaurant 2015 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,LOTN model in \'Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction\',73.29,Restaurant 2015 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-10,ONG model in \'Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning\',78.81,Restaurant 2015 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',80.52,Restaurant 2015 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2019-06,IOG model in \'Target-oriented Opinion Words Extraction with Target-fused Neural Sequence Labeling\',81.69,Restaurant 2016 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,LOTN model in \'Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction\',83.62,Restaurant 2016 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2020-10,ONG model in \'Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning\',86.01,Restaurant 2016 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Aspect-oriented  opinion extraction,SemEval 2014 Task 4 Sub Task 2,2021-06,BARTABSA model in \'A Unified Generative Framework for Aspect-Based Sentiment Analysis\',87.92,Restaurant 2016 (F1),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Extract aspect,SemEval 2015 Task 12,2017-10,EliXa model in \'Review highlights: opinion mining on reviews: a hybrid model for rule selection in aspect extraction\',0.7,F1 score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Extract aspect-polarity tuple,SemEval 2015 Task 12,2017-10,Syntactic Grammer Model model in \'Review highlights: opinion mining on reviews: a hybrid model for rule selection in aspect extraction\',0.51,F1 score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Fine-grained opinion analysis,MPQA,2017-11,FS-MTL model in \'SRL4ORL: Improving Opinion Role Labeling using Multi-task Learning with Semantic Role Labeling\',83.8,Holder Binary F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Fine-grained opinion analysis,MPQA,2019-06,SRL-SAWR model in \'Enhancing Opinion Role Labeling with Semantic-Aware Word Representations from Semantic Role Labeling\',84.91,Holder Binary F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Fine-grained opinion analysis,MPQA,2017-11,FS-MTL model in \'SRL4ORL: Improving Opinion Role Labeling using Multi-task Learning with Semantic Role Labeling\',72.06,Target Binary F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Fine-grained opinion analysis,MPQA,2019-06,SRL-SAWR model in \'Enhancing Opinion Role Labeling with Semantic-Aware Word Representations from Semantic Role Labeling\',73.29,Target Binary F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSI,2021-09,TEASEL model in \'TEASEL: A Transformer-Based Speech-Prefixed Language Model\',85.0,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSI,2021-09,TEASEL model in \'TEASEL: A Transformer-Based Speech-Prefixed Language Model\',-0.64,MAE,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSI,2020-09,Tri-TransModality model in \'TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis\',82.71,F1-score (Weighted),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSI,2021-09,TEASEL model in \'TEASEL: A Transformer-Based Speech-Prefixed Language Model\',87.5,Acc-2,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSI,2021-09,TEASEL model in \'TEASEL: A Transformer-Based Speech-Prefixed Language Model\',47.52,Acc-7,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSI,2021-09,TEASEL model in \'TEASEL: A Transformer-Based Speech-Prefixed Language Model\',0.836,Corr,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,MOSI,2017-07,bc-LSTM model in \'Context-Dependent Sentiment Analysis in User-Generated Videos\',80.3,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,MOSI,2018-10,MMMU-BA model in \'Contextual Inter-modal Attention for Multi-modal Sentiment Analysis\',82.31,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,MOSI,2019-06,MulT model in \'Multimodal Transformer for Unaligned Multimodal Language Sequences\',83.0,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,MOSI,2020-02,Proposed: B2 + B4 w/ multimodal fusion model in \'Gated Mechanism for Attention Based Multimodal Sentiment Analysis\',83.91,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,MOSI,2020-10,CM-BERT model in \'Cross-Modal BERT for Text-Audio Sentiment Analysis\',84.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,MOSI,2019-06,MulT model in \'Multimodal Transformer for Unaligned Multimodal Language Sequences\',82.8,F1 score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,MOSI,2020-10,CM-BERT model in \'Cross-Modal BERT for Text-Audio Sentiment Analysis\',84.5,F1 score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,B-T4SA,2021-02,AutoML-Based Fusion Approach model in \'An AutoML-based Approach to Multimodal Image Sentiment Analysis\',95.19,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSEI,2018-07,Graph-MFN model in \'Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph\',76.9,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSEI,2020-02,Multilogue-Net model in \'Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation\',82.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSEI,2020-06,Transformer-based joint-encoding model in \'A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis\',82.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSEI,2018-07,Graph-MFN model in \'Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph\',-0.71,MAE,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Multimodal sentiment analysis,CMU-MOSEI,2020-02,Multilogue-Net model in \'Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation\',-0.59,MAE,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,AJGT,2020-02,AraBERTv1 model in \'AraBERT: Transformer-based Model for Arabic Language Understanding\',93.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IITP Product Reviews Sentiment,2020-11,"IndicBERT Base model in \'IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages\'",71.32,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IITP Product Reviews Sentiment,2022-01,xlmindic-base-uniscript model in \'Does Transliteration Help Multilingual Language Modeling?\',77.18,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,DBRD,2019-12,BERTje model in \'BERTje: A Dutch BERT Model\',93.0,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,DBRD,2020-01,RobBERT v2 model in \'RobBERT: a Dutch RoBERTa-based Language Model\',95.144,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,DBRD,2020-01,RobBERT v2 model in \'RobBERT: a Dutch RoBERTa-based Language Model\',95.144,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SLUE,2021-11,"W2V2-L-LL60K (pipeline approach, uses LM) model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\'",60.4,Recall (%),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SLUE,2021-11,"W2V2-L-LL60K (pipeline approach, uses LM) model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\'",63.3,F1 (%),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Financial PhraseBank,2019-08,FinBERT model in \'FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\',86.0,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Financial PhraseBank,2019-08,FinBERT model in \'FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\',84.0,F1 score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,FiQA,2018-04,Deep Neural Networks (DNN) model in \'Financial Aspect and Sentiment Predictions with Deep Neural Networks: An Ensemble Approach\',-0.09,MSE,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,FiQA,2018-08,Deep Representations model in \'Financial Aspect-Based Sentiment Analysis using Deep Representations\',-0.08,MSE,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,FiQA,2019-08,FinBERT model in \'FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\',-0.07,MSE,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,FiQA,2018-04,Deep Neural Networks (DNN) model in \'Financial Aspect and Sentiment Predictions with Deep Neural Networks: An Ensemble Approach\',0.41,R^2,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,FiQA,2019-08,FinBERT model in \'FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\',0.55,R^2,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,ArSAS,2019-08,CNN-LSTM model in \'Mazajak: An Online Arabic Sentiment Analyser\',0.9,Average Recall,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Yelp Fine-grained classification,2015-09,Char-level CNN model in \'Character-level Convolutional Networks for Text Classification\',37.95,Error,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Yelp Fine-grained classification,2019-01,SVDCNN model in \'Squeezed Very Deep Convolutional Neural Networks for Text Classification\',46.8,Error,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SemEval 2017 Task 4-A,2017-04,LSTMs+CNNs ensemble with multiple conv. ops model in \'BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs\',0.685,Average Recall,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,ASTD,2019-08,CNN-LSTM model in \'Mazajak: An Online Arabic Sentiment Analyser\',0.62,Average Recall,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,HARD,2020-02,AraBERTv1 model in \'AraBERT: Transformer-based Model for Arabic Language Understanding\',96.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Latvian Twitter Eater Sentiment Dataset,2020-07,Naive Bayes model in \'What Can We Learn From Almost a Decade of Food Tweets\',61.23,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SemEval 2014 Task 4 Subtask 1+2,2018-11,E2E-TBSA model in \'A Unified Model for Opinion Target Extraction and Target Sentiment Prediction\',57.9,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SemEval 2014 Task 4 Subtask 1+2,2019-06,SPAN model in \'Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification\',68.06,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SemEval 2014 Task 4 Subtask 1+2,2020-09,GRACE model in \'GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Analysis\',70.71,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,1B Words,2017-08,"Random model in \'Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\'",17.0,1 in 10 R-at-1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,CR,2017-12,Block-sparse LSTM model in \'GPU Kernels for Block-Sparse Weights\',92.2,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,CR,2021-04,EFL model in \'Entailment as Few-Shot Learner\',92.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,CR,2022-01,RoBERTa+DualCL model in \'Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation\',94.39,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IITP Movie Reviews Sentiment,2020-11,"IndicBERT Base model in \'IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages\'",59.03,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IITP Movie Reviews Sentiment,2022-01,xlmindic-base-uniscript model in \'Does Transliteration Help Multilingual Language Modeling?\',66.34,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Amazon Review Polarity,2016-07,FastText model in \'Bag of Tricks for Efficient Text Classification\',94.6,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Amazon Review Polarity,2017-07,DPCNN model in \'Deep Pyramid Convolutional Neural Networks for Text Categorization\',96.68,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Amazon Review Polarity,2019-04,BERT large model in \'Unsupervised Data Augmentation for Consistency Training\',97.37,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,ChnSentiCorp Dev,2019-06,RoBERTa-wwm-ext-large model in \'Pre-Training with Whole Word Masking for Chinese BERT\',95.8,F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,RuSentiment,2018-08,NNC+VK model in \'RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian\',72.8,Weighted F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,RuSentiment,2021-07,RuBERT-RuSentiment model in \'Deep Transfer Learning Baselines for Sentiment Analysis in Russian\',75.71,Weighted F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,"LABR (2-class, unbalanced)",2020-02,AraBERTv1 model in \'AraBERT: Transformer-based Model for Arabic Language Understanding\',86.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-05,DANN model in \'Domain-Adversarial Training of Neural Networks\',75.4,DVD,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE model in \'The Variational Fair Autoencoder\',76.57,DVD,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2018-04,Multi-task tri-training model in \'Strong Baselines for Neural Semi-supervised Learning under Domain Shift\',78.14,DVD,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2018-10,Distributional Correspondence Indexing model in \'Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments\',81.0,DVD,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2021-04,UDALM: Unsupervised Domain Adaptation through Language Modeling model in \'UDALM: Unsupervised Domain Adaptation through Language Modeling\',89.78,DVD,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-05,DANN model in \'Domain-Adversarial Training of Neural Networks\',71.43,Books,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE model in \'The Variational Fair Autoencoder\',73.4,Books,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2018-04,Multi-task tri-training model in \'Strong Baselines for Neural Semi-supervised Learning under Domain Shift\',74.86,Books,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2018-10,Distributional Correspondence Indexing model in \'Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments\',81.4,Books,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2021-04,UDALM: Unsupervised Domain Adaptation through Language Modeling model in \'UDALM: Unsupervised Domain Adaptation through Language Modeling\',90.63,Books,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-05,DANN model in \'Domain-Adversarial Training of Neural Networks\',77.67,Electronics,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE model in \'The Variational Fair Autoencoder\',80.53,Electronics,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2018-04,Multi-task tri-training model in \'Strong Baselines for Neural Semi-supervised Learning under Domain Shift\',81.45,Electronics,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2021-04,UDALM: Unsupervised Domain Adaptation through Language Modeling model in \'UDALM: Unsupervised Domain Adaptation through Language Modeling\',92.78,Electronics,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-05,DANN model in \'Domain-Adversarial Training of Neural Networks\',80.53,Kitchen,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE model in \'The Variational Fair Autoencoder\',82.93,Kitchen,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2017-02,Asymmetric tri-training model in \'Asymmetric Tri-training for Unsupervised Domain Adaptation\',83.97,Kitchen,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2018-10,Distributional Correspondence Indexing model in \'Revisiting Distributional Correspondence Indexing: A Python Reimplementation and New Experiments\',85.9,Kitchen,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2021-04,UDALM: Unsupervised Domain Adaptation through Language Modeling model in \'UDALM: Unsupervised Domain Adaptation through Language Modeling\',93.77,Kitchen,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Multi-Domain Sentiment Dataset,2015-05,DANN model in \'Domain-Adversarial Training of Neural Networks\',-76.26,Average,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2013-10,RNTN model in \'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\',85.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2014-08,CNN-MC [kim:13] model in \'Convolutional Neural Networks for Sentence Classification\',88.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2015-06,DMN [ankit16] model in \'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\',88.6,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2016-03,CNN + Logic rules model in \'Harnessing Deep Neural Networks with Logic Rules\',89.3,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2016-07,Neural Semantic Encoder model in \'Neural Semantic Encoders\',89.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2017-04,bmLSTM model in \'Learning to Generate Reviews and Discovering Sentiment\',91.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2017-12,Block-sparse LSTM model in \'GPU Kernels for Block-Sparse Weights\',93.2,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2018-10,Snorkel MeTaL(ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',96.2,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2019-04,MT-DNN-ensemble model in \'Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding\',96.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',97.0,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',97.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2019-10,T5-3B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',97.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-2 Binary classification,2019-11,SMART-RoBERTa Large model in \'SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\',97.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Amazon Review Full,2016-07,FastText model in \'Bag of Tricks for Efficient Text Classification\',60.2,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Amazon Review Full,2017-07,DPCNN model in \'Deep Pyramid Convolutional Neural Networks for Text Categorization\',65.19,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Amazon Review Full,2019-04,BERT large model in \'Unsupervised Data Augmentation for Consistency Training\',65.83,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Twitter,2019-02,AEN-BERT model in \'Attentional Encoder Network for Targeted Sentiment Classification\',74.71,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Sogou News,2016-07,"fastText, h=10, bigram model in \'Bag of Tricks for Efficient Text Classification\'",96.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IMDb,2014-12,seq2-bown-CNN model in \'Effective Use of Word Order for Text Categorization with Convolutional Neural Networks\',92.33,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IMDb,2016-02,oh-LSTM model in \'Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings\',94.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IMDb,2017-05,CEN-tpc model in \'Contextual Explanation Networks\',94.52,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IMDb,2017-12,Block-sparse LSTM model in \'GPU Kernels for Block-Sparse Weights\',94.99,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IMDb,2018-01,ULMFiT model in \'Universal Language Model Fine-tuning for Text Classification\',95.4,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IMDb,2019-04,BERT large finetune UDA model in \'Unsupervised Data Augmentation for Consistency Training\',95.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,IMDb,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',96.21,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Urdu Online Reviews,2021-06,RCNN model in \'Sentiment analysis for Urdu online reviews using deep learning models\',84.48,Average F1,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2015-07,UPNN model in \'Learning Semantic Representations of Users and Products for Document Level Sentiment Classification\',43.5,IMDB (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2016-11,NSC model in \'Neural Sentiment Classification with User and Product Attention\',53.3,IMDB (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2017-11,CMA model in \'Cascading Multiway Attentions for Document-level Sentiment Classification\',54.0,IMDB (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2018-01,HUAPA model in \'Improving Review Representations with User Attention and Product Attention for Sentiment Classification\',55.0,IMDB (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2019-08,BiLSTM+CHIM model in \'Rethinking Attribute Representation and Injection for Sentiment Classification\',56.4,IMDB (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2015-07,UPNN model in \'Learning Semantic Representations of Users and Products for Document Level Sentiment Classification\',59.6,Yelp 2013 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2016-11,NSC model in \'Neural Sentiment Classification with User and Product Attention\',65.0,Yelp 2013 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2017-11,CMA model in \'Cascading Multiway Attentions for Document-level Sentiment Classification\',66.4,Yelp 2013 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2018-01,HUAPA model in \'Improving Review Representations with User Attention and Product Attention for Sentiment Classification\',68.3,Yelp 2013 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2015-07,UPNN model in \'Learning Semantic Representations of Users and Products for Document Level Sentiment Classification\',60.8,Yelp 2014 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2016-11,NSC model in \'Neural Sentiment Classification with User and Product Attention\',66.7,Yelp 2014 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2017-11,CMA model in \'Cascading Multiway Attentions for Document-level Sentiment Classification\',67.6,Yelp 2014 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2018-01,HUAPA model in \'Improving Review Representations with User Attention and Product Attention for Sentiment Classification\',68.6,Yelp 2014 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,User and product information,2019-08,BiLSTM+CHIM model in \'Rethinking Attribute Representation and Injection for Sentiment Classification\',69.2,Yelp 2014 (Acc),pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,DynaSent,2017-08,SVM model in \'Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM\',1.0,10 fold Cross validation,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-5 Fine-grained classification,2013-10,RNTN model in \'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\',45.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-5 Fine-grained classification,2014-06,"Epic model in \'Less Grammar, More Features\'",49.6,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-5 Fine-grained classification,2015-02,Constituency Tree-LSTM model in \'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\',51.0,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-5 Fine-grained classification,2017-08,BCN+Char+CoVe model in \'Learned in Translation: Contextualized Word Vectors\',53.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-5 Fine-grained classification,2018-02,BCN+ELMo model in \'Deep contextualized word representations\',54.7,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-5 Fine-grained classification,2018-05,BCN+Suffix BiLSTM-Tied+CoVe model in \'Improved Sentence Modeling using Suffix Bidirectional LSTM\',56.2,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SST-5 Fine-grained classification,2020-12,RoBERTa-large+Self-Explaining model in \'Self-Explaining Structures Improve NLP Models\',59.1,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,Yelp Binary classification,2015-09,Char-level CNN model in \'Character-level Convolutional Networks for Text Classification\',4.88,Error,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,SemEval,2017-04,LSTMs+CNNs ensemble with multiple conv. ops model in \'BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs\',0.685,F1-score,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MR,2017-02,GRU-RNN-WORD2VEC model in \'All-but-the-Top: Simple and Effective Postprocessing for Word Representations\',78.26,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MR,2018-02,RNN-Capsule model in \'Sentiment Analysis by Capsules\',83.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MR,2018-05,byte mLSTM7 model in \'A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors\',86.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MR,2019-02,VLAWE model in \'Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation\',93.3,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MR,2017-08,"Millions of Emoji model in \'Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm\'",1500.0,Training Time,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',-73.4,Sentiment,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-10,LSTM model in \'TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification\',-58.3,Sentiment,neg
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',33.4,Emoji,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',79.3,Emotion,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2021-04,RoB-RT model in \'XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond\',79.5,Emotion,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-10,LSTM model in \'TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification\',52.6,Hate,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',82.1,Irony,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',79.5,Offensive,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2021-04,RoB-RT model in \'XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond\',80.5,Offensive,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',71.2,Stance,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,TweetEval,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',67.9,ALL,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MPQA,2018-03,USE_T+DAN (w2v w.e.)  model in \'Universal Sentence Encoder\',88.14,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MPQA,2018-05,byte mLSTM7 model in \'A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors\',88.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MPQA,2019-05,STM+TSED+PT+2L model in \'The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning\',89.83,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Sentiment analysis,MPQA,2021-04,EFL model in \'Entailment as Few-Shot Learner\',90.8,Accuracy,pos
Natural language analysis,Pragmatics analysis,Sentiment analysis,Tweet-reply sentiment analysis,RETWEET,2021-04,Ensemble Model (Bi-LSTM + CNN) model in \'How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies\',73.2,Average F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Extracting Covid-19 events from twitter,W-NUT 2020 Shared Task-3,2020-12,- model in \'Leveraging Event Specific and Chunk Span features to Extract COVID Events from tweets\',0.66,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,MASSIVE,2022-04,XLM-R Base model in \'MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\',83.6,Slot F1 Score,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',9.02,Accuracy,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',47.42,Accuracy,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',13.52,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',54.75,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',57.43,R-Prec,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',60.47,Recall@5,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',46.79,KILT-F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: Zero Shot RE,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',41.34,KILT-AC,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,MixSNIPS,2021-06,GL-GIN model in \'GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling\',94.9,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',43.56,Accuracy,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',53.9,Accuracy,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",87.68,Accuracy,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',50.61,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',61.74,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",89.93,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',37.62,R-Prec,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",80.7,R-Prec,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',40.07,Recall@5,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",89.0,Recall@5,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',32.34,KILT-F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",77.05,KILT-F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',27.84,KILT-AC,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,KILT: T-REx,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",75.84,KILT-AC,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,MixATIS,2021-06,GL-GIN model in \'GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling\',88.3,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,SLURP,2020-11,Multi-SLURP model in \'SLURP: A Spoken Language Understanding Resource Package\',0.642,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,SLURP,2021-11,"Partially Fine-tuned HuBERT model in \'A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding\'",0.753,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,ATIS,2018-12,Bi-model SLU model in \'A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling\',0.969,F1,pos
Natural language analysis,Pragmatics analysis,Slot filling,Slot filling,Polyvore,2019-02,Fashion GAE model in \'Context-Aware Visual Compatibility Prediction\',96.9,FITB,pos
Natural language analysis,Pragmatics analysis,Slot filling,Zero-shot Slot Filling,zsRE,2021-08,DPRDNS+RAG model in \'Robust Retrieval Augmented Generation for Zero-shot Slot Filling\',99.7,R-at-5,pos
Natural language analysis,Pragmatics analysis,Slot filling,Zero-shot Slot Filling,zsRE,2021-08,DPRDNS+RAG model in \'Robust Retrieval Augmented Generation for Zero-shot Slot Filling\',98.6,R-Prec,pos
Natural language analysis,Pragmatics analysis,Slot filling,Zero-shot Slot Filling,T-REx,2021-08,DPRDNS+RAG model in \'Robust Retrieval Augmented Generation for Zero-shot Slot Filling\',82.89,R-at-5,pos
Natural language analysis,Pragmatics analysis,Slot filling,Zero-shot Slot Filling,T-REx,2021-08,DPRDNS+RAG model in \'Robust Retrieval Augmented Generation for Zero-shot Slot Filling\',74.34,R-Prec,pos
Natural language analysis,Pragmatics analysis,Slot filling,Zero-shot Slot Filling,MASSIVE,2022-04,XLM-R Base model in \'MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\',64.2,Slot F1 Score,pos
Natural language analysis,Pragmatics analysis,Speculation detection,Speculation scope resolution,BioScope : Full Papers,2020-01,XLNet model in \'Resolving the Scope of Speculation and Negation using Transformer-Based Architectures\',96.91,F1,pos
Natural language analysis,Pragmatics analysis,Speculation detection,Speculation scope resolution,BioScope : Abstracts,2020-01,XLNet model in \'Resolving the Scope of Speculation and Negation using Transformer-Based Architectures\',97.87,F1,pos
Natural language analysis,Pragmatics analysis,Speculation detection,Speculation scope resolution,SFU Review Corpus,2020-01,XLNet model in \'Resolving the Scope of Speculation and Negation using Transformer-Based Architectures\',91.0,F1,pos
Natural language analysis,Pragmatics analysis,Stance detection,Biden),Twitter Stance Election 2020 - Stance detection (US election 2020,2021-05,KE-MLM model in \'Knowledge Enhanced Masked Language Model for Stance Detection\',0.7577,Average F1,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,RumourEval,2017-04,Kochkina et al. 2017 model in \'Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM\',0.784,Accuracy,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,RuStance,2018-09,Boosting model in \'Stance Prediction for Russian: Data and Analysis\',0.865,F1,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,Turkish Elections 2018,2020-05,MUSE + UMAP (Unsupervised) model in \'Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey\',0.84,Avg F1,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,Turkish Elections 2018,2020-05,MUSE + UMAP (Unsupervised) model in \'Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey\',0.9,Macro Precision,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,Turkish Elections 2018,2020-05,MUSE + UMAP (Unsupervised) model in \'Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey\',0.79,Macro Recall,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,Trump Midterm Elections 2018,2020-05,MUSE + UMAP (Unsupervised) model in \'Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey\',0.86,Avg F1,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,Trump Midterm Elections 2018,2020-05,MUSE + UMAP (Unsupervised) model in \'Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey\',0.89,Macro Precision,pos
Natural language analysis,Pragmatics analysis,Stance detection,Stance detection,Trump Midterm Elections 2018,2020-05,MUSE + UMAP (Unsupervised) model in \'Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey\',0.84,Macro Recall,pos
Natural language analysis,Pragmatics analysis,Stance detection,Trump),Twitter Stance Election 2020 - Stance detection (US election 2020,2021-05,KE-MLM model in \'Knowledge Enhanced Masked Language Model for Stance Detection\',0.7877,Average F1,pos
Natural language analysis,Pragmatics analysis,Subjectivity analysis,Subjectivity analysis,SUBJ,2015-04,AdaSent model in \'Self-Adaptive Hierarchical Sentence Model\',95.5,Accuracy,pos
Natural language analysis,Pragmatics analysis,Subjectivity analysis,Subjectivity analysis,SUBJ,2021-04,BERT-Base + CLR + LSTM model in \'An Empirical Evaluation of Word Embedding Models for Subjectivity Analysis Tasks\',97.3,Accuracy,pos
Natural language analysis,Pragmatics analysis,Subjectivity analysis,Subjectivity analysis,SUBJ,2022-01,RoBERTa+DualCL model in \'Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation\',97.34,Accuracy,pos
Natural language analysis,Pragmatics analysis,Subjectivity analysis,Subjectivity analysis,Czech Subjectivity Dataset,2022-04,XLM-R-Large model in \'Czech Dataset for Cross-lingual Subjectivity Classification\',93.56,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC German-to-English,2015-11,Monolingual training data model in \'Improving Neural Machine Translation Models with Monolingual Data\',76.9,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC German-to-English,2018-11,Multilingual Sentence Embeddings model in \'Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings\',95.58,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC German-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',96.19,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC Chinese-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',92.27,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC Russian-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',93.3,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC French-to-English,2015-11,Monolingual training data model in \'Improving Neural Machine Translation Models with Monolingual Data\',75.8,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC French-to-English,2018-11,Multilingual Sentence Embeddings model in \'Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings\',92.89,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual bitext mining,Cross-lingual bitext mining,BUCC French-to-English,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',93.91,F1 score,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-German,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',81.2,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-German,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',84.78,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-German,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',96.95,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Japanese,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',67.63,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Japanese,2019-09,"MultiFiT, pseudo model in \'MultiFiT: Efficient Multi-lingual Language Model Fine-tuning\'",69.57,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Spanish,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',72.5,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Spanish,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',77.33,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Spanish,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',96.8,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 English-to-German,2013-12,biCVM+ model in \'Multilingual Distributed Representations without Word Alignment\',86.2,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 English-to-German,2014-04,Bi+ model in \'Multilingual Models for Compositional Distributed Semantics\',88.1,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 English-to-German,2014-12,Biinclusion (Euro500kReuters) model in \'Leveraging Monolingual Data for Crosslingual Compositional Word Representations\',92.7,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Russian,2018-05,BiLSTM (UN) model in \'A Corpus for Multilingual Document Classification in Eight Languages\',61.42,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Russian,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',67.78,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Russian,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',89.7,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Italian,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',69.38,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Italian,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',69.43,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Italian,2019-09,"MultiFiT, pseudo model in \'MultiFiT: Efficient Multi-lingual Language Model Fine-tuning\'",76.02,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-French,2018-05,BiLSTM (UN) model in \'A Corpus for Multilingual Document Classification in Eight Languages\',74.52,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-French,2018-12,Massively Multilingual Sentence Embeddings model in \'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\',77.95,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-French,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',96.05,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Chinese,2018-05,MultiCCA + CNN model in \'A Corpus for Multilingual Document Classification in Eight Languages\',74.73,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot English-to-Chinese,2019-09,XLMft UDA model in \'Bridging the domain gap in cross-lingual document classification\',93.32,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,MLDoc Zero-Shot German-to-French,2018-05,BiLSTM (Europarl) model in \'A Corpus for Multilingual Document Classification in Eight Languages\',75.45,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 German-to-English,2013-12,biCVM+ model in \'Multilingual Distributed Representations without Word Alignment\',76.9,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 German-to-English,2014-04,Bi+ model in \'Multilingual Models for Compositional Distributed Semantics\',79.2,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,Cross-lingual document classification,Reuters RCV1/RCV2 German-to-English,2014-12,Biinclusion (Euro500kReuters) model in \'Leveraging Monolingual Data for Crosslingual Compositional Word Representations\',84.4,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,News Classification,N15News,2021-08,"Multimodal(ViT+BERT, Input: Image + Body) model in \'N24News: A New Dataset for Multimodal News Classification\'",0.9249,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,News Classification,BBC Hindi News Article Classification,2022-01,xlmindic-base-uniscript model in \'Does Transliteration Help Multilingual Language Modeling?\',79.14,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,News Classification,Soham News Article Classification,2020-11,"IndicBERT Base model in \'IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages\'",78.45,Accuracy,pos
Natural language analysis,Semantic analysis,Cross-lingual document classification,News Classification,Soham News Article Classification,2022-01,xlmindic-base-uniscript model in \'Does Transliteration Help Multilingual Language Modeling?\',93.89,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,Switchboard corpus,2016-03,CNN[[Lee and Dernoncourt2016]] model in \'Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks\',73.1,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,Switchboard corpus,2017-09,Bi-LSTM-CRF model in \'Dialogue Act Sequence Labeling using Hierarchical encoder with CRF\',79.2,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,Switchboard corpus,2017-11,CRF-ASN model in \'Dialogue Act Recognition via CRF-Attentive Structured Network\',81.3,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,Switchboard corpus,2018-10,DAH-CRF  model in \'A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification\',82.3,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,Switchboard corpus,2019-04,Bi-RNN + Self-Attention + Context model in \'Dialogue Act Classification with Context-Aware Self-Attention\',82.9,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,Switchboard corpus,2020-02,HGRU + Beam Search + Guided attention model in \'Guiding attention in Sequence-to-sequence models for Dialogue Act prediction\',85.0,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,ICSI Meeting Recorder Dialog Act (MRDA) corpus,2017-09,Bi-LSTM-CRF model in \'Dialogue Act Sequence Labeling using Hierarchical encoder with CRF\',90.9,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,ICSI Meeting Recorder Dialog Act (MRDA) corpus,2017-11,CRF-ASN model in \'Dialogue Act Recognition via CRF-Attentive Structured Network\',91.7,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,ICSI Meeting Recorder Dialog Act (MRDA) corpus,2020-09,Pretrained Hierarchical Transformer model in \'Hierarchical Pre-training for Sequence Labelling in Spoken Dialog\',92.4,Accuracy,pos
Natural language analysis,Semantic analysis,Dialog act classification,Dialog act classification,Switchboard dialogue act corpus,2018-04,Probabilistic-LSTM model in \'Probabilistic Word Association for Dialogue Act Classification with Recurrent Neural Networks\',75.48,Accuracy (%),pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',84.4,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',88.1,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-10,StateNet model in \'Towards Universal Dialogue State Tracking\',88.9,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2019-10,BERT-based tracker model in \'A Simple but Effective BERT Model for Dialog State Tracking on Resource-Limited Systems\',90.5,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2020-11,Seq2Seq-DU-w/oSchema model in \'A Sequence-to-Sequence Approach to Dialogue State Tracking\',91.2,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2021-10,AG-DST model in \'Amendable Generation for Dialogue State Tracking\',91.37,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',96.5,Request,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',97.1,Request,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2018-12,GCE model in \'Toward Scalable Neural Dialogue State Tracking Model\',97.4,Request,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Wizard-of-Oz,2019-10,BERT-based tracker model in \'A Simple but Effective BERT Model for Dialog State Tracking on Resource-Limited Systems\',97.6,Request,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,CoSQL,2021-09,T5-3B + PICARD model in \'PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models\',54.6,question match accuracy,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,CoSQL,2022-05,RASAT+PICARD model in \'RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL\',55.7,question match accuracy,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,CoSQL,2021-09,T5-3B + PICARD model in \'PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models\',23.7,interaction match accuracy,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,CoSQL,2022-05,RASAT+PICARD model in \'RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL\',26.5,interaction match accuracy,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',73.4,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',74.5,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2018-10,StateNet model in \'Towards Universal Dialogue State Tracking\',75.5,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2020-11,Seq2Seq-DU-w/oSchema model in \'A Sequence-to-Sequence Approach to Dialogue State Tracking\',85.0,Joint,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',96.5,Request,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2018-05,Zhong et al. model in \'Global-Locally Self-Attentive Dialogue State Tracker\',97.5,Request,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',90.0,Area,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',84.0,Food,pos
Natural language analysis,Semantic analysis,Dialog state tracking,Dialog state tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker model in \'Neural Belief Tracker: Data-Driven Dialogue State Tracking\',94.0,Price,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k fr-en,2020-09,EVA model in \'Visual Pivoting for (Unsupervised) Entity Alignment\',0.793,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k fr-en,2020-10,Zero Shot model in \'A Critical Assessment of State-of-the-Art in Entity Alignment\',0.837,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k fr-en,2020-11,EMGCN model in \'Entity Alignment for Knowledge Graphs with Multi-order Convolutional Networks\',0.94,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k fr-en,2021-03,Dual-AMN model in \'Boosting the Speed of Entity Alignment 10*: Dual Attention Matching Network with Normalized Hard Sample Mining\',0.954,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k fr-en,2021-08,"PSR model in \'Are Negative Samples Necessary in Entity Alignment? An Approach with High Performance, Scalability and Robustness\'",0.958,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k fr-en,2021-09,SEU model in \'From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment\',0.988,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k ja-en,2020-09,EVA model in \'Visual Pivoting for (Unsupervised) Entity Alignment\',0.762,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k ja-en,2020-11,EMGCN model in \'Entity Alignment for Knowledge Graphs with Multi-order Convolutional Networks\',0.866,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k ja-en,2021-03,Dual-AMN model in \'Boosting the Speed of Entity Alignment 10*: Dual Attention Matching Network with Normalized Hard Sample Mining\',0.892,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k ja-en,2021-08,"PSR model in \'Are Negative Samples Necessary in Entity Alignment? An Approach with High Performance, Scalability and Robustness\'",0.908,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,dbp15k ja-en,2021-09,SEU model in \'From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment\',0.956,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP2.0 zh-en,2021-06,MTransE w/ background ranking model in \'Knowing the No-match: Entity Alignment with Dangling Cases\',0.767,dangling entity detection F1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP2.0 zh-en,2021-06,MTransE w/ background ranking model in \'Knowing the No-match: Entity Alignment with Dangling Cases\',0.335,Entity Alignment (Consolidated) F1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2016-11,MTransE model in \'Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment\',0.308,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2019-08,RDGCN model in \'Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs\',0.7075,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2019-09,HGCN-JE model in \'Jointly Learning Entity and Relation Representations for Entity Alignment\',0.7203,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2020-01,Deep Graph Matching Consensus (L=10) model in \'Deep Graph Matching Consensus\',0.8012,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2020-08,RREA(text) model in \'Relational Reflection Entity Alignment\',0.822,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2020-11,EMGCN model in \'Entity Alignment for Knowledge Graphs with Multi-order Convolutional Networks\',0.863,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2021-08,"PSR model in \'Are Negative Samples Necessary in Entity Alignment? An Approach with High Performance, Scalability and Robustness\'",0.883,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity alignment,Entity alignment,DBP15k zh-en,2021-09,SEU model in \'From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment\',0.9,Hits-at-1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,ACE2004,2017-04,Global model in \'Deep Joint Entity Disambiguation with Local Neural Attention\',88.5,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,ACE2004,2019-09,confidence-order model in \'Global Entity Disambiguation with BERT\',91.9,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,WNED-WIKI,2017-04,Glonal model in \'Deep Joint Entity Disambiguation with Local Neural Attention\',77.5,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,WNED-WIKI,2019-09,confidence-order model in \'Global Entity Disambiguation with BERT\',89.1,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,MSNBC,2017-04,Global model in \'Deep Joint Entity Disambiguation with Local Neural Attention\',93.7,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,MSNBC,2019-09,confidence-order model in \'Global Entity Disambiguation with BERT\',96.3,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,Mewsli-9,2020-11,Model F+ model in \'Entity Linking in 100 Languages\',89.0,Micro Precision,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,Mewsli-9,2021-03,mGENRE model in \'Multilingual Autoregressive Entity Linking\',90.6,Micro Precision,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,AIDA-CoNLL,2020-10,Bootleg model in \'Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation\',96.8,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,AIDA-CoNLL,2011-07,Hoffart et al. model in \'Robust Disambiguation of Named Entities in Text\',82.29,In-KB Accuracy,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,AIDA-CoNLL,2016-01,Wikipedia2Vec-GBRT model in \'Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation\',93.1,In-KB Accuracy,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,AIDA-CoNLL,2017-05,NTEE model in \'Learning Distributed Representations of Texts and Entities from Knowledge Base\',94.7,In-KB Accuracy,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,AIDA-CoNLL,2018-02,DeepType model in \'DeepType: Multilingual Entity Linking by Neural Type System Evolution\',94.88,In-KB Accuracy,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,AIDA-CoNLL,2019-09,confidence-order model in \'Global Entity Disambiguation with BERT\',95.0,In-KB Accuracy,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,AQUAINT,2019-09,confidence-order model in \'Global Entity Disambiguation with BERT\',93.5,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,WNED-CWEB,2017-04,Global model in \'Deep Joint Entity Disambiguation with Local Neural Attention\',77.9,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,WNED-CWEB,2019-09,confidence-order model in \'Global Entity Disambiguation with BERT\',78.9,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,TAC2010,2016-01,Wikipedia2Vec model in \'Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation\',85.2,Micro Precision,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,TAC2010,2017-05,NTEE model in \'Learning Distributed Representations of Texts and Entities from Knowledge Base\',87.7,Micro Precision,pos
Natural language analysis,Semantic analysis,Entity disambiguation,Entity disambiguation,TAC2010,2018-02,DeepType model in \'DeepType: Multilingual Entity Linking by Neural Type System Evolution\',90.85,Micro Precision,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Rare Diseases Mentions in MIMIC-III (Text-to-UMLS),2021-05,SemEHR+WS (rules+BlueBERT) model in \'Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision\',0.858,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Rare Diseases Mentions in MIMIC-III (Text-to-UMLS),2022-05,SemEHR+WS (rules+BlueBERT) with tuning number of training data model in \'Ontology-Based and Weakly Supervised Rare Disease Phenotyping from Clinical Notes\',0.861,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,GUM,2021-09,baseline model in \'WikiGUM: Exhaustive Entity Linking for Wikification in 12 Genres\',26.4,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',49.29,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',71.22,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',49.29,R-Prec,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',71.22,R-Prec,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',49.29,Recall@5,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',79.22,Recall@5,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',49.29,KILT-AC,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-CWEB,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',71.22,KILT-AC,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',74.05,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',89.85,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',74.05,R-Prec,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',89.85,R-Prec,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',74.05,Recall@5,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',94.76,Recall@5,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',74.05,KILT-AC,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: AIDA-YAGO2,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',89.85,KILT-AC,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,TAC-KBP 2010,2018-02,Raiman & Raiman 2018 model in \'DeepType: Multilingual Entity Linking by Neural Type System Evolution\',90.9,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,MedMentions,2021-09,ArboEL model in \'Entity Linking and Discovery via Arborescence-based Supervised Clustering\',72.3,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,MedMentions,2021-09,ArboEL model in \'Entity Linking and Discovery via Arborescence-based Supervised Clustering\',95.62,Recall@64,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Rare Diseases Mentions in MIMIC-III,2021-05,SemEHR+WS (rules+BlueBERT) model in \'Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision\',0.702,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Rare Diseases Mentions in MIMIC-III,2022-05,SemEHR+WS (rules+BlueBERT) with tuning number of training data model in \'Ontology-Based and Weakly Supervised Rare Disease Phenotyping from Clinical Notes\',0.711,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,ZESHEL,2021-09,ArboEL model in \'Entity Linking and Discovery via Arborescence-based Supervised Clustering\',85.11,Recall@64,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,ZESHEL,2021-09,ArboEL model in \'Entity Linking and Discovery via Arborescence-based Supervised Clustering\',50.4,Unnormalized Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,CoNLL-Aida,2018-02,Raiman & Raiman 2018 model in \'DeepType: Multilingual Entity Linking by Neural Type System Evolution\',94.9,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,FIGER,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',57.19,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,FIGER,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',76.51,Macro F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,FIGER,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',73.39,Micro F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Derczynski,2018-08,Kolitsas et al. (2018) model in \'End-to-End Neural Entity Linking\',34.1,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Derczynski,2020-06,van Hulst et al. (2020) model in \'REL: An Entity Linker Standing on the Shoulders of Giants\',41.1,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Derczynski,2020-10,De Cao et al. (2021a) model in \'Autoregressive Entity Retrieval\',54.1,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,transformers model in \'Word Sense Disambiguation with Transformer Models\',77.8,Task 1 Accuracy: all,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,transformers model in \'Word Sense Disambiguation with Transformer Models\',75.2,Task 1 Accuracy: general purpose,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,transformers model in \'Word Sense Disambiguation with Transformer Models\',81.0,Task 1 Accuracy: domain specific,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,CTLR model in \'CTLR@WiC-TSV: Target Sense Verification using Marked Inputs andPre-trained Models\',72.7,Task 2 Accuracy: all,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,CTLR model in \'CTLR@WiC-TSV: Target Sense Verification using Marked Inputs andPre-trained Models\',65.6,Task 2 Accuracy: general purpose,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,CTLR model in \'CTLR@WiC-TSV: Target Sense Verification using Marked Inputs andPre-trained Models\',81.5,Task 2 Accuracy: domain specific,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,CTLR model in \'CTLR@WiC-TSV: Target Sense Verification using Marked Inputs andPre-trained Models\',78.3,Task 3 Accuracy: all,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,transformers model in \'Word Sense Disambiguation with Transformer Models\',77.0,Task 3 Accuracy: general purpose,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WiC-TSV,2021-04,CTLR model in \'CTLR@WiC-TSV: Target Sense Verification using Marked Inputs andPre-trained Models\',85.7,Task 3 Accuracy: domain specific,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,OKE-2016,2018-08,E2E model in \'End-to-End Neural Entity Linking\',58.4,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,MSNBC,2018-08,Kolitsas et al. (2018) model in \'End-to-End Neural Entity Linking\',72.4,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,MSNBC,2020-10,De Cao et al. (2021a) model in \'Autoregressive Entity Retrieval\',73.7,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,MSNBC,2021-01,Kannan Ravi et al. (2021) model in \'CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata\',83.4,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,WebQSP-WD,2018-04,VCG model in \'Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories\',0.73,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',47.13,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',87.44,Accuracy,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',47.13,R-Prec,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',87.44,R-Prec,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',47.13,Recall@5,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',94.91,Recall@5,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',47.13,KILT-AC,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,KILT: WNED-WIKI,2020-10,GENRE model in \'Autoregressive Entity Retrieval\',87.44,KILT-AC,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,AIDA-CoNLL,2011-07,Hoffart et al. (2011) model in \'Robust Disambiguation of Named Entities in Text\',72.8,Micro-F1 strong,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,AIDA-CoNLL,2018-08,Kolitsas et al. (2018) model in \'End-to-End Neural Entity Linking\',82.4,Micro-F1 strong,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,AIDA-CoNLL,2020-10,De Cao et al. (2021a) model in \'Autoregressive Entity Retrieval\',83.7,Micro-F1 strong,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,AIDA-CoNLL,2021-09,De Cao et al. (2021b) model in \'Highly Parallel Autoregressive Entity Linking with Discriminative Correction\',85.5,Micro-F1 strong,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,AIDA-CoNLL,2021-10,Zhang et al. (2021) model in \'EntQA: Entity Linking as Question Answering\',85.8,Micro-F1 strong,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,OKE-2015,2018-08,E2E model in \'End-to-End Neural Entity Linking\',66.9,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,FUNSD,2021-10,SERA model in \'Entity Relation Extraction as Dependency Parsing in Visually Rich Documents\',65.96,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,BC7 NLM-Chem,2021-11,Sieve-based model in \'Chemical detection and indexing in PubMed full text articles using deep learning and rule-based methods\',0.8136,F1-score (strict),pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,BC7 NLM-Chem,2022-07,Sieve-based+SapBERT model in \'Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics\',0.8275,F1-score (strict),pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,Rare Diseases Mentions in MIMIC-III Radiology Reports (Text-to-UMLS),2022-05,SemEHR+WS (rules+BlueBERT) with tuning number of training data model in \'Ontology-Based and Weakly Supervised Rare Disease Phenotyping from Clinical Notes\',0.907,F1,pos
Natural language analysis,Semantic analysis,Entity linking,Entity linking,N3-Reuters-128,2018-08,E2E model in \'End-to-End Neural Entity Linking\',54.6,Micro-F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,FIGER,2022-02,LITE model in \'Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference\',80.1,Macro F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,FIGER,2022-02,LITE model in \'Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference\',83.3,Micro F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Open Entity,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',75.56,F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Open Entity,2020-02,K-Adapter ( fac-adapter ) model in \'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\',77.6916,F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Open Entity,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',78.42,Precision,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Open Entity,2020-02,K-Adapter ( fac-adapter ) model in \'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\',79.6712,Precision,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Open Entity,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',72.9,Recall,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Open Entity,2020-02,K-Adapter ( fac-adapter + lin-adapter ) model in \'K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\',76.2774,Recall,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2018-07,Choi et al. (2018) w augmentation model in \'Ultra-Fine Entity Typing\',32.0,F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2019-03,LabelGCN Xiong et al. (2019) model in \'Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing\',36.9,F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2019-05,ELMo (distant denoising data) model in \'Learning to Denoise Distantly-Labeled Data for Entity Typing\',40.2,F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2021-06,MLMET model in \'Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model\',49.1,F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2018-07,Choi et al. (2018) w augmentation model in \'Ultra-Fine Entity Typing\',47.1,Precision,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2019-03,LabelGCN Xiong et al. (2019) model in \'Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing\',50.3,Precision,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2019-05,ELMo (distant denoising data) model in \'Learning to Denoise Distantly-Labeled Data for Entity Typing\',51.5,Precision,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2021-06,MLMET model in \'Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model\',53.6,Precision,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2018-07,Choi et al. (2018) w augmentation model in \'Ultra-Fine Entity Typing\',24.2,Recall,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2019-03,LabelGCN Xiong et al. (2019) model in \'Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing\',29.2,Recall,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2019-05,ELMo (distant denoising data) model in \'Learning to Denoise Distantly-Labeled Data for Entity Typing\',33.0,Recall,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Ontonotes v5 (English),2021-06,MLMET model in \'Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model\',45.3,Recall,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,OntoNotes,2022-02,LITE model in \'Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference\',86.6,Macro F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,OntoNotes,2022-02,LITE model in \'Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference\',81.4,Micro F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Freebase FIGER,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',37.4,Accuracy,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Freebase FIGER,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',84.2,Macro F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Freebase FIGER,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',85.7,Micro F1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Freebase FIGER,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',93.2,P-at-1,pos
Natural language analysis,Semantic analysis,Entity typing,Entity typing,Freebase FIGER,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',94.8,BEP,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Medical domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',41.07,MRR,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Medical domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',54.64,MRR,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Medical domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',18.84,MAP,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Medical domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',34.05,MAP,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Medical domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',20.71,P-at-5,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Medical domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',36.77,P-at-5,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,General,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',23.83,MRR,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,General,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',36.1,MRR,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,General,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',10.6,MAP,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,General,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',19.78,MAP,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,General,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',9.91,P-at-5,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,General,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',19.03,P-at-5,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Music domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',39.36,MRR,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Music domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',60.93,MRR,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Music domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',12.99,MAP,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Music domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',40.97,MAP,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Music domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',12.41,P-at-5,pos
Natural language analysis,Semantic analysis,Hypernym discovery,Hypernym discovery,Music domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',41.31,P-at-5,pos
Natural language analysis,Semantic analysis,Phrase tagging,Phrase tagging,KPTimes,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',73.5,F1,pos
Natural language analysis,Semantic analysis,Phrase tagging,Phrase tagging,KPTimes,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',69.1,Precision,pos
Natural language analysis,Semantic analysis,Phrase tagging,Phrase tagging,KPTimes,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',78.9,Recall,pos
Natural language analysis,Semantic analysis,Phrase tagging,Phrase tagging,KP20k,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',73.9,F1,pos
Natural language analysis,Semantic analysis,Phrase tagging,Phrase tagging,KP20k,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',69.9,Precision,pos
Natural language analysis,Semantic analysis,Phrase tagging,Phrase tagging,KP20k,2021-05,UCPhrase model in \'UCPhrase: Unsupervised Context-aware Quality Phrase Tagging\',78.3,Recall,pos
Natural language analysis,Semantic analysis,Question quality assessment,Question quality assessment,60k Stack Overflow Questions,2022-04,Multi-view approach model in \'Multi-View Approach to Suggest Moderation Actions in Community Question Answering Sites\',0.917,F1 Score,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2015E86,2016-06,JAMR model in \'CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss\',67.0,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2015E86,2017-07,Mul-BiLSTM model in \'Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks\',70.7,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2015E86,2018-05,Joint model model in \'AMR Parsing as Graph Prediction with Latent Alignment\',73.7,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2017-05,ChSeq + 100K model in \'Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations\',71.0,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2018-05,Joint model model in \'AMR Parsing as Graph Prediction with Latent Alignment\',74.4,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2019-05,Sequence-to-Graph Transduction model in \'AMR Parsing as Sequence-to-Graph Transduction\',76.3,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2019-09,Zhang et al. model in \'Broad-Coverage Semantic Parsing as Transduction\',77.0,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2020-04,Cai and Lam model in \'AMR Parsing via Graph-Sequence Iterative Inference\',80.2,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2020-10,"Xu, et al. model in \'Improving AMR Parsing with Sequence-to-Sequence Pre-training\'",81.4,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2021-04,APT base model in \'AMR Parsing with Action-Pointer Transformer\',82.6,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2021-05,SPRING model in \'One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline\',84.3,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2021-10,Graphene Smatch model in \'Ensembling Graph Predictions for AMR Parsing\',86.26,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2017T10,2021-12,MBSE model in \'Maximum Bayes Smatch Ensemble Distillation for AMR Parsing\',86.7,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12:,2016-08,Imitation learning  model in \'Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing\',0.7,F1 Newswire,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12:,2016-11,Incremental joint model model in \'AMR Parsing with an Incremental Joint Model\',0.71,F1 Newswire,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12:,2018-10,Transition-based+improved aligner+ensemble model in \'An AMR Aligner Tuned by Transition-based Parser\',0.73,F1 Newswire,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12:,2019-05,Sequence-to-Graph Transduction model in \'AMR Parsing as Sequence-to-Graph Transduction\',0.75,F1 Newswire,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12:,2016-11,Incremental joint model model in \'AMR Parsing with an Incremental Joint Model\',0.66,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12:,2018-10,Transition-based+improved aligner+ensemble model in \'An AMR Aligner Tuned by Transition-based Parser\',0.68,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12:,2019-05,Sequence-to-Graph Transduction model in \'AMR Parsing as Sequence-to-Graph Transduction\',0.7,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2015-07,Transition-based transducer model in \'Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers\',70.0,F1 Newswire,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2016-11,Incremental joint model  model in \'AMR Parsing with an Incremental Joint Model\',71.0,F1 Newswire,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2018-10,Transition-based+improved aligner+ensemble model in \'An AMR Aligner Tuned by Transition-based Parser\',73.3,F1 Newswire,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2015-07,Transition-based transducer model in \'Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers\',66.0,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2017-09,Improved CAMR  model in \'Getting the Most out of AMR Parsing\',68.1,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2018-10,Transition-based+improved aligner+ensemble model in \'An AMR Aligner Tuned by Transition-based Parser\',68.4,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2019-05,Two-stage Sequence-to-Graph Transducer model in \'AMR Parsing as Sequence-to-Graph Transduction\',70.2,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2019-09,Broad-Coverage Semantic Parsing as Transduction model in \'Broad-Coverage Semantic Parsing as Transduction\',71.3,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2020-04,AMR Parsing via Graph-Sequence Iterative Inference model in \'AMR Parsing via Graph-Sequence Iterative Inference\',75.4,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2014T12,2020-10,Pushing the Limits of AMR Parsing with Self-Learning model in \'Pushing the Limits of AMR Parsing with Self-Learning\',78.2,F1 Full,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,New3,2021-05,SPRING DFS model in \'One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline\',73.7,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,New3,2021-10,Graphene Smatch model in \'Ensembling Graph Predictions for AMR Parsing\',76.32,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,New3,2022-03,AMRBART large model in \'Graph Pre-training for AMR Parsing and Generation\',76.9,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,Bio,2021-05,SPRING DFS model in \'One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline\',59.7,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,Bio,2021-10,Graphene Smatch model in \'Ensembling Graph Predictions for AMR Parsing\',62.8,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,Bio,2022-03,AMRBART large model in \'Graph Pre-training for AMR Parsing and Generation\',63.2,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2020T02,2021-04,APT+Silver model in \'AMR Parsing with Action-Pointer Transformer\',80.4,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2020T02,2021-05,SPRING DFS model in \'One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline\',83.0,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2020T02,2021-10,Graphene Smatch model in \'Ensembling Graph Predictions for AMR Parsing\',84.87,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,LDC2020T02,2021-12,MBSE model in \'Maximum Bayes Smatch Ensemble Distillation for AMR Parsing\',85.4,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,The Little Prince,2021-05,SPRING DFS + silver model in \'One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline\',77.5,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,The Little Prince,2021-10,Graphene Smatch model in \'Ensembling Graph Predictions for AMR Parsing\',79.52,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,AMR parsing,The Little Prince,2022-03,AMRBART large model in \'Graph Pre-training for AMR Parsing and Generation\',79.8,Smatch,pos
Natural language analysis,Semantic analysis,Semantic parsing,DRS parsing,PMB-3.0.0,2018-10,Character-level bi-LSTM seq2seq model in \'Exploring Neural Methods for Parsing Discourse Representation Structures\',84.9,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,DRS parsing,PMB-3.0.0,2019-05,Character-level bi-LSTM seq2seq + linguistic features model in \'Linguistic Information in Neural Semantic Parsing with Multiple Encoders\',87.7,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,DRS parsing,PMB-3.0.0,2020-11,Bi-LSTM seq2seq: BERT + characters in 1 encoder model in \'Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT\',89.3,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,DRS parsing,PMB-2.2.0,2018-10,Character-level bi-LSTM seq2seq model in \'Exploring Neural Methods for Parsing Discourse Representation Structures\',83.3,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,DRS parsing,PMB-2.2.0,2019-05,Transformer seq2seq model in \'Discourse Representation Structure Parsing with Recurrent Neural Networks and the Transformer Model\',87.1,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,DRS parsing,PMB-2.2.0,2020-11,Bi-LSTM seq2seq: BERT + characters in 1 encoder model in \'Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT\',88.3,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2018-07,Dozat et al. (2018) model in \'Simpler but More Accurate Semantic Dependency Parsing\',93.9,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2019-06,MFVI model in \'Second-Order Semantic Dependency Parsing with End-to-End Neural Networks\',94.1,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2020-07,Fernández-González & Gómez-Rodríguez (2020) model in \'Transition-based Semantic Dependency Parsing with Pointer Networks\',95.1,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',95.8,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2018-07,Dozat et al. (2018) model in \'Simpler but More Accurate Semantic Dependency Parsing\',90.6,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2019-06,MFVI model in \'Second-Order Semantic Dependency Parsing with End-to-End Neural Networks\',91.3,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2020-07,Fernández-González & Gómez-Rodríguez (2020) model in \'Transition-based Semantic Dependency Parsing with Pointer Networks\',93.4,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PAS,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',94.6,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2018-07,Dozat et al. (2018) model in \'Simpler but More Accurate Semantic Dependency Parsing\',93.7,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2019-06,MFVI model in \'Second-Order Semantic Dependency Parsing with End-to-End Neural Networks\',94.0,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2020-07,Fernández-González & Gómez-Rodríguez (2020) model in \'Transition-based Semantic Dependency Parsing with Pointer Networks\',94.4,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',95.6,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2018-07,Dozat et al. (2018) model in \'Simpler but More Accurate Semantic Dependency Parsing\',88.9,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2019-06,MFVI model in \'Second-Order Semantic Dependency Parsing with End-to-End Neural Networks\',89.7,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2020-07,Fernández-González & Gómez-Rodríguez (2020) model in \'Transition-based Semantic Dependency Parsing with Pointer Networks\',91.0,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,DM,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',92.6,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2018-07,Dozat et al. (2018) model in \'Simpler but More Accurate Semantic Dependency Parsing\',81.0,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2019-06,MFVI model in \'Second-Order Semantic Dependency Parsing with End-to-End Neural Networks\',81.4,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2020-07,Fernández-González & Gómez-Rodríguez (2020) model in \'Transition-based Semantic Dependency Parsing with Pointer Networks\',82.6,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',83.8,In-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2018-07,Dozat et al. (2018) model in \'Simpler but More Accurate Semantic Dependency Parsing\',79.4,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2019-06,MFVI model in \'Second-Order Semantic Dependency Parsing with End-to-End Neural Networks\',79.6,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2020-07,Fernández-González & Gómez-Rodríguez (2020) model in \'Transition-based Semantic Dependency Parsing with Pointer Networks\',82.0,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic dependency parsing,PSD,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',83.4,Out-of-domain,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WebQuestionsSP,2016-08,"STAGG (Yih et al., 2016) model in \'The Value of Semantic Parse Labeling for Knowledge Base Question Answering\'",63.9,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WebQuestionsSP,2021-01,NSM+h model in \'Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals\',74.3,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"AMR (chinese, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',45.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"AMR (chinese, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',80.52,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,ATIS,2014-11,"ZH15 (Zhao and Huang, 2015) model in \'Type-Driven Incremental Semantic Parsing with Polymorphism\'",84.2,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,ATIS,2017-04,"ASN (Rabinovich et al., 2017) model in \'Abstract Syntax Networks for Code Generation and Semantic Parsing\'",85.3,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,ATIS,2018-10,Tranx model in \'TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation\',86.2,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,Geo,2018-05,coarse2fine model in \'Coarse-to-Fine Decoding for Neural Semantic Parsing\',88.2,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,spider,2018-09,Exact Set Matching model in \'Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task\',19.7,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,spider,2019-11,RATSQL + BERT model in \'RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers\',65.6,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,spider,2020-09,RATSQL + Grammar-Augmented Pre-Training model in \'GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing\',69.6,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,spider,2020-12,RATSQL + GAP model in \'Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training\',69.7,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,spider,2021-09,T5-3B + PICARD model in \'PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models\',71.9,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"PTG (czech, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',58.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"PTG (czech, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',92.24,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"PTG (czech, MRP 2020)",2021-05,"PERIN + RobeCzech model in \'RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model\'",92.36,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"DRG (english, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',63.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"DRG (english, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',94.16,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"DRG (german, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',62.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"DRG (german, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',89.83,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"EDS (english, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',80.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"EDS (english, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',92.73,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"PTG (english, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',54.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"PTG (english, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',89.19,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"UCCA (english, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',73.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"UCCA (english, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',76.4,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"AMR (english, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',52.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"AMR (english, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',80.23,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,HotpotQA,2021-09,ReasonBERTB model in \'ReasonBERT: Pre-trained to Reason with Distant Supervision\',22.4,F1-Score,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,SQA,2020-04,TAPAS-Large model in \'TAPAS: Weakly Supervised Table Parsing via Pre-training\',67.2,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,SQA,2021-07,TAPEX-Large model in \'TAPEX: Table Pre-training via Learning a Neural SQL Executor\',74.5,Denotation Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"UCCA (german, MRP 2020)",2020-10,HUJI-KU model in \'HUJI-KU at MRP~2020: Two Transition-based Neural Parsers\',75.0,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,"UCCA (german, MRP 2020)",2020-11,PERIN model in \'ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN\',81.01,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiTableQuestions,2019-09,Structured Attention model in \'Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs\',43.7,Accuracy (Dev),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiTableQuestions,2020-05,MAPO + TABERTLarge (K = 3) model in \'TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data\',52.2,Accuracy (Dev),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiTableQuestions,2021-07,TAPEX-Large model in \'TAPEX: Table Pre-training via Learning a Neural SQL Executor\',57.0,Accuracy (Dev),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiTableQuestions,2019-09,Structured Attention model in \'Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs\',44.5,Accuracy (Test),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiTableQuestions,2020-04,TAPAS-Large (pre-trained on SQA) model in \'TAPAS: Weakly Supervised Table Parsing via Pre-training\',48.8,Accuracy (Test),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiTableQuestions,2020-05,MAPO + TABERTLarge (K = 3) model in \'TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data\',51.8,Accuracy (Test),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiTableQuestions,2021-07,TAPEX-Large model in \'TAPEX: Table Pre-training via Learning a Neural SQL Executor\',57.5,Accuracy (Test),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiSQL,2019-10,NL2SQL-BERT model in \'Content Enhanced BERT-based Text-to-SQL Generation\',89.0,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiSQL,2020-04,TAPAS-Large (weak supervision) model in \'TAPAS: Weakly Supervised Table Parsing via Pre-training\',83.6,Denotation accuracy (test),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,WikiSQL,2021-07,TAPEX-Large (weak supervision) model in \'TAPEX: Table Pre-training via Learning a Neural SQL Executor\',89.5,Denotation accuracy (test),pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,complexWebQuestions-V1.0,2019-07,HSP model in \'Complex Question Decomposition for Semantic Parsing\',66.18,EM,pos
Natural language analysis,Semantic analysis,Semantic parsing,Semantic parsing,GraphQuestions,2021-09,ReasonBERTR model in \'ReasonBERT: Pre-trained to Reason with Distant Supervision\',41.3,F1 Score,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,CoNLL 2019,2019-11,Transition-based  (+BERT + Efficient Training + Effective Encoding) model in \'HIT-SCIR at MRP 2019: A Unified Pipeline for Meaning Representation Parsing via Efficient Training and Effective Encoding\',66.7,Full UCCA F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,CoNLL 2019,2019-11,Transition-based  (+BERT + Efficient Training + Effective Encoding) model in \'HIT-SCIR at MRP 2019: A Unified Pipeline for Meaning Representation Parsing via Efficient Training and Effective Encoding\',81.7,Full MRP F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,CoNLL 2019,2019-11,Transition-based (+BERT) model in \'TUPA at MRP 2019: A Multi-Task Baseline System\',65.9,LPP UCCA F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,CoNLL 2019,2019-11,Transition-based  (+BERT + Efficient Training + Effective Encoding) model in \'HIT-SCIR at MRP 2019: A Unified Pipeline for Meaning Representation Parsing via Efficient Training and Effective Encoding\',82.6,LPP MRP F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,SemEval 2019 Task 1,2017-04,Transition-based model in \'A Transition-Based Directed Acyclic Graph Parser for UCCA\',72.8,English-Wiki (open) F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,SemEval 2019 Task 1,2018-05,Transition-based + MTL model in \'Multitask Parsing Across Semantic Representations\',73.5,English-Wiki (open) F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,SemEval 2019 Task 1,2019-06,Constituent Tree Parsing (+BERT) model in \'HLT@SUDA at SemEval-2019 Task 1: UCCA Graph Parsing as Constituent Tree Parsing\',80.5,English-Wiki (open) F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,SemEval 2019 Task 1,2017-04,Transition-based model in \'A Transition-Based Directed Acyclic Graph Parser for UCCA\',67.2,English-20K (open) F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,SemEval 2019 Task 1,2018-05,Transition-based + MTL model in \'Multitask Parsing Across Semantic Representations\',68.4,English-20K (open) F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,UCCA parsing,SemEval 2019 Task 1,2019-06,Constituent Tree Parsing (+BERT) model in \'HLT@SUDA at SemEval-2019 Task 1: UCCA Graph Parsing as Constituent Tree Parsing\',76.7,English-20K (open) F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Unsupervised semantic parsing,VG graph-text,2019-04,GT-BT (composed noise) model in \'An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing\',21.7,F1,pos
Natural language analysis,Semantic analysis,Semantic parsing,Unsupervised semantic parsing,WebNLG v2.1,2019-04,GT-BT (sampled noise) model in \'An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing\',39.1,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Predicate detection,CoNLL 2005,2017-07,DeepSRL model in \'Deep Semantic Role Labeling: What Works and What\'s Next\',96.4,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Predicate detection,CoNLL 2005,2018-04,LISA model in \'Linguistically-Informed Self-Attention for Semantic Role Labeling\',98.4,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Predicate detection,CoNLL 2012,2018-04,LISA model in \'Linguistically-Informed Self-Attention for Semantic Role Labeling\',97.2,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2017-07,He et al. model in \'Deep Semantic Role Labeling: What Works and What\'s Next\',81.7,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2017-12,Tan et al. model in \'Deep Semantic Role Labeling with Self-Attention\',82.7,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2018-02,"He et al., 2017 + ELMo model in \'Deep contextualized word representations\'",84.6,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2018-05,"He et al., model in \'Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling\'",85.5,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2018-10,BiLSTM-Span (Ensemble) model in \'A Span Selection Model for Semantic Role Labeling\',87.0,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2021-06,RoBERTa+RegCCRF model in \'Constraining Linear-chain CRFs to Regular Languages\',87.51,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2021-09,MRC-SRL model in \'An MRC Framework for Semantic Role Labeling\',88.3,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,OntoNotes,2021-10,CRF2o + RoBERTa model in \'Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures Inside Arguments\',88.32,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,CoNLL-2009,2021-01,Ours (High-Order model) model in \'End-to-end Semantic Role Labeling with Neural Transition-based Model\',90.2,F1 (Arg.),pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,CoNLL-2009,2021-01,Ours (High-Order model) model in \'End-to-end Semantic Role Labeling with Neural Transition-based Model\',95.5,F1 (Prd.),pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,CoNLL 2005,2018-04,LISA model in \'Linguistically-Informed Self-Attention for Semantic Role Labeling\',86.04,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,CoNLL 2005,2018-10,"BiLSTM-Span (Ensemble, predicates given) model in \'A Span Selection Model for Semantic Role Labeling\'",88.5,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,CoNLL 2005,2021-04,Mohammadshahi and Henderson (2021) model in \'Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling\',88.93,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling,CoNLL 2005,2021-09,MRC-SRL model in \'An MRC Framework for Semantic Role Labeling\',90.0,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling (predicted predicates),CoNLL 2012,2018-04,LISA + ELMo model in \'Linguistically-Informed Self-Attention for Semantic Role Labeling\',83.38,F1,pos
Natural language analysis,Semantic analysis,Semantic role labeling,Semantic role labeling (predicted predicates),CoNLL 2005,2018-04,LISA + ELMo model in \'Linguistically-Informed Self-Attention for Semantic Role Labeling\',86.9,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,MRPC,2020-09,PAR BERT Base model in \'Pay Attention when Required\',89.2,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,MSRP,2013-10,"FEAT2, TFKLD, SVM, Fine-grained features model in \'Discriminative Improvements to Distributional Sentence Similarity\'",80.41,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,MSRP,2013-10,"FEAT2, TFKLD, SVM, Fine-grained features model in \'Discriminative Improvements to Distributional Sentence Similarity\'",85.96,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2017-02,BiMPM model in \'Bilateral Multi-Perspective Matching for Natural Language Sentences\',88.17,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2017-04,pt-DecAtt model in \'Neural Paraphrase Identification of Questions with Noisy Pretraining\',88.4,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2017-09,DIIN model in \'Natural Language Inference over Interaction Space\',89.06,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2018-07,MwAN  model in \'Multiway Attention Networks for Modeling Sentence Pairs\',89.12,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2018-10,Snorkel MeTaL(ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',89.9,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2019-06,XLNet-Large (ensemble) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',90.3,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',90.7,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2020-12,RealFormer model in \'RealFormer: Transformer Likes Residual Attention\',91.34,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2021-06,Charformer-Tall model in \'Charformer: Fast Character Transformers via Gradient-based Subword Tokenization\',91.4,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2022-02,"data2vec model in \'data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language\'",92.4,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2018-10,Snorkel MeTaL(ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',73.1,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2019-06,XLNet-Large (ensemble) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',74.2,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',74.4,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2020-12,RealFormer model in \'RealFormer: Transformer Likes Residual Attention\',88.28,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2021-04,EFL model in \'Entailment as Few-Shot Learner\',89.2,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2020-12,BERT-Base model in \'Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\',8030.0,Structure Aware Intrinsic Dimension,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,Quora Question Pairs,2020-12,BERT-Base model in \'Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\',9295.0,Direct Intrinsic Dimension,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,2017_test set,2018-06,"CNN model in \'Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering\'",50.0,10 fold Cross validation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,WikiHop,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',90.7,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,PIT,2021-04,TSDAE model in \'TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning\',69.2,AP,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,TURL,2021-04,TSDAE model in \'TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning\',76.8,AP,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Paraphrase identification,AP,2021-06,RoBETRa base model in \'Improving Paraphrase Detection with the Adversarial Paraphrasing Task\',0.525,MCC,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Question similarity,Q2Q Arabic Benchmark,2019-12,Tha3aroon model in \'Tha3aroon at NSURL-2019 Task 8: Semantic Question Similarity in Arabic\',0.94848,F1 score,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Question similarity,Q2Q Arabic Benchmark,2020-04,Ensemble multilingual BERT model model in \'The Inception Team at NSURL-2019 Task 8: Semantic Question Similarity in Arabic\',0.95924,F1 score,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC Dev,2019-09,TinyBERT (M=6;d\'=768;d\'i=3072) model in \'TinyBERT: Distilling BERT for Natural Language Understanding\',86.3,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC Dev,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',91.2,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2013-10,TF-KLD model in \'Discriminative Improvements to Distributional Sentence Similarity\',80.4,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',88.2,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',90.8,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',92.3,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2019-09,ALBERT model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',93.4,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2019-11,SMART-RoBERTa Large model in \'SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\',93.7,Accuracy,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2013-10,TF-KLD model in \'Discriminative Improvements to Distributional Sentence Similarity\',85.9,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',93.6,F1,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2020-12,BERT-Base model in \'Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\',1608.0,Structure Aware Intrinsic Dimension,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,MRPC,2020-12,BERT-Base model in \'Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\',1861.0,Direct Intrinsic Dimension,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,SentEval,2017-05,InferSent model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',0.884,SICK-R,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,SentEval,2018-03,GenSen model in \'Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\',0.888,SICK-R,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,SentEval,2017-05,InferSent model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',86.3,SICK-E,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,SentEval,2018-03,GenSen model in \'Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\',87.8,SICK-E,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS13,2019-08,SBERT-NLI-large model in \'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\',0.7846,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS13,2021-04,SimCSE-RoBERTa-large model in \'SimCSE: Simple Contrastive Learning of Sentence Embeddings\',0.8727,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS13,2021-09,Trans-Encoder-BERT-large-bi (unsup.) model in \'Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations\',0.8851,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS13,2022-03,DCPCSE-RoBERTa-large model in \'Deep Continuous Prompt for Contrastive Learning of Sentence Embeddings\',0.8864,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS15,2019-08,SRoBERTa-NLI-large model in \'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\',0.8185,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS15,2021-04,SimCSE-RoBERTalarge model in \'SimCSE: Simple Contrastive Learning of Sentence Embeddings\',0.8666,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS15,2021-09,Trans-Encoder-RoBERTa-large-cross (unsup.) model in \'Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations\',0.8863,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS12,2019-08,SRoBERTa-NLI-large model in \'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\',0.7453,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS12,2021-04,SimCSE-RoBERTa-large model in \'SimCSE: Simple Contrastive Learning of Sentence Embeddings\',0.7746,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS12,2021-09,Trans-Encoder-RoBERTa-large-cross (unsup.) model in \'Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations\',0.7828,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS12,2022-03,DCPCSE-RoBERTa-large model in \'Deep Continuous Prompt for Contrastive Learning of Sentence Embeddings\',0.7914,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2018-03,USE_T model in \'Universal Sentence Encoder\',0.782,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',0.832,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',0.925,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',0.928,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2019-11,SMART-RoBERTa Large model in \'SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\',0.929,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',0.924,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2019-11,SMART-RoBERTa Large model in \'SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\',0.925,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS Benchmark,2021-11,Mnet-Sim model in \'MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity\',0.931,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,SICK,2019-08,SRoBERTa-NLI-base model in \'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\',0.7446,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,SICK,2021-04,SimCSE-RoBERTalarge model in \'SimCSE: Simple Contrastive Learning of Sentence Embeddings\',0.8195,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,SICK,2022-03,DCPCSE-RoBERTa-large model in \'Deep Continuous Prompt for Contrastive Learning of Sentence Embeddings\',0.8207,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS14,2019-08,SBERT-NLI-large model in \'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\',0.7490000000000001,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS14,2021-04,SimCSE-RoBERTalarge model in \'SimCSE: Simple Contrastive Learning of Sentence Embeddings\',0.8236,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS14,2022-03,DCPCSE-RoBERTa-large model in \'Deep Continuous Prompt for Contrastive Learning of Sentence Embeddings\',0.8373,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS16,2019-08,SRoBERTa-NLI-large model in \'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\',0.7682,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS16,2020-11,BERTlarge-flow (target) model in \'On the Sentence Embeddings from Pre-trained Language Models\',0.7763,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS16,2021-04,SimCSE-RoBERTalarge model in \'SimCSE: Simple Contrastive Learning of Sentence Embeddings\',0.8393,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Semantic textual similarity,Semantic textual similarity,STS16,2021-09,Trans-Encoder-RoBERTa-large-cross (unsup.) model in \'Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations\',0.8503,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (original corpus),2019-10,"BioBERT\n(pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, original corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",89.75,F1,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (original corpus),2019-10,"BioBERT\n(pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, original corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",88.93,Precision,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (original corpus),2019-10,"SciBERT cased\n(SciVocab, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, original corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",91.53,Recall,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,MedSTS,2019-06,NCBI_BERT(base) (P+M) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',0.848,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015) model in \'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\'",0.8676,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015) model in \'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\'",-0.2532,MSE,neg
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015) model in \'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\'",0.8083,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,ClinicalSTS,2020-10,"CharacterBERT (base, medical, ensemble) model in \'CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters\'",85.62,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (expanded corpus),2019-10,"BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",93.38,F1,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (expanded corpus),2019-10,"BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",92.98,Precision,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (expanded corpus),2019-10,"BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",93.85,Recall,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,BIOSSES,2019-06,NCBI_BERT(base) (P+M) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',0.916,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,BIOSSES,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',0.9363,Pearson Correlation,pos
Natural language analysis,Semantic analysis,Sentence pair modeling,Semantic similarity estimation,CHIP-STS,2021-06,MacBERT-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',85.6,Macro F1,pos
Natural language analysis,Semantic analysis,Text classification,Automated Essay Scoring,ASAP,2015-09,FDA model in \'Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression\',0.705,Quadratic Weighted Kappa,pos
Natural language analysis,Semantic analysis,Text classification,Automated Essay Scoring,ASAP,2016-11,AF model in \'Automatic Features for Essay Scoring -- An Empirical Study\',0.734,Quadratic Weighted Kappa,pos
Natural language analysis,Semantic analysis,Text classification,Automated Essay Scoring,ASAP,2017-11,SkipFlow model in \'SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring\',0.764,Quadratic Weighted Kappa,pos
Natural language analysis,Semantic analysis,Text classification,Automated Essay Scoring,ASAP,2018-04,HISK+BOSWE model in \'Automated essay scoring with string kernels and word embeddings\',0.785,Quadratic Weighted Kappa,pos
Natural language analysis,Semantic analysis,Text classification,Automated Essay Scoring,ASAP,2022-05,Tran-BERT-MS-ML-R model in \'On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation\',0.791,Quadratic Weighted Kappa,pos
Natural language analysis,Semantic analysis,Text classification,Citation intent classification,ACL-ARC,2013-06,SVM model in \'Purpose and Polarity of Citation: Towards NLP-based Bibliometrics\',41.0,F1,pos
Natural language analysis,Semantic analysis,Text classification,Citation intent classification,ACL-ARC,2016-06,BiLSTM-Attention model in \'Hierarchical Attention Networks for Document Classification\',51.8,F1,pos
Natural language analysis,Semantic analysis,Text classification,Citation intent classification,ACL-ARC,2018-01,Feature-rich Random Forest model in \'Measuring the Evolution of a Scientific Field through Citation Frames\',53.0,F1,pos
Natural language analysis,Semantic analysis,Text classification,Citation intent classification,ACL-ARC,2018-02,BiLSTM-Attention + ELMo model in \'Deep contextualized word representations\',54.6,F1,pos
Natural language analysis,Semantic analysis,Text classification,Citation intent classification,ACL-ARC,2019-04,Structural-scaffolds model in \'Structural Scaffolds for Citation Intent Classification in Scientific Publications\',67.9,F1,pos
Natural language analysis,Semantic analysis,Text classification,Citation intent classification,SciCite,2019-03,SciBERT model in \'SciBERT: A Pretrained Language Model for Scientific Text\',84.99,F1,pos
Natural language analysis,Semantic analysis,Text classification,Clinical assertion status detection,2010 i2b2/VA,2020-12,BiLSTM (SparkNLP) model in \'Improving Clinical Document Understanding on COVID-19 Research with Spark NLP\',0.939,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Coherence evaluation,GCDC + RST - Accuracy,2018-05,"ParSeq model in \'Discourse Coherence in the Wild: A Dataset, Evaluation and Methods\'",55.09,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Coherence evaluation,GCDC + RST - Accuracy,2020-09,RST-Ensemble model in \'Neural RST-based Evaluation of Discourse Coherence\',55.39,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Coherence evaluation,GCDC + RST - Accuracy,2021-09,MTL with Transformer model in \'Transformer Models for Text Coherence Assessment\',61.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Coherence evaluation,GCDC + RST - F1,2018-05,"ParSeq model in \'Discourse Coherence in the Wild: A Dataset, Evaluation and Methods\'",46.65,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Coherence evaluation,GCDC + RST - F1,2020-09,RST-Ensemble model in \'Neural RST-based Evaluation of Discourse Coherence\',46.98,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Context-specific Spam Detection,Traditional and Context-specific Spam Twitter,2022-06,BERT model in \'Traditional and context-specific spam detection in low resource settings\',0.8408,Avg F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Recipe,2019-04,ApproxRepSet model in \'Rep the Set: Neural Networks for Learning Set Representations\',59.06,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Reuters En-De,2014-10,BilBOWA model in \'BilBOWA: Fast Bilingual Distributed Representations without Word Alignments\',86.5,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Classic,2019-04,ApproxRepSet model in \'Rep the Set: Neural Networks for Learning Set Representations\',96.24,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Classic,2019-12,REL-RWMD k-NN model in \'Speeding up Word Mover\'s Distance and its variants via properties of distances between embeddings\',96.85,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,WOS-5736,2017-09,HDLTex model in \'HDLTex: Hierarchical Deep Learning for Text Classification\',90.93,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Yelp-14,2019-04,KD-LSTMreg model in \'DocBERT: BERT for Document Classification\',69.4,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,AAPD,2019-04,KD-LSTMreg model in \'DocBERT: BERT for Document Classification\',72.9,F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,SciDocs (MAG),2020-04,SPECTER model in \'SPECTER: Document-level Representation Learning using Citation-informed Transformers\',82.0,F1 (micro),pos
Natural language analysis,Semantic analysis,Text classification,Document classification,SciDocs (MeSH),2020-04,SPECTER model in \'SPECTER: Document-level Representation Learning using Citation-informed Transformers\',86.4,F1 (micro),pos
Natural language analysis,Semantic analysis,Text classification,Document classification,SciDocs (MeSH),2022-02,SciNCL model in \'Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings\',88.7,F1 (micro),pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Reuters De-En,2014-10,BilBOWA model in \'BilBOWA: Fast Bilingual Distributed Representations without Word Alignments\',75.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Reuters-21578,2019-04,ApproxRepSet model in \'Rep the Set: Neural Networks for Learning Set Representations\',97.17,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Reuters-21578,2019-08,MPAD-path model in \'Message Passing Attention Networks for Document Understanding\',97.44,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Reuters-21578,2019-02,VLAWE model in \'Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation\',89.3,F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Reuters-21578,2020-02,MAGNET model in \'MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network\',89.9,F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,HOC,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',82.32,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,HOC,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',84.87,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,HOC,2019-06,NCBI_BERT(large) (P) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',87.3,F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,HOC,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',88.1,F1,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Twitter,2019-04,ApproxRepSet model in \'Rep the Set: Neural Networks for Learning Set Representations\',72.6,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Cora,2014-03,DeepWalk model in \'DeepWalk: Online Learning of Social Representations\',67.2,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Cora,2016-03,Planetoid* model in \'Revisiting Semi-Supervised Learning with Graph Embeddings\',75.7,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Cora,2016-09,Graph-CNN model in \'Semi-Supervised Classification with Graph Convolutional Networks\',81.5,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Cora,2016-11,MoNet model in \'Geometric deep learning on graphs and manifolds using mixture model CNNs\',81.7,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Cora,2017-10,GAT model in \'Graph Attention Networks\',83.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Cora,2018-08,LGCN model in \'Large-Scale Learnable Graph Convolutional Networks\',83.3,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Cora,2019-04,ACNet model in \'Adaptively Connected Neural Networks\',83.5,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,BBCSport,2019-04,ApproxRepSet model in \'Rep the Set: Neural Networks for Learning Set Representations\',95.73,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,BBCSport,2019-08,MPAD-path model in \'Message Passing Attention Networks for Document Understanding\',99.59,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,WOS-46985,2017-09,HDLTex model in \'HDLTex: Hierarchical Deep Learning for Text Classification\',76.58,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,WOS-11967,2017-09,HDLTex model in \'HDLTex: Hierarchical Deep Learning for Text Classification\',86.07,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,IMDb-M,2019-06,LSTM-reg (single model) model in \'Rethinking Complex Neural Network Architectures for Document Classification\',52.8,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,IMDb-M,2021-03,Document Classification Using Importance of Sentences model in \'Improving Document-Level Sentiment Classification Using Importance of Sentences\',54.8,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,MPQA,2019-08,MPAD-path model in \'Message Passing Attention Networks for Document Understanding\',89.81,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document classification,Amazon,2019-04,ApproxRepSet model in \'Rep the Set: Neural Networks for Learning Set Representations\',94.31,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document text classification,Tobacco-3482,2020-04,Optimized Text CNN model in \'Light-Weighted CNN for Text Classification\',46.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document text classification,Tobacco-3482,2020-04,Optimized Text CNN model in \'Light-Weighted CNN for Text Classification\',2.0,Training time (hours),pos
Natural language analysis,Semantic analysis,Text classification,Document text classification,Food-101,2020-12,Bert model in \'Image and Text fusion for UPMC Food-101 \\\\using BERT and CNNs\',84.41,Accuracy (%),pos
Natural language analysis,Semantic analysis,Text classification,Document text classification,Tobacco small-3482,2020-04,Optimized Text CNN model in \'Light-Weighted CNN for Text Classification\',84.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Document text classification,Tobacco small-3482,2020-04,Optimized Text CNN model in \'Light-Weighted CNN for Text Classification\',9.0,Training time (min),pos
Natural language analysis,Semantic analysis,Text classification,Document text classification,CUB-200-2011,2020-01,Bert model in \'Are These Birds Similar: Learning Branched Networks for Fine-grained Representations\',65.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Domain labelling,BabelDomains,2021-01,A2T model in \'Ask2Transformers: Zero-Shot Domain labelling with Pre-trained Language Models\',92.14,F1-Score,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,EWALK,2020-03,ProxEmo (ours) model in \'ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation\',82.4,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,MFA,2021-12,MLKNN model in \'The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)\',0.34,F-F1 score (Comb.),pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,MFA,2021-12,MLKNN model in \'The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)\',0.4,F-F1 score (Persian),pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,MFA,2021-12,MLKNN model in \'The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)\',0.39,V-F1 score (Comb.),pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,MFA,2021-12,MLKNN model in \'The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)\',0.42,V-F1 score (NA),pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,MFA,2021-12,MLKNN model in \'The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)\',0.42,F-F1 score (NA),pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,MFA,2021-12,MLKNN model in \'The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)\',0.4,V-F1 score (Persian),pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,ROCStories,2018-05,NPN + Explanation Training model in \'Modeling Naive Psychology of Characters in Simple Commonsense Stories\',30.29,F1,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,ROCStories,2020-06,Semi-supervision model in \'Modeling Label Semantics for Predicting Emotional Reactions\',65.88,F1,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,GoEmotions,2020-05,BERT model in \'GoEmotions: A Dataset of Fine-Grained Emotions\',46.0,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,SemEval 2018 Task 1E-c,2019-11,BERT+DK model in \'Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge\',0.591,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,SemEval 2018 Task 1E-c,2021-01,SpanEmo model in \'SpanEmo: Casting Multi-label Emotion Classification as Span-prediction\',0.601,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,SemEval 2018 Task 1E-c,2019-11,BERT+DK model in \'Improving Multi-label Emotion Classification by Integrating both General and Domain-specific Knowledge\',0.713,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,SemEval 2018 Task 1E-c,2018-12,Transformer (finetune) model in \'Practical Text Classification With Large Pre-Trained Language Models\',0.561,Macro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,SemEval 2018 Task 1E-c,2020-08,BERT-GCN model in \'EmoGraph: Capturing Emotion Correlations using Graph Networks\',0.563,Macro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Emotion classification,SemEval 2018 Task 1E-c,2021-01,SpanEmo model in \'SpanEmo: Casting Multi-label Emotion Classification as Span-prediction\',0.578,Macro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,ODIC 5-way (5-shot),2019-02,Induction Networks model in \'Induction Networks for Few-Shot Text Classification\',87.16,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,ODIC 5-way (10-shot),2019-02,Induction Networks model in \'Induction Networks for Few-Shot Text Classification\',88.49,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,ODIC 10-way (5-shot),2019-02,Induction Networks model in \'Induction Networks for Few-Shot Text Classification\',78.27,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,ODIC 10-way (10-shot),2019-02,Induction Networks model in \'Induction Networks for Few-Shot Text Classification\',81.64,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.83,ADE,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.897,TC,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.735,Avg,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2022-05,T-Few model in \'Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\',0.758,Avg,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.607,B77,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2022-05,T-Few model in \'Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\',0.695,B77,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.857,NIS,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.646,OSE,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2022-05,T-Few model in \'Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\',0.676,OSE,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,GPT-3 model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.937,Over,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2022-05,T-Few model in \'Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\',0.95,Over,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.908,SOT,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2022-05,T-Few model in \'Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\',0.915,SOT,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,GPT-3 model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.516,SRI,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,GPT-3 model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.656,TAI,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2022-05,T-Few model in \'Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\',0.736,TAI,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.627,ToS,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2022-05,T-Few model in \'Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\',0.75,ToS,pos
Natural language analysis,Semantic analysis,Text classification,Few-Shot Text Classification,RAFT,2021-09,Human (crowdsourced) model in \'RAFT: A Real-World Few-Shot Text Classification Benchmark\',0.722,TEH,pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,ANIMAL,2019-06,SELFIE model in \'SELFIE: Refurbishing Unclean Samples for Robust Deep Learning\',81.8,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,ANIMAL,2021-03,PLC model in \'Learning with Feature-Dependent Label Noise: A Progressive Approach\',83.4,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,ANIMAL,2021-04,NestedCoTeaching model in \'Boosting Co-teaching with Compression Regularization for Label Noise\',84.1,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,ANIMAL,2021-11,S3 model in \'S3: Supervised Self-supervised Learning under Label Noise\',88.5,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,Chaoyang,2021-12,HSANR model in \'Hard Sample Aware Noise Robust Learning for Histopathology Image Classification\',83.4,ACCURACY,pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random3,2016-09,Forward-T model in \'Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\',87.04,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random3,2018-04,Co-Teaching model in \'Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\',90.15,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random3,2020-06,ELR+ model in \'Early-Learning Regularization Prevents Memorization of Noisy Labels\',94.34,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random3,2020-10,CORES* model in \'Learning with Instance-Dependent Label Noise: A Sample Sieve Approach\',94.74,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random2,2016-09,Backward-T model in \'Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\',86.28,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random2,2018-04,Co-Teaching model in \'Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\',90.3,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random2,2020-02,Divide-Mix model in \'DivideMix: Learning with Noisy Labels as Semi-supervised Learning\',90.9,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random2,2020-06,ELR+ model in \'Early-Learning Regularization Prevents Memorization of Noisy Labels\',94.2,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random2,2020-10,CORES* model in \'Learning with Instance-Dependent Label Noise: A Sample Sieve Approach\',94.88,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Worst,2016-09,Forward-T model in \'Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\',79.79,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Worst,2018-04,Co-Teaching model in \'Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\',83.83,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Worst,2020-02,Divide-Mix model in \'DivideMix: Learning with Noisy Labels as Semi-supervised Learning\',92.56,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random1,2016-09,Backward-T model in \'Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\',87.14,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random1,2018-04,Co-Teaching model in \'Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\',90.33,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random1,2020-06,ELR+ model in \'Early-Learning Regularization Prevents Memorization of Noisy Labels\',94.43,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Random1,2020-10,CORES* model in \'Learning with Instance-Dependent Label Noise: A Sample Sieve Approach\',94.45,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Aggregate,2016-09,Forward-T model in \'Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\',88.24,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Aggregate,2018-04,Co-Teaching model in \'Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\',91.2,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Aggregate,2020-03,Positive-LS model in \'Does label smoothing mitigate label noise?\',91.57,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Aggregate,2020-06,ELR+ model in \'Early-Learning Regularization Prevents Memorization of Noisy Labels\',94.83,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-10N-Aggregate,2020-10,CORES* model in \'Learning with Instance-Dependent Label Noise: A Sample Sieve Approach\',95.25,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-100N,2016-09,Backward-T model in \'Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\',57.14,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-100N,2018-04,Co-Teaching model in \'Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\',60.37,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Learning with noisy labels,CIFAR-100N,2020-02,Divide-Mix model in \'DivideMix: Learning with Noisy Labels as Semi-supervised Learning\',71.13,Accuracy (mean),pos
Natural language analysis,Semantic analysis,Text classification,Multi-label classification of biomedical texts,MIMIC-III,2020-07,Convolutional Neural Network with per-label Attention model in \'Predicting Multiple ICD-10 Codes from Brazilian-Portuguese Clinical Notes\',0.537,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People's Republic of China,2021-06,Bert model in \'Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People\'s Republic of China\',0.85431,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People's Republic of China,2021-06,Bert model in \'Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People\'s Republic of China\',0.80352,1:1 Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People's Republic of China,2021-06,Bert model in \'Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People\'s Republic of China\',0.20803,F1 - macro,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Amazon-12K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',94.87,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Amazon-12K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',63.16,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Amazon-12K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',87.57,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Amazon-12K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',79.16,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Amazon-12K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',89.13,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III-50,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',64.1,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III-50,2021-06,D2SBERT using Sequence Attention model in \'Medical Code Prediction from Discharge Summary: Document to Sequence BERT using Sequence Attention\',68.555,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,LF-AmzonTitles-131K,2021-07,ECLARE model in \'ECLARE: Extreme Classification with Label Graph Correlations\',40.74,Precision@1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Reuters-21578,2019-02,VLAWE model in \'Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation\',89.3,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Reuters-21578,2020-02,MAGNET model in \'MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network\',89.9,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Reuters-21578,2021-09,CB-NTR model in \'Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution\',90.74,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,LF-AmazonTitles-131K,2021-07,DECAF model in \'ECLARE: Extreme Classification with Label Graph Correlations\',38.4,Precision@1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,RCV1-v2,2020-02,MAGNET model in \'MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network\',88.5,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Wiki-30K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',84.18,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Wiki-30K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',62.87,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Wiki-30K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',67.82,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Wiki-30K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',73.14,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Wiki-30K,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',75.64,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,SVICTOR (theme),2020-05,XGBoost model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.8887,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,SVICTOR (theme),2020-05,XGBoost model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.8634,Weighted F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',57.1,Macro F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',0.919,AUC,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2018-02,Feed-forward NN model in \'An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes\',0.249,Precision,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2018-02,Feed-forward NN model in \'An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes\',0.1138,Recall,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',62.5,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',40.7,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',65.0,Macro Precision,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',51.0,Macro Recall,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',72.9,Micro Precision,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MIMIC-III,2020-10,HLAN model in \'Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation\',57.3,Micro Recall,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,USPTO-3M,2019-05,BERT model in \'PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model\',66.83,F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Slashdot,2020-02,MAGNET model in \'MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network\',56.8,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Kan-Shan Cup,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',54.38,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Kan-Shan Cup,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',25.88,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Kan-Shan Cup,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',54.65,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Kan-Shan Cup,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',34.6,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Kan-Shan Cup,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',51.7,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2020-08,LW-PT model in \'Label-Wise Document Pre-Training for Multi-Label Text Classification\',0.728,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',84.48,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-11,LSAN model in \'Label-Specific Document Representation for Multi-Label Text Classification\',85.28,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2020-02,MAGNET model in \'MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network\',69.6,F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',41.19,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-11,LSAN model in \'Label-Specific Document Representation for Multi-Label Text Classification\',41.84,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',83.7,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-11,LSAN model in \'Label-Specific Document Representation for Multi-Label Text Classification\',84.78,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',60.72,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-11,LSAN model in \'Label-Specific Document Representation for Multi-Label Text Classification\',61.12,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',80.11,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,AAPD,2019-11,LSAN model in \'Label-Specific Document Representation for Multi-Label Text Classification\',80.84,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MVICTOR (theme),2020-05,XGBoost model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.8882,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,MVICTOR (theme),2020-05,XGBoost model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.9072,Weighted F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,Freecode,2020-10,TagBERT model in \'Tag Recommendation for Online Q&A Communities based on BERT Pre-Training Technique\',46.0,F1-score,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,BVICTOR,2020-05,XGBoost model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.8843,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,BVICTOR,2020-05,XGBoost model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.8957,Weighted F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,RCV1,2021-01,HiddeN model in \'Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification\',79.3,Micro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,RCV1,2021-01,HiddeN model in \'Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification\',47.3,Macro-F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,bert-base model in \'Large-Scale Multi-Label Text Classification on EU Legislation\',73.2,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',74.95,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',80.2,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',50.71,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,bert-base model in \'Large-Scale Multi-Label Text Classification on EU Legislation\',68.7,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',59.28,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,bert-base model in \'Large-Scale Multi-Label Text Classification on EU Legislation\',82.3,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',61.48,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',65.48,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-05,LAHA model in \'Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\',64.89,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',71.11,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,bert-base model in \'Large-Scale Multi-Label Text Classification on EU Legislation\',79.6,RP-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Multi-label text classification,EUR-Lex,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',80.2,nDCG-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Poem meters classification,PCD,2019-05,BiLSTM model in \'Learning meters of Arabic and English poems with Recurrent Neural Networks: a step forward for language understanding and synthesis\',96.38,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Political Salient Issue Orientation Detection,Persian Twitter,2022-01,XGBoost model in \'Tracking Legislators’ Expressed Policy Agendas in Real Time\',0.99,Average Recall,pos
Natural language analysis,Semantic analysis,Text classification,Rumor Detection,Sepehr_RumTel01,2019-01,Common context features + Four SA classes model in \'A Speech Act Classifier for Persian Texts and its Application in Identifying Rumors\',0.791,F-Measure,pos
Natural language analysis,Semantic analysis,Text classification,Rumor Detection,Sepehr_RumTel01,2020-02,Jahanbakhsh-Nagadeh et al. model in \'A Model to Measure the Spread Power of Rumors\',0.828,F-Measure,pos
Natural language analysis,Semantic analysis,Text classification,Rumor Detection,Sepehr_RumTel01,2020-11,ParsBERT+PCapsNet +SA+Title+ Auxiliary model in \'A Deep Content-Based Model for Persian Rumor Verification\',0.947,F-Measure,pos
Natural language analysis,Semantic analysis,Text classification,Semi-supervised text classification,Yahoo! Answers (800 Labels),2019-12,FlowGMM model in \'Semi-Supervised Learning with Normalizing Flows\',57.9,Accuracy (%),pos
Natural language analysis,Semantic analysis,Text classification,Semi-supervised text classification,AG News (200 Labels),2019-12,FlowGMM model in \'Semi-Supervised Learning with Normalizing Flows\',82.1,Accuracy (%),pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,SciCite,2018-01,Feature-Rich Random Forest model in \'Measuring the Evolution of a Scientific Field through Citation Frames\',79.6,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,SciCite,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',84.4,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,SciCite,2019-03,SciBERT model in \'SciBERT: A Pretrained Language Model for Scientific Text\',84.9,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,PubMed 20k RCT,2018-08,Hierarchical Neural Networks model in \'Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts\',92.6,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,Paper Field,2019-03,SciBERT (SciVocab) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',65.71,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,ACL-ARC,2018-01,Random Forest model in \'Measuring the Evolution of a Scientific Field through Citation Frames\',53.0,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,ACL-ARC,2019-03,SciBERT model in \'SciBERT: A Pretrained Language Model for Scientific Text\',70.98,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,ACL-ARC,2020-10,FE-MLM + Span model in \'Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model\',78.1,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,ScienceCite,2019-03,SciBERT (SciVocab) model in \'SciBERT: A Pretrained Language Model for Scientific Text\',84.99,F1,pos
Natural language analysis,Semantic analysis,Text classification,Sentence classification,CHIP-CTC,2021-06,RoBERTa-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',70.9,Macro F1,pos
Natural language analysis,Semantic analysis,Text classification,Spam detection,Traditional and Context-specific Spam Twitter,2022-06,BERT model in \'Traditional and context-specific spam detection in low resource settings\',0.8553,Avg F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Terms of Service,2021-04,Custom Legal-BERT model in \'When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset\',78.7,F1(10-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TREC-6,2015-04,TBCNN model in \'Discriminative Neural Sentence Modeling by Tree-Based Convolution\',4.0,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TREC-6,2015-11,C-LSTM model in \'A C-LSTM Neural Network for Text Classification\',5.4,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TREC-6,2017-02,GRU-RNN-GLOVE model in \'All-but-the-Top: Simple and Effective Postprocessing for Word Representations\',7.0,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TREC-6,2018-03,Capsule-B model in \'Investigating Capsule Networks with Dynamic Routing for Text Classification\',7.2,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TREC-6,2018-05,byte mLSTM7 model in \'A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors\',9.6,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TREC-6,2021-04,TM-Glove model in \'Enhancing Interpretable Clauses Semantically using Pretrained Word Representation\',9.96,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,AG News,2015-09,Char-level CNN model in \'Character-level Convolutional Networks for Text Classification\',9.51,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,AG News,2018-05,Seq2CNN with GWS(50) model in \'Abstractive Text Classification Using Sequence-to-convolution Neural Networks\',9.64,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,AG News,2018-08,ToWE-SG model in \'Task-oriented Word Embedding for Text Classification\',14.0,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Amazon-5,2019-04,BERT Finetune + UDA model in \'Unsupervised Data Augmentation for Consistency Training\',37.12,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,BLURB,2020-07,PubMedBERT (uncased; abstracts) model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',82.32,F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,BLURB,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',84.88,F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,MuLD (Character Type),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',82.58,F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TREC-50,2016-01,SVM model in \'Improving Question Classification by Feature Extraction and Selection\',8.4,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,SILICONE Benchmark,2020-09,Pretrained Hierarchical Transformer model in \'Hierarchical Pre-training for Sequence Labelling in Spoken Dialog\',71.25,1:1 Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RusAge: Corpus for Age-Based Text Classification,2020-09,LSVC + linguistic features + publishing attributes model in \'A Comparative Study of Feature Types for Age-Based Text Classification\',95.77,F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Facebook Media,2020-01,Our proposed method Model Averaging(D + E + F) model in \'A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts\',0.677,F1 (Hidden Test Set),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,DODF Data,2020-03,"ULMFiT (pre-trained vocab, no gradual unfreezing) model in \'Inferring the source of official texts: can SVM beat ULMFiT?\'",0.8918,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,DODF Data,2020-03,"ULMFiT (pre-trained vocab, no gradual unfreezing) model in \'Inferring the source of official texts: can SVM beat ULMFiT?\'",0.9257,Weighted F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Ohsumed,2017-07,CNN+Lowercased model in \'On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis\',36.2,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Ohsumed,2018-09,Text GCN model in \'Graph Convolutional Networks for Text Classification\',68.36,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Ohsumed,2019-02,SGC model in \'Simplifying Graph Convolutional Networks\',68.5,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Ohsumed,2019-10,Our Model* model in \'Text Level Graph Neural Network for Text Classification\',69.4,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Ohsumed,2020-03,Orthogonalized Soft VSM model in \'Text classification with word embedding regularization and soft similarity measure\',75.86,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,MR,2021-01,SSGC model in \'Simple Spectral Graph Convolution\',76.7,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,MR,2021-05,RoBERTaGCN model in \'BertGCN: Transductive Text Classification by Combining GCN and BERT\',89.7,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yelp-5,2019-01,HAHNN (CNN) model in \'Hierarchical Attentional Hybrid Neural Networks for Document Classification\',73.28,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Amazon-2,2019-04,BERT Finetune + UDA model in \'Unsupervised Data Augmentation for Consistency Training\',3.5,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Amazon-2,2019-09,ULMFiT (Small data) model in \'Sampling Bias in Deep Active Classification: An Empirical Study\',3.9,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Amazon-2,2020-03,"LHTR model in \'Heavy-tailed Representations, Text Polarity Classification & Data Augmentation\'",5.7,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,arXiv,2020-07,BigBird model in \'Big Bird: Transformers for Longer Sequences\',92.31,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,An Amharic News Text classification Dataset,2021-03,Naive Bayes using Tf-idf features model in \'An Amharic News Text classification Dataset\',62.3,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,MVICTOR (type),2020-05,CNN + CRF model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.7505,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,MVICTOR (type),2020-05,CNN + CRF model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.9537,Weighted F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2018-05,RMDL (15 RDLs) model in \'RMDL: Random Multimodel Deep Learning for Classification\',87.91,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2019-02,SGC model in \'Simplifying Graph Convolutional Networks\',88.5,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2021-01,SSGC model in \'Simple Spectral Graph Convolution\',88.6,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2021-05,RoBERTaGCN model in \'BertGCN: Transductive Text Classification by Combining GCN and BERT\',89.5,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2019-11,SCDV-MS model in \'Improving Document Classification with Multi-Sense Embeddings\',86.2,Precision,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2021-05,Sparse Tensor Classifier model in \'An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics\',87.1,Precision,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2019-11,SCDV-MS model in \'Improving Document Classification with Multi-Sense Embeddings\',86.18,Recall,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2021-05,Sparse Tensor Classifier model in \'An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics\',86.6,Recall,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',83.9,F-measure,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2019-09,NABoE-full model in \'Neural Attentive Bag-of-Entities Model for Text Classification\',86.2,F-measure,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,20NEWS,2021-05,Sparse Tensor Classifier model in \'An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics\',86.6,F-measure,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,SVICTOR (type),2020-05,CNN + CRF model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.774,Average F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,SVICTOR (type),2020-05,CNN + CRF model in \'VICTOR: a Dataset for Brazilian Legal Documents Classification\',0.9533,Weighted F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R52,2018-09,Text GCN model in \'Graph Convolutional Networks for Text Classification\',93.56,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R52,2019-02,SGCN model in \'Simplifying Graph Convolutional Networks\',94.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R52,2019-06,GraphStar model in \'Graph Star Net for Generalized Multi-Task Learning\',95.0,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R52,2021-05,1-6 BertGCN model in \'BertGCN: Transductive Text Classification by Combining GCN and BERT\',96.6,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,OneStopEnglish (Readability Assessment),2018-06,SMO (Sequential Minimal Optimization) model in \'OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification\',0.781,Accuracy (5-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,OneStopEnglish (Readability Assessment),2019-07,HAN (Hierarchical Attention Network) model in \'Supervised and Unsupervised Neural Approaches to Text Readability\',0.787,Accuracy (5-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,OneStopEnglish (Readability Assessment),2021-09,RoBERTa-RF-T1 hybrid model in \'Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features\',0.99,Accuracy (5-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yelp-2,2019-04,BERT Finetune + UDA model in \'Unsupervised Data Augmentation for Consistency Training\',97.95,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yelp-2,2019-05,BERT-ITPT-FiT model in \'How to Fine-Tune BERT for Text Classification?\',98.08,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yelp-2,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',98.63,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Sogou News,2018-10,CCCapsNet model in \'Compositional Coding Capsule Network with K-Means Routing for Text Classification\',97.25,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Sogou News,2019-05,BERT-ITPT-FiT model in \'How to Fine-Tune BERT for Text Classification?\',98.07,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,DBpedia,2015-09,Char-level CNN model in \'Character-level Convolutional Networks for Text Classification\',1.55,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,DBpedia,2018-05,Seq2CNN(50) model in \'Abstractive Text Classification Using Sequence-to-convolution Neural Networks\',2.77,Error,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,WeeBit (Readability Assessment),2019-06,SVM (Support Vector Machine) with Handcrafted Features model in \'Text Readability Assessment for Second Language Learners\',0.803,Accuracy (5-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,WeeBit (Readability Assessment),2019-07,BERT model in \'Supervised and Unsupervised Neural Approaches to Text Readability\',0.857,Accuracy (5-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,WeeBit (Readability Assessment),2021-09,BART-RF-T1 hybrid model in \'Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features\',0.905,Accuracy (5-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2016-02,oh-CNN + two LSTM tv-embed. model in \'Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings\',92.85,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-08,HiLAP (bow-CNN) model in \'Hierarchical Text Classification with Reinforced Label Assignment\',60.1,Macro F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-08,HiLAP (bow-CNN) model in \'Hierarchical Text Classification with Reinforced Label Assignment\',83.3,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2020-02,MAGNET model in \'MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network\',88.5,Micro F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',97.05,P-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',56.33,P-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',93.11,nDCG-at-5,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',81.27,P-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',92.47,nDCG-at-3,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,RCV1,2019-06,NLP-Cap model in \'Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications\',97.05,nDCG-at-1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Patents,2020-07,BigBird model in \'Big Bird: Transformers for Longer Sequences\',69.3,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,WNUT-2020 Task 2,2020-10,NutCracker model in \'NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative COVID-19 Tweets using Ensembling and Adversarial Training\',0.9096,F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TRAC2-English. Task2.,2020-05,"BERT model in \'BERT of all trades, master of some\'",0.871585052,F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,AffCon 2020 Emotion Detection,2020-06,BERT-based Ensembles model in \'BERT-based Ensembles for Modeling Disclosure and Support in Conversational Social Media Text\',0.558,F1 score,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Twitter-US,2020-01,Our proposed method Model Averaging(D + E + F) model in \'A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts\',0.648,F1 (Hidden Test Set),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Overruling,2021-04,Custom Legal-BERT model in \'When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset\',97.4,F1(10-fold),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Adverse Drug Events (ADE) Corpus,2022-01,Spark NLP model in \'Mining Adverse Drug Reactions from Unstructured Mediums at Scale\',85.96,F1 - macro,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',96.7,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2018-09,Text GCN model in \'Graph Convolutional Networks for Text Classification\',97.07,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2019-02,SGC model in \'Simplifying Graph Convolutional Networks\',97.2,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2019-06,GraphStar model in \'Graph Star Net for Generalized Multi-Task Learning\',97.4,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2019-09,NABoE-full model in \'Neural Attentive Bag-of-Entities Model for Text Classification\',97.9,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2021-05,RoBERTaGCN model in \'BertGCN: Transductive Text Classification by Combining GCN and BERT\',98.2,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2018-06,TextEnt-full model in \'Representation Learning of Entities and Documents from Knowledge Base Descriptions\',91.0,F-measure,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,R8,2019-09,NABoE-full model in \'Neural Attentive Bag-of-Entities Model for Text Classification\',91.7,F-measure,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Hyperpartisan,2020-07,BigBird model in \'Big Bird: Transformers for Longer Sequences\',92.2,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,TRAC2-Benghali. Task 2.,2020-05,"BERT model in \'BERT of all trades, master of some\'",0.929702403,F1,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,GLUE RTE,2020-03,TRANS-BLSTM model in \'TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding\',79.78,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yahoo! Answers,2016-07,FastText model in \'Bag of Tricks for Efficient Text Classification\',72.3,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yahoo! Answers,2018-05,SWEM-concat model in \'Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms\',73.53,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yahoo! Answers,2018-07,DRNN model in \'Disconnected Recurrent Neural Networks for Text Categorization\',76.26,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,Yahoo! Answers,2019-05,BERT-ITPT-FiT model in \'How to Fine-Tune BERT for Text Classification?\',77.62,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,GLUE COLA,2020-03,TRANS-BLSTM model in \'TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding\',64.81,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,GLUE SST2,2020-03,TRANS-BLSTM model in \'TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding\',94.38,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,GLUE MRPC,2020-03,TRANS-BLSTM model in \'TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding\',90.45,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,GLUE STSB,2020-03,TRANS-BLSTM model in \'TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding\',90.43,Accuracy,pos
Natural language analysis,Semantic analysis,Text classification,Text classification,IMDb,2014-05,Paragraph Vectors Le & Mikolov (2014) model in \'Distributed Representations of Sentences and Documents\',92.58,Accuracy (2 classes),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,IMDb,2019-01,HAHNN (CNN) model in \'Hierarchical Attentional Hybrid Neural Networks for Document Classification\',95.17,Accuracy (2 classes),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,IMDb,2019-04,BERT Finetune + UDA model in \'Unsupervised Data Augmentation for Consistency Training\',95.8,Accuracy (2 classes),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,IMDb,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',96.8,Accuracy (2 classes),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,IMDb,2020-12,ERNIE-Doc-Large model in \'ERNIE-Doc: A Retrospective Long-Document Modeling Transformer\',97.1,Accuracy (2 classes),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,IMDb,2019-04,KD-LSTMreg model in \'DocBERT: BERT for Document Classification\',53.7,Accuracy (10 classes),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,IMDb,2021-03,Document Classification Using Importance of Sentences model in \'Improving Document-Level Sentiment Classification Using Importance of Sentences\',54.8,Accuracy (10 classes),pos
Natural language analysis,Semantic analysis,Text classification,Text classification,LOCAL DATASET,2018-05,RMDL (15 RDLs model in \'RMDL: Random Multimodel Deep Learning for Classification\',90.79,Accuracy (%),pos
Natural language analysis,Semantic analysis,Text classification,Topic coverage,Topic modeling topic coverage dataset - news,2020-12,NMF-200 model in \'A Topic Coverage Approach to Evaluation of Topic Models\',0.65,AuCDC,pos
Natural language analysis,Semantic analysis,Text classification,Topic coverage,Topic modeling topic coverage dataset - news,2020-12,PYP model in \'A Topic Coverage Approach to Evaluation of Topic Models\',0.64,SupCov,pos
Natural language analysis,Semantic analysis,Text classification,Topic coverage,Topic modeling topic coverage dataset - bio,2020-12,NMF-200 model in \'A Topic Coverage Approach to Evaluation of Topic Models\',0.67,AuCDC,pos
Natural language analysis,Semantic analysis,Text classification,Topic coverage,Topic modeling topic coverage dataset - bio,2020-12,NMF-200 model in \'A Topic Coverage Approach to Evaluation of Topic Models\',0.44,SupCov,pos
Natural language analysis,Semantic analysis,Text classification,Topic coverage,Topic modeling topic coverage dataset,2020-12,AuCDC model in \'A Topic Coverage Approach to Evaluation of Topic Models\',0.95,Spearman Correlation,pos
Natural language analysis,Semantic analysis,Text classification,Topic modeling,NYT,2020-07,JoSH model in \'Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding\',90.91,MACC,pos
Natural language analysis,Semantic analysis,Text classification,Topic modeling,NYT,2020-07,JoSH model in \'Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding\',0.0166,Topic coherence@5,pos
Natural language analysis,Semantic analysis,Text classification,Topic modeling,arXiv,2020-07,JoSH model in \'Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding\',83.24,MACC,pos
Natural language analysis,Semantic analysis,Text classification,Topic modeling,arXiv,2020-07,JoSH model in \'Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding\',0.0074,Topic coherence@5,pos
Natural language analysis,Semantic analysis,Text classification,Topic modeling,arXiv,2019-02,TopicEq model in \'TopicEq: A Joint Topic and Mathematical Equation Model for Scientific Texts\',0.097,Topic Coherence@50,pos
Natural language analysis,Semantic analysis,Text classification,Topic modeling,20 Newsgroups,2015-11,NVDM model in \'Neural Variational Inference for Text Processing\',-836.0,Test perplexity,neg
Natural language analysis,Semantic analysis,Text classification,Topic modeling,20 Newsgroups,2019-08,Bayesian SMM model in \'Learning document embeddings along with their uncertainties\',-515.0,Test perplexity,neg
Natural language analysis,Semantic analysis,Text classification,Toxic Comment Classification,GermEval 2021 - Toxic Comments test set,2021-09,"GBERT/GELECTRA Ensemble model in \'FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning\'",71.8,F1,pos
Natural language analysis,Semantic analysis,Text classification,Traditional Spam Detection,Traditional and Context-specific Spam Twitter,2022-06,BERT model in \'Traditional and context-specific spam detection in low resource settings\',0.9079,Avg F1,pos
Natural language analysis,Semantic analysis,Text classification,Weakly supervised classification,THYME-2016,2020-08,Trove model in \'Ontology-driven weak supervision for clinical entity classification in electronic health records\',72.9,F1,pos
Natural language analysis,Semantic analysis,Text classification,Weakly supervised classification,ShARe/CLEF 2014: Task 2 Disorders,2020-08,Trove model in \'Ontology-driven weak supervision for clinical entity classification in electronic health records\',92.7,F1,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,SNIPS,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',89.3,NMI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,SNIPS,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',86.82,ARI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,SNIPS,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',93.63,ACC,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,Stackoverflow,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',69.84,NMI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,Stackoverflow,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',52.59,ARI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,Stackoverflow,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',73.48,ACC,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,BANKING77,2020-12,DeepAligned model in \'Discovering New Intents with Deep Aligned Clustering\',0.7956,NMI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,BANKING77,2020-12,DeepAligned model in \'Discovering New Intents with Deep Aligned Clustering\',0.5364,ARI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,BANKING77,2020-12,DeepAligned model in \'Discovering New Intents with Deep Aligned Clustering\',64.9,ACC,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,ATIS,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',94.74,NMI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,ATIS,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',89.41,ARI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,ATIS,2019-11,CDAC+ model in \'Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement\',91.66,ACC,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,CLINC150,2020-12,DeepAligned model in \'Discovering New Intents with Deep Aligned Clustering\',0.9389,NMI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,CLINC150,2020-12,DeepAligned model in \'Discovering New Intents with Deep Aligned Clustering\',0.7975,ARI,pos
Natural language analysis,Semantic analysis,Text clustering,Open Intent Discovery,CLINC150,2020-12,DeepAligned model in \'Discovering New Intents with Deep Aligned Clustering\',86.49,ACC,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,GoogleNews-T,2021-03,SCCL model in \'Supporting Clustering with Contrastive Learning\',75.8,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,Searchsnippets,2017-01,STC2-LE model in \'Self-Taught Convolutional Neural Networks for Short Text Clustering\',77.09,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,Searchsnippets,2019-08,"SIF + Aut., Self-Train. model in \'A Self-Training Approach for Short Text Clustering\'",77.1,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,Searchsnippets,2021-03,SCCL model in \'Supporting Clustering with Contrastive Learning\',85.2,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,GoogleNews-S,2021-03,SCCL model in \'Supporting Clustering with Contrastive Learning\',83.1,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,Tweet,2021-03,SCCL model in \'Supporting Clustering with Contrastive Learning\',78.2,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,AG News,2021-03,SCCL model in \'Supporting Clustering with Contrastive Learning\',88.2,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,Biomedical,2017-01,STC2-LE model in \'Self-Taught Convolutional Neural Networks for Short Text Clustering\',43.62,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,Biomedical,2021-03,SCCL model in \'Supporting Clustering with Contrastive Learning\',46.2,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Short text clustering,GoogleNews-TS,2021-03,SCCL model in \'Supporting Clustering with Contrastive Learning\',89.8,Acc,pos
Natural language analysis,Semantic analysis,Text clustering,Text clustering,20 Newsgroups,2020-04,G-BAT model in \'Neural Topic Modeling with Bidirectional Adversarial Training\',41.25,Accuracy,pos
Natural language analysis,Semantic analysis,Text data mining,Argument Pair Extraction (APE),RR,2021-08,MLMC model in \'Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding\',32.81,Overall F1,pos
Natural language analysis,Semantic analysis,Text data mining,Claim Extraction with Stance Classification (CESC),IAM Dataset,2022-03,Multi-label model in \'IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks\',60.25,Macro F1,pos
Natural language analysis,Semantic analysis,Text data mining,Claim-Evidence Pair Extraction (CEPE),IAM Dataset,2022-03,Multi-task model in \'IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks\',35.92,F1,pos
Natural language analysis,Semantic analysis,Text data mining,Component classification,CDCP,2021-02,ResAttArg model in \'Multi-Task Attentive Residual Networks for Argument Mining\',78.71,Macro F1,pos
Natural language analysis,Semantic analysis,Text data mining,Fact Selection,ArgSciChat,2022-02,TF-IDF model in \'ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers\',16.22,Fact-F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 3 Task 1,2016-03,"LSTMLP (T:SemCor, U:1K) model in \'Semi-supervised Word Sense Disambiguation with Neural Models\'",71.8,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 3 Task 1,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",77.8,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-01,Babelfy model in \'Entity Linking meets Word Sense Disambiguation: a Unified Approach\',67.0,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-03,UKBppr_w2w model in \'Random Walks for Knowledge-Based Word Sense Disambiguation\',68.8,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2020-02,KEF model in \'Word Sense Disambiguation: A comprehensive knowledge exploitation framework\',69.6,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-01,Babelfy model in \'Entity Linking meets Word Sense Disambiguation: a Unified Approach\',63.5,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-03,UKBppr_w2w model in \'Random Walks for Knowledge-Based Word Sense Disambiguation\',66.1,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2017-04,WN 1st sense baseline model in \'Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison\',66.2,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-01,Babelfy model in \'Entity Linking meets Word Sense Disambiguation: a Unified Approach\',51.6,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-03,UKBppr_w2w model in \'Random Walks for Knowledge-Based Word Sense Disambiguation\',53.0,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2017-04,WN 1st sense baseline model in \'Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison\',55.2,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2020-02,KEF model in \'Word Sense Disambiguation: A comprehensive knowledge exploitation framework\',56.9,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-01,Babelfy model in \'Entity Linking meets Word Sense Disambiguation: a Unified Approach\',66.4,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2020-02,KEF model in \'Word Sense Disambiguation: A comprehensive knowledge exploitation framework\',68.4,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-03,UKBppr_w2w-nf model in \'Random Walks for Knowledge-Based Word Sense Disambiguation\',64.5,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2017-04,WN 1st sense baseline model in \'Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison\',67.8,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2018-01,WSD-TM model in \'Knowledge-based Word Sense Disambiguation using Topic Models\',69.6,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2020-02,KEF model in \'Word Sense Disambiguation: A comprehensive knowledge exploitation framework\',72.3,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2014-01,Babelfy model in \'Entity Linking meets Word Sense Disambiguation: a Unified Approach\',65.5,All,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2018-01,WSD-TM model in \'Knowledge-based Word Sense Disambiguation using Topic Models\',66.9,All,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Knowledge-based:,2020-02,KEF model in \'Word Sense Disambiguation: A comprehensive knowledge exploitation framework\',68.0,All,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 2 Lexical Sample,2015-05,IMS + adapted CW model in \'Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains\',66.2,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 2 Lexical Sample,2016-06,BiLSTM with GloVe model in \'Word Sense Disambiguation using a Bidirectional LSTM\',66.9,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 2 Lexical Sample,2019-09,kNN-BERT model in \'Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings\',76.52,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2015 Task 13,2018-05,GASext (Concatenation) model in \'Incorporating Glosses into Neural Word Sense Disambiguation\',72.6,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2015 Task 13,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",74.46,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2015 Task 13,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",82.6,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 3 Lexical Sample,2015-05,IMS + adapted CW model in \'Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains\',73.4,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 3 Lexical Sample,2019-09,kNN-BERT model in \'Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings\',80.12,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX+POS</sub> model in \'Neural Sequence Learning Models for Word Sense Disambiguation\',72.0,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-05,GAS<sub>ext</sub> model in \'Incorporating Glosses into Neural Word Sense Disambiguation\',72.2,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",75.15,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",79.7,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2020-07,EWISER+WNGC model in \'Breaking Through the 80\\% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information\',80.8,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2021-06,ESCHER SemCor model in \'ESC: Redesigning WSD with Extractive Sense Comprehension\',81.7,Senseval 2,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX</sub> model in \'Neural Sequence Learning Models for Word Sense Disambiguation\',69.4,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-02,ELMo model in \'Deep contextualized word representations\',69.6,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-05,GAS<sub>ext</sub> model in \'Incorporating Glosses into Neural Word Sense Disambiguation\',70.5,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",77.8,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2020-07,EWISER+WNGC model in \'Breaking Through the 80\\% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information\',79.0,Senseval 3,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-02,ELMo model in \'Deep contextualized word representations\',62.2,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",66.81,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",73.4,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2020-07,EWISER+WNGC model in \'Breaking Through the 80\\% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information\',75.2,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2021-06,ESCHER SemCor model in \'ESC: Redesigning WSD with Extractive Sense Comprehension\',76.3,SemEval 2007,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX+POS</sub> model in \'Neural Sequence Learning Models for Word Sense Disambiguation\',66.9,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-05,GAS<sub>ext</sub> model in \'Incorporating Glosses into Neural Word Sense Disambiguation\',67.2,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",72.63,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",78.7,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2020-07,EWISER+WNGC model in \'Breaking Through the 80\\% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information\',80.7,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2021-06,ESCHER SemCor model in \'ESC: Redesigning WSD with Extractive Sense Comprehension\',82.2,SemEval 2013,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX</sub> model in \'Neural Sequence Learning Models for Word Sense Disambiguation\',72.4,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-05,GAS<sub>ext</sub> model in \'Incorporating Glosses into Neural Word Sense Disambiguation\',72.6,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",74.46,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",82.6,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Supervised:,2021-06,ESCHER SemCor model in \'ESC: Redesigning WSD with Extractive Sense Comprehension\',83.2,SemEval 2015,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 2,2016-03,"LSTMLP (T:OMSTI, U:1K) model in \'Semi-supervised Word Sense Disambiguation with Neural Models\'",74.4,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 2,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",75.15,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SensEval 2,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",79.7,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2007 Task 7,2016-03,"LSTMLP (T:SemCor, U:OMSTI) model in \'Semi-supervised Word Sense Disambiguation with Neural Models\'",84.3,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2007 Task 7,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",86.02,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2007 Task 7,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",90.4,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2007 Task 17,2016-03,LSTM (T:SemCor) model in \'Semi-supervised Word Sense Disambiguation with Neural Models\',64.2,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2007 Task 17,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",66.81,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2007 Task 17,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",73.4,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2013 Task 12,2016-03,"LSTMLP (T:SemCor, U:1K) model in \'Semi-supervised Word Sense Disambiguation with Neural Models\'",69.5,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2013 Task 12,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble model in \'Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships\'",72.63,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,SemEval 2013 Task 12,2019-05,"SemCor+WNGC, hypernyms model in \'Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation\'",78.7,F1,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,RUSSE,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.805,Accuracy,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2019-08,GlossBert-ws model in \'GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge\',75.9,Task 1 Accuracy: all,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2019-08,GlossBert-ws model in \'GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge\',75.2,Task 1 Accuracy: general purpose,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2019-08,GlossBert-ws model in \'GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge\',76.7,Task 1 Accuracy: domain specific,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2020-04,Bert-base model in \'WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\',77.9,Task 1 Accuracy: domain specific,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2020-04,Bert-base model in \'WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\',71.7,Task 2 Accuracy: all,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2020-04,Bert-base model in \'WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\',68.6,Task 2 Accuracy: general purpose,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2020-04,Bert-base model in \'WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\',74.7,Task 2 Accuracy: domain specific,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2020-04,Human model in \'WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\',85.3,Task 3 Accuracy: all,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2020-04,Human model in \'WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\',82.1,Task 3 Accuracy: general purpose,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,WiC-TSV,2020-04,Human model in \'WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\',89.2,Task 3 Accuracy: domain specific,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Words in Context,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',76.9,Accuracy,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Words in Context,2020-06,DeBERTa-Ensemble model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',77.5,Accuracy,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense disambiguation,Words in Context,2020-10,COSINE + Transductive Learning model in \'Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach\',85.3,Accuracy,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2013-06,AI-KU model in \'AI-KU: Using Substitute Vectors and Co-Occurrence Modeling For Word Sense Induction and Disambiguation\',15.92,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2015-01,STM+w2v model in \'A Sense-Topic Model for Word Sense Induction with Unsupervised Data Enrichment\',19.89,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2016-12,MCC-S model in \'Structured Generative Models of Continuous Features for Word Sense Induction\',20.58,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2018-08,LSDP model in \'Word Sense Induction with Neural biLM and Symmetric Patterns\',25.4,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2019-05,BERT+DP model in \'Towards better substitution-based word sense induction\',37.0,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2013-06,AI-KU model in \'AI-KU: Using Substitute Vectors and Co-Occurrence Modeling For Word Sense Induction and Disambiguation\',39.0,F-BC,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2015-01,STM+w2v model in \'A Sense-Topic Model for Word Sense Induction with Unsupervised Data Enrichment\',55.4,F-BC,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2016-12,MCC-S model in \'Structured Generative Models of Continuous Features for Word Sense Induction\',55.6,F-BC,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2018-08,LSDP model in \'Word Sense Induction with Neural biLM and Symmetric Patterns\',57.5,F-BC,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2018-11,AutoSense model in \'AutoSense Model for Word Sense Induction\',61.7,F-BC,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2019-05,BERT+DP model in \'Towards better substitution-based word sense induction\',64.0,F-BC,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2013-06,AI-KU model in \'AI-KU: Using Substitute Vectors and Co-Occurrence Modeling For Word Sense Induction and Disambiguation\',6.5,F_NMI,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2015-01,STM+w2v model in \'A Sense-Topic Model for Word Sense Induction with Unsupervised Data Enrichment\',7.14,F_NMI,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2016-12,MCC-S model in \'Structured Generative Models of Continuous Features for Word Sense Induction\',7.62,F_NMI,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2018-08,LSDP model in \'Word Sense Induction with Neural biLM and Symmetric Patterns\',11.3,F_NMI,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2013,2019-05,BERT+DP model in \'Towards better substitution-based word sense induction\',21.4,F_NMI,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2014-08,LDA model in \'Unsupervised Word Sense Induction using Distributional Statistics\',60.7,F-Score,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2018-11,AutoSense model in \'AutoSense Model for Word Sense Induction\',61.7,F-Score,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2019-05,BERT+DP model in \'Towards better substitution-based word sense induction\',71.3,F-Score,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2014-08,BNP-HC model in \'Inducing Word Sense with Automatically Learned Hidden Concepts\',21.4,V-Measure,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2019-05,BERT+DP model in \'Towards better substitution-based word sense induction\',40.4,V-Measure,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2014-08,BNP-HC model in \'Inducing Word Sense with Automatically Learned Hidden Concepts\',22.23,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2016-06,SE-WSI-fix model in \'Sense Embedding Learning for Word Sense Induction\',23.24,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2018-11,AutoSense model in \'AutoSense Model for Word Sense Induction\',24.59,AVG,pos
Natural language analysis,Semantic analysis,Word sense disambiguation,Word sense induction,SemEval 2010 WSI,2019-05,BERT+DP model in \'Towards better substitution-based word sense induction\',53.6,AVG,pos
Natural language analysis,Sentence embedding,Sentence compression,Sentence compression,Google Dataset,2015-09,LSTM model in \'Sentence Compression by Deletion with LSTMs\',0.82,F1,pos
Natural language analysis,Sentence embedding,Sentence compression,Sentence compression,Google Dataset,2018-06,Higher-Order Syntactic Attention Network model in \'Higher-Order Syntactic Attention Network for Longer Sentence Compression\',0.835,F1,pos
Natural language analysis,Sentence embedding,Sentence compression,Sentence compression,Google Dataset,2018-07,BiRNN + LM Evaluator model in \'A Language Model based Evaluator for Sentence Compression\',0.851,F1,pos
Natural language analysis,Sentence embedding,Sentence compression,Sentence compression,Google Dataset,2020-02,SLAHAN (LSTM+syntactic-information) model in \'Syntactically Look-Ahead Attention Network for Sentence Compression\',0.855,F1,pos
Natural language analysis,Sentence embedding,Sentence compression,Sentence compression,Google Dataset,2015-09,LSTM model in \'Sentence Compression by Deletion with LSTMs\',0.38,CR,pos
Natural language analysis,Sentence embedding,Sentence compression,Sentence compression,Google Dataset,2017-07,BiLSTM model in \'Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains\',0.43,CR,pos
Natural language analysis,Sentence embedding,Sentence embeddings for biomedical texts,Sentence embeddings for biomedical texts,MedSTS,2018-10,BioSentVec (PubMed + MIMIC-III) model in \'BioSentVec: creating sentence embeddings for biomedical texts\',0.767,Pearson Correlation,pos
Natural language analysis,Sentence embedding,Sentence embeddings for biomedical texts,Sentence embeddings for biomedical texts,BIOSSES,2017-07,Paragraph Vector model in \'BIOSSES: A Semantic Sentence Similarity Estimation System for the Biomedical Domain\',0.787,Pearson Correlation,pos
Natural language analysis,Sentence embedding,Sentence embeddings for biomedical texts,Sentence embeddings for biomedical texts,BIOSSES,2018-10,BioSentVec (PubMed) model in \'BioSentVec: creating sentence embeddings for biomedical texts\',0.817,Pearson Correlation,pos
Natural language analysis,Sentence embedding,Sentence embeddings for biomedical texts,Sentence embeddings for biomedical texts,BIOSSES,2021-10,"Supervised combination of: Jaccard, Q-gram, sent2vec, Paragraph vector DM, skip-thoughts, fastText model in \'Neural sentence embedding models for semantic similarity estimation in the biomedical domain\'",0.871,Pearson Correlation,pos
Natural language analysis,Syntactic analysis,Anaphora resolution,Abstract anaphora resolution,The ARRAU Corpus,2017-06,MR-LSTM model in \'A Mention-Ranking Model for Abstract Anaphora Resolution\',43.83,Average Precision,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,CoNLL 2000,2017-11,Adversarial Training model in \'Robust Multilingual Part-of-Speech Tagging via Adversarial Training\',95.25,Exact Span F1,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,CoNLL 2000,2018-01,CVT+Multi-Task+Large model in \'Cross-View Training for Semi-Supervised Learning\',96.98,Exact Span F1,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,CoNLL 2000,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',97.3,Exact Span F1,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,CoNLL 2003 (English),2020-09,"Wang et al., 2020 model in \'More Embeddings, Better Sequence Labelers?\'",92.0,F1,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,CoNLL 2003 (English),2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',92.5,F1,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,CoNLL 2003 (German),2020-09,"Wang et al., 2020 model in \'More Embeddings, Better Sequence Labelers?\'",94.4,F1,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,CoNLL 2003 (German),2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',95.0,F1,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,Penn Treebank,2016-08,Low supervision model in \'Deep multi-task learning with low level tasks supervised at lower layers\',95.57,F1 score,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,Penn Treebank,2016-11,JMT model in \'A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks\',95.77,F1 score,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,Penn Treebank,2018-08,Flair embeddings model in \'Contextual String Embeddings for Sequence Labeling\',96.72,F1 score,pos
Natural language analysis,Syntactic analysis,Chunking,Chunking,Penn Treebank,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',97.3,F1 score,pos
Natural language analysis,Syntactic analysis,Combinatory categorical grammar (CCG) supertagging,Combinatory categorical grammar (CCG) supertagging,CCGbank,2016-06,Lewis et al. model in \'LSTM CCG Parsing\',94.7,Accuracy,pos
Natural language analysis,Syntactic analysis,Combinatory categorical grammar (CCG) supertagging,Combinatory categorical grammar (CCG) supertagging,CCGbank,2018-09,CVT + Multi-task + Large model in \'Semi-Supervised Sequence Modeling with Cross-View Training\',96.1,Accuracy,pos
Natural language analysis,Syntactic analysis,Combinatory categorical grammar (CCG) supertagging,Combinatory categorical grammar (CCG) supertagging,CCGbank,2022-03,Heterogeneous Dynamic Convolutions model in \'Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions\',96.29,Accuracy,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2017-11,PRPN (tuned) model in \'Neural Language Modeling by Jointly Learning Syntax and Lexicon\',47.3,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2018-08,DMV + invertible projector model in \'Unsupervised Learning of Syntactic Structure with Invertible Neural Projections\',47.9,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2018-10,ON-LSTM (tuned) model in \'Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\',48.1,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2019-06,DIORA (+PP) model in \'Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders\',55.7,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2021-04,TN-PCFG (p=500) model in \'PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols\',57.7,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2021-05,NBL-PCFG model in \'Neural Bi-Lexicalized PCFG Induction\',60.4,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2021-10,inside-outside co-training + weak supervision model in \'Co-training an Unsupervised Constituency Parser with Weak Supervision\',63.1,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2022-05,DP in rank space model in \'Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs\',64.1,Mean F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2017-11,PRPN (tuned) model in \'Neural Language Modeling by Jointly Learning Syntax and Lexicon\',47.9,Max F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2018-10,ON-LSTM (tuned) model in \'Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\',50.0,Max F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2019-04,URNNG model in \'Unsupervised Recurrent Neural Network Grammars\',52.4,Max F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2019-06,Compound PCFG model in \'Compound Probabilistic Context-Free Grammars for Grammar Induction\',60.1,Max F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2021-04,TN-PCFG (p=500) model in \'PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols\',61.4,Max F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2021-10,inside-outside co-training + weak supervision model in \'Co-training an Unsupervised Constituency Parser with Weak Supervision\',66.8,Max F1 (WSJ),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2018-08,DMV + invertible projector model in \'Unsupervised Learning of Syntactic Structure with Invertible Neural Projections\',60.2,Mean F1 (WSJ10),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2018-10,ON-LSTM model in \'Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\',65.1,Mean F1 (WSJ10),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2019-06,DIORA model in \'Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders\',67.7,Mean F1 (WSJ10),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2021-10,inside-outside co-training + weak supervision model in \'Co-training an Unsupervised Constituency Parser with Weak Supervision\',74.2,Mean F1 (WSJ10),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2018-10,ON-LSTM model in \'Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\',66.8,Max F1 (WSJ10),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency grammar induction,PTB,2019-06,DIORA model in \'Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders\',68.5,Max F1 (WSJ10),pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,CTB7,2020-08,CRF Parser + Electra model in \'Fast and Accurate Neural CRF Constituency Parsing\',91.92,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2006-06,Self-training model in \'Effective Self-Training for Parsing\',92.1,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2015-09,Parse fusion model in \'Syntactic Parse Fusion\',92.6,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2016-11,Semi-supervised LSTM-LM model in \'Parsing as Language Modeling\',93.8,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2017-07,Model combination model in \'Improving Neural Parsing by Disentangling Model Combination and Reranking Effects\',94.66,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2018-05,Self-attentive encoder + ELMo model in \'Constituency Parsing with a Self-Attentive Encoder\',95.13,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2019-03,CNN Large + fine-tune model in \'Cloze-driven Pretraining of Self-attention Networks\',95.6,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2019-07,Head-Driven Phrase Structure Grammar Parsing (Joint) + XLNet model in \'Head-Driven Phrase Structure Grammar Parsing on Penn Treebank\',96.33,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,Penn Treebank,2019-11,Label Attention Layer + HPSG + XLNet model in \'Rethinking Self-Attention: Towards Interpretability in Neural Parsing\',96.38,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,CTB5,2018-05,Kitaev etal. 2018 model in \'Constituency Parsing with a Self-Attentive Encoder\',87.43,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,CTB5,2018-12,Kitaev etal. 2019 model in \'Multilingual Constituency Parsing with Self-Attention and Pre-Training\',91.75,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,CTB5,2020-08,CRF Parser + BERT model in \'Fast and Accurate Neural CRF Constituency Parsing\',92.27,F1 score,pos
Natural language analysis,Syntactic analysis,Constituency parsing,Constituency parsing,CTB5,2020-10,Attach-Juxtapose Parser + BERT model in \'Strongly Incremental Constituency Parsing with Graph Neural Networks\',93.52,F1 score,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Cross-lingual zero-shot dependency parsing,Universal Dependency Treebank,2016-02,"MaLOPa model in \'Many Languages, One Parser\'",70.5,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Cross-lingual zero-shot dependency parsing,Universal Dependency Treebank,2019-02,"Cross-Lingual ELMo model in \'Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing\'",77.3,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Cross-lingual zero-shot dependency parsing,Universal Dependency Treebank,2016-03,MULTI-PROJ model in \'A Representation Learning Framework for Multi-Source Transfer Parsing\',76.4,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Cross-lingual zero-shot dependency parsing,Universal Dependency Treebank,2019-02,"Cross-Lingual ELMo model in \'Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing\'",84.2,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency grammar induction,WSJ,2020-10,Joint training: sibling-NDMV + L-NDMV * model in \'Second-Order Unsupervised Neural Dependency Parsing\',67.5,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency grammar induction,WSJ10,2019-07,D-NDMV model in \'Enhancing Unsupervised Generative Dependency Parser with Contextual Information\',75.6,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency grammar induction,WSJ10,2020-10,Joint training: sibling-NDMV + L-NDMV * model in \'Second-Order Unsupervised Neural Dependency Parsing\',79.9,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Chinese Treebank,2020-10,MFVI model in \'Second-Order Neural Dependency Parsing with Message Passing and End-to-End Training\',91.69,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Chinese Treebank,2020-10,MFVI model in \'Second-Order Neural Dependency Parsing with Message Passing and End-to-End Training\',92.78,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,UD2.5 test,2021-01,Trankit model in \'Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing\',87.06,Macro-averaged F1,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2018-07,"HIT-SCIR model in \'Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation\'",75.84,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",80.43,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2019-08,"UDPipe 2.0 + mBERT + FLAIR model in \'Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing\'",84.6,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2018-10,UDPipe 2.0 model in \'UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task\',61.25,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2019-01,Stanford+ model in \'Universal Dependency Parsing from Scratch\',62.08,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",85.69,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2019-08,"UDPipe 2.0 + mBERT + FLAIR model in \'Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing\'",87.64,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Universal Dependencies,2018-10,TurkuNLP model in \'Turku Neural Parser Pipeline: An End-to-End System for the CoNLL 2018 Shared Task\',66.09,BLEX,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,ParTUT,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",88.06,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,ParTUT,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',92.9,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,ParTUT,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",90.55,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,ParTUT,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',95.21,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,NLPCC-2019,2020-05,CRFPar model in \'Efficient Second-Order TreeCRF for Neural Dependency Parsing\',72.33,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,NLPCC-2019,2020-05,CRFPar model in \'Efficient Second-Order TreeCRF for Neural Dependency Parsing\',78.02,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Sequoia Treebank,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",90.05,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Sequoia Treebank,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',94.39,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Sequoia Treebank,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",92.53,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Sequoia Treebank,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',95.56,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2015-06,Weiss et al. model in \'Structured Training for Neural Network Transition-Based Parsing\',92.06,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2016-03,Andor et al. model in \'Globally Normalized Transition-Based Neural Networks\',92.79,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2016-11,Deep Biaffine + RoBERTa model in \'Deep Biaffine Attention for Neural Dependency Parsing\',95.75,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2019-11,Label Attention Layer + HPSG + XLNet model in \'Rethinking Self-Attention: Towards Interpretability in Neural Parsing\',96.26,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2015-06,Weiss et al. model in \'Structured Training for Neural Network Transition-Based Parsing\',94.01,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2016-03,Andor et al. model in \'Globally Normalized Transition-Based Neural Networks\',94.61,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2016-11,Deep Biaffine + RoBERTa model in \'Deep Biaffine Attention for Neural Dependency Parsing\',97.29,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2019-11,Label Attention Layer + HPSG + XLNet model in \'Rethinking Self-Attention: Towards Interpretability in Neural Parsing\',97.42,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2015-06,Weiss et al. model in \'Structured Training for Neural Network Transition-Based Parsing\',97.3,POS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2016-03,BIST transition-based parser model in \'Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations\',97.44,POS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Penn Treebank,2018-07,jPTDP model in \'An improved neural network model for joint POS tagging and dependency parsing\',97.97,POS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,CoNLL-2009,2016-11,Biaffine Parser model in \'Deep Biaffine Attention for Neural Dependency Parsing\',85.38,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,CoNLL-2009,2020-05,CRFPar model in \'Efficient Second-Order TreeCRF for Neural Dependency Parsing\',86.52,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,CoNLL-2009,2016-11,Biaffine Parser model in \'Deep Biaffine Attention for Neural Dependency Parsing\',88.9,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,CoNLL-2009,2020-05,CRFPar model in \'Efficient Second-Order TreeCRF for Neural Dependency Parsing\',89.63,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,GENIA - UAS,2018-08,BiLSTM-CRF model in \'From POS tagging to dependency parsing for biomedical event extraction\',92.84,F1,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Spoken Corpus,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",80.01,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Spoken Corpus,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',81.37,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Spoken Corpus,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",85.24,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,Spoken Corpus,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',86.05,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,DaNE,2021-07,da_dacy_large_tft-0.0.0 model in \'DaCy: A Unified Framework for Danish NLP\',88.44,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,DaNE,2021-07,da_dacy_large_tft-0.0.0 model in \'DaCy: A Unified Framework for Danish NLP\',90.85,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,GENIA - LAS,2018-08,BiLSTM-CRF model in \'From POS tagging to dependency parsing for biomedical event extraction\',91.92,F1,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,French GSD,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",91.45,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,French GSD,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',92.47,LAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,French GSD,2019-04,"UDify model in \'75 Languages, 1 Model: Parsing Universal Dependencies Universally\'",93.6,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Dependency parsing,French GSD,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',94.82,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Unsupervised dependency parsing,Penn Treebank,2004-07,DMV model in \'Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency\',35.9,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Unsupervised dependency parsing,Penn Treebank,2009-06,Shared Logistic Normal DMV model in \'Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction\',41.4,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Unsupervised dependency parsing,Penn Treebank,2010-10,Tree Substitution Grammar DMV model in \'Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing\',55.7,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Unsupervised dependency parsing,Penn Treebank,2013-10,Combined System model in \'Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction\',64.4,UAS,pos
Natural language analysis,Syntactic analysis,Dependency parsing,Unsupervised dependency parsing,Penn Treebank,2015-04,Iterative reranking model in \'Unsupervised Dependency Parsing: Let\'s Use Supervised Parsers\',66.2,UAS,pos
Natural language analysis,Syntactic analysis,Grammar checking,Chinese spell checking,SIGHAN 2015,2021-05,"ReaLiSe model in \'Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking\'",77.8,Correction F1,pos
Natural language analysis,Syntactic analysis,Grammar checking,Chinese spell checking,SIGHAN 2015,2021-08,PHMOSpell model in \'PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check\',78.1,Correction F1,pos
Natural language analysis,Syntactic analysis,Grammar checking,Chinese spell checking,SIGHAN 2015,2021-05,"ReaLiSe model in \'Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking\'",79.3,Detection F1,pos
Natural language analysis,Syntactic analysis,Grammar checking,Chinese spell checking,SIGHAN 2015,2021-08,PHMOSpell model in \'PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check\',80.5,Detection F1,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,Unrestricted,2018-07,CNN Seq2Seq + Fluency Boost model in \'Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study\',61.34,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,Unrestricted,2018-07,CNN Seq2Seq + Fluency Boost and inference model in \'Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study\',62.37,GLEU,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task (10 annotations),2018-01,CNN Seq2Seq model in \'A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction\',70.14,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task (10 annotations),2018-04,SMT + BiGRU model in \'Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation\',72.04,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,BEA-2019 (test),2019-06,BEA Combination model in \'Learning to combine Grammatical Error Corrections\',73.2,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,BEA-2019 (test),2020-05,"Sequence tagging + token-level transformations + two-stage fine-tuning (+BERT, RoBERTa, XLNet) model in \'GECToR -- Grammatical Error Correction: Tag, Not Rewrite\'",73.6,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder) model in \'Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data\',71.57,Precision,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2020-05,"Sequence tagging + token-level transformations + two-stage fine-tuning (+BERT, RoBERTa, XLNet) model in \'GECToR -- Grammatical Error Correction: Tag, Not Rewrite\'",78.2,Precision,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder) model in \'Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data\',38.65,Recall,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2020-05,BART model in \'Stronger Baselines for Grammatical Error Correction Using Pretrained Encoder-Decoder Model\',45.1,Recall,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2018-01,CNN Seq2Seq model in \'A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction\',54.79,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2018-04,SMT + BiGRU model in \'Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation\',56.25,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2018-10,CNN Seq2Seq + Quality Estimation model in \'Neural Quality Estimation of Grammatical Error Correction\',56.52,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder) model in \'Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data\',61.15,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2019-09,Transformer + Pre-train with Pseudo Data model in \'An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction\',65.0,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2020-05,"Sequence tagging + token-level transformations + two-stage fine-tuning (+BERT, RoBERTa, XLNet) model in \'GECToR -- Grammatical Error Correction: Tag, Not Rewrite\'",66.5,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,CoNLL-2014 Shared Task,2021-06,T5 model in \'A Simple Recipe for Multilingual Grammatical Error Correction\',68.87,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,GMEG-wiki,2021-09,LM-Critic (Unsupervised) model in \'LM-Critic: Language Models for Unsupervised Grammatical Error Correction\',50.6,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,JFLEG,2018-01,CNN Seq2Seq model in \'A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction\',57.47,GLEU,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,JFLEG,2018-04,SMT + BiGRU model in \'Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation\',61.5,GLEU,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,JFLEG,2020-05,Transformer + Pre-train with Pseudo Data + BERT model in \'Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction\',62.0,GLEU,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,JFLEG,2021-05,VERNet model in \'Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction\',62.1,GLEU,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,Falko-MERLIN,2018-11,Multilayer Convolutional Encoder-Decoder model in \'Using Wikipedia Edits in Low Resource Grammatical Error Correction\',43.35,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,Falko-MERLIN,2019-10,Transformer model in \'Grammatical Error Correction in Low-Resource Scenarios\',73.71,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,Falko-MERLIN,2021-06,gT5 xxl model in \'A Simple Recipe for Multilingual Grammatical Error Correction\',75.96,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error correction,GMEG-yahoo,2021-09,LM-Critic (Unsupervised) model in \'LM-Critic: Language Models for Unsupervised Grammatical Error Correction\',52.2,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,CoNLL-2014 A1,2016-07,Bi-LSTM (unrestricted data) model in \'Compositional Sequence Labeling Models for Error Detection in Learner Writing\',34.3,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,CoNLL-2014 A1,2017-07,Bi-LSTM + POS (unrestricted data) model in \'Auxiliary Objectives for Neural Error Detection Models\',36.1,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,CoNLL-2014 A1,2021-05,VERNet model in \'Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction\',54.3,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,FCE,2016-07,Bi-LSTM model in \'Compositional Sequence Labeling Models for Error Detection in Learner Writing\',41.1,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,FCE,2016-11,Bi-LSTM + charattn model in \'Attending to Characters in Neural Sequence Labeling Models\',41.88,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,FCE,2017-04,Bi-LSTM + LMcost model in \'Semi-supervised Multitask Learning for Sequence Labeling\',48.48,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,FCE,2017-07,Ann+PAT+MT model in \'Artificial Error Generation with Machine Translation and Syntactic Patterns\',49.11,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,FCE,2018-11,BiLSTM-JOINT model in \'Jointly Learning to Label Sentences and Tokens\',52.07,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,FCE,2021-05,VERNet model in \'Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction\',72.2,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,JFLEG,2018-11,BiLSTM-JOINT (trained on FCE) model in \'Jointly Learning to Label Sentences and Tokens\',52.52,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,CoNLL-2014 A2,2016-07,Bi-LSTM (unrestricted data) model in \'Compositional Sequence Labeling Models for Error Detection in Learner Writing\',44.0,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,CoNLL-2014 A2,2017-07,Bi-LSTM + POS (unrestricted data) model in \'Auxiliary Objectives for Neural Error Detection Models\',45.1,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Grammatical error detection,CoNLL-2014 A2,2021-05,VERNet model in \'Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction\',63.1,F0.5,pos
Natural language analysis,Syntactic analysis,Grammar checking,Linguistic acceptability assessment,CoLA,2019-01,MT-DNN model in \'Multi-Task Deep Neural Networks for Natural Language Understanding\',68.4,Accuracy,pos
Natural language analysis,Syntactic analysis,Grammar checking,Linguistic acceptability assessment,CoLA,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',69.0,Accuracy,pos
Natural language analysis,Syntactic analysis,Grammar checking,Linguistic acceptability assessment,CoLA,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',69.2,Accuracy,pos
Natural language analysis,Syntactic analysis,Grammar checking,Linguistic acceptability assessment,CoLA,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',70.8,Accuracy,pos
Natural language analysis,Syntactic analysis,Grammar checking,Linguistic acceptability assessment,CoLA,2021-04,EFL model in \'Entailment as Few-Shot Learner\',86.4,Accuracy,pos
Natural language analysis,Syntactic analysis,Grammar checking,Linguistic acceptability assessment,CoLA Dev,2019-09,TinyBERT (M=6;d\' =768;d\'i=3072) model in \'TinyBERT: Distilling BERT for Natural Language Understanding\',54.0,Accuracy,pos
Natural language analysis,Syntactic analysis,Grammar checking,Linguistic acceptability assessment,CoLA Dev,2020-06,DeBERTa (large) model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',69.5,Accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,ANTILLES,2022-06,Bi-LSTM-CRF + Flair Embeddings + CamemBERT (oscar−138gb−base) Embeddings model in \'ANTILLES: An Open French Linguistically Enriched Part-of-Speech Corpus\',97.98,Weighted Average F1-score,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Penn Treebank,2015-08,Char Bi-LSTM model in \'Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation\',97.78,Accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Penn Treebank,2018-05,Meta BiLSTM model in \'Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings\',97.96,Accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Penn Treebank,2022-07,SALE-BART encoder model in \'Sequence Alignment Ensemble with a Single Neural Network for Sequence Labeling\',98.15,Accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Penn Treebank,2022-03,SALE model in \'Sequential Alignment Methods for Ensemble Part-of-Speech Tagging\',97.81,CoNLL F1,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,ParTUT,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',97.63,UPOS,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Social media,2013-09,GATE model in \'Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data\',88.69,Accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Social media,2019-04,PretRand model in \'Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging\',91.46,Accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,French GSD,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',98.19,UPOS,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,UD2.5 test,2021-01,Trankit model in \'Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing\',95.65,Macro-averaged F1,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Spoken Corpus,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',96.68,UPOS,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Ritter,2017-09,"Gui et al., 2017 model in \'Part-of-Speech Tagging for Twitter with Adversarial Neural Networks\'",90.9,Acc,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Ritter,2018-10,"Gui et al., 2018 model in \'Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging\'",91.2,Acc,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Ritter,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',93.4,Acc,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,ARK,2013-06,"Owoputi et al., 2013 model in \'Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters\'",93.2,Acc,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,ARK,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',94.4,Acc,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Sequoia Treebank,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',99.21,UPOS,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,XGLUE,2022-04,mGPT model in \'mGPT: Few-Shot Learners Go Multilingual\',0.56,Avg. F1,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,DaNE,2021-07,da_dacy_large_tft-0.0.0 model in \'DaCy: A Unified Framework for Danish NLP\',98.37,Accuracy (%),pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,UD,2016-04,Bi-LSTM model in \'Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss\',96.4,Avg accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,UD,2017-11,Adversarial Bi-LSTM model in \'Robust Multilingual Part-of-Speech Tagging via Adversarial Training\',96.65,Avg accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,UD,2019-08,BiLSTM-LAN model in \'Hierarchically-Refined Label Attention Network for Sequence Labeling\',96.88,Avg accuracy,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Tweebank,2017-09,"Gui et al., 2017 model in \'Part-of-Speech Tagging for Twitter with Adversarial Neural Networks\'",92.8,Acc,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Tweebank,2020-05,BERTweet model in \'BERTweet: A pre-trained language model for English Tweets\',95.2,Acc,pos
Natural language analysis,Syntactic analysis,Part-of-speech tagging,Part-of-speech tagging,Tweebank,2020-10,ACE model in \'Automated Concatenation of Embeddings for Structured Prediction\',95.8,Acc,pos
Natural language analysis,Syntactic analysis,Query wellformedness,Query wellformedness,Query Wellformedness,2018-08,"word-1, 2 POS-1, 2, 3 model in \'Identifying Well-formed Natural Language Questions\'",70.7,Accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Arabic text diacritization,Tashkeela,2019-04,Shakkala model in \'Arabic Text Diacritization Using Deep Neural Networks\',-0.1119,Word Error Rate (WER),neg
Natural language analysis,Syntactic analysis,Text diacritization,Arabic text diacritization,Tashkeela,2019-11,Shakkelha model in \'Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation\',-0.0509,Word Error Rate (WER),neg
Natural language analysis,Syntactic analysis,Text diacritization,Arabic text diacritization,Tashkeela,2020-11,CBHG model model in \'Effective Deep Learning Models for Automatic Diacritization of Arabic Text\',-0.0443,Word Error Rate (WER),neg
Natural language analysis,Syntactic analysis,Text diacritization,Arabic text diacritization,Tashkeela,2019-04,Shakkala model in \'Arabic Text Diacritization Using Deep Neural Networks\',0.0373,Diacritic Error Rate,pos
Natural language analysis,Syntactic analysis,Text diacritization,Croatian Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',99.73,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Czech Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2018-05,RNN + LM model in \'Diacritics Restoration Using Neural Networks\',99.06,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Czech Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',99.22,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,French Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',99.71,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Hungarian Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',99.41,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Irish Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',98.88,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Latvian Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',98.63,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Romanian Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',98.64,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Slovak Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',99.32,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Spanish Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',99.62,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Turkish Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',98.95,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Vietnamese Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2018-05,RNN + LM model in \'Diacritics Restoration Using Neural Networks\',97.73,Alpha-Word accuracy,pos
Natural language analysis,Syntactic analysis,Text diacritization,Vietnamese Text Diacritization,Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems,2021-05,BERT model in \'Diacritics Restoration using BERT with Analysis on Czech language\',98.53,Alpha-Word accuracy,pos
Natural language analysis,Temporal processing,Document dating,Document dating,APW,2012-07,Chambers model in \'Labeling Documents with Timestamps: Learning from their Time Expressions\',52.5,Accuracy,pos
Natural language analysis,Temporal processing,Document dating,Document dating,APW,2019-02,NeuralDater model in \'Dating Documents using Graph Convolution Networks\',64.1,Accuracy,pos
Natural language analysis,Temporal processing,Document dating,Document dating,NYT,2012-07,Chambers model in \'Labeling Documents with Timestamps: Learning from their Time Expressions\',42.3,Accuracy,pos
Natural language analysis,Temporal processing,Document dating,Document dating,NYT,2019-02,NeuralDater model in \'Dating Documents using Graph Convolution Networks\',58.9,Accuracy,pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',90.43,Type,pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',96.37,Strict Detection (Pr.),pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',96.37,Strict Detection (Re.),pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',96.37,Strict Detection (F1),pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,B2B model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',100.0,Relaxed Detection (Pr.),pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',100.0,Relaxed Detection (Re.),pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal Tagging,TempEval-3,2021-09,R2R model in \'BERT got a Date: Introducing Transformers to Temporal Tagging\',100.0,Relaxed Detection (F1),pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal information extraction,TempEval-3,2013-06,ClearTK model in \'ClearTK-TimeML: A minimalist approach to TempEval 2013\',30.98,Temporal awareness,pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal information extraction,TempEval-3,2019-06,Ning et al. model in \'A Structured Learning Approach to Temporal Relation Extraction\',67.2,Temporal awareness,pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal information extraction,TimeBank,2014-01,CAEVO model in \'Dense Event Ordering with a Multi-Pass Architecture\',0.507,F1 score,pos
Natural language analysis,Temporal processing,Temporal information extraction,Temporal information extraction,TimeBank,2016-12,Catena model in \'CATENA: CAusal and TEmporal relation extraction from NAtural language texts\',0.511,F1 score,pos
Natural language analysis,Temporal processing,Timex normalization,Timex normalization,TimeBank,2012-05,TIMEN model in \'TIMEN: An Open Temporal Expression Normalisation Resource\',0.89,F1-Score,pos
Natural language analysis,Temporal processing,Timex normalization,Timex normalization,PNT,2015-09,HeidelTime model in \'A Baseline Temporal Tagger for all Languages\',0.74,F1-Score,pos
Natural language analysis,Temporal processing,Timex normalization,Timex normalization,PNT,2018-01,Laparra et al. model in \'From Characters to Time Intervals: New Paradigms for Evaluation and Neural Parsing of Time Normalizations\',0.764,F1-Score,pos
Natural language analysis,Text annotation,Table annotation,Cell Entity Annotation,ToughTables-WD,2021-10,DAGOBAH model in \'DAGOBAH: Table and Graph Contexts for Eﬀicient Semantic Annotation of Tabular Data\',92.3,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Cell Entity Annotation,WikiTables-TURL-CEA,2020-06,TURL model in \'TURL: Table Understanding through Representation Learning\',68.0,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Cell Entity Annotation,ToughTables-DBP,2021-10,DAGOBAH model in \'DAGOBAH: Table and Graph Contexts for Eﬀicient Semantic Annotation of Tabular Data\',94.5,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Cell Entity Annotation,WikipediaGS,2020-06,TURL model in \'TURL: Table Understanding through Representation Learning\',67.0,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,WikiTables-TURL-CTA,2020-06,TURL model in \'TURL: Table Understanding through Representation Learning\',94.75,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,GitTables-SemTab-SCH,2021-10,DAGOBAH model in \'DAGOBAH: Table and Graph Contexts for Eﬀicient Semantic Annotation of Tabular Data\',18.3,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,VizNet-Sato-Full,2019-11,Sato model in \'Sato: Contextual Semantic Type Detection in Tables\',75.6,Macro-F1,pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,VizNet-Sato-Full,2021-04,DODUO model in \'Annotating Columns with Pre-trained Language Models\',84.6,Macro-F1,pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,VizNet-Sato-Full,2019-11,Sato model in \'Sato: Contextual Semantic Type Detection in Tables\',90.2,Weighted-F1,pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,VizNet-Sato-Full,2021-05,TaBERT model in \'TABBIE: Pretrained Representations of Tabular Data\',97.2,Weighted-F1,pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,VizNet-Sato-MultiColumn,2019-11,Sato model in \'Sato: Contextual Semantic Type Detection in Tables\',73.5,Macro-F1,pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,VizNet-Sato-MultiColumn,2021-04,DODUO model in \'Annotating Columns with Pre-trained Language Models\',83.8,Macro-F1,pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,VizNet-Sato-MultiColumn,2019-11,Sato model in \'Sato: Contextual Semantic Type Detection in Tables\',92.5,Weighted-F1,pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,ToughTables-WD,2021-10,DAGOBAH model in \'DAGOBAH: Table and Graph Contexts for Eﬀicient Semantic Annotation of Tabular Data\',83.2,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,WikipediaGS-CTA,2019-05,HNN model in \'Learning Semantic Annotations for Tabular Data\',65.5,Accuracy (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,WikipediaGS-CTA,2020-06,TURL model in \'TURL: Table Understanding through Representation Learning\',74.6,Accuracy (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,GitTables-SemTab-DBP,2021-10,DAGOBAH model in \'DAGOBAH: Table and Graph Contexts for Eﬀicient Semantic Annotation of Tabular Data\',7.0,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,ToughTables-DBP,2021-10,JenTab model in \'JenTab Meets SemTab 2021\'s New Challenges\',46.0,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,T2Dv2,2019-05,HNN + P2Vec model in \'Learning Semantic Annotations for Tabular Data\',96.6,Accuracy (%),pos
Natural language analysis,Text annotation,Table annotation,Column Type Annotation,T2Dv2,2018-11,ColNet - Ensemble model in \'ColNet: Embedding the Semantics of Web Tables for Column Type Prediction\',94.9,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Columns Property Annotation,T2Dv2,2017-03,T2K model in \'Matching web tables to DBpedia-A feature utility study\',81.0,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Columns Property Annotation,WikiTables-TURL-CPA,2020-06,TURL model in \'TURL: Table Understanding through Representation Learning\',94.91,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Row Annotation,T2Dv2,2017-03,T2K model in \'Matching web tables to DBpedia-A feature utility study\',80.0,F1 (%),pos
Natural language analysis,Text annotation,Table annotation,Table Type Detection,T2Dv2,2017-03,T2K model in \'Matching web tables to DBpedia-A feature utility study\',92.0,F1 (%),pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-08,SARG (greedy) model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',92.2,BLEU-1,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-08,SARG (greedy) model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',89.6,BLEU-2,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-08,SARG (greedy) model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',92.1,ROUGE-1,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-08,SARG (greedy) model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',86.0,ROUGE-2,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-08,SARG (n_beam=5) model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',46.4,Rewriting F3,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-09,RUN+BERT model in \'Incomplete Utterance Rewriting as Semantic Segmentation\',47.7,Rewriting F3,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-08,SARG (greedy) model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',62.4,Rewriting F1,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Multi-Rewrite,2020-08,SARG (n_beam=5) model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',52.5,Rewriting F2,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,CANARD,2020-08,SARG model in \'SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\',54.8,BLEU,pos
Natural language generation,Dialogue rewriting,Dialogue rewriting,Dialogue rewriting,Rewrite,2020-09,RUN+BERT model in \'Incomplete Utterance Rewriting as Semantic Segmentation\',93.5,ROUGE-L,pos
Natural language generation,Explanation Generation,Explanation Generation,Explanation Generation,CLEVR-X,2022-04,FM model in \'CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\',80.3,Acc,pos
Natural language generation,Explanation Generation,Explanation Generation,Explanation Generation,CLEVR-X,2022-04,PJ-X model in \'CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\',87.4,B4,pos
Natural language generation,Explanation Generation,Explanation Generation,Explanation Generation,CLEVR-X,2022-04,PJ-X model in \'CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\',639.8,C,pos
Natural language generation,Explanation Generation,Explanation Generation,Explanation Generation,CLEVR-X,2022-04,PJ-X model in \'CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\',58.9,M,pos
Natural language generation,Explanation Generation,Explanation Generation,Explanation Generation,CLEVR-X,2022-04,PJ-X model in \'CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations\',93.4,RL,pos
Natural language generation,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',0.1831,BLEU,pos
Natural language generation,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',95.5,Embedding Average,pos
Natural language generation,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',62.5,Greedy Matching,pos
Natural language generation,Goal-oriented dialog,Goal-oriented dialog,Goal-oriented dialog,Kvret,2020-01,IJEEL-KVL model in \'Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning\',97.4,Vector Extrema,pos
Natural language generation,Image captioning,Hindi Image Captioning,Hindi Image Captioning,Hindi Visual Genome (Challenge Set),2021-07,En-De model in \'An encoder-decoder based framework for hindi image caption generation\',0.66,BLEU,pos
Natural language generation,Image captioning,Hindi Image Captioning,Hindi Image Captioning,Hindi Visual Genome (Test Set),2021-07,En-De model in \'An encoder-decoder based framework for hindi image caption generation\',3.28,BLEU,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',103.1,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-08,SimVLM_huge model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',113.7,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',116.9,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',14.2,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',15.8,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',5700000.0,Pre-train (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-02,Enc-Dec model in \'Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts\',15000000.0,Pre-train (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-in-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',200000000.0,Pre-train (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-near-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',96.1,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-near-domain,2021-08,SimVLM_huge model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',110.9,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-near-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',113.3,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-near-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',13.8,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-near-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',15.1,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-near-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',5700000.0,Pre-train (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-near-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',200000000.0,Pre-train (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',112.82,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',62.48,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',15.22,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',37.97,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',86.33,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',72.83,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',55.94,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',32.7,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions test,2014-11,From Captions to Visual Concepts and Back model in \'From Captions to Visual Concepts and Back\',56.7,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions test,2014-11,From Captions to Visual Concepts and Back model in \'From Captions to Visual Concepts and Back\',92.5,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions test,2019-09,Unified VLP model in \'Unified Vision-Language Pre-Training for Image Captioning and VQA\',116.9,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions test,2019-09,Unified VLP model in \'Unified Vision-Language Pre-Training for Image Captioning and VQA\',21.2,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions test,2014-11,From Captions to Visual Concepts and Back model in \'From Captions to Visual Concepts and Back\',33.1,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-overall,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',95.5,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-overall,2021-08,SimVLM_huge model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',112.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-overall,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',113.4,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-overall,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',13.5,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-overall,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',15.0,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-overall,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',5700000.0,Pretrain (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-overall,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',200000000.0,Pretrain (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2021-11,VLKD(ViT-B/16) model in \'Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation\',16.7,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',24.5,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2021-11,VLKD(ViT-B/16) model in \'Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation\',58.3,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',82.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2021-11,VLKD(ViT-B/16) model in \'Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation\',13.4,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',15.7,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2021-11,VLKD(ViT-B/16) model in \'Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation\',19.7,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions Karpathy Test,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',22.5,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',88.3,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2021-08,SimVLM_huge model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',115.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2022-01,BLIP_ViT-L model in \'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\',115.3,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',12.1,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',14.0,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2022-01,BLIP_ViT-L model in \'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\',14.4,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',5700000.0,Pretrain (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-val-out-domain,2021-11,LEMON_large model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',200000000.0,Pretrain (#images),pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',65.1,BLEU-1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',17.5,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',42.6,BLEU-2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',27.8,BLEU-3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',57.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',43.4,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',35.7,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,BanglaLekhaImageCaptions,2021-02,CNN + 1D CNN model in \'Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model\',29.7,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Conceptual Captions,2021-11,ClipCap (MLP + GPT2 tuning) model in \'ClipCap: CLIP Prefix for Image Captioning\',87.26,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Conceptual Captions,2021-11,ClipCap (MLP + GPT2 tuning) model in \'ClipCap: CLIP Prefix for Image Captioning\',26.71,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Conceptual Captions,2021-11,ClipCap (MLP + GPT2 tuning) model in \'ClipCap: CLIP Prefix for Image Captioning\',18.5,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,FlickrStyle10K,2020-04,MSCap model in \'MemCap: Memorizing Style Knowledge for Image Captioning\',50.8,BLEU-1 (Romantic),pos
Natural language generation,Image captioning,Image captioning,Image captioning,Localized Narratives,2019-12,RCNN + trace positions model in \'Connecting Vision and Language with Localized Narratives\',106.5,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Localized Narratives,2021-08,LoopCAG model in \'Control Image Captioning Spatially and Temporally\',114.0,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',115.54,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',61.9,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',15.06,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',36.31,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',86.48,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',72.6,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',55.26,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',31.8,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2014-12,BRNN model in \'Deep Visual-Semantic Alignments for Generating Image Descriptions\',15.7,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2017-06,Cornia et al model in \'Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention\',21.3,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2019-09,Unified VLP model in \'Unified Vision-Language Pre-Training for Image Captioning and VQA\',30.1,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2014-12,BRNN model in \'Deep Visual-Semantic Alignments for Generating Image Descriptions\',24.7,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2017-06,Cornia et al model in \'Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention\',46.4,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2019-09,Unified VLP model in \'Unified Vision-Language Pre-Training for Image Captioning and VQA\',67.4,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2019-09,Unified VLP model in \'Unified Vision-Language Pre-Training for Image Captioning and VQA\',17.0,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2014-12,BRNN model in \'Deep Visual-Semantic Alignments for Generating Image Descriptions\',15.3,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2017-06,Cornia et al model in \'Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention\',20.0,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,Flickr30k Captions test,2019-09,Unified VLP model in \'Unified Vision-Language Pre-Training for Image Captioning and VQA\',23.0,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,AIC-ICC,2021-03,CMCL model in \'WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training\',66.1,BLEU,pos
Natural language generation,Image captioning,Image captioning,Image captioning,AIC-ICC,2021-03,CMCL model in \'WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training\',220.7,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,AIC-ICC,2021-03,CMCL model in \'WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training\',71.9,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,AIC-ICC,2021-03,CMCL model in \'WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training\',41.1,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,SCICAP,2021-10,"CNN+LSTM (Vision only, First sentence) model in \'SciCap: Generating Captions for Scientific Figures\'",0.0219,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-05,"NIC (ResNet-50, CutMix) model in \'CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\'",64.2,BLEU-1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-05,"NIC (ResNet-50, CutMix) model in \'CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\'",24.9,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2020-12,UNIMO-large model in \'UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning\',39.6,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-05,"NIC (ResNet-50, CutMix) model in \'CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\'",46.3,BLEU-2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-05,"NIC (ResNet-50, CutMix) model in \'CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\'",33.6,BLEU-3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-05,"NIC (ResNet-50, CutMix) model in \'CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\'",77.6,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-08,RDN model in \'Reflective Decoding Network for Image Captioning\',125.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-12,M2 Transformer model in \'Meshed-Memory Transformer for Image Captioning\',131.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-05,"NIC (ResNet-50, CutMix) model in \'CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\'",49.0,ROUGE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO,2019-05,"NIC (ResNet-50, CutMix) model in \'CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\'",23.1,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',110.14,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',57.57,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',13.74,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2021-08,Single Model model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',13.89,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',25.78,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',81.73,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',65.48,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',45.58,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',28.17,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',114.25,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',61.2,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',14.85,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',34.65,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',85.62,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',71.36,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',53.62,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',31.27,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',95.5,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',122.04,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',55.49,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',60.96,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',12.66,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',15.7,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',21.79,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',30.04,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',79.44,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',85.99,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',61.15,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',71.28,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',41.03,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',52.66,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',26.56,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD out-of-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',30.45,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',100.12,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',114.25,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',123.39,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',58.26,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',61.2,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',63.12,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',14.04,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',14.85,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',15.94,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',28.95,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',34.65,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',37.35,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',82.27,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',85.62,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',88.1,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',66.04,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',71.36,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',74.81,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',47.48,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',53.62,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',57.68,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',29.47,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2021-11,Microsoft Cognitive Services team model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',31.27,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD entire,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',32.5,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2018-02,Xmodal-Ctx model in \'007: Democratically Finding The Cause of Packet Drops\',83.4,BLEU-1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2022-07,GRIT (No VL pretraining - base) model in \'GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features\',84.2,BLEU-1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2014-11,From Captions to Visual Concepts and Back model in \'From Captions to Visual Concepts and Back\',25.7,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2018-02,Xmodal-Ctx model in \'007: Democratically Finding The Cause of Packet Drops\',41.4,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-04,Oscar model in \'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\',41.7,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-11,LEMON model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',42.6,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",44.9,BLEU-4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-03,X-Transformer model in \'X-Linear Attention Networks for Image Captioning\',65.8,BLEU-2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-03,X-Transformer model in \'X-Linear Attention Networks for Image Captioning\',51.5,BLEU-3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2018-02,Xmodal-Ctx model in \'007: Democratically Finding The Cause of Packet Drops\',60.4,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2022-07,GRIT (No VL pretraining - base) model in \'GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features\',60.7,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2018-02,Xmodal-Ctx model in \'007: Democratically Finding The Cause of Packet Drops\',24.0,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-04,Oscar model in \'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\',24.5,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',25.2,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',25.4,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-11,LEMON model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',25.5,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",26.6,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2018-02,Xmodal-Ctx model in \'007: Democratically Finding The Cause of Packet Drops\',139.9,CIDER,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-04,Oscar model in \'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\',140.0,CIDER,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',140.9,CIDER,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',143.3,CIDER,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-11,LEMON model in \'Scaling Up Vision-Language Pre-training for Image Captioning\',145.5,CIDER,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",154.9,CIDER,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2019-08,RDN model in \'Reflective Decoding Network for Image Captioning\',125.2,CIDEr-D,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-02,AoANet + VC model in \'Visual Commonsense R-CNN\',131.6,CIDEr-D,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-03,X-Transformer model in \'X-Linear Attention Networks for Image Captioning\',132.8,CIDEr-D,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2022-07,GRIT (No VL pretraining - base) model in \'GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features\',144.2,CIDEr-D,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2014-11,From Captions to Visual Concepts and Back model in \'From Captions to Visual Concepts and Back\',23.6,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2018-02,Xmodal-Ctx model in \'007: Democratically Finding The Cause of Packet Drops\',30.4,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2020-04,Oscar model in \'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\',30.6,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-01,VinVL model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',31.1,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,COCO Captions,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',33.4,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps val,2021-02,VL-T5 model in \'Unifying Vision-and-Language Tasks via Text Generation\',4.4,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps val,2021-10,FewVLM model in \'A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models\',42.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps val,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',58.7,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps val,2021-02,VL-T5 model in \'Unifying Vision-and-Language Tasks via Text Generation\',5.3,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps val,2021-10,FewVLM model in \'A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models\',8.5,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps val,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',8.6,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',101.2,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',123.92,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',58.76,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',63.5,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',14.27,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',15.96,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',30.21,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',38.44,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',82.88,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',88.56,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',67.01,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',75.48,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',48.73,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',58.46,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',30.0,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD near-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',32.86,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',100.62,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',122.4,CIDEr,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',59.43,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',64.02,ROUGE-L,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',14.7,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',16.18,SPICE,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',32.07,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',41.65,B4,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',82.94,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',88.55,B1,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',67.56,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',76.1,B2,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',49.66,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',60.53,B3,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2020-09,Microsoft Cognitive Services team model in \'VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\',30.62,METEOR,pos
Natural language generation,Image captioning,Image captioning,Image captioning,nocaps-XD in-domain,2022-05,GIT model in \'GIT: A Generative Image-to-text Transformer for Vision and Language\',33.41,METEOR,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k,2018-11,COCO_ELMo_PNASNet model in \'Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding\',69.19,Pointing Game Accuracy,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Visual Genome,2018-11,VG_ELMo_PNASNet model in \'Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding\',55.16,Pointing Game Accuracy,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,ReferIt,2016-06,MCB model in \'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\',28.91,Accuracy,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,ReferIt,2018-11,VG_BiLSTM_VGG model in \'Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding\',62.76,Pointing Game Accuracy,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2016-06,MCB model in \'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\',48.69,Accuracy,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2015-11,GroundeR 100.0% annot. model in \'Grounding of Textual Phrases in Images by Reconstruction\',48.38,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2018-05,DDPN (ResNet-101) model in \'Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding\',73.3,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2019-09,Soft-Label Chain CRF (SL-CCRF) model in \'Phrase Grounding by Soft-Label Chain Conditional Random Field\',74.69,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2020-02,LCMCG model in \'Learning Cross-modal Context Graph for Visual Grounding\',76.74,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2021-04,MDETR-ENB5 model in \'MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding\',84.3,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2021-12,GLIP model in \'Grounded Language-Image Pre-training\',87.1,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2022-06,GLIPv2 model in \'GLIPv2: Unifying Localization and Vision-Language Understanding\',87.7,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2018-05,BAN (Bottom-Up detector) model in \'Bilinear Attention Networks\',86.35,R-at-10,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',86.51,R-at-10,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2021-04,MDETR-ENB5 model in \'MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding\',95.8,R-at-10,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2021-12,GLIP model in \'Grounded Language-Image Pre-training\',98.1,R-at-10,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2018-05,BAN (Bottom-Up detector) model in \'Bilinear Attention Networks\',84.22,R-at-5,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',84.98,R-at-5,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2021-04,MDETR-ENB5 model in \'MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding\',93.9,R-at-5,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Test,2021-12,GLIP model in \'Grounded Language-Image Pre-training\',96.9,R-at-5,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Dev,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',70.4,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Dev,2022-05,PEVL model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',84.1,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Dev,2022-06,Fiber-B model in \'Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone\',87.1,R-at-1,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Dev,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',86.31,R-at-10,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Dev,2022-06,Fiber-B model in \'Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone\',97.4,R-at-10,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Dev,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',84.49,R-at-5,pos
Natural language generation,Image captioning,Phrase grounding,Phrase grounding,Flickr30k Entities Dev,2022-06,Fiber-B model in \'Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone\',96.1,R-at-5,pos
Natural language generation,Image captioning,Relational captioning,Relational captioning,relational captioning dataset,2019-03,MTTSNet model in \'Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning\',34.27,Image-Level Recall,pos
Natural language generation,Image captioning,Relational captioning,Relational captioning,relational captioning dataset,2020-10,MTTSNet (extended) model in \'Dense Relational Image Captioning via Multi-task Triple-Stream Networks\',45.96,Image-Level Recall,pos
Natural language generation,Image captioning,Semi Supervised Learning for Image Captioning,Semi Supervised Learning for Image Captioning,COCO,2021-08,"Perturb, Predict & Paraphrase model in \'Perturb, Predict & Paraphrase: Semi-Supervised Learning using Noisy Student for Image Captioning\'",84.5,CIDEr,pos
Natural language generation,Language modelling,Cross-document language modeling,Cross-document language modeling,MultiNews val,2021-01,CD-LM model in \'CDLM: Cross-Document Language Modeling\',-1.69,Perplexity,neg
Natural language generation,Language modelling,Cross-document language modeling,Cross-document language modeling,MultiNews test,2021-01,CD-LM model in \'CDLM: Cross-Document Language Modeling\',-1.76,Perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2016-12,GCNN-8 model in \'Language Modeling with Gated Convolutional Networks\',-37.2,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2018-03,"LSTM (Hebbian, Cache, MbPA) model in \'Fast Parametric Learning with Activation Memorization\'",-29.2,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2018-09,Transformer (Adaptive inputs) model in \'Adaptive Input Representations for Neural Language Modeling\',-18.7,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2019-01,Transformer-XL Large model in \'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\',-18.3,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2019-02,GPT-2 Full model in \'Language Models are Unsupervised Multitask Learners\',-17.48,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2019-04,Transformer-XL (RMS dynamic eval) model in \'Dynamic Evaluation of Transformer Language Models\',-16.4,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2019-09,Megatron-LM model in \'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\',-10.81,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2018-03,LSTM model in \'Fast Parametric Learning with Activation Memorization\',36.0,Validation perplexity,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2020-05,Decay RNN model in \'How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?\',76.67,Validation perplexity,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2018-03,4 layer QRNN model in \'An Analysis of Neural Language Modeling at Multiple Scales\',151000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2018-09,Transformer (Adaptive inputs) model in \'Adaptive Input Representations for Neural Language Modeling\',247000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2019-01,Transformer-XL Large model in \'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\',257000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2019-02,GPT-2 Full model in \'Language Models are Unsupervised Multitask Learners\',1542000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2019-09,Megatron-LM model in \'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\',8300000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2021-03,GLM-XXLarge (unidirectional) model in \'GLM: General Language Model Pretraining with Autoregressive Blank Infilling\',10000000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-103,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.566,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,enwiki8,2020-09,PAR Transformer 24B model in \'Pay Attention when Required\',1.11,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,GitHub,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.377,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Character Level),2016-09,2-layer Norm HyperLSTM model in \'HyperNetworks\',14400000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Character Level),2016-11,NAS-RL model in \'Neural Architecture Search with Reinforcement Learning\',16300000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Character Level),2017-05,FS-LSTM-4 model in \'Fast-Slow Recurrent Neural Networks\',27000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Character Level),2016-09,2-layer Norm HyperLSTM model in \'HyperNetworks\',1.219,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Character Level),2018-03,Temporal Convolutional Network model in \'An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\',1.31,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Character Level),2019-05,Bipartite Flow model in \'Discrete Flows: Invertible Generative Models of Discrete Data\',1.38,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,USPTO Backgrounds,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.546,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,100 sleep nights of 8 caregivers,2018-02,Gpt3 model in \'007: Democratically Finding The Cause of Packet Drops\',1.0,10%,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Wiki-40B,2021-07,Combiner-Axial-8k model in \'Combiner: Full Attention Transformer with Sparse Computation Cost\',-16.49,Perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Wiki-40B,2022-02,FLASH-Quad-8k model in \'Transformer Quality in Linear Time\',-14.998,Perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,OpenWebtext2,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.677,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2014-09,Zaremba et al. (2014) - LSTM (large) model in \'Recurrent Neural Network Regularization\',-78.4,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2015-12,Gal & Ghahramani (2016) - Variational LSTM (large) model in \'A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\',-75.2,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2016-07,Recurrent highway networks model in \'Recurrent Highway Networks\',-65.4,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2016-11,NAS-RL model in \'Neural Architecture Search with Reinforcement Learning\',-64.0,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2017-08,GL-LWGC + AWD-MoS-LSTM + dynamic eval model in \'Gradual Learning of Recurrent Neural Networks\',-46.34,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2019-02,GPT-2 model in \'Language Models are Unsupervised Multitask Learners\',-35.76,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2019-04,BERT-Large-CAS model in \'Language Models with Transformers\',-31.3,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2020-05,GPT-3 (Zero-Shot) model in \'Language Models are Few-Shot Learners\',-20.5,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2014-09,Zaremba et al. (2014) - LSTM (medium) model in \'Recurrent Neural Network Regularization\',86.2,Validation perplexity,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2016-07,Recurrent highway networks model in \'Recurrent Highway Networks\',23000000.0,Params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2016-11,NAS-RL model in \'Neural Architecture Search with Reinforcement Learning\',25000000.0,Params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2017-08,GL-LWGC + AWD-MoS-LSTM + dynamic eval model in \'Gradual Learning of Recurrent Neural Networks\',26000000.0,Params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2018-08,AWD-LSTM-DOC x5 model in \'Direct Output Connection for a High-Rank Language Model\',185000000.0,Params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2019-02,GPT-2 model in \'Language Models are Unsupervised Multitask Learners\',1542000000.0,Params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Penn Treebank (Word Level),2020-05,GPT-3 (Zero-Shot) model in \'Language Models are Few-Shot Learners\',175000000000.0,Params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,PTB,2019-11,I-DARTS model in \'Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition\',-56.0,PPL,neg
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2016-10,Gated-Attention Reader (+ features) model in \'Broad Context Language Modeling as Reading Comprehension\',49.0,Accuracy,pos
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2018-07,Universal Transformer (w/ dynamic halting) model in \'Universal Transformers\',56.25,Accuracy,pos
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2019-02,GPT-2 1.5B (Zero Shot) model in \'Language Models are Unsupervised Multitask Learners\',63.24,Accuracy,pos
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',86.4,Accuracy,pos
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2022-01,"Megatron-Turing NLG 530B (Few-Shot) model in \'Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\'",87.2,Accuracy,pos
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2022-04,PaLM-540B (Few-Shot) model in \'PaLM: Scaling Language Modeling with Pathways\',89.7,Accuracy,pos
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2019-02,GPT-2 1.5B (Zero Shot) model in \'Language Models are Unsupervised Multitask Learners\',-8.63,Perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,LAMBADA,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',-1.92,Perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,Pile CC,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.691,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2018-09,Adaptive Input Large model in \'Adaptive Input Representations for Neural Language Modeling\',23.83,Validation perplexity,pos
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2021-07,H-Transformer-1D Nr=16 (Base) model in \'H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences\',23.95,Validation perplexity,pos
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2021-02,SRU++ Large model in \'When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute\',465000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2013-12,RNN-1024 + 9 Gram model in \'One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling\',-51.3,PPL,neg
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2016-02,10 LSTM+CNN inputs + SNM10-SKIP (ensemble) model in \'Exploring the Limits of Language Modeling\',-23.7,PPL,neg
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2018-09,Adaptive Input Very Large model in \'Adaptive Input Representations for Neural Language Modeling\',-23.02,PPL,neg
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2019-01,Transformer-XL Large model in \'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\',-21.8,PPL,neg
Natural language generation,Language modelling,Language modelling,Language modelling,One Billion Word,2021-03,OmniNetT (Large) model in \'OmniNet: Omnidirectional Representations from Transformers\',-21.5,PPL,neg
Natural language generation,Language modelling,Language modelling,Language modelling,enwik8,2016-07,Recurrent Highway Networks model in \'Recurrent Highway Networks\',46000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,enwik8,2017-05,Large FS-LSTM-4 model in \'Fast-Slow Recurrent Neural Networks\',47000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,enwik8,2018-08,Transformer (64 layers) model in \'Character-Level Language Modeling with Deeper Self-Attention\',235000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,enwik8,2019-01,Transformer-XL (24 layers) model in \'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\',277000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,enwik8,2019-02,"GPT-2 (48 layers, h=1600) model in \'Language Models are Unsupervised Multitask Learners\'",1542000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,enwik8,2013-08,LSTM (7 layers) model in \'Generating Sequences With Recurrent Neural Networks\',1.67,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,PubMed Cognitive Control Abstracts,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.577,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Bookcorpus2,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.741,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,HackerNews,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.89,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,The Pile,2019-02,GPT-2 (Zero-Shot) model in \'Language Models are Unsupervised Multitask Learners\',1.2253,Bits per byte,pos
Natural language generation,Language modelling,Language modelling,Language modelling,FreeLaw,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.513,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,DM Mathematics,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",1.14,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Ubuntu IRC,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",1.09,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,PubMed Central,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.525,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Hutter Prize,2016-07,Large RHN model in \'Recurrent Highway Networks\',46000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Hutter Prize,2017-05,Large FS-LSTM-4 model in \'Fast-Slow Recurrent Neural Networks\',47000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Hutter Prize,2018-08,64-layer Character Transformer Model model in \'Character-Level Language Modeling with Deeper Self-Attention\',235000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Hutter Prize,2019-01,24-layer Transformer-XL model in \'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\',277000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Hutter Prize,2016-07,RHN - depth 5 [zilly2016recurrent] model in \'Recurrent Highway Networks\',1.31,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,OpenSubtitles,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.899,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,C4,2021-09,Primer model in \'Primer: Searching for Efficient Transformers for Language Modeling\',-12.35,Perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,C4,2021-09,Primer model in \'Primer: Searching for Efficient Transformers for Language Modeling\',17300.0,TPUv3 Hours,pos
Natural language generation,Language modelling,Language modelling,Language modelling,C4,2021-09,Original T5 model in \'Primer: Searching for Efficient Transformers for Language Modeling\',1000000.0,Steps,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Books3,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.712,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Text8,2016-03,BN LSTM model in \'Recurrent Batch Normalization\',16000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Text8,2016-09,Unregularised mLSTM model in \'Multiplicative LSTM for sequence modelling\',45000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Text8,2018-08,64-layer Character Transformer Model model in \'Character-Level Language Modeling with Deeper Self-Attention\',235000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Text8,2019-01,Transformer-XL - 24 layers model in \'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\',277000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Text8,2019-02,GPT-2 model in \'Language Models are Unsupervised Multitask Learners\',1542000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Text8,2016-02,"td-LSTM (Zhang et al., 2016) model in \'Architectural Complexity Measures of Recurrent Neural Networks\'",1.63,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2016-11,Inan et al. (2016) - Variational LSTM (tied) (h=650) + augmented loss model in \'Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\',-87.0,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2016-12,Grave et al. (2016) - LSTM + continuous cache pointer model in \'Improving Neural Language Models with a Continuous Cache\',-68.9,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2017-07,Melis et al. (2017) - 1-layer LSTM (tied) model in \'On the State of the Art of Evaluation in Neural Language Models\',-65.9,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2017-08,GL-LWGC + AWD-MoS-LSTM + dynamic eval model in \'Gradual Learning of Recurrent Neural Networks\',-40.46,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2018-08,Past Decode Reg. + AWD-LSTM-MoS + dyn. eval. model in \'Improved Language Modeling by Decoding the Past\',-40.3,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2018-09,FRAGE + AWD-LSTM-MoS + dynamic eval model in \'FRAGE: Frequency-Agnostic Word Representation\',-39.14,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2019-02,GPT-2 model in \'Language Models are Unsupervised Multitask Learners\',-18.34,Test perplexity,neg
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2016-11,Inan et al. (2016) - Variational LSTM (tied) (h=650) model in \'Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\',92.3,Validation perplexity,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2017-07,Melis et al. (2017) - 1-layer LSTM (tied) model in \'On the State of the Art of Evaluation in Neural Language Models\',24000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2017-08,GL-LWGC + AWD-MoS-LSTM + dynamic eval model in \'Gradual Learning of Recurrent Neural Networks\',38000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2018-08,AWD-LSTM-DOC x5 model in \'Direct Output Connection for a High-Rank Language Model\',185000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,WikiText-2,2019-02,GPT-2 model in \'Language Models are Unsupervised Multitask Learners\',1542000000.0,Number of params,pos
Natural language generation,Language modelling,Language modelling,Language modelling,enwik8 dev,2021-07,Transformer-LS (small) model in \'Long-Short Transformer: Efficient Transformers for Language and Vision\',1.01,Bit per Character (BPC),pos
Natural language generation,Language modelling,Language modelling,Language modelling,Curation Corpus,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.475,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,PhilPapers,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.695,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,arXiv,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.662,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Gutenberg PG-19,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.656,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,NIH ExPorter,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.59,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,language-modeling-recommendation,2021-12,GPT2 model in \'Zero-Shot Recommendation as Language Modeling\',48.8,1:1 Accuracy,pos
Natural language generation,Language modelling,Language modelling,Language modelling,StackExchange,2021-12,"Gopher model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",0.641,BPB,pos
Natural language generation,Language modelling,Language modelling,Language modelling,Text8 dev,2021-07,Transformer-LS (small) model in \'Long-Short Transformer: Efficient Transformers for Language and Vision\',1.03,Bit per Character (BPC),pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,SCROLLS,2021-12,LongT5 XL model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',41.89,Avg.,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,SCROLLS,2021-12,LongT5 XL model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',53.1,Qspr,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,SCROLLS,2021-12,LongT5 XL model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',29.3,Nrtv,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,SCROLLS,2021-12,LongT5 XL model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',88.2,CNLI,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,SCROLLS,2022-05,UL2 model in \'Unifying Language Learning Paradigms\',88.7,CNLI,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2020-11,BigBird model in \'Long Range Arena: A Benchmark for Efficient Transformers\',55.01,Avg,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-09,PSF model in \'Sparse Factorization of Large Square Matrices\',63.64,Avg,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-10,S4 model in \'Efficiently Modeling Long Sequences with Structured State Spaces\',80.48,Avg,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2020-11,Linear Trans. model in \'Long Range Arena: A Benchmark for Efficient Transformers\',65.9,Text,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-09,PSF model in \'Sparse Factorization of Large Square Matrices\',77.32,Text,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2022-01,CDIL model in \'Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks\',87.61,Text,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2020-11,Sparse Trans. model in \'Long Range Arena: A Benchmark for Efficient Transformers\',44.24,Image,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-09,PSF model in \'Sparse Factorization of Large Square Matrices\',45.01,Image,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-10,S4 model in \'Efficiently Modeling Long Sequences with Structured State Spaces\',87.26,Image,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2020-11,Local Attention model in \'Long Range Arena: A Benchmark for Efficient Transformers\',37.27,ListOps,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-09,PSF model in \'Sparse Factorization of Large Square Matrices\',38.85,ListOps,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-10,S4 model in \'Efficiently Modeling Long Sequences with Structured State Spaces\',58.35,ListOps,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2020-11,Performer model in \'Long Range Arena: A Benchmark for Efficient Transformers\',77.05,Pathfinder,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-09,PSF model in \'Sparse Factorization of Large Square Matrices\',80.49,Pathfinder,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-10,S4 model in \'Efficiently Modeling Long Sequences with Structured State Spaces\',86.05,Pathfinder,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2022-01,CDIL model in \'Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks\',91.0,Pathfinder,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2020-11,Sparse Trans. model in \'Long Range Arena: A Benchmark for Efficient Transformers\',59.59,Retrieval,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-09,PSF model in \'Sparse Factorization of Large Square Matrices\',76.51,Retrieval,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-10,S4 model in \'Efficiently Modeling Long Sequences with Structured State Spaces\',87.09,Retrieval,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2022-03,DSS model in \'Diagonal State Spaces are as Effective as Structured State Spaces\',87.6,Retrieval,pos
Natural language generation,Language modelling,Long-range modeling,Long-range modeling,LRA,2021-10,S4 model in \'Efficiently Modeling Long Sequences with Structured State Spaces\',88.1,Pathfinder-X,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (original corpus),2019-10,"BioBERT\n(pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, original corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",89.75,F1,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (original corpus),2019-10,"BioBERT\n(pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, original corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",88.93,Precision,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (original corpus),2019-10,"SciBERT cased\n(SciVocab, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, original corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",91.53,Recall,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,MedSTS,2019-06,NCBI_BERT(base) (P+M) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',0.848,Pearson Correlation,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015) model in \'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\'",0.8676,Pearson Correlation,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015) model in \'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\'",-0.2532,MSE,neg
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015) model in \'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\'",0.8083,Spearman Correlation,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,ClinicalSTS,2020-10,"CharacterBERT (base, medical, ensemble) model in \'CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters\'",85.62,Pearson Correlation,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (expanded corpus),2019-10,"BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",93.38,F1,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (expanded corpus),2019-10,"BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",92.98,Precision,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,Annotated corpus for semantic similarity of clinical trial outcomes (expanded corpus),2019-10,"BioBERT (pre-trained on PubMed abstracts + PMC, fine-tuned on \""Annotated corpus for semantic similarity of clinical trial outcomes, expanded corpus\"") model in \'Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations\'",93.85,Recall,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,BIOSSES,2019-06,NCBI_BERT(base) (P+M) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',0.916,Pearson Correlation,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,BIOSSES,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',0.9363,Pearson Correlation,pos
Natural language generation,Language modelling,Sentence pair modeling,Semantic similarity estimation,CHIP-STS,2021-06,MacBERT-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',85.6,Macro F1,pos
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge European,2014-12,Deep Speech model in \'Deep Speech: Scaling up end-to-end speech recognition\',-31.2,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge European,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-17.55,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge Commonwealth,2014-12,Deep Speech model in \'Deep Speech: Scaling up end-to-end speech recognition\',-28.46,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge Commonwealth,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-13.56,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge Indian,2014-12,Deep Speech model in \'Deep Speech: Scaling up end-to-end speech recognition\',-45.35,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge Indian,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-22.44,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge American-Canadian,2014-12,Deep Speech model in \'Deep Speech: Scaling up end-to-end speech recognition\',-15.01,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Accented speech recognition,VoxForge American-Canadian,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-7.55,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Automatic Speech Recognition,LRS2,2018-09,TM-CTC model in \'Deep Audio-Visual Speech Recognition\',10.1,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Distant speech recognition,DIRHA English WSJ,2018-11,Li-GRU model in \'The PyTorch-Kaldi Speech Recognition Toolkit\',-23.9,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Distant speech recognition,CHiME-4 real 6ch,2018-03,HMM-TDNN(LFMMI) + LSTMLM + NN-GEV model in \'Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline\',-2.74,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Lip to speech synthesis,LRW,2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.197,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Lip to speech synthesis,LRW,2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.344,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Lip to speech synthesis,LRW,2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.543,STOI,pos
Natural language generation,Language modelling,Speech recognition,Noisy speech recognition,CHiME real,2014-12,CNN + Bi-RNN + CTC (speech to letters) model in \'Deep Speech: Scaling up end-to-end speech recognition\',-67.94,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Noisy speech recognition,CHiME real,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-21.79,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Noisy speech recognition,CHiME real,2018-03,HMM-TDNN(LFMMI) + LSTMLM  model in \'Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline\',-11.4,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Noisy speech recognition,CHiME clean,2014-12,CNN + Bi-RNN + CTC (speech to letters) model in \'Deep Speech: Scaling up end-to-end speech recognition\',-6.3,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Noisy speech recognition,CHiME clean,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-3.34,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (DL),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.671,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (DL),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.183,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (DL),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.402,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (DL),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.282,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (DL),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.576,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chess),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.4,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chess),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',1.503,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chess),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.29,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chess),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.334,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chess),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.418,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chess),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.506,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (EH),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.367,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (EH),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.22,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (EH),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.304,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (EH),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.369,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (EH),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.463,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,GRID corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.772,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,GRID corpus (mixed-speech),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',1.984,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,GRID corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.535,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,GRID corpus (mixed-speech),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.579,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,GRID corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.731,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,GRID corpus (mixed-speech),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.738,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (HS),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.29,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (HS),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',1.366,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (HS),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.311,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (HS),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.337,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (HS),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.446,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (HS),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.504,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chem),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.3,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chem),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',1.529,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chem),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.284,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chem),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.429,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chem),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.416,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,Lip2Wav (Chem),2021-11,Visual Voice Memory model in \'Speech Reconstruction with Reminiscent Sound via Visual Voice Memory\',0.566,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,TCD-TIMIT corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',1.35,PESQ,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,TCD-TIMIT corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',36.5,ESTOI,pos
Natural language generation,Language modelling,Speech recognition,Speaker-specific lip to speech synthesis,TCD-TIMIT corpus (mixed-speech),2020-05,Lip2Wav model in \'Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\',0.558,STOI,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,ShEMO,2022-04,CNN model in \'Emotion Recognition In Persian Speech Using Deep Neural Networks\',65.2,Unweighted Accuracy,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,MSP-Podcast (Valence),2021-02,preCPC model in \'Contrastive Unsupervised Learning for Speech Emotion Recognition\',0.377,CCC,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',82.99,Accuracy,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',0.82,Precision,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',0.82,Recall,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,RAVDESS,2020-07,CNN-X  (Shallow CNN) model in \'Shallow over Deep Neural Networks: A empirical analysis for human emotion classification using audio data\',0.82,F1 Score,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,MSP-Podcast (Dominance),2021-02,preCPC model in \'Contrastive Unsupervised Learning for Speech Emotion Recognition\',0.639,CCC,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,MSP-Podcast (Activation),2021-02,preCPC model in \'Contrastive Unsupervised Learning for Speech Emotion Recognition\',0.706,CCC,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,IEMOCAP,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression) model in \'Multimodal Speech Emotion Recognition and Ambiguity Resolution\',0.718,F1,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,IEMOCAP,2018-02,CNN+LSTM model in \'CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation\',0.65,UA,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,IEMOCAP,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression) model in \'Multimodal Speech Emotion Recognition and Ambiguity Resolution\',0.701,UA,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,IEMOCAP,2020-10,SYSCOMB: BLSTMATT with CSA model in \'Empirical Interpretation of Speech Emotion Perception with Attention Based Model for Speech Emotion Recognition\',0.74,UA,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,IEMOCAP,2021-09,SER with MTL model in \'Speech Emotion Recognition with Multi-Task Learning\',0.78,UA,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,IEMOCAP,2019-11,LSTM+FC model in \'Speech Emotion Recognition Using Speech Feature and Word Embedding\',0.755,WAP,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,IEMOCAP,2020-10,DANN model in \'Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition\',0.827,WAP,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,CREMA-D,2020-01,GRU model in \'Visually Guided Self Supervised Learning of Speech Representations\',55.01,Accuracy,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,CREMA-D,2020-02,ResNet-18 + PyNADA model in \'Non-linear Neurons with Human-like Apical Dendrite Activations\',65.15,Accuracy,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,CREMA-D,2021-03,ResNet-18 + SPEL model in \'Self-paced ensemble learning for speech and audio classification\',68.12,Accuracy,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,CREMA-D,2022-03,SepTr model in \'SepTr: Separable Transformer for Audio Spectrogram Processing\',70.47,Accuracy,pos
Natural language generation,Language modelling,Speech recognition,Speech emotion recognition,CREMA-D,2022-05,SepTr + LeRaC model in \'LeRaC: Learning Rate Curriculum\',70.95,Accuracy,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,GigaSpeech DEV,2021-06,"Conformer/Transformer-AED model in \'GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio\'",-10.9,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice,2021-04,SpeechStew (1B) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',10.8,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice Spanish,2021-01,"VoxPopuli-50K model in \'VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation\'",10.0,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice Spanish,2021-10,QuartzNet15x5ES (CV-only) model in \'Scribosermo: Fast Speech-to-Text models for German and other Languages\',10.5,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,AISHELL-1,2018-08,Att model in \'End-to-end Speech Recognition with Adaptive Computation Steps\',-18.7,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,AISHELL-1,2019-09,CTC/Att model in \'A Comparative Study on Transformer vs RNN in Speech Applications\',-6.7,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,AISHELL-1,2020-05,CTC-CRF 4gram-LM model in \'CAT: A CTC-CRF based ASR Toolkit Bridging the Hybrid and the End-to-end Approaches towards Data Efficiency and Low Latency\',-6.34,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,AISHELL-1,2020-12,CTC/Att model in \'Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition\',-4.72,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-other,2019-12,TDS 60k pseudo-label + CTC fine-tuning + 4gram-LM model in \'Libri-Light: A Benchmark for ASR with Limited or No Supervision\',-56.6,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-other,2020-06,wav2vec 2.0 Large-10h-LV-60k model in \'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\',-5.0,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-other,2019-12,CPC unlab-60k model in \'Libri-Light: A Benchmark for ASR with Limited or No Supervision\',-13.42,ABX-across,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-other,2019-12,CPC unlab-60k model in \'Libri-Light: A Benchmark for ASR with Limited or No Supervision\',8.14,ABX-within,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-other,2020-05,S6000h-n42-τ2 → 0.1 model in \'Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization\',12.05,ABX-within,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Tedlium,2021-04,SpeechStew (100M) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-5.3,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Google Speech Commands - Musan,2021-12,ImportantAug model in \'ImportantAug: a data augmentation agent for speech\',13.3,Error rate - SNR 0dB,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard CallHome,2021-04,SpeechStew (100M) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-8.3,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 CallHome,2019-09,Espresso model in \'Espresso: A Fast End-to-end Neural Speech Recognition Toolkit\',-19.1,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,GigaSpeech TEST,2021-06,"Conformer/Transformer-AED model in \'GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio\'",-10.8,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,SLUE,2021-11,W2V2-B-VP100K  model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\',21.6,VoxPopuli (Dev),pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,SLUE,2021-11,W2V2-B-VP100K  model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\',22.4,VoxPopuli (Test),pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,SLUE,2021-11,W2V2-B-VP100K  model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\',29.9,VoxCeleb (Dev),pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,SLUE,2021-11,W2V2-B-VP100K  model in \'SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech\',33.4,VoxCeleb (Test),pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,AMI SDM1,2021-04,SpeechStew (100M) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-21.7,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2014-06,DNN + Dropout model in \'Building DNN Acoustic Models for Large Vocabulary Speech Recognition\',-15.0,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2014-12,"CNN + Bi-RNN + CTC (speech to letters), 25.9% WER if trainedonlyon SWB model in \'Deep Speech: Scaling up end-to-end speech recognition\'",-12.6,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2015-05,IBM 2015 model in \'The IBM 2015 English Conversational Telephone Speech Recognition System\',-8.0,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2016-04,"RNN + VGG + LSTM acoustic model trained on SWB+Fisher+CH, N-gram + \""model M\"" + NNLM language model model in \'The IBM 2016 English Conversational Telephone Speech Recognition System\'",-6.6,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2016-09,Microsoft 2016 model in \'The Microsoft 2016 Conversational Speech Recognition System\',-6.2,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2016-10,Microsoft 2016b model in \'Achieving Human Parity in Conversational Speech Recognition\',-5.8,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2017-03,ResNet + BiLSTMs acoustic model model in \'English Conversational Telephone Speech Recognition by Humans and Machines\',-5.5,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2020-01,IBM (LSTM encoder-decoder) model in \'Single headed attention based sequence-to-sequence model for state-of-the-art results on Switchboard\',-4.7,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard + Hub500,2021-05,IBM (LSTM+Conformer encoder-decoder) model in \'On the limit of English conversational speech recognition\',-4.3,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-5.33,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2017-12,Gated ConvNets model in \'Letter-Based Speech Recognition with Gated ConvNets\',-4.8,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2018-04,tdnn + chain + rnnlm rescoring model in \'Neural Network Language Modeling with Letter-based Features and Importance Sampling\',-3.06,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2019-04,LAS + SpecAugment  model in \'SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\',-2.5,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2019-05,Hybrid model with Transformer rescoring model in \'RWTH ASR Systems for LibriSpeech: Hybrid vs Attention -- w/o Data Augmentation\',-2.3,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2019-10,Multi-Stream Self-Attention With Dilated 1D Convolutions model in \'State-of-the-Art Speech Recognition Using Multi-Stream Self-Attention With Dilated 1D Convolutions\',-2.2,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2019-11,Conv + Transformer AM + Pseudo-Labeling (ConvLM with Transformer Rescoring) model in \'End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures\',-2.03,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2020-05,ContextNet + SpecAugment-based Noisy Student Training with Libri-Light model in \'Improved Noisy Student Training for Automatic Speech Recognition\',-1.7,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-clean,2020-10,Conformer + Wav2vec 2.0 + SpecAugment-based Noisy Student Training with Libri-Light model in \'Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition\',-1.4,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,WSJ eval93,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-4.98,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 SwitchBoard,2019-04,Jasper DR 10x5 model in \'Jasper: An End-to-End Convolutional Neural Acoustic Model\',16.2,CallHome,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 SwitchBoard,2020-05,CTC-CRF model in \'CAT: A CTC-CRF based ASR Toolkit Bridging the Hybrid and the End-to-end Approaches towards Data Efficiency and Low Latency\',18.4,CallHome,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 SwitchBoard,2019-04,Jasper DR 10x5 model in \'Jasper: An End-to-End Convolutional Neural Acoustic Model\',7.8,SwitchBoard,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 SwitchBoard,2020-05,CTC-CRF model in \'CAT: A CTC-CRF based ASR Toolkit Bridging the Hybrid and the End-to-end Approaches towards Data Efficiency and Low Latency\',9.7,SwitchBoard,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 SwitchBoard,2019-09,Espresso model in \'Espresso: A Fast End-to-end Neural Speech Recognition Toolkit\',9.2,Eval2000,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 SwitchBoard,2020-05,CTC-CRF model in \'CAT: A CTC-CRF based ASR Toolkit Bridging the Hybrid and the End-to-end Approaches towards Data Efficiency and Low Latency\',14.1,Hub5'00,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Hub5'00 FISHER-SWBD,2020-05,CTC-CRF model in \'CAT: A CTC-CRF based ASR Toolkit Bridging the Hybrid and the End-to-end Approaches towards Data Efficiency and Low Latency\',-12.0,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2015-12,Deep Speech 2 model in \'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\',-13.25,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2018-04,tdnn + chain + rnnlm rescoring model in \'Neural Network Language Modeling with Letter-based Features and Importance Sampling\',-7.63,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2019-04,LAS + SpecAugment model in \'SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\',-5.8,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2019-05,Hybrid model with Transformer rescoring model in \'RWTH ASR Systems for LibriSpeech: Hybrid vs Attention -- w/o Data Augmentation\',-5.0,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2019-10,hybrid + Transformer LM rescoring model in \'Transformer-based Acoustic Modeling for Hybrid Speech Recognition\',-4.85,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2019-11,Conv + Transformer AM (ConvLM with Transformer Rescoring) model in \'End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures\',-4.11,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2020-05,ContextNet + SpecAugment-based Noisy Student Training with Libri-Light model in \'Improved Noisy Student Training for Automatic Speech Recognition\',-3.4,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2020-06,wav2vec 2.0 with Libri-Light model in \'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\',-3.3,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2020-10,Conformer + Wav2vec 2.0 + SpecAugment-based Noisy Student Training with Libri-Light model in \'Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition\',-2.6,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech test-other,2021-08,w2v-BERT XXL model in \'W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training\',-2.5,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech train-clean-100 test-clean,2020-10,wav2vec_wav2letter model in \'Self-training and Pre-training are Complementary for Speech Recognition\',-2.8,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,LibriSpeech train-clean-100 test-other,2020-10,wav2vec_wav2letter model in \'Self-training and Pre-training are Complementary for Speech Recognition\',-3.6,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard (300hr),2018-09,End-to-end LF-MMI model in \'End-to-end speech recognition using lattice-free MMI\',-9.3,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,SPGISpeech,2021-04,"Conformer model in \'SPGISpeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition\'",-5.7,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,MediaSpeech,2021-03,wav2vec model in \'MediaSpeech: Multilanguage ASR Benchmark and Dataset\',0.9596,WER for Arabic,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,MediaSpeech,2021-03,Deepspeech model in \'MediaSpeech: Multilanguage ASR Benchmark and Dataset\',0.4741,WER for French,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,MediaSpeech,2021-03,Deepspeech model in \'MediaSpeech: Multilanguage ASR Benchmark and Dataset\',0.4236,WER for Spanish,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,MediaSpeech,2021-03,wav2vec model in \'MediaSpeech: Multilanguage ASR Benchmark and Dataset\',0.5812,WER for Turkish,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-clean,2019-12,TDS 60k pseudo-label + CTC fine-tuning + 4gram-LM model in \'Libri-Light: A Benchmark for ASR with Limited or No Supervision\',-29.3,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-clean,2020-06,wav2vec 2.0 Large-10h-LV-60k model in \'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\',-2.5,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-clean,2019-12,CPC unlab-60k model in \'Libri-Light: A Benchmark for ASR with Limited or No Supervision\',-7.56,ABX-across,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-clean,2019-12,CPC unlab-60k model in \'Libri-Light: A Benchmark for ASR with Limited or No Supervision\',5.83,ABX-within,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Libri-Light test-clean,2020-05,S6000h-n42-τ2 → 0.1 model in \'Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization\',9.33,ABX-within,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,swb_hub_500 WER fullSWBCH,2014-06,DNN + Dropout model in \'Building DNN Acoustic Models for Large Vocabulary Speech Recognition\',-19.1,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,swb_hub_500 WER fullSWBCH,2014-12,"CNN + Bi-RNN + CTC (speech to letters), 25.9% WER if trainedonlyon SWB model in \'Deep Speech: Scaling up end-to-end speech recognition\'",-16.0,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,swb_hub_500 WER fullSWBCH,2016-04,"RNN + VGG + LSTM acoustic model trained on SWB+Fisher+CH, N-gram + \""model M\"" + NNLM language model model in \'The IBM 2016 English Conversational Telephone Speech Recognition System\'",-12.2,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,swb_hub_500 WER fullSWBCH,2016-09,"VGG/Resnet/LACE/BiLSTM acoustic model trained on SWB+Fisher+CH, N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast model in \'The Microsoft 2016 Conversational Speech Recognition System\'",-11.9,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,swb_hub_500 WER fullSWBCH,2017-03,ResNet + BiLSTMs acoustic model model in \'English Conversational Telephone Speech Recognition by Humans and Machines\',-10.3,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,swb_hub_500 WER fullSWBCH,2020-01,IBM (LSTM encoder-decoder) model in \'Single headed attention based sequence-to-sequence model for state-of-the-art results on Switchboard\',-7.8,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,swb_hub_500 WER fullSWBCH,2021-05,IBM (LSTM+Conformer encoder-decoder) model in \'On the limit of English conversational speech recognition\',-6.8,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,TUDA,2015-12,PocketSphinx model in \'Open Source German Distant Speech Recognition: Corpus and Acoustic Model\',39.6,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice vi,2021-09,Vietnamese end-to-end speech recognition using wav2vec 2.0 by VietAI model in \'Vietnamese end-to-end speech recognition using wav2vec 2.0\',11.52,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,AMI IMH,2021-04,SpeechStew (100M) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-9.0,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,WenetSpeech,2021-10,Espnet model in \'WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition\',9.7,Character Error Rate (CER),pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,CHiME-6 dev_gss12,2020-04,RNN-T model in \'Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription\',-55.0,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,CHiME-6 dev_gss12,2021-04,SpeechStew (1B) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-31.9,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,CHiME-6 eval,2021-04,SpeechStew (1B) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-38.9,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,WSJ eval92,2015-04,TC-DNN-BLSTM-DNN model in \'Deep Recurrent Neural Networks for Acoustic Modelling\',-3.5,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,WSJ eval92,2016-09,tdnn + chain model in \'Purely sequence-trained neural networks for ASR based on lattice-free MMI\',-2.32,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,WSJ eval92,2021-04,Speechstew 100M model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-1.3,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Switchboard SWBD,2021-04,SpeechStew (100M) model in \'SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\',-4.7,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice French,2021-01,"VoxPopuli-50K model in \'VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation\'",9.6,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice French,2021-10,QuartzNet15x5FR (CV-only) model in \'Scribosermo: Fast Speech-to-Text models for German and other Languages\',12.1,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,TIMIT,2013-03,Bi-LSTM + skip connections w/ CTC model in \'Speech Recognition with Deep Recurrent Neural Networks\',-17.7,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,TIMIT,2015-06,Bi-RNN + Attention model in \'Attention-Based Models for Speech Recognition\',-17.6,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,TIMIT,2016-03,RNN-CRF on 24(x3) MFSC model in \'Segmental Recurrent Neural Networks for End-to-end Speech Recognition\',-17.3,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,TIMIT,2018-03,Li-GRU + fMLLR features model in \'Light Gated Recurrent Units for Speech Recognition\',-14.9,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,TIMIT,2018-11,LiGRU + Dropout + BatchNorm + Monophone Reg model in \'The PyTorch-Kaldi Speech Recognition Toolkit\',-14.2,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,TIMIT,2019-10,vq-wav2vec model in \'vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations\',-11.6,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,TIMIT,2020-06,wav2vec 2.0 model in \'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\',-8.3,Percentage error,neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,Fongbe  audio,2017-01,Triphone (39 features) + LDA and MLLT + SGMM model in \'First Automatic Fongbe Continuous Speech Recognition System: Development of Acoustic Models and Language Models\',-16.57,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Speech recognition,VIVOS,2021-09,Vietnamese end-to-end speech recognition using wav2vec 2.0 by VietAI model in \'Vietnamese end-to-end speech recognition using wav2vec 2.0\',6.15,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,VIVOS,2022-05,wav2vec2-base-vietnamese-160h (No Language Model) model in \'Wav2vec2 Base Vietnamese 160h\',15.05,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice Portuguese,2022-02,XLSR53 Wav2Vec2 Portuguese by Orlem Santos model in \'XLSR53 Wav2Vec2 Portuguese by Orlem Santos\',10.74,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice German,2021-01,"VoxPopuli model in \'VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation\'",7.8,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice German,2022-06,wav2vec 2.0 XLS-R (no LM) model in \'TEVR: Improving Speech Recognition by Token Entropy Variance Reduction\',12.06,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice German,2022-06,wav2vec 2.0 XLS-R 1B (5-gram) model in \'TEVR: Improving Speech Recognition by Token Entropy Variance Reduction\',1.62,Test CER,pos
Natural language generation,Language modelling,Speech recognition,Speech recognition,Common Voice Italian,2021-10,QuartzNet15x5IT (D5) model in \'Scribosermo: Fast Speech-to-Text models for German and other Languages\',11.5,Test WER,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C EN->ES,2020-11,Dual-decoder Transformer model in \'Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation\',28.12,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C EN->ES,2021-06,Transformer with Adapters model in \'Lightweight Adapter Tuning for Multilingual Speech Translation\',28.73,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C,2020-11,Dual-decoder Transformer model in \'Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation\',25.62,SacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C,2021-06,Transformer with Adapters model in \'Lightweight Adapter Tuning for Multilingual Speech Translation\',26.61,SacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C EN->FR,2020-11,Dual-decoder Transformer model in \'Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation\',33.45,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C EN->DE,2020-07,Transformer + Meta Learning(ASR/MT) + Data Augmentation model in \'End-to-End Offline Speech Translation System for IWSLT 2020 using Modality Agnostic Meta-Learning\',27.51,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C EN->DE,2021-05,Wav2Vec2.0+mBART+Adaptors model in \'End-to-End Speech Translation with Pre-trained Models and Adapters: UPC at IWSLT 2021\',28.22,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C EN->DE,2021-06,Task Modulation + Multitask Learning(ASR/MT) + Data Augmentation model in \'TASK AWARE MULTI-TASK LEARNING FOR SPEECH TO TEXT TASKS\',28.88,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,libri-trans,2020-12,Transformer + ASR Pretrain + SpecAug model in \'NeurST: Neural Speech Translation Toolkit\',16.3,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,libri-trans,2020-12,Transformer + ASR Pretrain + SpecAug model in \'NeurST: Neural Speech Translation Toolkit\',17.2,Case-insensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,libri-trans,2020-12,Transformer + ASR Pretrain + SpecAug model in \'NeurST: Neural Speech Translation Toolkit\',18.7,Case-insensitive tokenized BLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,libri-trans,2020-12,Transformer + ASR Pretrain + SpecAug model in \'NeurST: Neural Speech Translation Toolkit\',17.8,Case-sensitive tokenized BLEU,pos
Natural language generation,Language modelling,Speech recognition,Speech-to-text translation,MuST-C EN->NL,2021-09,Speechformer model in \'Speechformer: Reducing Information Loss in Direct Speech Translation\',27.7,Case-sensitive sacreBLEU,pos
Natural language generation,Language modelling,Speech recognition,Visual speech recognition,LRS2,2021-10,VTP with more data model in \'Sub-word Level Lip Reading With Visual Attention\',-22.6,Word Error Rate (WER),neg
Natural language generation,Language modelling,Speech recognition,Visual speech recognition,LRS3-TED,2021-10,VTP with more data model in \'Sub-word Level Lip Reading With Visual Attention\',-30.7,Word Error Rate (WER),neg
Natural language generation,Machine translation,Low-Resource Neural Machine Translation,Low-Resource Neural Machine Translation,Umsuka,2022-05,https://huggingface.co/MUNasir/umsuka-en-zu model in \'Geographical Distance Is The New Hyperparameter: A Case Study Of Finding The Optimal Pre-trained Language For English-isiZulu Machine Translation\',13.73,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2017 Latvian-English,2017-09,mLSTM with factored data model in \'Tilde\'s Machine Translation Systems for WMT 2017\',20.8,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2017 Latvian-English,2018-10,Transformer trained on highly filtered data model in \'Impact of Corpora Quality on Neural Machine Translation\',24.37,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 French-English,2018-09,SMT + iterative backtranslation (unsupervised) model in \'Unsupervised Statistical Machine Translation\',25.87,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 French-English,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',36.5,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 German-English,2016-06,Attentional encoder-decoder + BPE model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',38.6,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 German-English,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',39.8,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 German-English,2019-11,Exploiting Mono at Scale (single) model in \'Exploiting Monolingual Data at Scale for Neural Machine Translation\',47.5,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-French,2020-08,DeLighT model in \'DeLighT: Deep and Light-weight Transformer\',40.5,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 Romanian-English,2016-06,Attentional encoder-decoder + BPE model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',33.3,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 Romanian-English,2019-01,MLM pretraining model in \'Cross-lingual Language Model Pretraining\',35.3,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 Romanian-English,2020-11,fast-noisy-channel-modeling model in \'Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling\',40.3,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 Russian-English,2016-06,Attentional encoder-decoder + BPE model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',28.0,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2017 English-Chinese,2018-03,Hassan et al. (2018) model in \'Achieving Human Parity on Automatic Chinese to English News Translation\',24.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2017 English-Chinese,2019-01,DynamicConv model in \'Pay Less Attention with Lightweight and Dynamic Convolutions\',24.4,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,ACCURAT balanced test corpus for under resourced languages Russian-Estonian,2018-05,Multilingual Transformer model in \'Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages\',18.03,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 English-German,2020-07,MixedRepresentations model in \'Sequence Generation with Mixed Representations\',29.93,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 English-German,2021-04,Unidrop model in \'UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost\',29.99,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 English-German,2022-06,Bi-SimCut model in \'Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation\',31.16,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2015-12,LSTM+Attention+Ensemble model in \'Stanford Neural Machine Translation Systems for Spoken Language Domains\',26.4,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2018-06,DeconvDec model in \'Deconvolution-Based Global Decoding for Neural Machine Translation\',28.47,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2018-08,Self-Adaptive Control of Temperature model in \'Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation\',29.12,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2018-09,CVT model in \'Semi-Supervised Sequence Modeling with Cross-View Training\',29.6,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2019-10,Transformer+BPE-dropout model in \'BPE-Dropout: Simple and Effective Subword Regularization\',33.27,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2021-04,Tall Transformer with Style-Augmented Training model in \'Better Translation for Vietnamese\',37.8,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2022-03,Transformer Tall 18 model in \'MTet: Multi-domain Translation for English-Vietnamese\',40.2,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-Vietnamese,2022-07,NLLB-200 model in \'No Language Left Behind: Scaling Human-Centered Machine Translation\',34.8,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2017 Finnish-English,2019-06,CT+B/S construction model in \'The University of Sydney\'s Machine Translation System for WMT19\',35.5,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 Chinese-English,2019-11,BP-Transformer model in \'BP-Transformer: Modelling Long-Range Context via Binary Partitioning\',19.84,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,V_A (trained on T_H),2021-02,M_C model in \'On Automatic Parsing of Log Records\',0.28,Median Relative Edit Distance,pos
Natural language generation,Machine translation,Machine translation,Machine translation,Business Scene Dialogue EN-JA,2020-08,Transformer-base model in \'Designing the Business Conversation Corpus\',13.53,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 Finnish-English,2019-06,CT+B/S construction model in \'The University of Sydney\'s Machine Translation System for WMT19\',32.4,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2014-06,CSLM + RNN + WP model in \'Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\',34.54,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2014-09,SMT+LSTM5 model in \'Sequence to Sequence Learning with Neural Networks\',36.5,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2014-10,LSTM6 + PosUnk model in \'Addressing the Rare Word Problem in Neural Machine Translation\',37.5,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2016-06,Deep-Att + PosUnk model in \'Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\',39.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2016-09,GNMT+RL model in \'Google\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\',39.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2017-01,MoE model in \'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\',40.56,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2017-05,ConvS2S (ensemble) model in \'Convolutional Sequence to Sequence Learning\',41.3,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2017-11,Weighted Transformer (large) model in \'Weighted Transformer Network for Machine Translation\',41.4,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2018-03,Transformer (big) + Relative Position Representations model in \'Self-Attention with Relative Position Representations\',41.5,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2018-06,Transformer Big model in \'Scaling Neural Machine Translation\',43.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2018-08,Noisy back-translation model in \'Understanding Back-Translation at Scale\',45.6,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2020-08,Transformer+BT (ADMIN init) model in \'Very Deep Transformers for Neural Machine Translation\',46.4,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2018-08,Noisy back-translation model in \'Understanding Back-Translation at Scale\',43.8,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-French,2020-08,Transformer+BT (ADMIN init) model in \'Very Deep Transformers for Neural Machine Translation\',44.4,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2019 Finnish-English,2019-06,CT+B/S construction model in \'The University of Sydney\'s Machine Translation System for WMT19\',34.1,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,Itihasa,2021-06,Baseline (en->sn) model in \'Itihasa: A large-scale corpus for Sanskrit to English translation\',7.59,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2019 English-German,2019-07,Facebook FAIR (ensemble) model in \'Facebook FAIR\'s WMT19 News Translation Task Submission\',43.1,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2019 English-German,2019-07,Facebook FAIR (ensemble) model in \'Facebook FAIR\'s WMT19 News Translation Task Submission\',42.7,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2019 English-German,2019-11,Exploiting Mono at Scale (single) model in \'Exploiting Monolingual Data at Scale for Neural Machine Translation\',43.8,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2018 English-Estonian,2018-10,Multi-pass backtranslated adapted transformer model in \'Tilde\'s Machine Translation Systems for WMT 2018\',24.1,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 Thai-English,2016-06,Seq-KD + Seq-Inter + Word-KD model in \'Sequence-Level Knowledge Distillation\',14.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2015 English-German,2015-08,BPE word segmentation model in \'Neural Machine Translation of Rare Words with Subword Units\',22.8,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2015 English-German,2016-03,Enc-Dec Att (char) model in \'A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation\',23.5,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2015 English-German,2016-10,ByteNet model in \'Neural Machine Translation in Linear Time\',26.3,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-German,2016-06,Attentional encoder-decoder + BPE model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',34.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-German,2019-05,MADL model in \'Multi-Agent Dual Learning\',40.68,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-German,2019-11,Exploiting Mono at Scale (single) model in \'Exploiting Monolingual Data at Scale for Neural Machine Translation\',40.9,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2017 English-Latvian,2018-10,Transformer trained on highly filtered data model in \'Impact of Corpora Quality on Neural Machine Translation\',22.89,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-German,2016-07,RNNsearch model in \'An Actor-Critic Algorithm for Sequence Prediction\',25.04,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-German,2017-05,ConvS2S model in \'Convolutional Sequence to Sequence Learning\',26.73,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-German,2017-06,Transformer model in \'Attention Is All You Need\',28.5,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 English-German,2020-06,PS-KD model in \'Self-Knowledge Distillation with Progressive Refinement of Targets\',30.0,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 English-Arabic,2022-07,NLLB-200 model in \'No Language Left Behind: Scaling Human-Centered Machine Translation\',25.2,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 English-Arabic,2019-10,Transformer base + BPE-Dropout model in \'BPE-Dropout: Simple and Effective Subword Regularization\',15.2,Cased sacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2019 English-Japanese,2020-05,fiore model in \'Parallel Corpus Filtering via Pre-trained Language Models\',527424878.0,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2018 English-Finnish,2018-10,Transformer trained on highly filtered data model in \'Impact of Corpora Quality on Neural Machine Translation\',17.4,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2018 Estonian-English,2018-10,Multi-pass backtranslated adapted transformer model in \'Tilde\'s Machine Translation Systems for WMT 2018\',29.0,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2017 English-Finnish,2021-03,OmniNetP model in \'OmniNet: Omnidirectional Representations from Transformers\',20.9,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2017 Russian-English,2021-03,OmniNetP model in \'OmniNet: Omnidirectional Representations from Transformers\',36.2,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-Russian,2016-06,Attentional encoder-decoder + BPE model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',26.0,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,V_B (trained on T_H),2021-02,M_C model in \'On Automatic Parsing of Log Records\',0.25,Median Relative Edit Distance,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 German-English,2017-11,NAT +FT + NPD model in \'Non-Autoregressive Neural Machine Translation\',23.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 German-English,2018-02,Denoising autoencoders (non-autoregressive) model in \'Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement\',25.43,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 German-English,2019-09,FlowSeq-large (NPD n = 30) model in \'FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow\',28.29,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 German-English,2020-03,MAT+Knee model in \'Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule\',31.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 German-English,2020-11,CMLM+LAT+4 iterations model in \'Incorporating a Local Translation Mechanism into Non-autoregressive Translation\',32.04,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 German-English,2021-09,"BiBERT model in \'BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation\'",34.94,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 German-English,2022-06,Bi-SimCut model in \'Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation\',35.15,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-Czech,2019-01,Evolved Transformer Big model in \'The Evolved Transformer\',28.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-Romanian,2016-06,BiGRU model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',28.1,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-Romanian,2016-08,GRU BPE90k model in \'The QT21/HimL Combined Machine Translation System\',28.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-Romanian,2017-05,ConvS2S BPE40k model in \'Convolutional Sequence to Sequence Learning\',29.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-Romanian,2019-09,FlowSeq-large (NPD n = 30) model in \'FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow\',32.35,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-Romanian,2020-08,DeLighT model in \'DeLighT: Deep and Light-weight Transformer\',34.7,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 Arabic-English,2022-07,NLLB-200 model in \'No Language Left Behind: Scaling Human-Centered Machine Translation\',44.7,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 Arabic-English,2019-10,Transformer base + BPE-Dropout model in \'BPE-Dropout: Simple and Effective Subword Regularization\',33.0,Cased sacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2019 German-English,2019-11,Exploiting Mono at Scale (single) model in \'Exploiting Monolingual Data at Scale for Neural Machine Translation\',41.9,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,Arba Sicula,2021-10,Larger model in \'Sicilian Translator: A Recipe for Low-Resource NMT\',35.0,BLEU (En-Scn),pos
Natural language generation,Machine translation,Machine translation,Machine translation,Arba Sicula,2021-10,Many-to-Many model in \'Sicilian Translator: A Recipe for Low-Resource NMT\',36.5,BLEU (It-Scn),pos
Natural language generation,Machine translation,Machine translation,Machine translation,Arba Sicula,2021-10,Larger model in \'Sicilian Translator: A Recipe for Low-Resource NMT\',36.8,BLEU (Scn-En),pos
Natural language generation,Machine translation,Machine translation,Machine translation,Arba Sicula,2021-10,Many-to-Many model in \'Sicilian Translator: A Recipe for Low-Resource NMT\',30.9,BLEU (Scn-It),pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 French-English,2022-07,NLLB-200 model in \'No Language Left Behind: Scaling Human-Centered Machine Translation\',45.8,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 French-English,2019-10,Transformer base + BPE-Dropout model in \'BPE-Dropout: Simple and Effective Subword Regularization\',38.6,Cased sacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2018 Finnish-English,2018-10,Transformer trained on highly filtered data model in \'Impact of Corpora Quality on Neural Machine Translation\',24.0,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT 2018 Finnish-English,2019-06,CT+B/S construction model in \'The University of Sydney\'s Machine Translation System for WMT19\',26.5,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2015-08,RNN Enc-Dec Att model in \'Effective Approaches to Attention-based Neural Machine Translation\',20.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2016-09,GNMT+RL model in \'Google\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\',26.3,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2017-05,ConvS2S (ensemble) model in \'Convolutional Sequence to Sequence Learning\',26.4,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2017-06,Transformer Big model in \'Attention Is All You Need\',28.4,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2017-11,Weighted Transformer (large) model in \'Weighted Transformer Network for Machine Translation\',28.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2018-03,Transformer (big) + Relative Position Representations model in \'Self-Attention with Relative Position Representations\',29.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2018-06,Transformer Big model in \'Scaling Neural Machine Translation\',29.3,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2018-08,Noisy back-translation model in \'Understanding Back-Translation at Scale\',35.0,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2021-04,Transformer Cycle (Rev) model in \'Lessons on Parameter Sharing across Layers in Transformers\',35.14,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2018-08,Noisy back-translation model in \'Understanding Back-Translation at Scale\',33.8,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2014 English-German,2021-03,Mask Attention Network (big) model in \'Mask Attention Networks: Rethinking and Strengthen Transformer\',215000000.0,Number of Params,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2015 English-Russian,2015-08,C2-50k Segmentation model in \'Neural Machine Translation of Rare Words with Subword Units\',20.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2017 English-French,2021-03,OmniNetP model in \'OmniNet: Omnidirectional Representations from Transformers\',43.1,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2016-07,Actor-Critic [Bahdanau2017] model in \'An Actor-Critic Algorithm for Sequence Prediction\',28.53,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2017-06,Transformer model in \'Attention Is All You Need\',34.44,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2019-01,DynamicConv model in \'Pay Less Attention with Lightweight and Dynamic Convolutions\',35.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2019-05,Local Joint Self-attention model in \'Joint Source-Target Self Attention with Locality Constraints\',35.7,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2019-11,Data Diversification model in \'Data Diversification: A Simple Strategy For Neural Machine Translation\',37.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2020-03,Cutoff+Knee model in \'Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule\',37.78,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2021-06,Transformer + R-Drop + Cutoff model in \'R-Drop: Regularized Dropout for Neural Networks\',37.9,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2021-09,"BiBERT model in \'BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation\'",38.61,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2014 German-English,2021-03,Mask Attention Network (small) model in \'Mask Attention Networks: Rethinking and Strengthen Transformer\',37000000.0,Number of Params,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 English-Czech,2016-06,Attentional encoder-decoder + BPE model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',25.8,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2016 Czech-English,2016-06,Attentional encoder-decoder + BPE model in \'Edinburgh Neural Machine Translation Systems for WMT 16\',31.4,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2017 Chinese-English,2021-03,T2R + Pretrain model in \'Finetuning Pretrained Transformers into RNNs\',23.8,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,20NEWS,2015-08,12 model in \'Effective Approaches to Attention-based Neural Machine Translation\',1.0,Accuracy,pos
Natural language generation,Machine translation,Machine translation,Machine translation,20NEWS,2017-09,tensorflow/tensor2tensor model in \'Neural Machine Translation\',5.0,1-of-100 Accuracy,pos
Natural language generation,Machine translation,Machine translation,Machine translation,Tatoeba (EN-to-EL),2021-03,PENELOPIE Transformers-based NMT (EN2EL) model in \'PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation\',76.9,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,Tatoeba (EL-to-EN),2021-03,PENELOPIE (Transformers-based Greek-to-English NMT) model in \'PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation\',79.3,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,V_C (trained on T_H),2021-02,M_C model in \'On Automatic Parsing of Log Records\',0.27,Median Relative Edit Distance,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 German-English,2014-09,Bi-GRU (MLE+SLE) model in \'Neural Machine Translation by Jointly Learning to Align and Translate\',28.53,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 German-English,2016-07,RNNsearch model in \'An Actor-Critic Algorithm for Sequence Prediction\',29.98,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 German-English,2016-11,Conv-LSTM (deep+pos) model in \'A Convolutional Encoder Model for Neural Machine Translation\',30.4,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 German-English,2017-05,ConvS2S model in \'Convolutional Sequence to Sequence Learning\',32.31,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 German-English,2017-11,ConvS2S+Risk model in \'Classical Structured Prediction Losses for Sequence to Sequence Learning\',32.93,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 German-English,2018-08,Pervasive Attention model in \'Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction\',34.18,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2015 German-English,2020-06,PS-KD model in \'Self-Knowledge Distillation with Progressive Refinement of Targets\',36.2,BLEU score,pos
Natural language generation,Machine translation,Machine translation,Machine translation,Business Scene Dialogue JA-EN,2020-08,Transformer-base model in \'Designing the Business Conversation Corpus\',12.88,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,WMT2017 English-German,2021-03,OmniNetP model in \'OmniNet: Omnidirectional Representations from Transformers\',29.0,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,ACCURAT balanced test corpus for under resourced languages Estonian-Russian,2018-05,Multilingual Transformer model in \'Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages\',19.18,BLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 English-French,2022-07,NLLB-200 model in \'No Language Left Behind: Scaling Human-Centered Machine Translation\',43.0,SacreBLEU,pos
Natural language generation,Machine translation,Machine translation,Machine translation,IWSLT2017 English-French,2019-10,Transformer base + BPE-Dropout model in \'BPE-Dropout: Simple and Effective Subword Regularization\',39.83,Cased sacreBLEU,pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal lexical translation,MultiSubs English-Portuguese,2021-03,Multimodal BRNN model in \'MultiSubs: A Large-scale Multimodal and Multilingual Dataset\',0.8,ALI,pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal lexical translation,MultiSubs English-French,2021-03,Multimodal BRNN model in \'MultiSubs: A Large-scale Multimodal and Multilingual Dataset\',0.81,ALI,pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal lexical translation,MultiSubs English-German,2021-03,Multimodal BRNN model in \'MultiSubs: A Large-scale Multimodal and Multilingual Dataset\',0.94,ALI,pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Hindi Visual Genome (Challenge Set),2021-06,ViTA model in \'ViTA: Visual-Linguistic Translation by Aligning Object Tags\',51.6,BLEU (EN-HI),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2017-01,IMGD model in \'Incorporating Global Visual Features into Attention-Based Neural Machine Translation\',37.3,BLEU (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2018-11,VMMTF model in \'Latent Variable Model for Multi-modal Translation\',37.6,BLEU (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2019-06,del+obj model in \'Distilling Translations with Visual Awareness\',38.0,BLEU (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2019-11,Caglayan model in \'Multimodal Machine Translation through Visuals and Speech\',39.4,BLEU (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2020-09,DCCN model in \'Dynamic Context-guided Capsule Network for Multimodal Machine Translation\',39.7,BLEU (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2017-01,IMGD model in \'Incorporating Global Visual Features into Attention-Based Neural Machine Translation\',55.1,Meteor (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2018-11,VMMTF model in \'Latent Variable Model for Multi-modal Translation\',56.0,Meteor (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2019-11,Caglayan model in \'Multimodal Machine Translation through Visuals and Speech\',58.7,Meteor (EN-DE),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2018-08,VAG-NMT model in \'A Visual Attention Grounding Neural Model for Multimodal Machine Translation\',70.3,Meteor (EN-FR),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2019-06,del model in \'Distilling Translations with Visual Awareness\',74.6,Meteor (EN-FR),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2020-09,DCCN model in \'Dynamic Context-guided Capsule Network for Multimodal Machine Translation\',76.4,Meteor (EN-FR),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2017-06,Transformer model in \'Attention Is All You Need\',29.0,BLUE (DE-EN),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Multi30K,2020-06,PS-KD model in \'Self-Knowledge Distillation with Progressive Refinement of Targets\',32.3,BLUE (DE-EN),pos
Natural language generation,Machine translation,Multimodal machine translation,Multimodal machine translation,Hindi Visual Genome (Test Set),2021-06,ViTA model in \'ViTA: Visual-Linguistic Translation by Aligning Object Tags\',44.6,BLEU (EN-HI),pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 German-English,2018-04,PBSMT model in \'Phrase-Based & Neural Unsupervised Machine Translation\',25.2,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 German-English,2018-10,Synthetic bilingual data init model in \'Unsupervised Neural Machine Translation Initialized by Unsupervised Statistical Machine Translation\',26.7,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 German-English,2019-01,MLM pretraining for encoder and decoder model in \'Cross-lingual Language Model Pretraining\',34.3,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 German-English,2019-02,SMT + NMT (tuning and joint refinement) model in \'An Effective Approach to Unsupervised Machine Translation\',34.4,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 German-English,2019-05,MASS (6-layer Transformer) model in \'MASS: Masked Sequence to Sequence Pre-training for Language Generation\',35.2,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 German-English,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',40.6,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 German-English,2019-01,SMT as posterior regularization model in \'Unsupervised Neural Machine Translation with SMT as Posterior Regularization\',20.4,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 German-English,2019-02,SMT + NMT (tuning and joint refinement) model in \'An Effective Approach to Unsupervised Machine Translation\',27.0,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English--Romanian,2019-01,MLM pretraining for encoder and decoder model in \'Cross-lingual Language Model Pretraining\',33.3,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English--Romanian,2020-02,BERT-fused NMT model in \'Incorporating BERT into Neural Machine Translation\',36.02,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 Romanian-English,2019-01,MLM pretraining for encoder and decoder model in \'Cross-lingual Language Model Pretraining\',31.8,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 Romanian-English,2019-05,MASS (6-layer Transformer) model in \'MASS: Masked Sequence to Sequence Pre-training for Language Generation\',33.1,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 Romanian-English,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',39.5,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English-German,2018-04,PBSMT + NMT model in \'Phrase-Based & Neural Unsupervised Machine Translation\',20.2,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English-German,2019-01,MLM pretraining for encoder and decoder model in \'Cross-lingual Language Model Pretraining\',26.4,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English-German,2019-02,SMT + NMT (tuning and joint refinement) model in \'An Effective Approach to Unsupervised Machine Translation\',26.9,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English-German,2019-05,MASS (6-layer Transformer) model in \'MASS: Masked Sequence to Sequence Pre-training for Language Generation\',28.3,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English-German,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',29.7,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 English-German,2019-01,SMT as posterior regularization model in \'Unsupervised Neural Machine Translation with SMT as Posterior Regularization\',17.0,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 English-German,2019-02,SMT + NMT (tuning and joint refinement) model in \'An Effective Approach to Unsupervised Machine Translation\',22.5,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 French-English,2018-04,PBSMT + NMT model in \'Phrase-Based & Neural Unsupervised Machine Translation\',27.7,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 French-English,2019-01,SMT as posterior regularization model in \'Unsupervised Neural Machine Translation with SMT as Posterior Regularization\',28.9,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 French-English,2019-02,SMT + NMT (tuning and joint refinement) model in \'An Effective Approach to Unsupervised Machine Translation\',33.5,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 French-English,2019-05,MASS (6-layer Transformer) model in \'MASS: Masked Sequence to Sequence Pre-training for Language Generation\',34.9,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 French-English,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',39.2,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2016 English-Romanian,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',21.0,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 English-French,2018-04,PBSMT + NMT model in \'Phrase-Based & Neural Unsupervised Machine Translation\',27.6,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 English-French,2019-01,MLM pretraining for encoder and decoder model in \'Cross-lingual Language Model Pretraining\',33.4,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 English-French,2019-02,SMT + NMT (tuning and joint refinement) model in \'An Effective Approach to Unsupervised Machine Translation\',36.2,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 English-French,2019-05,MASS (6-layer Transformer) model in \'MASS: Masked Sequence to Sequence Pre-training for Language Generation\',37.5,BLEU,pos
Natural language generation,Machine translation,Unsupervised machine translation,Unsupervised machine translation,WMT2014 English-French,2020-02,BERT-fused NMT model in \'Incorporating BERT into Neural Machine Translation\',38.27,BLEU,pos
Natural language generation,Machine translation,Word alignment,Word alignment,es-en,2017-10,Adv - Refine - CSLS model in \'Word Translation Without Parallel Data\',83.3,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,es-en,2020-01,Barycenter Alignment model in \'Unsupervised Multilingual Alignment using Wasserstein Barycenter\',83.5,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,MUSE en-de,2020-01,Barycenter Alignment model in \'Unsupervised Multilingual Alignment using Wasserstein Barycenter\',74.08,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,en-fr,2017-10,Adv - Refine - CSLS model in \'Word Translation Without Parallel Data\',82.3,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,en-fr,2020-01,Barycenter Alignment model in \'Unsupervised Multilingual Alignment using Wasserstein Barycenter\',82.94,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,fr-en,2017-10,Adv - Refine - CSLS model in \'Word Translation Without Parallel Data\',82.1,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,fr-en,2020-01,Barycenter Alignment model in \'Unsupervised Multilingual Alignment using Wasserstein Barycenter\',83.23,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,en-it,2020-01,Barycenter Alignment model in \'Unsupervised Multilingual Alignment using Wasserstein Barycenter\',81.45,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,MUSE en-pt,2020-01,Barycenter Alignment model in \'Unsupervised Multilingual Alignment using Wasserstein Barycenter\',84.65,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,en-es,2017-10,Adv - Refine - CSLS model in \'Word Translation Without Parallel Data\',81.7,P-at-1,pos
Natural language generation,Machine translation,Word alignment,Word alignment,en-es,2020-01,Barycenter Alignment model in \'Unsupervised Multilingual Alignment using Wasserstein Barycenter\',84.26,P-at-1,pos
Natural language generation,Multimodal text prediction,Multimodal text prediction,Multimodal text prediction,MultiSubs,2021-03,9-gram LM with back-off model in \'MultiSubs: A Large-scale Multimodal and Multilingual Dataset\',30.35,Accuracy,pos
Natural language generation,Multimodal text prediction,Multimodal text prediction,Multimodal text prediction,MultiSubs,2021-03,9-gram LM with back-off model in \'MultiSubs: A Large-scale Multimodal and Multilingual Dataset\',0.44,Word similarity,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v2, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.915,1 in 2 R-at-1,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v2, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.815,1 in 10 R-at-2,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v2, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.652,1 in 10 R-at-1,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v2, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.966,1 in 10 R-at-5,pos
Natural language generation,Question answering,Answer selection,Answer selection,CICERO,2022-03,T5-large model in \'CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues\',77.68,Exact Match,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v1, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.916,1 in 2 R-at-1,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v1, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.822,1 in 10 R-at-2,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v1, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.684,1 in 10 R-at-1,pos
Natural language generation,Question answering,Answer selection,Answer selection,"Ubuntu Dialogue (v1, Ranking)",2017-10,HRDE-LTC model in \'Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering\',0.96,1 in 10 R-at-5,pos
Natural language generation,Question answering,Community question answering,Community question answering,Quora Question Pairs,2020-05,MFAE model in \'What Do Questions Exactly Ask? MFAE: Duplicate Question Identification with Multi-Fusion Asking Emphasis\',90.54,Accuracy,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,XQuAD,2020-10,Decoupled model in \'Rethinking embedding coupling in pre-trained language models\',63.8,F1,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,XQuAD,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',79.7,F1,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,XQuAD,2020-10,Decoupled model in \'Rethinking embedding coupling in pre-trained language models\',46.9,EM,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,XQuAD,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',63.6,EM,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,XQuAD,2021-10,mLUKE-E model in \'mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models\',74.2,Average F1,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,TyDiQA-GoldP,2020-10,Decoupled model in \'Rethinking embedding coupling in pre-trained language models\',58.1,F1,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,TyDiQA-GoldP,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',75.3,F1,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,TyDiQA-GoldP,2020-10,Decoupled model in \'Rethinking embedding coupling in pre-trained language models\',42.8,EM,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,TyDiQA-GoldP,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',60.0,EM,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,MLQA,2020-10,Coupled model in \'Rethinking embedding coupling in pre-trained language models\',53.1,F1,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,MLQA,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',71.6,F1,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,MLQA,2020-10,Coupled model in \'Rethinking embedding coupling in pre-trained language models\',37.3,EM,pos
Natural language generation,Question answering,Cross-lingual question answering,Cross-lingual question answering,MLQA,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',54.9,EM,pos
Natural language generation,Question answering,Generative question answering,Generative question answering,CICERO,2022-03,T5-large pre-trained on GLUCOSE model in \'CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues\',0.298,ROUGE,pos
Natural language generation,Question answering,Generative question answering,Generative question answering,CoQA,2018-08,PGNet model in \'CoQA: A Conversational Question Answering Challenge\',45.4,F1-Score,pos
Natural language generation,Question answering,Generative question answering,Generative question answering,CoQA,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',82.5,F1-Score,pos
Natural language generation,Question answering,Generative question answering,Generative question answering,CoQA,2020-01,ERNIE-GEN model in \'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\',84.5,F1-Score,pos
Natural language generation,Question answering,Graph question answering,Graph question answering,GQA,2021-04,GraphVQA model in \'GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering\',96.3,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSVD-QA,2017-04,ST-VQA model in \'TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\',0.313,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSVD-QA,2018-03,Co-Mem model in \'Motion-Appearance Co-Memory Networks for Video Question Answering\',0.317,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSVD-QA,2019-04,HMEMA model in \'Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering\',0.337,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSVD-QA,2020-02,HCRN model in \'Hierarchical Conditional Relation Networks for Video Question Answering\',0.361,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSVD-QA,2020-12,Just Ask model in \'Just Ask: Learning to Answer Questions from Millions of Narrated Videos\',0.463,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSVD-QA,2022-03,All-in-one-B model in \'All in One: Exploring Unified Video-Language Pre-training\',0.483,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA test-std,2019-02,MAC model in \'GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\',54.06,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA test-std,2019-05,single-hop + LCGN (ours) model in \'Language-Conditioned Graph Networks for Relational Reasoning\',56.1,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA test-std,2019-07,NSM model in \'Learning by Abstraction: The Neural State Machine\',63.17,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA test-std,2021-10,ProTo model in \'ProTo: Program-Guided Transformer for Program-Guided Tasks\',65.14,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-04,Pythia v0.3  model in \'Towards VQA Models That Can Read\',54.72,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-08,"LXR955, No Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",55.4,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-08,"LXR955, No Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",74.0,yes/no,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-08,"LXR955, No Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",24.76,number,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-09,B-Ultra model in \'Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\',28.81,number,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-08,"LXR955, No Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",39.0,other,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-08,"LXR955, No Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",82.26,unanswerable,pos
Natural language generation,Question answering,Image question answering,Image question answering,VizWiz 2018,2019-09,B-Ultra model in \'Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\',84.03,unanswerable,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (QA-R) dev,2019-08,VL-BERTLARGE model in \'VL-BERT: Pre-training of Generic Visual-Linguistic Representations\',77.9,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) abstract images 1.0 open ended,2015-05,Dualnet ensemble model in \'VQA: Visual Question Answering\',69.73,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) abstract images 1.0 open ended,2016-09,Graph VQA model in \'Graph-Structured Representations for Visual Question Answering\',70.42,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA-CE,2021-04,RandImg model in \'Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering\',34.41,Accuracy (Counterexamples),pos
Natural language generation,Question answering,Image question answering,Image question answering,DocVQA val,2020-07,BERT LARGE Baseline model in \'DocVQA: A Dataset for VQA on Document Images\',54.48,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,DocVQA val,2020-07,BERT LARGE Baseline model in \'DocVQA: A Dataset for VQA on Document Images\',0.655,ANLS,pos
Natural language generation,Question answering,Image question answering,Image question answering,DocVQA,2021-02,TILT-Large model in \'Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer\',87.05,ANLS,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) abstract 1.0 multiple choice,2015-05,Dualnet ensemble model in \'VQA: Visual Question Answering\',71.18,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) abstract 1.0 multiple choice,2016-09,Graph VQA model in \'Graph-Structured Representations for Visual Question Answering\',74.37,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2016-06,MCB model in \'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\',64.7,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2017-04,"N2NMN (ResNet-152, policy search) model in \'Learning to Reason: End-to-End Module Networks for Visual Question Answering\'",64.9,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2017-05,MUTAN model in \'MUTAN: Multimodal Tucker Fusion for Visual Question Answering\',67.42,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2017-08,"Image features from bottom-up attention (adaptive K, ensemble) model in \'Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge\'",69.87,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2018-05,BAN+Glove+Counter model in \'Bilinear Attention Networks\',70.04,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2019-06,MCANed-6 model in \'Deep Modular Co-Attention Networks for Visual Question Answering\',70.63,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2019-08,VL-BERTLARGE model in \'VL-BERT: Pre-training of Generic Visual-Linguistic Representations\',71.79,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2019-09,UNITER (Large) model in \'UNITER: UNiversal Image-TExt Representation Learning\',73.24,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2020-04,Oscar model in \'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\',73.82,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2021-07,ALBEF (14M) model in \'Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\',75.84,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',80.03,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-dev,2021-11,VLMo model in \'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts\',82.78,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',82.66,Sub-tasks (Img.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',75.19,Sub-tasks (Txt.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',83.62,Sub-tasks (Blank),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,ViLT model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',82.61,Reasoning (Geo.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',77.81,Reasoning (Cou.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',87.0,Reasoning (Com.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',55.62,Reasoning (Spa.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,ViT model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',68.8,Reasoning (Sce.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',68.75,Reasoning (Pat.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',77.98,Reasoning (Tim.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',82.13,Reasoning (Fra.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Top-Down model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',99.54,Reasoning (Est.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',56.73,Reasoning (Alg.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Top-Down model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',99.46,Reasoning (Mea.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',92.49,Reasoning (Sen.),pos
Natural language generation,Question answering,Image question answering,Image question answering,IconQA,2021-10,Patch-TRM model in \'IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning\',95.73,Reasoning (Pro.),pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-dev,2015-11,NMN+LSTM+FT model in \'Neural Module Networks\',58.6,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-dev,2016-03,DMN+ model in \'Dynamic Memory Networks for Visual and Textual Question Answering\',60.3,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-dev,2016-05,HieCoAtt (ResNet) model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',61.8,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-dev,2016-06,MCB (ResNet) model in \'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\',64.2,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-dev,2016-11,DAN (ResNet) model in \'Dual Attention Networks for Multimodal Reasoning and Matching\',64.3,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-dev,2017-04,"SAAA (ResNet) model in \'Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering\'",64.5,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR,2017-05,IEP-700K model in \'Inferring and Executing Programs for Visual Reasoning\',96.9,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR,2017-09,CNN+GRU+FiLM model in \'FiLM: Visual Reasoning with a General Conditioning Layer\',97.7,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR,2018-03,TbD + reg + hres model in \'Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning\',99.1,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR,2018-10,NS-VQA (1K programs) model in \'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding\',99.8,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA,2022-04,RelViT model in \'RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning\',65.54,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA,2022-05,PEVL+ model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',77.0,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 open ended,2015-05,LSTM Q+I model in \'VQA: Visual Question Answering\',58.2,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 open ended,2015-11,SAN model in \'Stacked Attention Networks for Image Question Answering\',58.9,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 open ended,2016-03,DMN+ [xiong2016dynamic] model in \'Dynamic Memory Networks for Visual and Textual Question Answering\',60.4,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 open ended,2016-05,HQI+ResNet model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',62.1,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 open ended,2016-06,MCB 7 att. model in \'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\',66.5,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,DocVQA test,2020-07,Human model in \'DocVQA: A Dataset for VQA on Document Images\',94.36,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,DocVQA test,2020-07,Human model in \'DocVQA: A Dataset for VQA on Document Images\',0.981,ANLS,pos
Natural language generation,Question answering,Image question answering,Image question answering,PlotQA-D2,2019-08,PReFIL model in \'Answering Questions about Data Visualizations using Efficient Bimodal Fusion\',10.37,1:1 Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,PlotQA-D2,2019-09,PlotQA model in \'PlotQA: Reasoning over Scientific Plots\',22.52,1:1 Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,PlotQA-D2,2021-11,CRCT model in \'Classification-Regression for Chart Comprehension\',34.44,1:1 Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,Visual Genome (subjects),2016-11,CMN model in \'Modeling Relationships in Referential Expressions with Compositional Modular Networks\',44.24,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 val,2021-06,Frozen model in \'Multimodal Few-Shot Learning with Frozen Language Models\',29.5,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 val,2021-11,VLKD(ViT-B/16) model in \'Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation\',38.6,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 val,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',41.1,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,Visual Genome (pairs),2016-11,CMN model in \'Modeling Relationships in Referential Expressions with Compositional Modular Networks\',28.52,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 multiple choice,2015-05,LSTM Q+I model in \'VQA: Visual Question Answering\',63.1,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 multiple choice,2016-04,FDA model in \'A Focused Dynamic Attention Model for Visual Question Answering\',64.2,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 multiple choice,2016-05,HQI+ResNet model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',66.1,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 1.0 multiple choice,2016-06,MCB 7 att. model in \'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\',70.1,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,GRIT,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",72.4,VQA (ablation),pos
Natural language generation,Question answering,Image question answering,Image question answering,GRIT,2022-06,"Unified-IOXL model in \'Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks\'",74.5,VQA (ablation),pos
Natural language generation,Question answering,Image question answering,Image question answering,GRIT,2022-02,GPV-2 model in \'Webly Supervised Concept Expansion for General Purpose Vision Models\',63.2,VQA (test),pos
Natural language generation,Question answering,Image question answering,Image question answering,GRIT,2022-06,"Unified-IOXL model in \'Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks\'",74.5,VQA (test),pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-A) dev,2019-08,VL-BERTLARGE model in \'VL-BERT: Pre-training of Generic Visual-Linguistic Representations\',75.5,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-AR) test,2019-08,VL-BERTLARGE model in \'VL-BERT: Pre-training of Generic Visual-Linguistic Representations\',59.7,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-AR) test,2019-09,UNITER (Large) model in \'UNITER: UNiversal Image-TExt Representation Learning\',62.8,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-AR) test,2020-06,ERNIE-ViL-large(ensemble of 15 models) model in \'ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph\',70.5,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (QA-R) test,2019-08,VL-BERTLARGE model in \'VL-BERT: Pre-training of Generic Visual-Linguistic Representations\',78.4,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (QA-R) test,2019-09,UNITER-large (ensemble of 10 models) model in \'UNITER: UNiversal Image-TExt Representation Learning\',83.4,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (QA-R) test,2020-06,ERNIE-ViL-large(ensemble of 15 models) model in \'ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph\',86.1,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR-Humans,2017-05,IEP-18K model in \'Inferring and Executing Programs for Visual Reasoning\',66.6,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR-Humans,2017-09,CNN+GRU+FiLM model in \'FiLM: Visual Reasoning with a General Conditioning Layer\',75.9,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR-Humans,2018-03,MAC model in \'Compositional Attention Networks for Machine Reasoning\',81.5,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,CLEVR-Humans,2021-04,MDETR model in \'MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding\',81.7,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,ZS-F-VQA,2021-07,SAN † - hard mask model in \'Zero-shot Visual Question Answering using Knowledge Graph\',29.39,Hit-at-1,pos
Natural language generation,Question answering,Image question answering,Image question answering,TDIUC,2019-02,Accuracy model in \'MUREL: Multimodal Relational Reasoning for Visual Question Answering\',88.2,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA-CP,2018-08,HAN model in \'Learning Visual Question Answering by Bootstrapping Hard Attention\',28.65,Score,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA-CP,2019-02,MuRel model in \'MUREL: Multimodal Relational Reasoning for Visual Question Answering\',39.54,Score,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA-CP,2019-05,UpDn+SCR (VQA-X) model in \'Self-Critical Reasoning for Robust Visual Question Answering\',49.45,Score,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA-CP,2019-09,Learned-Mixin +H model in \'Don\'t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases\',52.05,Score,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA-CP,2020-03,CSS model in \'Counterfactual Samples Synthesizing for Robust Visual Question Answering\',58.95,Score,pos
Natural language generation,Question answering,Image question answering,Image question answering,Visual7W,2016-06,MCB+Att. model in \'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\',62.2,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,Visual7W,2016-11,CMN model in \'Modeling Relationships in Referential Expressions with Compositional Modular Networks\',72.53,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-AR) dev,2019-08,VL-BERTLARGE model in \'VL-BERT: Pre-training of Generic Visual-Linguistic Representations\',58.9,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,OK-VQA,2021-06,Frozen model in \'Multimodal Few-Shot Learning with Frozen Language Models\',5.9,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,OK-VQA,2021-11,VLKD(ViT-B/16) model in \'Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation\',10.5,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,OK-VQA,2022-06,MetaLM model in \'Language Models are General-Purpose Interfaces\',11.4,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,FigureQA - test 1,2017-10,RN model in \'FigureQA: An Annotated Figure Dataset for Visual Reasoning\',76.52,1:1 Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,FigureQA - test 1,2019-08,PReFIL model in \'Answering Questions about Data Visualizations using Efficient Bimodal Fusion\',94.88,1:1 Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-A) test,2019-08,VL-BERTLARGE model in \'VL-BERT: Pre-training of Generic Visual-Linguistic Representations\',75.8,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-A) test,2019-09,UNITER-large (10 ensemble) model in \'UNITER: UNiversal Image-TExt Representation Learning\',79.8,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VCR (Q-A) test,2020-06,ERNIE-ViL-large(ensemble of 15 models) model in \'ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph\',81.6,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2017-07,BottomUp model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',49.74,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-07,GRN model in \'Bilinear Graph Networks for Visual Question Answering\',61.22,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-08,"LXR955, Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",62.71,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2021-01,Single Model model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',64.65,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2017-07,BottomUp model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',78.71,Consistency,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-07,GRN model in \'Bilinear Graph Networks for Visual Question Answering\',90.31,Consistency,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-08,"LXR955, Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",93.1,Consistency,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2021-01,Single Model model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',94.35,Consistency,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2017-07,BottomUp model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',66.64,Binary,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-07,GRN model in \'Bilinear Graph Networks for Visual Question Answering\',78.69,Binary,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-08,"LXR955, Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",79.79,Binary,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2021-01,Single Model model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',82.63,Binary,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2017-07,BottomUp model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',34.83,Open,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-07,GRN model in \'Bilinear Graph Networks for Visual Question Answering\',45.81,Open,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-08,"LXR955, Ensemble model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\'",47.64,Open,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2021-01,Single Model model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',48.77,Open,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2017-07,BottomUp model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',84.57,Plausibility,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-07,GRN model in \'Bilinear Graph Networks for Visual Question Answering\',85.43,Plausibility,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2017-07,BottomUp model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',96.18,Validity,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-07,GRN model in \'Bilinear Graph Networks for Visual Question Answering\',96.36,Validity,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2021-01,Single Model model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\',96.62,Validity,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2017-07,BottomUp model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',5.98,Distribution,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA Test2019,2019-07,GRN model in \'Bilinear Graph Networks for Visual Question Answering\',6.77,Distribution,pos
Natural language generation,Question answering,Image question answering,Image question answering,QLEVR,2022-05,MAC model in \'QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning\',66.5,Overall Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-std,2015-11,SAN (VGG) model in \'Stacked Attention Networks for Image Question Answering\',58.9,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-std,2016-03,DMN+ model in \'Dynamic Memory Networks for Visual and Textual Question Answering\',60.4,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-std,2016-05,HieCoAtt (ResNet) model in \'Hierarchical Question-Image Co-Attention for Visual Question Answering\',62.1,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-std,2016-06,RAU (ResNet) model in \'Training Recurrent Answering Units with Joint Loss Minimization for VQA\',63.2,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v1 test-std,2017-04,"SAAA (ResNet) model in \'Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering\'",64.6,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSRVTT-QA,2017-04,ST-VQA model in \'TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\',0.309,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSRVTT-QA,2018-03,Co-Mem model in \'Motion-Appearance Co-Memory Networks for Video Question Answering\',0.32,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSRVTT-QA,2019-04,HMEMA model in \'Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering\',0.33,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSRVTT-QA,2020-02,HCRN model in \'Hierarchical Conditional Relation Networks for Video Question Answering\',0.356,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSRVTT-QA,2020-12,Just Ask model in \'Just Ask: Learning to Answer Questions from Millions of Narrated Videos\',0.415,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSRVTT-QA,2021-12,ALPRO model in \'Align and Prompt: Video-and-Language Pre-training with Entity Prompts\',0.421,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,MSRVTT-QA,2022-03,All-in-one-B model in \'All in One: Exploring Unified Video-Language Pre-training\',0.443,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA test-dev,2019-05,single-hop + LCGN (ours) model in \'Language-Conditioned Graph Networks for Relational Reasoning\',55.8,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA test-dev,2019-07,NSM model in \'Learning by Abstraction: The Neural State Machine\',62.95,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,GQA test-dev,2021-10,CFR model in \'Coarse-to-Fine Reasoning for Visual Question Answering\',72.1,Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,PlotQA-D1,2019-08,PReFIL model in \'Answering Questions about Data Visualizations using Efficient Bimodal Fusion\',57.91,1:1 Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,PlotQA-D1,2021-11,CRCT model in \'Classification-Regression for Chart Comprehension\',76.94,1:1 Accuracy,pos
Natural language generation,Question answering,Image question answering,Image question answering,COCO Visual Question Answering (VQA) real images 2.0 open ended,2015-05,HDU-USYD-UNCC model in \'VQA: Visual Question Answering\',68.16,Percentage correct,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2016-12,"MCB [11, 12] model in \'Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\'",62.27,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2017-05,MUTAN model in \'MUTAN: Multimodal Tucker Fusion for Visual Question Answering\',67.4,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2017-07,Up-Down model in \'Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\',70.34,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2018-05,BAN+Glove+Counter model in \'Bilinear Attention Networks\',70.4,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2019-06,MCANed-6 model in \'Deep Modular Co-Attention Networks for Visual Question Answering\',70.9,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2019-07,"BGN, ensemble model in \'Bilinear Graph Networks for Visual Question Answering\'",75.92,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-01,"MSR + MS Cog. Svcs., X10 models model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\'",77.45,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',80.34,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-11,VLMo model in \'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts\',81.3,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",81.98,overall,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2019-07,"BGN, ensemble model in \'Bilinear Graph Networks for Visual Question Answering\'",90.89,yes/no,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-01,"MSR + MS Cog. Svcs., X10 models model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\'",92.38,yes/no,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-11,VLMo model in \'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts\',94.68,yes/no,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2019-07,"BGN, ensemble model in \'Bilinear Graph Networks for Visual Question Answering\'",61.13,number,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-01,"MSR + MS Cog. Svcs., X10 models model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\'",62.55,number,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-11,VLMo model in \'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts\',67.26,number,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",71.44,number,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2019-07,"BGN, ensemble model in \'Bilinear Graph Networks for Visual Question Answering\'",66.28,other,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-01,"MSR + MS Cog. Svcs., X10 models model in \'VinVL: Revisiting Visual Representations in Vision-Language Models\'",67.87,other,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2021-11,VLMo model in \'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts\',72.87,other,pos
Natural language generation,Question answering,Image question answering,Image question answering,VQA v2 test-std,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",73.35,other,pos
Natural language generation,Question answering,Image question answering,Machine reading comprehension,ReClor,2020-02,XLNet-large model in \'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\',56.0,Accuracy,pos
Natural language generation,Question answering,Image question answering,Machine reading comprehension,ReClor,2020-02,XLNet-large model in \'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\',75.7,Accuracy (easy),pos
Natural language generation,Question answering,Image question answering,Machine reading comprehension,ReClor,2020-02,XLNet-large model in \'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\',40.5,Accuracy (hard),pos
Natural language generation,Question answering,Image question answering,Machine reading comprehension,BIOMRC,2022-06,MLP-based-weighting (on BIOMRC Lite) model in \'Contextual embedding and model weighting by fusing domain knowledge on Biomedical Question Answering\',88.0,Acc,pos
Natural language generation,Question answering,Image question answering,Machine reading comprehension,UQuAD,2021-11,XLM-RoBERTa model in \'UQuAD1.0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension\',66.0,F1,pos
Natural language generation,Question answering,Image question answering,Machine reading comprehension,UQuAD,2021-11,BERT model in \'UQuAD1.0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension\',66.0,Exact Match,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,QALD-9-Plus,2022-01,QAnswer-Wikidata-English model in \'QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers\',0.4459,Macro F1,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,WebQuestionsSP,2020-12,UniK-QA (T5-large) model in \'UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering\',79.1,Hits-at-1,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,WebQuestionsSP,2021-08,ReTraCk Oracle EL model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',74.7,F1,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,ComplexWebQuestions,2021-01,NSM+h model in \'Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals\',53.9,Accuracy,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,ComplexWebQuestions,2021-04,CBR-KBQA model in \'Case-based Reasoning for Natural Language Queries over Knowledge Bases\',70.4,Accuracy,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',65.3,Overall F1,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',61.5,Compositional EM,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',70.9,Compositional F1,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',84.4,I.I.D. EM,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',87.5,I.I.D. F1,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',58.1,Overall EM,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',44.6,Zero-shot EM,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,GrailQA,2021-08,ReTraCk model in \'ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering\',52.5,Zero-shot F1,pos
Natural language generation,Question answering,Knowledge base question answering,Knowledge base question answering,WebQSP-WD,2018-08,GGNN model in \'Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering\',0.2588,Avg F1,pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,Math23K,2019-07,GROUP-ATT model in \'Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions\',66.9,Accuracy (5-fold),pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,Math23K,2019-08,GTS model in \'A Goal-Driven Tree-Structured Neural Model for Math Word Problems\',74.3,Accuracy (5-fold),pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,Math23K,2020-07,Graph2Tree model in \'Graph-to-Tree Learning for Solving Math Word Problems\',75.5,Accuracy (5-fold),pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,Math23K,2019-07,GROUP-ATT model in \'Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions\',69.5,Accuracy (training-test),pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,Math23K,2020-07,Graph2Tree model in \'Graph-to-Tree Learning for Solving Math Word Problems\',77.4,Accuracy (training-test),pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,Math23K,2020-12,LBF model in \'Learning by Fixing: Solving Math Word Problems with Weak Supervision\',59.8,weakly-supervised,pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,MATH,2021-03,GPT-2 (1.5B) model in \'Measuring Mathematical Problem Solving With the MATH Dataset\',6.9,Accuracy,pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,MATH,2022-06,Minerva 540B-maj1@k model in \'Solving Quantitative Reasoning Problems with Language Models\',50.3,Accuracy,pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,MATH,2021-03,GPT-3-175B (few-shot) model in \'Measuring Mathematical Problem Solving With the MATH Dataset\',175.0,Parameters (Billions),pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,MATH,2022-06,PaLM 540B model in \'Solving Quantitative Reasoning Problems with Language Models\',540.0,Parameters (Billions),pos
Natural language generation,Question answering,Mathematical question answering,Math word problem solving,SVAMP,2021-03,Graph2Tree with RoBERTa model in \'Are NLP Models really able to Solve Simple Math Word Problems?\',43.8,Execution Accuracy,pos
Natural language generation,Question answering,Mathematical question answering,Mathematical question answering,Geometry3K,2021-05,Human Expert model in \'Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning\',90.9,Accuracy (%),pos
Natural language generation,Question answering,Mathematical question answering,Mathematical question answering,GeoS,2015-09,GEOS model in \'Solving Geometry Problems: Combining Text and Diagram Interpretation\',49.0,Accuracy (%),pos
Natural language generation,Question answering,Mathematical question answering,Mathematical question answering,GeoS,2021-05,Inter-GPS model in \'Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning\',67.0,Accuracy (%),pos
Natural language generation,Question answering,Memex question answering,Memex question answering,MemexQA,2018-06,FVTA model in \'Focal Visual-Text Attention for Visual Question Answering\',0.357,Accuracy,pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',76.51,F1 (QE-PE),pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',64.29,EM(QE-PE),pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',57.31,F1 (QE-PH),pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',44.71,EM(QE-PH),pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',51.04,F1(QH-PE),pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',41.01,EM(QH-PE),pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',59.8,F1(QH-PH),pos
Natural language generation,Question answering,Multilingual machine comprehension in English Hindi,Multilingual machine comprehension in English Hindi,Extended XQuAD,2020-06,m-BERT augmented with Hindi QA model in \'BERT Based Multilingual Machine Comprehension in English and Hindi\',45.63,EM(QH-PH),pos
Natural language generation,Question answering,Multiple Choice Question Answering (MCQA),Multiple Choice Question Answering (MCQA),MedMCQA (with Context),2022-03,"PubmedBERT(Gu et al., 2022) model in \'MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering\'",0.47,Test Set (Acc-%),pos
Natural language generation,Question answering,Multiple Choice Question Answering (MCQA),Multiple Choice Question Answering (MCQA),MedMCQA (with Context),2022-03,"PubmedBERT(Gu et al., 2022) model in \'MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering\'",0.43,Dev Set (Acc-%),pos
Natural language generation,Question answering,Multiple Choice Question Answering (MCQA),Multiple Choice Question Answering (MCQA),IndicGLUE WSTP Pa,2020-11,"IndicBERT Large model in \'IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages\'",77.54,Accuracy,pos
Natural language generation,Question answering,Multiple Choice Question Answering (MCQA),Multiple Choice Question Answering (MCQA),IndicGLUE WSTP Pa,2022-01,xlmindic-base-uniscript model in \'Does Transliteration Help Multilingual Language Modeling?\',77.55,Accuracy,pos
Natural language generation,Question answering,Multiple Choice Question Answering (MCQA),Multiple Choice Question Answering (MCQA),MedMCQA (w/o Context),2022-03,"PubmedBERT(Gu et al., 2022) model in \'MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering\'",0.41,Test Set (Acc-%),pos
Natural language generation,Question answering,Multiple Choice Question Answering (MCQA),Multiple Choice Question Answering (MCQA),MedMCQA (w/o Context),2022-03,"PubmedBERT(Gu et al., 2022) model in \'MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering\'",0.4,Dev Set (Acc-%),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SQuAD1.1,2016-11,DCN model in \'Dynamic Coattention Networks For Question Answering\',66.2,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SQuAD1.1,2017-03,DrQA model in \'Reading Wikipedia to Answer Open-Domain Questions\',70.0,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,WebQuestions,2020-12,UniK-QA model in \'UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering\',57.7,Exact Match,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Natural Questions,2020-12,UniK-QA model in \'UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering\',54.9,Exact Match,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Natural Questions,2021-09,R2-D2 \\w HN-DPR model in \'R2-D2: A Modular Baseline for Open-Domain Question Answering\',55.9,Exact Match,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SQuAD1.1 dev,2019-02,BERTserini model in \'End-to-End Open-Domain Question Answering with BERTserini\',38.6,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SQuAD1.1 dev,2019-04,BERTserini model in \'Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering\',50.2,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SQuAD1.1 dev,2020-09,SPARTA model in \'SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval\',59.3,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,TriviaQA,2021-01,UnitedQA (Hybrid) model in \'UnitedQA: A Hybrid Approach for Open Domain Question Answering\',70.5,Exact Match,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Quasar,2016-06,GA model in \'Gated-Attention Readers for Text Comprehension\',26.4,EM (Quasar-T),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Quasar,2017-08,R^3 model in \'R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering\',35.3,EM (Quasar-T),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Quasar,2017-11,Evidence Aggregation via R^3 Re-Ranking model in \'Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering\',42.3,EM (Quasar-T),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Quasar,2016-06,GA model in \'Gated-Attention Readers for Text Comprehension\',26.4,F1 (Quasar-T),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Quasar,2016-11,BiDAF model in \'Bidirectional Attention Flow for Machine Comprehension\',28.5,F1 (Quasar-T),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Quasar,2017-08,R^3 model in \'R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering\',41.7,F1 (Quasar-T),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Quasar,2017-11,Evidence Aggregation via R^3 Re-Ranking model in \'Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering\',49.6,F1 (Quasar-T),pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,TQA,2020-12,UniK-QA model in \'UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering\',65.5,Exact Match,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2017-08,R^3 model in \'R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering\',55.3,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-07,Denoising QA model in \'Denoising Distantly Supervised Open-Domain Question Answering\',64.5,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2019-07,SpanBERT model in \'SpanBERT: Improving Pre-training by Representing and Predicting Spans\',84.8,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2017-03,DrQA model in \'Reading Wikipedia to Answer Open-Domain Questions\',41.9,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2017-08,R^3 model in \'R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering\',49.0,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-07,Denoising QA model in \'Denoising Distantly Supervised Open-Domain Question Answering\',58.8,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-11,DECAPROP model in \'Densely Connected Attention Propagation for Reading Comprehension\',62.2,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2019-04,Sparse Attention model in \'Generating Long Sequences with Sparse Transformers\',64.7,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2019-08,Multi-passage BERT model in \'Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering\',65.1,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2020-01,Locality-Sensitive Hashing model in \'Reformer: The Efficient Transformer\',66.0,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2020-09,Cluster-Former (#C=512) model in \'Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding\',68.0,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2016-03,ASR model in \'Text Understanding with the Attention Sum Reader Network\',22.8,N-gram F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-01,AMANDA model in \'A Question-Focused Multi-Factor Attention Network for Question Answering\',56.6,N-gram F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-10,Bi-Attention + DCU-LSTM model in \'Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension\',59.5,N-gram F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-11,DecaProp model in \'Densely Connected Attention Propagation for Reading Comprehension\',70.8,N-gram F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2016-03,ASR model in \'Text Understanding with the Attention Sum Reader Network\',41.3,Unigram Acc,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-01,AMANDA model in \'A Question-Focused Multi-Factor Attention Network for Question Answering\',46.8,Unigram Acc,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-10,Bi-Attention + DCU-LSTM model in \'Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension\',49.4,Unigram Acc,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,SearchQA,2018-11,DecaProp model in \'Densely Connected Attention Propagation for Reading Comprehension\',62.2,Unigram Acc,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,DuReader,2019-07,ERNIE 2.0 Large model in \'ERNIE 2.0: A Continual Pre-training Framework for Language Understanding\',64.2,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: HotpotQA,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',19.57,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: HotpotQA,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',12.64,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',27.73,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",60.97,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',19.6,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",51.73,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",43.56,KILT-EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",70.78,R-Prec,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",76.63,Recall@5,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: Natural Questions,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",49.8,KILT-F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,Natural Questions (short),2021-06,EMDR2 model in \'End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\',52.5,Exact Match,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',27.83,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",81.4,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',18.11,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",76.27,EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",57.91,KILT-EM,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",72.68,R-Prec,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",74.23,Recall@5,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: TriviaQA,2022-07,"Re2G model in \'Re2G: Retrieve, Rerank, Generate\'",61.78,KILT-F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',16.1,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2021-03,arxiv.org/abs/2103.06332 model in \'Hurdles to Progress in Long-form Question Answering\',22.88,F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2020-09,T5-base model in \'KILT: a Benchmark for Knowledge Intensive Language Tasks\',19.08,ROUGE-L,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2021-03,arxiv.org/abs/2103.06332 model in \'Hurdles to Progress in Long-form Question Answering\',23.19,ROUGE-L,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2021-03,arxiv.org/abs/2103.06332 model in \'Hurdles to Progress in Long-form Question Answering\',10.67,R-Prec,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2021-03,arxiv.org/abs/2103.06332 model in \'Hurdles to Progress in Long-form Question Answering\',24.56,Recall@5,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2021-03,arxiv.org/abs/2103.06332 model in \'Hurdles to Progress in Long-form Question Answering\',2.34,KILT-F1,pos
Natural language generation,Question answering,Open-domain question answering,Open-domain question answering,KILT: ELI5,2021-03,arxiv.org/abs/2103.06332 model in \'Hurdles to Progress in Long-form Question Answering\',2.36,KILT-RL,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (HotpotQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',30.49,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (HotpotQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',30.38,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (HotpotQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',16.76,BLEU-4,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (HotpotQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',4.98,METEOR,pos
Natural language generation,Question answering,Question answering,Question answering,SWAG,2021-11,DeBERTaV3large model in \'DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\',93.4,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,PubMedQA,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',55.84,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,PubMedQA,2021-06,BioELECTRA uncased model in \'BioELECTRA:Pretrained Biomedical text Encoder using Discriminators\',64.2,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,PubMedQA,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',72.2,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Aristo Kaggle Allen AI 8th grade questions,2016-04,Cardal model in \'Moving Beyond the Turing Test with the Allen AI Science Challenge\',59.31,1:1 Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,BioASQ,2020-07,PubMedBERT uncased model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',87.56,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,BioASQ,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',94.8,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,MedQA-USMLE,2019-01,BioBERT (large) model in \'BioBERT: a pre-trained biomedical language representation model for biomedical text mining\',36.7,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,MedQA-USMLE,2021-04,QA-GNN model in \'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering\',38.0,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,MedQA-USMLE,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',44.6,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,TweetQA,2021-05,ByT5 (small) model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',72.0,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,TweetQA,2021-05,ByT5 model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',75.7,ROUGE-L,pos
Natural language generation,Question answering,Question answering,Question answering,CommonsenseQA,2020-05,UnifiedQA model in \'UnifiedQA: Crossing Format Boundaries With a Single QA System\',79.1,Test Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,NQ (BEIR),2021-04,BM25+CE model in \'BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\',0.533,nDCG-at-10,pos
Natural language generation,Question answering,Question answering,Question answering,NQ (BEIR),2022-06,monoT5-3B model in \'No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval\',0.633,nDCG-at-10,pos
Natural language generation,Question answering,Question answering,Question answering,OTT-QA,2020-10,Fusion Retriever+ETC model in \'Open Question Answering over Tables and Text\',27.2,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,OTT-QA,2022-01,CARP model in \'Reasoning over Hybrid Chain for Table-and-Text Open Domain QA\',32.5,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,FinQA,2021-09,FinQANet (RoBERTa-large) model in \'FinQA: A Dataset of Numerical Reasoning over Financial Data\',65.05,Execution Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,FinQA,2021-09,FinQANet (RoBERTa-large) model in \'FinQA: A Dataset of Numerical Reasoning over Financial Data\',63.52,Program Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,EfficientQA dev,2021-01,UnitedQA model in \'UnitedQA: A Hybrid Approach for Open Domain Question Answering\',54.1,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2017-03,DrQA model in \'Reading Wikipedia to Answer Open-Domain Questions\',35.7,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2019-01,BERTjoint model in \'A BERT Baseline for the Natural Questions\',52.7,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2019-04,Sparse Attention model in \'Generating Long Sequences with Sparse Transformers\',56.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2019-09,BERTwwm + SQuAD 2 model in \'Frustratingly Easy Natural Question Answering\',57.2,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2020-02,REALM model in \'REALM: Retrieval-Augmented Language Model Pre-Training\',40.4,Exact Match (EM),pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2020-04,DPR model in \'Dense Passage Retrieval for Open-Domain Question Answering\',41.5,Exact Match (EM),pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2020-05,RAG model in \'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\',44.5,Exact Match (EM),pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2020-07,Fusion-in-Decoder (large) model in \'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\',51.4,Exact Match (EM),pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (short),2020-12,FiD+Distil model in \'Distilling Knowledge from Reader to Retriever for Question Answering\',53.7,Exact Match (EM),pos
Natural language generation,Question answering,Question answering,Question answering,SberQuAD,2019-12,DeepPavlov R-Net model in \'SberQuAD -- Russian Reading Comprehension Dataset: Description and Analysis\',80.04,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SberQuAD,2019-12,DeepPavlov R-Net model in \'SberQuAD -- Russian Reading Comprehension Dataset: Description and Analysis\',60.62,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Quasart-T,2017-03,DrQA model in \'Reading Wikipedia to Answer Open-Domain Questions\',37.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Quasart-T,2018-07,Denoising QA model in \'Denoising Distantly Supervised Open-Domain Question Answering\',42.2,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Quasart-T,2019-04,Sparse Attention model in \'Generating Long Sequences with Sparse Transformers\',52.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Quasart-T,2020-01,Locality-Sensitive Hashing model in \'Reformer: The Efficient Transformer\',53.2,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Quasart-T,2020-09,Cluster-Former (#C=512) model in \'Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding\',54.0,EM,pos
Natural language generation,Question answering,Question answering,Question answering,AI2 Kaggle Dataset,2017-08,OUR APPROACH model in \'Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification\',54.0,P-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,EfficientQA test,2021-01,UnitedQA model in \'UnitedQA: A Hybrid Approach for Open Domain Question Answering\',54.0,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,StepGame,2022-04,TP-MANN model in \'StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts\',52.99,1-of-100 Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,RecipeQA,2021-01,multimodal+LXMERT+ConstrainedMaxPooling model in \'Latent Alignment of Procedural Concepts in Multimodal Recipes\',0.475,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,HybridQA,2020-04,HYBRIDER model in \'HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data\',43.8,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HybridQA,2021-06,DocHopper model in \'Iterative Hierarchical Attention for Answering Complex Questions over Long Documents\',46.3,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HybridQA,2021-09,MATE Ponter model in \'MATE: Multi-view Attention for Table Transformer Efficiency\',62.7,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HybridQA,2021-12,MITQA model in \'Multi-Instance Training for Question Answering Across Table and Linked Text\',64.3,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,SCDE,2020-04,bert-large-uncased + APN model in \'SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations\',0.717,BA,pos
Natural language generation,Question answering,Question answering,Question answering,SCDE,2020-04,bert-large-uncased + APN model in \'SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations\',0.299,PA,pos
Natural language generation,Question answering,Question answering,Question answering,SCDE,2020-04,bert-large-uncased + APN model in \'SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations\',0.661,DE,pos
Natural language generation,Question answering,Question answering,Question answering,QASPER,2021-05,Longformer Encoder Decoder (base) model in \'A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers\',33.63,Token F1,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (NarrativeQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',22.09,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (NarrativeQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',19.84,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (NarrativeQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',62.0,BLEU-4,pos
Natural language generation,Question answering,Question answering,Question answering,MuLD (NarrativeQA),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',4.52,METEOR,pos
Natural language generation,Question answering,Question answering,Question answering,SimpleQuestions,2015-06,Memory Networks (ensemble) model in \'Large-scale Simple Question Answering with Memory Networks\',63.9,F1,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2017-03,FastQAExt model in \'Making Neural QA as Simple as Possible but not Simpler\',56.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2018-01,AMANDA model in \'A Question-Focused Multi-Factor Attention Network for Question Answering\',63.7,F1,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2018-11,DecaProp model in \'Densely Connected Attention Propagation for Reading Comprehension\',66.3,F1,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2019-07,SpanBERT model in \'SpanBERT: Improving Pre-training by Representing and Predicting Spans\',73.6,F1,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2017-03,FastQAExt model in \'Making Neural QA as Simple as Possible but not Simpler\',43.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2018-01,AMANDA model in \'A Question-Focused Multi-Factor Attention Network for Question Answering\',48.4,EM,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2018-05,MINIMAL(Dyn) model in \'Efficient and Robust Question Answering from Minimal Context over Documents\',50.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,NewsQA,2018-11,DecaProp model in \'Densely Connected Attention Propagation for Reading Comprehension\',53.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,StoryCloze,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',93.4,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (long),2017-03,DrQA model in \'Reading Wikipedia to Answer Open-Domain Questions\',46.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (long),2019-01,BERTjoint model in \'A BERT Baseline for the Natural Questions\',64.7,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (long),2019-04,Sparse Attention model in \'Generating Long Sequences with Sparse Transformers\',74.5,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (long),2020-01,Locality-Sensitive Hashing model in \'Reformer: The Efficient Transformer\',75.5,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (long),2020-09,Cluster-Former (#C=512) model in \'Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding\',76.5,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (long),2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',79.6,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions (long),2020-12,DensePhrases model in \'Learning Dense Representations of Phrases at Scale\',71.9,EM,pos
Natural language generation,Question answering,Question answering,Question answering,MS MARCO,2016-11,BiDaF Baseline model in \'Bidirectional Attention Flow for Machine Comprehension\',23.96,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,MS MARCO,2018-05,VNET model in \'Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification\',51.63,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,MS MARCO,2018-11,Deep Cascade QA model in \'A Deep Cascade Model for Multi-Document Reading Comprehension\',52.01,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,MS MARCO,2019-01,Masque Q&A Style model in \'Multi-style Generative Reading Comprehension\',52.2,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,MS MARCO,2016-11,BiDaF Baseline model in \'Bidirectional Attention Flow for Machine Comprehension\',10.64,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,MS MARCO,2018-05,VNET model in \'Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification\',54.37,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,MS MARCO,2018-11,Deep Cascade QA model in \'A Deep Cascade Model for Multi-Document Reading Comprehension\',54.64,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,COPA,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',94.8,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,COPA,2020-06,DeBERTa-Ensemble model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',98.4,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,COPA,2022-04,PaLM 540B (finetuned)  model in \'PaLM: Scaling Language Modeling with Pathways\',100.0,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,FQuAD,2020-02,CamemBERT-Large model in \'FQuAD: French Question Answering Dataset\',92.2,F1,pos
Natural language generation,Question answering,Question answering,Question answering,FQuAD,2020-02,CamemBERT-Large model in \'FQuAD: French Question Answering Dataset\',82.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA (BEIR),2021-04,BM25+CE model in \'BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\',0.707,nDCG-at-10,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA (BEIR),2022-06,monoT5-3B model in \'No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval\',0.759,nDCG-at-10,pos
Natural language generation,Question answering,Question answering,Question answering,FairytaleQA,2022-03,BART fine-tuned on FairytaleQA model in \'Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension\',0.536,F1,pos
Natural language generation,Question answering,Question answering,Question answering,FairytaleQA,2022-03,BART fine-tuned on FairytaleQA model in \'Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension\',0.533,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2018-10,BERT-large(single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',70.0,F1,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',88.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',88.2,F1,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2022-04,PaLM 540B (finetuned)  model in \'PaLM: Scaling Language Modeling with Pathways\',90.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2018-10,BERT-large(single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',24.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',63.3,EM,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',63.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,MultiRC,2022-04,PaLM 540B (finetuned)  model in \'PaLM: Scaling Language Modeling with Pathways\',69.2,EM,pos
Natural language generation,Question answering,Question answering,Question answering,ARC-c,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',63.1,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Molweni,2021-04,DADgraph model in \'DADgraph: A Discourse-aware Dialogue Graph Neural Network for Multiparty Dialogue Machine Reading Comprehension\',61.5,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Molweni,2021-09,Li and Zhao - ELECTRA model in \'Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension\',72.9,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Molweni,2021-04,DADgraph model in \'DADgraph: A Discourse-aware Dialogue Graph Neural Network for Multiparty Dialogue Machine Reading Comprehension\',46.5,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Molweni,2021-09,Ma et al. - ELECTRA model in \'Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension\',58.6,EM,pos
Natural language generation,Question answering,Question answering,Question answering,CaseHOLD,2021-04,Custom Legal-BERT model in \'When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset\',69.5,Macro F1 (10-fold),pos
Natural language generation,Question answering,Question answering,Question answering,QASent,2014-05,Paragraph vector (lexical overlap + dist output) model in \'Distributed Representations of Sentences and Documents\',0.7514,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,QASent,2014-12,Bigram-CNN (lexical overlap + dist output) model in \'Deep Learning for Answer Sentence Selection\',0.7846,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,QASent,2015-11,Attentive LSTM model in \'Neural Variational Inference for Text Processing\',0.8117,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,QASent,2014-05,Paragraph vector (lexical overlap + dist output) model in \'Distributed Representations of Sentences and Documents\',0.6762,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,QASent,2014-12,Bigram-CNN (lexical overlap + dist output) model in \'Deep Learning for Answer Sentence Selection\',0.7113,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,QASent,2015-11,Attentive LSTM model in \'Neural Variational Inference for Text Processing\',0.7339,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,UnifiedQA 3B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',-0.16,BLEU,pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,UnifiedQA 3B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',1.76,ROUGE,pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,UnifiedQA 3B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',53.86,% true,pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,GPT-3 175B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',97.55,% info,pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,UnifiedQA 3B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',53.24,% true (GPT-judge),pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,UnifiedQA 3B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',0.08,BLEURT,pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,GPT-2 1.5B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',0.22,MC1,pos
Natural language generation,Question answering,Question answering,Question answering,TruthfulQA,2021-09,GPT-2 1.5B model in \'TruthfulQA: Measuring How Models Mimic Human Falsehoods\',0.39,MC2,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-08,DrQA + seq2seq with copy attention (single model) model in \'CoQA: A Conversational Question Answering Challenge\',67.0,In-domain,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-09,"BiDAF++ (single model) model in \'A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC\'",69.4,In-domain,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-10,BERT Large Augmented (single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',82.5,In-domain,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-08,DrQA + seq2seq with copy attention (single model) model in \'CoQA: A Conversational Question Answering Challenge\',60.4,Out-of-domain,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-09,"BiDAF++ (single model) model in \'A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC\'",63.8,Out-of-domain,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-10,BERT Large Augmented (single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',77.6,Out-of-domain,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-08,DrQA + seq2seq with copy attention (single model) model in \'CoQA: A Conversational Question Answering Challenge\',65.1,Overall,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-09,"BiDAF++ (single model) model in \'A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC\'",67.8,Overall,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2018-10,BERT Large Augmented (single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',81.1,Overall,pos
Natural language generation,Question answering,Question answering,Question answering,CoQA,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',85.0,Overall,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2014-12,CNN model in \'Deep Learning for Answer Sentence Selection\',0.785,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2016-06,PWIN model in \'Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement\',0.8219,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.825,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2019-05,Comp-Clip + LM + LC model in \'A Compare-Aggregate Model with Latent Clustering for Answer Selection\',0.928,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2019-11,"TANDA-RoBERTa (ASNQ, TREC-QA) model in \'TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection\'",0.974,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2014-12,CNN model in \'Deep Learning for Answer Sentence Selection\',0.711,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2016-06,PWIN model in \'Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement\',0.7588,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.77,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2019-05,Comp-Clip + LM + LC model in \'A Compare-Aggregate Model with Latent Clustering for Answer Selection\',0.868,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,TrecQA,2019-11,"TANDA-RoBERTa (ASNQ, TREC-QA) model in \'TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection\'",0.943,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,DROP Test,2019-03,NAQA Net model in \'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\',47.01,F1,pos
Natural language generation,Question answering,Question answering,Question answering,DROP Test,2019-08,BERT+Calculator (ensemble) model in \'Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension\',81.78,F1,pos
Natural language generation,Question answering,Question answering,Question answering,DROP Test,2020-09,QDGAT (ensemble) model in \'Question Directed Graph Attention Network for Numerical Reasoning over Text\',88.38,F1,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2017-05,Mnemonic Reader model in \'Reinforced Mnemonic Reader for Machine Reading Comprehension\',52.85,F1,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2017-06,Reading Twice for NLU model in \'Dynamic Integration of Background Knowledge in Neural NLU Systems\',56.73,F1,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2017-10,S-Norm model in \'Simple and Effective Multi-Paragraph Reading Comprehension\',71.32,F1,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2018-10,MemoReader model in \'MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller\',73.26,F1,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2019-07,SpanBERT model in \'SpanBERT: Improving Pre-training by Representing and Predicting Spans\',83.6,F1,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2017-05,Mnemonic Reader model in \'Reinforced Mnemonic Reader for Machine Reading Comprehension\',46.94,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2017-06,Reading Twice for NLU model in \'Dynamic Integration of Background Knowledge in Neural NLU Systems\',50.56,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2017-10,S-Norm model in \'Simple and Effective Multi-Paragraph Reading Comprehension\',66.37,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2018-10,MemoReader model in \'MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller\',67.21,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',71.2,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2020-12,FiD+Distil model in \'Distilling Knowledge from Reader to Retriever for Question Answering\',72.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2021-12,GLaM 62B/64E (Few-shot) model in \'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\',75.8,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TriviaQA,2022-04,PaLM-540B (One-Shot) model in \'PaLM: Scaling Language Modeling with Pathways\',81.4,EM,pos
Natural language generation,Question answering,Question answering,Question answering,TACRED,2020-10,LUKE model in \'LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\',72.7,Relation F1,pos
Natural language generation,Question answering,Question answering,Question answering,Children's Book Test,2016-03,AS reader (avg) model in \'Text Understanding with the Attention Sum Reader Network\',68.9,Accuracy-CN,pos
Natural language generation,Question answering,Question answering,Question answering,Children's Book Test,2016-06,NSE model in \'Gated-Attention Readers for Text Comprehension\',71.9,Accuracy-CN,pos
Natural language generation,Question answering,Question answering,Question answering,Children's Book Test,2018-02,GPT-2 model in \'007: Democratically Finding The Cause of Packet Drops\',93.3,Accuracy-CN,pos
Natural language generation,Question answering,Question answering,Question answering,Children's Book Test,2016-03,AS reader (greedy) model in \'Text Understanding with the Attention Sum Reader Network\',71.0,Accuracy-NE,pos
Natural language generation,Question answering,Question answering,Question answering,Children's Book Test,2016-06,GA + feature + fix L(w) model in \'Gated-Attention Readers for Text Comprehension\',74.9,Accuracy-NE,pos
Natural language generation,Question answering,Question answering,Question answering,Children's Book Test,2018-02,GPT-2 model in \'007: Democratically Finding The Cause of Packet Drops\',89.05,Accuracy-NE,pos
Natural language generation,Question answering,Question answering,Question answering,ComplexWebQuestions,2021-10,TOME-2 model in \'Mention Memory: incorporating textual knowledge into Transformers through entity mention attention\',47.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,DaNetQA,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.915,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2014-05,Paragraph vector (lexical overlap + dist output) model in \'Distributed Representations of Sentences and Documents\',0.6058,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2014-12,Bigram-CNN (lexical overlap + dist output) model in \'Deep Learning for Answer Sentence Selection\',0.6652,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2015-11,Attentive LSTM model in \'Neural Variational Inference for Text Processing\',0.7069,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2016-02,LDC model in \'Sentence Similarity Learning by Lexical Decomposition and Composition\',0.7226,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2016-06,Key-Value Memory Network model in \'Key-Value Memory Networks for Directly Reading Documents\',0.7265,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.727,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2019-05,Comp-Clip + LM + LC model in \'A Compare-Aggregate Model with Latent Clustering for Answer Selection\',0.784,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2019-11,"TANDA-RoBERTa (ASNQ, WikiQA) model in \'TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection\'",0.933,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2014-05,Paragraph vector (lexical overlap + dist output) model in \'Distributed Representations of Sentences and Documents\',0.5976,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2014-12,Bigram-CNN (lexical overlap + dist output) model in \'Deep Learning for Answer Sentence Selection\',0.652,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2015-11,Attentive LSTM model in \'Neural Variational Inference for Text Processing\',0.6886,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2016-02,LDC model in \'Sentence Similarity Learning by Lexical Decomposition and Composition\',0.7058,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2016-06,PWIM model in \'Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement\',0.709,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.712,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2019-05,Comp-Clip + LM + LC model in \'A Compare-Aggregate Model with Latent Clustering for Answer Selection\',0.764,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,WikiQA,2019-11,"TANDA-RoBERTa (ASNQ, WikiQA) model in \'TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection\'",0.92,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,catbAbI LM-mode,2020-11,Fast Weight Memory model in \'Learning Associative Inference Using Fast Weight Memory\',93.04,Accuracy (mean),pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2016-11,BiDAF model in \'Bidirectional Attention Flow for Machine Comprehension\',36.74,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-09,MHPGM + NOIC model in \'Commonsense for Generative Multi-Hop Question Answering Tasks\',44.16,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-10,ConZNet model in \'Cut to the Chase: A Context Zoom-in Network for Reading Comprehension\',46.67,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO) model in \'Multi-style Generative Reading Comprehension\',59.87,Rouge-L,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2016-11,BiDAF model in \'Bidirectional Attention Flow for Machine Comprehension\',33.45,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-09,MHPGM + NOIC model in \'Commonsense for Generative Multi-Hop Question Answering Tasks\',43.63,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-11,DecaProp model in \'Densely Connected Attention Propagation for Reading Comprehension\',44.35,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO) model in \'Multi-style Generative Reading Comprehension\',54.11,BLEU-1,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2016-11,BiDAF model in \'Bidirectional Attention Flow for Machine Comprehension\',15.69,BLEU-4,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-09,MHPGM + NOIC model in \'Commonsense for Generative Multi-Hop Question Answering Tasks\',21.07,BLEU-4,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-10,ConZNet model in \'Cut to the Chase: A Context Zoom-in Network for Reading Comprehension\',22.49,BLEU-4,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-11,DecaProp model in \'Densely Connected Attention Propagation for Reading Comprehension\',27.61,BLEU-4,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO) model in \'Multi-style Generative Reading Comprehension\',30.43,BLEU-4,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2016-11,BiDAF model in \'Bidirectional Attention Flow for Machine Comprehension\',15.68,METEOR,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-09,MHPGM + NOIC model in \'Commonsense for Generative Multi-Hop Question Answering Tasks\',19.03,METEOR,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-10,ConZNet model in \'Cut to the Chase: A Context Zoom-in Network for Reading Comprehension\',19.24,METEOR,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2018-11,DecaProp model in \'Densely Connected Attention Propagation for Reading Comprehension\',21.8,METEOR,pos
Natural language generation,Question answering,Question answering,Question answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO) model in \'Multi-style Generative Reading Comprehension\',26.13,METEOR,pos
Natural language generation,Question answering,Question answering,Question answering,MRQA out-of-domain,2021-03,RGX model in \'Cooperative Self-training of Machine Reading Comprehension\',68.4,Average F1,pos
Natural language generation,Question answering,Question answering,Question answering,bAbi,2015-03,End-To-End Memory Networks model in \'End-To-End Memory Networks\',-7.5,Mean Error Rate,neg
Natural language generation,Question answering,Question answering,Question answering,bAbi,2016-06,QRN model in \'Query-Reduction Networks for Question Answering\',-0.3,Mean Error Rate,neg
Natural language generation,Question answering,Question answering,Question answering,bAbi,2015-03,End-To-End Memory Networks model in \'End-To-End Memory Networks\',93.4,Accuracy (trained on 10k),pos
Natural language generation,Question answering,Question answering,Question answering,bAbi,2016-06,QRN model in \'Query-Reduction Networks for Question Answering\',99.7,Accuracy (trained on 10k),pos
Natural language generation,Question answering,Question answering,Question answering,bAbi,2020-02,STM model in \'Self-Attentive Associative Memory\',99.85,Accuracy (trained on 10k),pos
Natural language generation,Question answering,Question answering,Question answering,bAbi,2015-03,End-To-End Memory Networks model in \'End-To-End Memory Networks\',86.1,Accuracy (trained on 1k),pos
Natural language generation,Question answering,Question answering,Question answering,bAbi,2016-06,QRN model in \'Query-Reduction Networks for Question Answering\',90.1,Accuracy (trained on 1k),pos
Natural language generation,Question answering,Question answering,Question answering,WikiHop,2017-10,BiDAF model in \'Constructing Datasets for Multi-hop Reading Comprehension Across Documents\',42.9,Test,pos
Natural language generation,Question answering,Question answering,Question answering,WikiHop,2018-04,Coref-GRU model in \'Neural Models for Reasoning over Multiple Mentions using Coreference\',59.3,Test,pos
Natural language generation,Question answering,Question answering,Question answering,WikiHop,2018-09,MHQA model in \'Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks\',65.4,Test,pos
Natural language generation,Question answering,Question answering,Question answering,WikiHop,2019-01,CFC model in \'Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering\',70.6,Test,pos
Natural language generation,Question answering,Question answering,Question answering,WikiHop,2019-10,"MultiHop (Chen et al., [2019a]) model in \'Multi-hop Question Answering via Reasoning Chains\'",76.5,Test,pos
Natural language generation,Question answering,Question answering,Question answering,WikiHop,2020-04,Longformer-large model in \'Longformer: The Long-Document Transformer\',81.9,Test,pos
Natural language generation,Question answering,Question answering,Question answering,WikiHop,2020-07,BigBird-etc model in \'Big Bird: Transformers for Longer Sequences\',82.3,Test,pos
Natural language generation,Question answering,Question answering,Question answering,MCTest-500,2016-03,Parallel-Hierarchical model in \'A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data\',71.0,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Mathematics Dataset,2019-04,Transformer model in \'Analysing Mathematical Reasoning Abilities of Neural Models\',0.76,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Mathematics Dataset,2019-10,TP-Transformer model in \'Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving\',0.8192,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Mathematics Dataset,2021-02,Graph Logic Reasoner model in \'Logic Embeddings for Complex Query Answering\',0.98667,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0 dev,2018-08,RMR + ELMo (Model-III) model in \'Read + Verify: Machine Reading Comprehension with Unanswerable Questions\',74.8,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0 dev,2018-10,BERT large model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',81.9,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0 dev,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',90.6,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0 dev,2018-08,RMR + ELMo (Model-III) model in \'Read + Verify: Machine Reading Comprehension with Unanswerable Questions\',72.3,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0 dev,2018-10,BERT large model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',78.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0 dev,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',87.9,EM,pos
Natural language generation,Question answering,Question answering,Question answering,FiQA-2018 (BEIR),2021-04,BM25+CE model in \'BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\',0.347,nDCG-at-10,pos
Natural language generation,Question answering,Question answering,Question answering,FiQA-2018 (BEIR),2022-02,SGPT-CE-6.1B model in \'SGPT: GPT Sentence Embeddings for Semantic Search\',0.401,nDCG-at-10,pos
Natural language generation,Question answering,Question answering,Question answering,FiQA-2018 (BEIR),2022-06,monoT5-3B model in \'No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval\',0.513,nDCG-at-10,pos
Natural language generation,Question answering,Question answering,Question answering,ChAII - Hindi and Tamil Question Answering,2022-04,MuCoT model in \'MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages\',0.53,Jaccard,pos
Natural language generation,Question answering,Question answering,Question answering,BoolQ,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',91.2,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,BoolQ,2022-02,ST-MoE-32B model in \'ST-MoE: Designing Stable and Transferable Sparse Expert Models\',92.4,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,JD Product Question Answer,2019-01,PAAG model in \'Product-Aware Answer Generation in E-Commerce Question-Answering\',2.0189,BLEU,pos
Natural language generation,Question answering,Question answering,Question answering,TAT-QA,2021-05,TagOp model in \'TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance\',50.1,Exact Match (EM),pos
Natural language generation,Question answering,Question answering,Question answering,Quora Question Pairs,2018-05,SWEM-concat model in \'Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms\',83.03,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Quora Question Pairs,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',92.3,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,CliCR,2018-03,Gated-Attention Reader model in \'CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension\',33.9,F1,pos
Natural language generation,Question answering,Question answering,Question answering,JaQuAD,2022-02,BERT-Japanese model in \'JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension\',78.92,F1,pos
Natural language generation,Question answering,Question answering,Question answering,JaQuAD,2022-02,BERT-Japanese model in \'JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension\',63.38,Exact Match,pos
Natural language generation,Question answering,Question answering,Question answering,catbAbI QA-mode,2020-11,Fast Weight Memory model in \'Learning Associative Inference Using Fast Weight Memory\',96.75,1:1 Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,NaturalQA,2019-07,SpanBERT model in \'SpanBERT: Improving Pre-training by Representing and Predicting Spans\',82.5,F1,pos
Natural language generation,Question answering,Question answering,Question answering,NaturalQA,2020-04,DPR model in \'Dense Passage Retrieval for Open-Domain Question Answering\',41.5,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Reverb,2014-04,Weakly Supervised Embeddings model in \'Open Question Answering with Weakly Supervised Embedding Models\',73.0,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,YahooCQA,2016-02,AP-BiLSTM model in \'Attentive Pooling Networks\',0.731,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,YahooCQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.801,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,YahooCQA,2020-02,sMIM (1024) + model in \'SentenceMIM: A Latent Variable Language Model\',0.863,MRR,pos
Natural language generation,Question answering,Question answering,Question answering,YahooCQA,2016-02,AP-BiLSTM model in \'Attentive Pooling Networks\',0.568,P-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,YahooCQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.683,P-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,YahooCQA,2020-02,sMIM (1024) + model in \'SentenceMIM: A Latent Variable Language Model\',0.757,P-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,CronQuestions,2021-06,CronKGQA model in \'Question Answering Over Temporal Knowledge Graphs\',64.7,Hits-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,CronQuestions,2021-12,TempoQR-Hard model in \'TempoQR: Temporal Question Reasoning over Knowledge Graphs\',91.8,Hits-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,COMPLEXQUESTIONS,2017-07,WebQA model in \'Evaluating Semantic Parsing against a Simple Web-based Question Answering Model\',32.6,F1,pos
Natural language generation,Question answering,Question answering,Question answering,CNN / Daily Mail,2015-06,MemNNs (ensemble) model in \'Teaching Machines to Read and Comprehend\',69.4,CNN,pos
Natural language generation,Question answering,Question answering,Question answering,CNN / Daily Mail,2016-03,AS Reader (ensemble model) model in \'Text Understanding with the Attention Sum Reader Network\',75.4,CNN,pos
Natural language generation,Question answering,Question answering,Question answering,CNN / Daily Mail,2016-06,GA Reader model in \'Gated-Attention Readers for Text Comprehension\',77.9,CNN,pos
Natural language generation,Question answering,Question answering,Question answering,CNN / Daily Mail,2017-03,GA+MAGE (32) model in \'Linguistic Knowledge as Memory for Recurrent Neural Networks\',78.6,CNN,pos
Natural language generation,Question answering,Question answering,Question answering,CNN / Daily Mail,2015-06,Attentive Reader model in \'Teaching Machines to Read and Comprehend\',69.0,Daily Mail,pos
Natural language generation,Question answering,Question answering,Question answering,CNN / Daily Mail,2016-03,AS Reader (ensemble model) model in \'Text Understanding with the Attention Sum Reader Network\',77.7,Daily Mail,pos
Natural language generation,Question answering,Question answering,Question answering,CNN / Daily Mail,2016-06,GA Reader model in \'Gated-Attention Readers for Text Comprehension\',80.9,Daily Mail,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2018-03,BiAttention MRU model in \'Multi-range Reasoning for Machine Comprehension\',60.2,RACE-m,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',62.9,RACE-m,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-01,DCMN_large  model in \'Dual Co-Matching Network for Multi-choice Reading Comprehension\',73.4,RACE-m,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-03,OCN_large model in \'Option Comparison Network for Multiple-choice Reading Comprehension\',76.7,RACE-m,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',85.45,RACE-m,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2018-03,BiAttention MRU model in \'Multi-range Reasoning for Machine Comprehension\',50.3,RACE-h,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',57.4,RACE-h,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-01,DCMN_large  model in \'Dual Co-Matching Network for Multi-choice Reading Comprehension\',68.1,RACE-h,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-03,OCN_large model in \'Option Comparison Network for Multiple-choice Reading Comprehension\',69.6,RACE-h,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2018-03,BiAttention MRU model in \'Multi-range Reasoning for Machine Comprehension\',53.3,RACE,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',59.0,RACE,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-01,DCMN_large  model in \'Dual Co-Matching Network for Multi-choice Reading Comprehension\',69.7,RACE,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-03,OCN_large model in \'Option Comparison Network for Multiple-choice Reading Comprehension\',71.7,RACE,pos
Natural language generation,Question answering,Question answering,Question answering,RACE,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',81.75,RACE,pos
Natural language generation,Question answering,Question answering,Question answering,BLURB,2020-07,PubMedBERT (uncased; abstracts) model in \'Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing\',71.7,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,BLURB,2022-03,BioLinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',83.5,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2021-09,ReasonBERTR model in \'ReasonBERT: Pre-trained to Reason with Distant Supervision\',34.1,Joint F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2018-09,"SAFSR model model in \'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\'",0.598,JOINT-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2019-05,DFGN model in \'Dynamically Fused Graph Network for Multi-hop Reasoning\',0.5982,JOINT-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2019-11,Robustly Fine-tuned Graph-based Recurrent Retriever model in \'Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\',0.612,JOINT-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-07,BigBird-etc model in \'Big Bird: Transformers for Longer Sequences\',0.736,JOINT-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2018-09,"SAFSR model model in \'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\'",0.589,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2019-11,Robustly Fine-tuned Graph-based Recurrent Retriever model in \'Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\',0.6,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-09,DDRQA model in \'Answering Any-hop Open-domain Questions with Iterative Document Reranking\',0.625,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-10,IRRR+ model in \'Answering Open-Domain Questions of Varying Reasoning Steps from Text\',0.663,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-12,HopRetriever + Sp-search model in \'HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions\',0.671,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2021-09,AISO model in \'Adaptive Information Seeking for Open-Domain Question Answering\',0.675,ANS-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2018-09,"SAFSR model model in \'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\'",0.716,ANS-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2019-11,Robustly Fine-tuned Graph-based Recurrent Retriever model in \'Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\',0.73,ANS-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-07,BigBird-etc model in \'Big Bird: Transformers for Longer Sequences\',0.812,ANS-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2018-09,"SAFSR model model in \'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\'",0.48,SUP-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2019-11,HGN + SemanticRetrievalMRS IR model in \'Hierarchical Graph Network for Multi-hop Question Answering\',0.5,SUP-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-09,Recursive Dense Retriever model in \'Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\',0.575,SUP-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2021-09,AISO model in \'Adaptive Information Seeking for Open-Domain Question Answering\',0.612,SUP-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2018-09,"SAFSR model model in \'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\'",0.757,SUP-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2019-11,HGN + SemanticRetrievalMRS IR model in \'Hierarchical Graph Network for Multi-hop Question Answering\',0.764,SUP-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-07,BigBird-etc model in \'Big Bird: Transformers for Longer Sequences\',0.891,SUP-F1,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2018-09,"SAFSR model model in \'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\'",0.345,JOINT-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2019-11,HGN + SemanticRetrievalMRS IR model in \'Hierarchical Graph Network for Multi-hop Question Answering\',0.356,JOINT-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-09,Recursive Dense Retriever model in \'Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\',0.418,JOINT-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-10,IRRR+ model in \'Answering Open-Domain Questions of Varying Reasoning Steps from Text\',0.428,JOINT-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2020-12,HopRetriever + Sp-search model in \'HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions\',0.432,JOINT-EM,pos
Natural language generation,Question answering,Question answering,Question answering,HotpotQA,2021-09,AISO model in \'Adaptive Information Seeking for Open-Domain Question Answering\',0.449,JOINT-EM,pos
Natural language generation,Question answering,Question answering,Question answering,QuAC,2018-10,FlowQA (single model) model in \'FlowQA: Grasping Flow in History for Conversational Machine Comprehension\',64.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,QuAC,2018-10,FlowQA (single model) model in \'FlowQA: Grasping Flow in History for Conversational Machine Comprehension\',5.8,HEQD,pos
Natural language generation,Question answering,Question answering,Question answering,QuAC,2018-10,FlowQA (single model) model in \'FlowQA: Grasping Flow in History for Conversational Machine Comprehension\',59.6,HEQQ,pos
Natural language generation,Question answering,Question answering,Question answering,CODAH,2019-04,BERT Large model in \'CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense\',69.6,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,CODAH,2020-04,G-DAUG-Combo + RoBERTa-Large model in \'Generative Data Augmentation for Commonsense Reasoning\',84.0,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,OpenBookQA,2019-07,Careful Selection model in \'Careful Selection of Knowledge to solve Open Book Question Answering\',72.0,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,OpenBookQA,2021-04,QA-GNN model in \'QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering\',82.8,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,PIQA,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',82.8,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Story Cloze Test,2016-06,Memory chains and semantic supervision model in \'UNIMELB at SemEval-2016 Tasks 4A and 4B: An Ensemble of Neural Networks and a Word2Vec Based Model for Sentiment Classification\',78.7,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Story Cloze Test,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',86.5,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Story Cloze Test,2018-10,Reading Strategies Model model in \'Improving Machine Reading Comprehension with General Reading Strategies\',88.3,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,Torque,2020-05,RoBERTa-large model in \'TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions\',75.2,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Torque,2020-12,ECONET model in \'ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning\',76.3,F1,pos
Natural language generation,Question answering,Question answering,Question answering,Torque,2020-05,RoBERTa-large model in \'TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions\',51.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Torque,2020-12,ECONET model in \'ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning\',52.0,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Torque,2020-05,RoBERTa-large model in \'TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions\',34.5,C,pos
Natural language generation,Question answering,Question answering,Question answering,Torque,2020-12,ECONET model in \'ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning\',37.0,C,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',29.9,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions,2021-12,GLaM 62B/64E (Few-Shot) model in \'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\',32.5,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions,2022-04,PaLM-540B (Few-Shot) model in \'PaLM: Scaling Language Modeling with Pathways\',39.6,EM,pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions,2019-01,BERT-joint model in \'A BERT Baseline for the Natural Questions\',66.2,F1 (Long),pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions,2020-07,BigBird-etc model in \'Big Bird: Transformers for Longer Sequences\',77.7,F1 (Long),pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions,2019-01,BERT-joint model in \'A BERT Baseline for the Natural Questions\',52.1,F1 (Short),pos
Natural language generation,Question answering,Question answering,Question answering,Natural Questions,2020-07,BigBird-etc model in \'Big Bird: Transformers for Longer Sequences\',57.8,F1 (Short),pos
Natural language generation,Question answering,Question answering,Question answering,MRQA,2018-10,BERT (large) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',78.5,Average F1,pos
Natural language generation,Question answering,Question answering,Question answering,MRQA,2022-03,LinkBERT (large) model in \'LinkBERT: Pretraining Language Models with Document Links\',81.0,Average F1,pos
Natural language generation,Question answering,Question answering,Question answering,OBQA,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',78.4,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2019-10,T5.1.1-XXL+SSM model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',42.8,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2014-04,Weakly Supervised Embeddings model in \'Open Question Answering with Weakly Supervised Embedding Models\',29.7,F1,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2014-06,Subgraph embeddings model in \'Question Answering with Subgraph Embeddings\',39.2,F1,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2015-06,Memory Networks (ensemble) model in \'Large-scale Simple Question Answering with Memory Networks\',42.2,F1,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2019-06,ORQA model in \'Latent Retrieval for Weakly Supervised Open Domain Question Answering\',36.4,EM,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2020-02,REALM model in \'REALM: Retrieval-Augmented Language Model Pre-Training\',40.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2020-04,DPR model in \'Dense Passage Retrieval for Open-Domain Question Answering\',42.4,EM,pos
Natural language generation,Question answering,Question answering,Question answering,WebQuestions,2020-05,RAG model in \'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\',45.2,EM,pos
Natural language generation,Question answering,Question answering,Question answering,MCTest-160,2016-03,"syntax, frame, coreference, and word embedding features model in \'A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data\'",75.27,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2016-08,Match-LSTM with Bi-Ans-Ptr (Boundary+Search+b)  model in \'Machine Comprehension Using Match-LSTM and Answer Pointer\',64.7,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2016-10,DCR model in \'End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension\',71.2,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2016-11,BIDAF (single) model in \'Bidirectional Attention Flow for Machine Comprehension\',77.3,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2017-03,DrQA (Document Reader only) model in \'Reading Wikipedia to Answer Open-Domain Questions\',78.8,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2017-04,Ruminating Reader model in \'Ruminating Reader: Reasoning with Gated Multi-Hop Attention\',79.5,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2017-05,R.M-Reader (single) model in \'Reinforced Mnemonic Reader for Machine Reading Comprehension\',86.3,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2018-10,BERT large (+TriviaQA) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',91.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',95.1,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',95.64,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2019-11,XLNet+DSC model in \'Dice Loss for Data-imbalanced NLP Tasks\',95.77,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2016-08,Match-LSTM with Bi-Ans-Ptr (Boundary+Search+b)  model in \'Machine Comprehension Using Match-LSTM and Answer Pointer\',64.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2016-11,BIDAF (single) model in \'Bidirectional Attention Flow for Machine Comprehension\',67.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2017-03,FastQAExt (beam-size 5) model in \'Making Neural QA as Simple as Possible but not Simpler\',70.3,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2017-04,Ruminating Reader model in \'Ruminating Reader: Reasoning with Gated Multi-Hop Attention\',70.6,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2017-05,R.M-Reader (single) model in \'Reinforced Mnemonic Reader for Machine Reading Comprehension\',78.9,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2018-10,BERT large (+TriviaQA) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',84.2,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',89.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1 dev,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',90.06,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SemEvalCQA,2015-03,ARC-II model in \'Convolutional Neural Network Architectures for Matching Natural Language Sentences\',0.753,P-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,SemEvalCQA,2016-02,AP-CNN model in \'Attentive Pooling Networks\',0.755,P-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,SemEvalCQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.809,P-at-1,pos
Natural language generation,Question answering,Question answering,Question answering,SemEvalCQA,2015-03,ARC-II model in \'Convolutional Neural Network Architectures for Matching Natural Language Sentences\',0.78,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,SemEvalCQA,2016-06,Kelp model in \'KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers\',0.792,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,SemEvalCQA,2017-07,HyperQA model in \'Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\',0.795,MAP,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2016-08,Match-LSTM with Ans-Ptr (Boundary) (ensemble) model in \'Machine Comprehension Using Match-LSTM and Answer Pointer\',77.022,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2016-09,ReasoNet (ensemble) model in \'ReasoNet: Learning to Stop Reading in Machine Comprehension\',82.552,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2017-05,Reinforced Mnemonic Reader (ensemble model) model in \'Reinforced Mnemonic Reader for Machine Reading Comprehension\',88.533,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2018-10,BERT (ensemble) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',93.16,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',95.08,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2020-10,LUKE model in \'LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\',95.4,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2016-08,Match-LSTM with Ans-Ptr (Boundary) (ensemble) model in \'Machine Comprehension Using Match-LSTM and Answer Pointer\',67.901,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2016-09,ReasoNet (ensemble) model in \'ReasoNet: Learning to Stop Reading in Machine Comprehension\',75.034,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2017-05,Reinforced Mnemonic Reader (ensemble model) model in \'Reinforced Mnemonic Reader for Machine Reading Comprehension\',82.283,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2018-10,BERT (ensemble) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',87.433,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',89.898,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD1.1,2020-10,LUKE (single model) model in \'LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\',90.202,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2017-11,FusionNet++ (ensemble) model in \'FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension\',72.484,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2017-12,SAN (ensemble model) model in \'Stochastic Answer Networks for Machine Reading Comprehension\',73.704,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2018-08,Reinforced Mnemonic Reader + Answer Verifier (single model) model in \'Read + Verify: Machine Reading Comprehension with Unanswerable Questions\',74.295,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2018-10,BERT (single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',83.061,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',90.689,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2019-08,XLNet + SG-Net Verifier (ensemble) model in \'SG-Net: Syntax-Guided Machine Reading Comprehension\',90.702,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2019-09,ALBERT (ensemble model) model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',92.215,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2020-01,Retro-Reader (ensemble) model in \'Retrospective Reader for Machine Reading Comprehension\',92.978,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2017-11,FusionNet++ (ensemble) model in \'FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension\',70.3,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2017-12,SAN (ensemble model) model in \'Stochastic Answer Networks for Machine Reading Comprehension\',71.316,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2018-08,Reinforced Mnemonic Reader + Answer Verifier (single model) model in \'Read + Verify: Machine Reading Comprehension with Unanswerable Questions\',71.767,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2018-10,BERT (single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',80.005,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',87.926,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2019-08,XLNet + SG-Net Verifier (ensemble) model in \'SG-Net: Syntax-Guided Machine Reading Comprehension\',88.174,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2019-09,ALBERT (ensemble model) model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',89.731,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD2.0,2020-01,Retro-Reader (ensemble) model in \'Retrospective Reader for Machine Reading Comprehension\',90.578,EM,pos
Natural language generation,Question answering,Question answering,Question answering,ARC-e,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',79.6,Accuracy,pos
Natural language generation,Question answering,Question answering,Question answering,FriendsQA,2020-04,Li and Choi - RoBERTa model in \'Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering\',69.6,F1,pos
Natural language generation,Question answering,Question answering,Question answering,FriendsQA,2021-09,Ma et al. - ELECTRA model in \'Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension\',75.4,F1,pos
Natural language generation,Question answering,Question answering,Question answering,FriendsQA,2020-04,Li and Choi - RoBERTa model in \'Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering\',53.5,EM,pos
Natural language generation,Question answering,Question answering,Question answering,FriendsQA,2021-09,Ma et al. - ELECTRA model in \'Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension\',58.7,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD,2020-07,Fusion-in-Decoder (large) model in \'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\',56.7,F1,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD,2020-04,DPR model in \'Dense Passage Retrieval for Open-Domain Question Answering\',24.1,EM,pos
Natural language generation,Question answering,Question answering,Question answering,SQuAD,2021-06,RAG-end2end model in \'Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering\',40.02,Exact Match,pos
Natural language generation,Question answering,Question quality assessment,Question quality assessment,60k Stack Overflow Questions,2022-04,Multi-view approach model in \'Multi-View Approach to Suggest Moderation Actions in Community Question Answering Sites\',0.917,F1 Score,pos
Natural language generation,Question answering,Question-Answer categorization,Question-Answer categorization,QC-Science,2021-07,TagRec(BERT+Sent BERT) model in \'TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy\',0.93,R-at-10,pos
Natural language generation,Question answering,Question-Answer categorization,Question-Answer categorization,QC-Science,2021-07,TagRec(BERT+USE) model in \'TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy\',0.86,R-at-5,pos
Natural language generation,Question answering,Question-Answer categorization,Question-Answer categorization,QC-Science,2021-07,TagRec(BERT+Sent BERT) model in \'TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy\',0.97,R@20,pos
Natural language generation,Question answering,Question-Answer categorization,Question-Answer categorization,QC-Science,2021-07,TagRec(BERT+USE) model in \'TagRec: Automated Tagging of Questions with Hierarchical Learning Taxonomy\',0.95,R@15,pos
Natural language generation,Question answering,Temporal/Casual QA,Temporal/Casual QA,NExT-QA,2022-04,Flamingo model in \'Flamingo: a Visual Language Model for Few-Shot Learning\',33.5,WUPS,pos
Natural language generation,Question answering,Video question answering,Video question answering,Howto100M-QA,2020-05,Hero w/ pre-training model in \'HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\',77.75,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2015-05,VIS+LST model in \'Exploring Models and Data for Image Question Answering\',29.91,1/4,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2018-09,"TVQA model in \'TVQA: Localized, Compositional Video Question Answering\'",35.16,1/4,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2020-02,HCRN model in \'Hierarchical Conditional Relation Networks for Video Question Answering\',36.49,1/4,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2021-03,Eclipse model in \'SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events\',37.05,1/4,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2015-05,VIS+LST model in \'Exploring Models and Data for Image Question Answering\',54.25,1/2,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2018-09,"TVQA model in \'TVQA: Localized, Compositional Video Question Answering\'",63.15,1/2,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2020-02,HCRN model in \'Hierarchical Conditional Relation Networks for Video Question Answering\',63.79,1/2,pos
Natural language generation,Question answering,Video question answering,Video question answering,SUTD-TrafficQA,2021-03,Eclipse model in \'SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events\',64.77,1/2,pos
Natural language generation,Question answering,Video question answering,Video question answering,MSRVTT-QA,2022-06,Singularity-temporal model in \'Revealing Single Frame Bias for Video-and-Language Learning\',43.9,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,ActivityNet-QA,2019-06,E-SA model in \'ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering\',0.318,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,ActivityNet-QA,2020-12,Just Ask model in \'Just Ask: Learning to Answer Questions from Millions of Narrated Videos\',0.389,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,ActivityNet-QA,2022-06,Singularity-temporal model in \'Revealing Single Frame Bias for Video-and-Language Learning\',0.441,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,MSRVTT-MC,2022-06,Singularity-temporal model in \'Revealing Single Frame Bias for Video-and-Language Learning\',93.7,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,TVQA,2019-04,"STAGE (Lei et al., 2019) model in \'TVQA+: Spatio-Temporal Grounding for Video Question Answering\'",70.5,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,TVQA,2020-05,Hero w/ pre-training model in \'HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\',74.24,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,TVQA,2020-11,"iPerceive (Chadha et al., 2020) model in \'iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering\'",76.96,Accuracy,pos
Natural language generation,Question answering,Video question answering,Video question answering,iVQA,2020-12,Just Ask model in \'Just Ask: Learning to Answer Questions from Millions of Narrated Videos\',0.354,Accuracy,pos
Natural language generation,Question generation,Question generation,Question generation,SQuAD1.1,2017-04,NQG++ model in \'Neural Question Generation from Text: A Preliminary Study\',13.27,BLEU-4,pos
Natural language generation,Question generation,Question generation,Question generation,SQuAD1.1,2018-06,MPQG model in \'Leveraging Context Information for Natural Question Generation\',13.91,BLEU-4,pos
Natural language generation,Question generation,Question generation,Question generation,SQuAD1.1,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',22.78,BLEU-4,pos
Natural language generation,Question generation,Question generation,Question generation,SQuAD1.1,2020-01,ERNIE-GENLARGE (large-scale text corpora) model in \'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\',25.41,BLEU-4,pos
Natural language generation,Question generation,Question generation,Question generation,Natural Questions,2020-05,Info-HCVAE model in \'Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs\',37.18,QAE,pos
Natural language generation,Question generation,Question generation,Question generation,Natural Questions,2020-05,HCVAE model in \'Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs\',32.78,R-QAE,pos
Natural language generation,Question generation,Question generation,Question generation,FairytaleQA,2022-03,BART fine-tuned on FairytaleQA model in \'Fantastic Questions and Where to Find Them: FairytaleQA -- An Authentic Dataset for Narrative Comprehension\',0.527,ROUGE-L,pos
Natural language generation,Question generation,Question generation,Question generation,Visual Question Generation,2018-08,MDN model in \'Multimodal Differential Network for Visual Question Generation\',36.0,BLEU-1,pos
Natural language generation,Question generation,Question generation,Question generation,TriviaQA,2020-05,Info-HCVAE model in \'Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs\',35.45,QAE,pos
Natural language generation,Question generation,Question generation,Question generation,TriviaQA,2020-05,HCVAE model in \'Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs\',34.41,R-QAE,pos
Natural language generation,Question generation,Question generation,Question generation,SQuAD,2020-05,Info-HCVAE model in \'Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs\',71.18,QAE,pos
Natural language generation,Question generation,Question generation,Question generation,SQuAD,2020-05,Info-HCVAE model in \'Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs\',38.8,R-QAE,pos
Natural language generation,Question generation,Question generation,Question generation,COCO Visual Question Answering (VQA) real images 1.0 open ended,2014-12,coco-Caption [[Karpathy and Li2014]] model in \'Deep Visual-Semantic Alignments for Generating Image Descriptions\',62.5,BLEU-1,pos
Natural language generation,Question generation,Question generation,Question generation,COCO Visual Question Answering (VQA) real images 1.0 open ended,2018-08,MDN model in \'Multimodal Differential Network for Visual Question Generation\',65.1,BLEU-1,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-maxmin,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',90.46,Java,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-maxmin,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',82.2,Python,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-maxmin,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',90.79,Go,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-maxmin,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',86.84,Ruby,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-maxmin,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',86.4,JS,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-maxmin,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',88.21,PHP,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-all,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',80.63,Java,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-all,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',87.21,Python,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-all,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',83.31,Go,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-all,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',80.17,Ruby,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-all,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',81.77,JS,pos
Natural language generation,Sentence Completion,Cloze Test,Cloze Test,CodeXGLUE - CT-all,2021-02,CodeBERT(MLM) model in \'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\',85.05,PHP,pos
Natural language generation,Sentence Completion,Hurtful Sentence Completion,Hurtful Sentence Completion,HONEST,2021-06,BERT-large model in \'HONEST: Measuring Hurtful Sentence Completion in Language Models\',3.33,HONEST,pos
Natural language generation,Sentence Completion,Sentence Completion,Sentence Completion,HellaSwag,2020-05,GPT-3 175B (Few-Shot) model in \'Language Models are Few-Shot Learners\',79.3,Accuracy,pos
Natural language generation,Sentence Completion,Sentence Completion,Sentence Completion,HellaSwag,2021-01,MUPPET Roberta Large model in \'Muppet: Massive Multi-task Representations with Pre-Finetuning\',86.4,Accuracy,pos
Natural language generation,Task-oriented dialog systems,SSTOD,SSTOD,SSD_NAME,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',57.73,Dialogue Success Rate,pos
Natural language generation,Task-oriented dialog systems,SSTOD,SSTOD,SSD_NAME,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',84.96,Joint Acc,pos
Natural language generation,Task-oriented dialog systems,SSTOD,SSTOD,SSD_NAME,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',93.12,Slot Acc,pos
Natural language generation,Task-oriented dialog systems,SSTOD,SSTOD,automata,2022-03,UBAR+ model in \'A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\',45.8,Dialogue Success Rate,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2017-05,KV Retrieval Net model in \'Key-Value Retrieval Networks for Task-Oriented Dialogue\',48.0,Entity F1,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2018-06,DSR model in \'Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation\',51.9,Entity F1,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2019-01,GLMP model in \'Global-to-local Memory Pointer Networks for Task-Oriented Dialogue\',59.97,Entity F1,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-04,DF-Net model in \'Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog\',62.5,Entity F1,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-10,COMET model in \'Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems\',63.6,Entity F1,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2022-01,T5-3b(UnifiedSKG) model in \'UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\',70.07,Entity F1,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2017-05,KV Retrieval Net model in \'Key-Value Retrieval Networks for Task-Oriented Dialogue\',13.2,BLEU,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2019-01,GLMP model in \'Global-to-local Memory Pointer Networks for Task-Oriented Dialogue\',14.79,BLEU,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-04,DF-Net model in \'Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog\',15.2,BLEU,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,KVRET,2020-10,COMET model in \'Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems\',17.3,BLEU,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,SGD,2021-02,"T5 model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.331,METEOR,pos
Natural language generation,Task-oriented dialog systems,Task-oriented dialog systems,Task-oriented dialog systems,Kvret,2020-04,DF-Net model in \'Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog\',62.7,Entity F1,pos
Natural language generation,Text generation,Concept-to-text generation,Concept-to-text generation,COCO Captions,2019-07,tecpic model in \'Fake News Detection as Natural Language Inference\',2.0,BLEU-2,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Relation Generation),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',94.4,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Relation Generation),2022-02,SeqPlan model in \'Data-to-text Generation with Variational Sequential Planning\',95.9,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Relation Generation),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',30.8,count,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2019-10,tgen model in \'Neural Generation for Czech: Data and Baselines\',21.96,BLEU score,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2020-04,binmt model in \'Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech\',26.35,BLEU score,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2019-10,tgen model in \'Neural Generation for Czech: Data and Baselines\',4.77,NIST,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2020-04,binmt model in \'Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech\',5.24,NIST,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2019-10,tgen model in \'Neural Generation for Czech: Data and Baselines\',2.18,CIDER,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2020-04,binmt model in \'Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech\',2.6,CIDER,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2019-10,tgen model in \'Neural Generation for Czech: Data and Baselines\',23.32,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Czech Restaurant NLG,2020-04,binmt model in \'Machine Translation Pre-training for Data-to-Text Generation -- A Case Study in Czech\',25.81,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,ViGGO,2019-10,Bo3 model in \'ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation\',52.1,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,ViGGO,2020-04,DataTuner_FC model in \'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity\',53.6,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2017-12,Gong model in \'Technical Report for E2E NLG Challenge\',64.22,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-03,Zhang model in \'Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge\',65.45,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-04,"Sys1-Primary model in \'TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation\'",65.61,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-05,Slug model in \'A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation\',66.19,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2019-04,S_1^R model in \'Pragmatically Informative Text Generation\',68.6,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2017-12,Gong model in \'Technical Report for E2E NLG Challenge\',2.2721,CIDEr,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2019-04,S_1^R model in \'Pragmatically Informative Text Generation\',2.37,CIDEr,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2017-12,Gong model in \'Technical Report for E2E NLG Challenge\',8.3453,NIST,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-04,"Sys1-Primary model in \'TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation\'",8.5105,NIST,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-05,Slug model in \'A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation\',8.613,NIST,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2019-04,S_1^R model in \'Pragmatically Informative Text Generation\',8.73,NIST,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2017-12,Gong model in \'Technical Report for E2E NLG Challenge\',66.45,ROUGE-L,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-03,Zhang model in \'Attention Regularized Sequence-to-Sequence Learning for E2E NLG Challenge\',70.83,ROUGE-L,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2017-12,Gong model in \'Technical Report for E2E NLG Challenge\',44.69,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-04,"Sys1-Primary model in \'TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation\'",45.17,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,E2E NLG Challenge,2018-11,TUDA model in \'E2E NLG Challenge: Neural Models vs. Templates\',45.29,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Rotowire (Content Selection),2017-07,Encoder-decoder + conditional copy model in \'Challenges in Data-to-Document Generation\',29.49,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Rotowire (Content Selection),2018-09,Neural Content Planning + conditional copy model in \'Data-to-Text Generation with Content Selection and Planning\',34.18,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Rotowire (Content Selection),2019-12,Hierarchical Transformer Encoder + conditional copy model in \'A Hierarchical Model for Data-to-Text Generation\',39.47,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Rotowire (Content Selection),2017-07,Encoder-decoder + conditional copy model in \'Challenges in Data-to-Document Generation\',36.18,Recall,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Rotowire (Content Selection),2018-09,Neural Content Planning + conditional copy model in \'Data-to-Text Generation with Content Selection and Planning\',51.22,Recall,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Rotowire (Content Selection),2019-12,Hierarchical Transformer Encoder + conditional copy model in \'A Hierarchical Model for Data-to-Text Generation\',51.64,Recall,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Rotowire (Content Selection),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',57.8,Recall,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2017-07,Encoder-decoder + conditional copy model in \'Challenges in Data-to-Document Generation\',74.8,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2018-09,Neural Content Planning + conditional copy model in \'Data-to-Text Generation with Content Selection and Planning\',87.47,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2019-12,Hierarchical Transformer Encoder +  conditional copy model in \'A Hierarchical Model for Data-to-Text Generation\',89.46,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',97.6,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2017-07,Encoder-decoder + conditional copy model in \'Challenges in Data-to-Document Generation\',23.72,count,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2018-09,Neural Content Planning + conditional copy model in \'Data-to-Text Generation with Content Selection and Planning\',34.28,count,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',42.1,count,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Relation Generation),2022-02,SeqPlan model in \'Data-to-text Generation with Variational Sequential Planning\',46.7,count,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,SR11Deep,2018-10,GCN + feat model in \'Deep Graph Convolutional Encoders for Structured Data to Text Generation\',0.666,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,SR11Deep,2019-11,Transition based Deep Input Linearization model in \'Transition-Based Deep Input Linearization\',80.49,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,AMR3.0,2021-03,StructAdapt model in \'Structural Adapters in Pretrained Language Models for AMR-to-text Generation\',48.0,Bleu,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG Full,2019-08,Transformer (Pipeline) model in \'Neural data-to-text generation: A comparison between pipeline and end-to-end architectures\',51.68,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG Full,2020-04,DATATUNER_NO_FC model in \'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity\',52.9,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG Full,2020-05,T5-Large model in \'Text-to-Text Pre-Training for Data-to-Text Tasks\',57.1,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG Full,2020-07,T5-large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',59.7,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG Full,2021-05,T5-large + Wiki + Position model in \'Stage-wise Fine-tuning for Graph-to-Text Generation\',60.56,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG Full,2021-10,"Control Prefixes (A1, A2, T5-large) model in \'Control Prefixes for Parameter-Efficient Text Generation\'",62.27,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Content Ordering),2017-07,Encoder-decoder + conditional copy model in \'Challenges in Data-to-Document Generation\',14.49,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Content Ordering),2018-09,Neural Content Planning + conditional copy model in \'Data-to-Text Generation with Content Selection and Planning\',16.5,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Content Ordering),2019-12,Hierarchical Transformer Encoder + conditional copy model in \'A Hierarchical Model for Data-to-Text Generation\',17.5,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Content Ordering),2017-07,Encoder-decoder + conditional copy model in \'Challenges in Data-to-Document Generation\',8.68,DLD,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Content Ordering),2018-09,Neural Content Planning + conditional copy model in \'Data-to-Text Generation with Content Selection and Planning\',18.58,DLD,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire (Content Ordering),2019-12,Hierarchical Transformer Encoder + conditional copy model in \'A Hierarchical Model for Data-to-Text Generation\',18.9,DLD,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Content Ordering),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',21.8,DLD,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Content Ordering),2022-02,SeqPlan model in \'Data-to-text Generation with Variational Sequential Planning\',22.7,DLD,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG ru,2021-02,"mBART model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.613,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Content Selection),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',40.8,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Content Selection),2021-12,Force-Copy model in \'May the Force Be with Your Copy Mechanism: Enhanced Supervised-Copy Method for Natural Language Generation\',49.39,Precision,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset (Content Selection),2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',54.9,Recall,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG,2018-07,GTR-LSTM (entity masking) model in \'GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data\',58.6,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG,2020-01,CGE-LW (Levi Graph) model in \'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs\',63.69,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG,2020-05,T5-Base model in \'Text-to-Text Pre-Training for Data-to-Text Tasks\',64.7,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG,2020-07,T5-small model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',65.05,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG,2021-05,T5-large + Wiki + Position model in \'Stage-wise Fine-tuning for Graph-to-Text Generation\',66.07,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG,2021-10,"Control Prefixes (A1, T5-large) model in \'Control Prefixes for Parameter-Efficient Text Generation\'",67.32,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire,2017-07,Encoder-decoder + conditional copy model in \'Challenges in Data-to-Document Generation\',14.19,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire,2018-09,Neural Content Planning + conditional copy model in \'Data-to-Text Generation with Content Selection and Planning\',16.5,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire,2019-12,Hierarchical transformer encoder + conditional copy model in \'A Hierarchical Model for Data-to-Text Generation\',17.5,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,RotoWire,2021-08,HierarchicalEncoder + NR + IR model in \'Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation\',17.96,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MULTIWOZ 2.1,2019-05,HDSA model in \'Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention\',26.48,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MULTIWOZ 2.1,2020-02,SC-GPT2 model in \'Few-shot Natural Language Generation for Task-Oriented Dialog\',30.76,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MULTIWOZ 2.1,2020-04,T5-small model in \'Template Guided Text Generation for Task-Oriented Dialogue\',34.96,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MULTIWOZ 2.1,2020-05,T5-Base model in \'Text-to-Text Pre-Training for Data-to-Text Tasks\',35.1,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset,2019-06,ENT model in \'Data-to-text Generation with Entity Modeling\',11.5,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset,2021-02,Macro model in \'Data-to-text Generation with Macro Planning\',12.62,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,MLB Dataset,2022-02,SeqPlan model in \'Data-to-text Generation with Variational Sequential Planning\',14.29,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Cleaned E2E NLG Challenge,2019-11,TGen model in \'Semantic Noise Matters for Neural Natural Language Generation\',40.73,BLEU (Test set),pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Cleaned E2E NLG Challenge,2020-04,DataTuner_FC model in \'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity\',43.6,BLEU (Test set),pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Cleaned E2E NLG Challenge,2021-10,Control Prefixes (T5-large) model in \'Control Prefixes for Parameter-Efficient Text Generation\',44.15,BLEU (Test set),pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Cleaned E2E NLG Challenge,2021-02,"LSTM model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.394,METEOR (Validation set),pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,Wikipedia Person and Animal Dataset,2020-05,Ours model in \'Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints\',24.56,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,WebNLG en,2021-02,"mBART model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.462,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,ToTTo,2020-04,BERT-to-BERT model in \'ToTTo: A Controlled Table-To-Text Generation Dataset\',44.0,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,ToTTo,2020-05,T5-3B model in \'Text-to-Text Pre-Training for Data-to-Text Tasks\',49.5,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,ToTTo,2020-04,BERT-to-BERT model in \'ToTTo: A Controlled Table-To-Text Generation Dataset\',52.6,PARENT,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,ToTTo,2020-05,T5-3B model in \'Text-to-Text Pre-Training for Data-to-Text Tasks\',58.4,PARENT,pos
Natural language generation,Text generation,Data-to-text generation,Data-to-text generation,ToTTo,2021-02,"T5 model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.363,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,PathQuestion,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',61.48,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,PathQuestion,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',65.89,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,PathQuestion,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',77.72,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,PathQuestion,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',78.87,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,PathQuestion,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',44.57,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,PathQuestion,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',48.25,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Constrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',48.0,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Constrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',61.01,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Constrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',65.0,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Constrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',73.57,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Constrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',36.0,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Constrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',46.32,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WikiGraphs,2021-07,Unconditional model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',-25.85,Test perplexity,neg
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',26.22,rBLEU (Test),pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',31.39,rBLEU (Valid),pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',28.35,rBLEU(w/title)(Test),pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',32.65,rBLEU(w/title)(Valid),pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (Seen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',64.71,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (Seen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',45.85,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (Seen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',78.29,chrF++,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (All),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',59.7,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (All),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',44.18,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (All),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',75.4,chrF++,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,ENT-DESC,2020-04,MGCN+sum model in \'ENT-DESC: Entity Description Generation by Exploring Knowledge Graph\',26.4,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (Unseen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',53.67,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (Unseen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',42.26,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG (Unseen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',72.25,chrF++,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',61.0,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2020-10,KGPT model in \'KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation\',64.11,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',66.14,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',71.0,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2020-10,KGPT model in \'KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation\',74.57,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',76.1,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',42.0,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2020-10,KGPT model in \'KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation\',46.3,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebNLG 2.0 (Unconstrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',47.25,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebQuestions,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',29.45,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebQuestions,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',30.02,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebQuestions,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',55.45,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebQuestions,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',55.6,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebQuestions,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',30.96,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,WebQuestions,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',32.05,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,AGENDA,2019-04,GraphWriter model in \'Text Generation from Knowledge Graphs with Graph Transformers\',14.3,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,AGENDA,2020-01,CGE-LW model in \'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs\',18.01,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,KG-to-text generation,AGENDA,2020-07,BART-large+ STA model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',25.66,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,WebNLG v2.1,2019-04,GT-BT (sampled noise) model in \'An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing\',37.7,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Fine),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',41.59,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Fine),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',3.57,CIDEr,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Fine),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',63.31,ROUGE-L,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Fine),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',35.72,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Full),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',41.29,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Full),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',3.53,CIDEr,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Full),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',63.73,ROUGE-L,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,GenWiki (Full),2020-12,CycleGT_Base model in \'GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\',35.39,METEOR,pos
Natural language generation,Text generation,Data-to-text generation,Unsupervised KG-to-Text Generation,VG graph-text,2019-04,GT-BT (composed noise) model in \'An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing\',23.2,BLEU,pos
Natural language generation,Text generation,Data-to-text generation,Visual storytelling,VIST,2018-04,AREL-t-100 model in \'No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\',63.8,BLEU-1,pos
Natural language generation,Text generation,Data-to-text generation,Visual storytelling,VIST,2018-04,AREL-t-100 model in \'No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\',14.1,BLEU-4,pos
Natural language generation,Text generation,Data-to-text generation,Visual storytelling,VIST,2018-04,AREL-t-100 model in \'No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\',39.1,BLEU-2,pos
Natural language generation,Text generation,Data-to-text generation,Visual storytelling,VIST,2018-04,AREL-t-100 model in \'No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\',23.2,BLEU-3,pos
Natural language generation,Text generation,Data-to-text generation,Visual storytelling,VIST,2018-04,AREL-t-100 model in \'No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\',9.4,CIDEr,pos
Natural language generation,Text generation,Data-to-text generation,Visual storytelling,VIST,2018-04,AREL-t-100 model in \'No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\',29.5,ROUGE,pos
Natural language generation,Text generation,Data-to-text generation,Visual storytelling,VIST,2018-04,AREL-t-100 model in \'No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\',35.0,METEOR,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Amazon-5,2017-01,mm model in \'Adversarial Learning for Neural Dialogue Generation\',5.0,1 in 10 R-at-2,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',12.17,BLEU,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Two-in-one model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',-10.49,PPL,neg
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',60.9,Success,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.51,Specificity,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.973,Slot Accuracy,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.6,Joint SA,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',75.1,Inform,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',90.8,Inform_mct,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',74.4,Success_mct,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.58,Sensibleness,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,FusedChat,2021-09,Classification-based model model in \'Fusing task-oriented and open-domain dialogues in conversational agents\',0.55,SSA,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,WikiText-103,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',-32.48,Perplexity,neg
Natural language generation,Text generation,Dialog generation,Dialog generation,Reddit (multi-ref),2019-02,SpaceFusion model in \'Jointly Optimizing Diversity and Relevance in Neural Response Generation\',2.53,interest (human),pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Reddit (multi-ref),2019-02,SpaceFusion model in \'Jointly Optimizing Diversity and Relevance in Neural Response Generation\',2.72,relevance (human),pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Twitter Dialogue (Tense),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',34.48,Accuracy,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Twitter Dialogue (Noun),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',4.63,F1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Twitter Dialogue (Noun),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',4.82,Precision,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Twitter Dialogue (Noun),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',5.22,Recall,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Persona-Chat,2014-09,Seq2Seq + Attention model in \'Neural Machine Translation by Jointly Learning to Align and Translate\',16.18,Avg F1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Persona-Chat,2019-01,TransferTransfo model in \'TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents\',19.09,Avg F1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Persona-Chat,2020-04,P^2 Bot model in \'You Impress Me: Dialogue Generation via Mutual Persona Perception\',19.77,Avg F1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',14.7,BLEU-1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',14.79,ROUGE-L,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',19.09,CIDr,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Persona-Chat,2020-05,Synthesizer (R+V) model in \'Synthesizer: Rethinking Self-Attention in Transformer Models\',6.39,METEOR,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Tense),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',29.01,Accuracy,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Entity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',3.72,F1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Entity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',4.91,Precision,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Entity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',3.36,Recall,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Activity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',11.43,F1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Activity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',16.84,Precision,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Activity),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',9.72,Recall,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',9.01,F1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',12.56,Rouge-L,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',15.37,ROUGE-1,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,CMU-DoG,2021-09,[?]-former (SM) model in \'$\\infty$-former: Infinite Memory Transformer\',7.55,Meteor,pos
Natural language generation,Text generation,Dialog generation,Dialog generation,Ubuntu Dialogue (Cmd),2016-06,MrRNN Act.-Ent. model in \'Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation\',95.04,Accuracy,pos
Natural language generation,Text generation,Dialog generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",1.99,BLEU,pos
Natural language generation,Text generation,Dialog generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.0056,Dis-1,pos
Natural language generation,Text generation,Dialog generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.0431,Dis-2,pos
Natural language generation,Text generation,Dialog generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.125,Dis-3,pos
Natural language generation,Text generation,Dialog generation,Multi-modal Dialogue Generation,OpenViDial 2.0,2021-09,"FV (w/o MI) model in \'OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts\'",0.2215,Dis-4,pos
Natural language generation,Text generation,Dialog generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.1403,BLEU,pos
Natural language generation,Text generation,Dialog generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',-17.48,PPL,neg
Natural language generation,Text generation,Dialog generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.0614,Distinct-1,pos
Natural language generation,Text generation,Dialog generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.343,Distinct-2,pos
Natural language generation,Text generation,Dialog generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo+da} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',5012.0,Greedy Embedding,pos
Natural language generation,Text generation,Dialog generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.5617,Average Embedding,pos
Natural language generation,Text generation,Dialog generation,Personalized and Emotional Conversation,CPED,2022-05,GPT-{per+emo+da} model in \'CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\',0.5722,bertscore,pos
Natural language generation,Text generation,Distractor generation,Distractor generation,RACE,2020-10,BDG p.m. model in \'A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies\',39.81,BLEU-1,pos
Natural language generation,Text generation,Distractor generation,Distractor generation,RACE,2020-10,BDG p.m. model in \'A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies\',13.56,BLEU-4,pos
Natural language generation,Text generation,Distractor generation,Distractor generation,RACE,2020-10,BDG p.m. model in \'A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies\',24.81,BLEU-2,pos
Natural language generation,Text generation,Distractor generation,Distractor generation,RACE,2020-10,BDG p.m. model in \'A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies\',17.66,BLEU-3,pos
Natural language generation,Text generation,Distractor generation,Distractor generation,RACE,2020-10,BDG p.m. model in \'A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies\',34.01,ROUGE-L,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',93.17,F1,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,Table-to-Text model in \'Fact-based Text Editing\',98.23,Precision,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',89.74,Recall,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',75.68,BLEU,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',24.8,Exact Match,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',47.69,ADD,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',0.7707,DELETE,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',0.9184,KEEP,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,WebEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',72.2,SARI,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',63.39,F1,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',78.84,Precision,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',52.3,Recall,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',84.43,BLEU,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',2.65,Exact Match,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',41.5,ADD,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',84.24,DELETE,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',98.41,KEEP,pos
Natural language generation,Text generation,Fact-based text editing,Fact-based text editing,RotoEdit,2020-07,FactEditor model in \'Fact-based Text Editing\',74.72,SARI,pos
Natural language generation,Text generation,Graph-to-sequence,Graph-to-sequence,LDC2015E86:,2018-05,GRN model in \'A Graph-to-Sequence Model for AMR-to-Text Generation\',33.6,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,Graph-to-sequence,WebNLG,2020-01,CGE-LW model in \'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs\',63.69,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,PathQuestion,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',61.48,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,PathQuestion,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',65.89,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,PathQuestion,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',77.72,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,PathQuestion,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',78.87,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,PathQuestion,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',44.57,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,PathQuestion,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',48.25,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Constrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',48.0,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Constrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',61.01,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Constrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',65.0,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Constrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',73.57,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Constrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',36.0,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Constrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',46.32,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WikiGraphs,2021-07,Unconditional model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',-25.85,Test perplexity,neg
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',26.22,rBLEU (Test),pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',31.39,rBLEU (Valid),pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',28.35,rBLEU(w/title)(Test),pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WikiGraphs,2021-07,GNN model in \'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset\',32.65,rBLEU(w/title)(Valid),pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (Seen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',64.71,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (Seen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',45.85,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (Seen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',78.29,chrF++,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (All),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',59.7,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (All),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',44.18,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (All),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',75.4,chrF++,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,ENT-DESC,2020-04,MGCN+sum model in \'ENT-DESC: Entity Description Generation by Exploring Knowledge Graph\',26.4,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (Unseen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',53.67,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (Unseen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',42.26,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG (Unseen),2020-07,T5_large model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',72.25,chrF++,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',61.0,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2020-10,KGPT model in \'KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation\',64.11,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',66.14,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',71.0,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2020-10,KGPT model in \'KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation\',74.57,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',76.1,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2018-11,SOTA-NPT  model in \'Handling Rare Items in Data-to-Text Generation\',42.0,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2020-10,KGPT model in \'KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation\',46.3,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebNLG 2.0 (Unconstrained),2021-06,JointGT (T5) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',47.25,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebQuestions,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',29.45,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebQuestions,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',30.02,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebQuestions,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',55.45,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebQuestions,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',55.6,ROUGE,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebQuestions,2020-04,SOTA-NPT model in \'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks\',30.96,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,WebQuestions,2021-06,JointGT (BART) model in \'JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs\',32.05,METEOR,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,AGENDA,2019-04,GraphWriter model in \'Text Generation from Knowledge Graphs with Graph Transformers\',14.3,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,AGENDA,2020-01,CGE-LW model in \'Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs\',18.01,BLEU,pos
Natural language generation,Text generation,Graph-to-sequence,KG-to-text generation,AGENDA,2020-07,BART-large+ STA model in \'Investigating Pretrained Language Models for Graph-to-Text Generation\',25.66,BLEU,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,DUC 2004,2017-06,GCN: Personalized Discourse Graph model in \'Graph-based Neural Multi-Document Summarization\',38.23,ROUGE-1,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,WCEP,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',37.9,ROUGE-L,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,WCEP,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',46.1,ROUGE-1,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,WCEP,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',25.2,ROUGE-2,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',25.9,ROUGE-L,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2018-08,CopyTransformer model in \'Bottom-Up Abstractive Summarization\',43.57,ROUGE-1,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2021-07,CTF+DPP model in \'Multi-Document Summarization withDeterminantal Point Process Attention\',45.84,ROUGE-1,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',49.9,ROUGE-1,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2018-08,PG-BRNN model in \'Bottom-Up Abstractive Summarization\',14.19,ROUGE-2,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2019-06,Hi-MAP model in \'Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\',14.89,ROUGE-2,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2021-07,CTF+DPP model in \'Multi-Document Summarization withDeterminantal Point Process Attention\',15.94,ROUGE-2,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',21.1,ROUGE-2,pos
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,Multi-News,2018-08,PG-BRNN model in \'Bottom-Up Abstractive Summarization\',-16.75,ROUGE-SU4,neg
Natural language generation,Text generation,Multi-document summarization,Multi-document summarization,review,2018-12,solar model in \'Solar Cell Surface Defect Inspection Based on Multispectral Convolutional Neural Network\',100.0,1-of-100 Accuracy,pos
Natural language generation,Text generation,Natural Language Landmark Navigation Instructions Generation,Natural Language Landmark Navigation Instructions Generation,map2seq,2020-12,graph2text+pretrain model in \'Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem\',66.4,SNT,pos
Natural language generation,Text generation,Paper generation,Paper generation,ACL Title and Abstract Dataset,2018-05,Writing-editing Network model in \'Paper Abstract Writing through Editing Mechanism\',20.3,ROUGE-L,pos
Natural language generation,Text generation,Paper generation,Paper generation,ACL Title and Abstract Dataset,2018-05,Writing-editing Network model in \'Paper Abstract Writing through Editing Mechanism\',14.0,METEOR,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,Quora Question Pairs,2022-03,HRQ-VAE model in \'Hierarchical Sketch Induction for Paraphrase Generation\',33.11,BLEU,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,Quora Question Pairs,2021-05,Separator model in \'Factorising Meaning and Form for Intent-Preserving Paraphrasing\',5.84,iBLEU,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,Quora Question Pairs,2022-03,HRQ-VAE model in \'Hierarchical Sketch Induction for Paraphrase Generation\',18.42,iBLEU,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,Paralex,2022-03,HRQ-VAE model in \'Hierarchical Sketch Induction for Paraphrase Generation\',39.49,BLEU,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,Paralex,2021-05,Separator model in \'Factorising Meaning and Form for Intent-Preserving Paraphrasing\',14.84,iBLEU,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,Paralex,2022-03,HRQ-VAE model in \'Hierarchical Sketch Induction for Paraphrase Generation\',24.93,iBLEU,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,MSCOCO,2022-03,HRQ-VAE model in \'Hierarchical Sketch Induction for Paraphrase Generation\',27.9,BLEU,pos
Natural language generation,Text generation,Paraphrase generation,Paraphrase generation,MSCOCO,2022-03,HRQ-VAE model in \'Hierarchical Sketch Induction for Paraphrase Generation\',19.04,iBLEU,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',28.1,BLEU,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',-18.3,Perplexity,neg
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',66.2,ROUGE-L,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',67.0,ROUGE-1,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',40.9,ROUGE-2,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',30.9,BLEU,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',-15.7,Perplexity,neg
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',67.5,ROUGE-L,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',68.3,ROUGE-1,pos
Natural language generation,Text generation,Story Generation,Story Generation,TVMegaSite dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',44.0,ROUGE-2,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',28.4,BLEU,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',-18.2,Perplexity,neg
Natural language generation,Text generation,Story Generation,Story Generation,Fandom test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',61.5,ROUGE-L,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',63.2,ROUGE-1,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom test,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',32.9,ROUGE-2,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',28.4,BLEU,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',-17.9,Perplexity,neg
Natural language generation,Text generation,Story Generation,Story Generation,Fandom dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',61.2,ROUGE-L,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',63.0,ROUGE-1,pos
Natural language generation,Text generation,Story Generation,Story Generation,Fandom dev,2021-09,(NN) Oracle plot + summary + oracle char. desc. model in \'TVRecap: A Dataset for Generating Stories with Character Descriptions\',32.8,ROUGE-2,pos
Natural language generation,Text generation,Table-to-text generation,KB-to-language generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model model in \'Describing a Knowledge Base\',23.2,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,KB-to-language generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model model in \'Describing a Knowledge Base\',42.0,ROUGE,pos
Natural language generation,Text generation,Table-to-text generation,KB-to-language generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model model in \'Describing a Knowledge Base\',23.4,METEOR,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (Unseen),2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',48.4,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (Unseen),2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.39,METEOR,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (Unseen),2021-07,GPT-2-Large (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.53,TER,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WikiBio,2016-03,Table NLM model in \'Neural Text Generation from Structured Data with Application to the Biography Domain\',34.7,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WikiBio,2017-11,Field-gating Seq2seq + dual attention model in \'Table-to-text Generation by Structure-aware Seq2seq Learning\',44.89,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WikiBio,2016-03,Table NLM model in \'Neural Text Generation from Structured Data with Application to the Biography Domain\',25.8,ROUGE,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WikiBio,2017-11,Field-gating Seq2seq + dual attention + beam search model in \'Table-to-text Generation by Structure-aware Seq2seq Learning\',41.65,ROUGE,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WikiBio,2021-02,MBD model in \'Controlling Hallucinations at Word Level in Data-to-Text Generation\',56.16,PARENT,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (Seen),2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',65.4,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (Seen),2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.46,METEOR,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (Seen),2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.33,TER,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,RotoWire,2021-08,HierarchicalEncoder + NR + IR model in \'Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation\',17.96,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,RotoWire,2021-08,HierarchicalEncoder + NR + IR model in \'Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation\',25.3,Content Ordering,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,RotoWire,2021-08,HierarchicalEncoder + NR + IR model in \'Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation\',55.88,Content Selection (F1),pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,E2E,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',70.3,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,E2E,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',2.47,CIDEr,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,E2E,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',8.9,NIST,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,E2E,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',70.8,ROUGE-L,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,E2E,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',46.3,METEOR,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (All),2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',55.6,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (All),2021-07,GPT-2-Large (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.42,METEOR,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,WebNLG (All),2021-07,GPT-2-Large (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.42,TER,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model model in \'Describing a Knowledge Base\',23.2,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,Wikipedia Person and Animal Dataset,2020-02,VTM model in \'Variational Template Machine for Data-to-Text Generation\',25.22,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model model in \'Describing a Knowledge Base\',23.4,ROUGE,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,Wikipedia Person and Animal Dataset,2020-02,VTM model in \'Variational Template Machine for Data-to-Text Generation\',45.36,ROUGE,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model model in \'Describing a Knowledge Base\',42.0,METEOR,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,DART,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',47.2,BLEU,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,DART,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.39,METEOR,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,DART,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.4,BLEURT,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,DART,2021-07,GPT-2-Large (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.46,TER,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,DART,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.94,BERT,pos
Natural language generation,Text generation,Table-to-text generation,Table-to-text generation,DART,2021-07,HTLM (fine-tuning) model in \'HTLM: Hyper-Text Pre-Training and Prompting of Language Models\',0.51,Mover,pos
Natural language generation,Text generation,Text generation,Text generation,CNN/Daily Mail,2020-04,PALM model in \'PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation\',41.41,ROUGE-L,pos
Natural language generation,Text generation,Text generation,Text generation,CMU-SE,2018-08,STWGAN-GP model in \'Generating Text through Adversarial Training using Skip-Thought Vectors\',0.617,BLEU-3,pos
Natural language generation,Text generation,Text generation,Text generation,CommonGen,2019-11,UniLM model in \'CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\',14.92,CIDEr,pos
Natural language generation,Text generation,Text generation,Text generation,CommonGen,2021-02,"BART model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.301,METEOR,pos
Natural language generation,Text generation,Text generation,Text generation,One Billion Word,2020-12,WGANGP + DGflow model in \'Refining Deep Generative Models via Discriminator Gradient Flow\',0.186,JS-4,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.521,BLEU-4,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-05,RankGAN model in \'Adversarial Ranking for Language Generation\',0.557,BLEU-4,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.778,BLEU-4,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.831,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-05,RankGAN model in \'Adversarial Ranking for Language Generation\',0.85,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.95,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.642,BLEU-3,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-05,RankGAN model in \'Adversarial Ranking for Language Generation\',0.672,BLEU-3,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.88,BLEU-3,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.427,BLEU-5,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-05,RankGAN model in \'Adversarial Ranking for Language Generation\',0.544,BLEU-5,pos
Natural language generation,Text generation,Text generation,Text generation,COCO Captions,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.686,BLEU-5,pos
Natural language generation,Text generation,Text generation,Text generation,Chinese Poems,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.738,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,Chinese Poems,2017-05,RankGAN model in \'Adversarial Ranking for Language Generation\',0.812,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,LDC2016E25,2018-05,Graph2Seq model in \'A Graph-to-Sequence Model for AMR-to-Text Generation\',22.0,BLEU,pos
Natural language generation,Text generation,Text generation,Text generation,DART,2021-02,"T5 model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.115,METEOR,pos
Natural language generation,Text generation,Text generation,Text generation,DART,2021-10,Control Prefixes (T5-large) model in \'Control Prefixes for Parameter-Efficient Text Generation\',0.411,METEOR,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.4541,BLEU-4,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.627,BLEU-4,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.859,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.956,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.6015,BLEU-3,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.819,BLEU-3,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2016-09,SeqGAN model in \'SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\',0.4498,BLEU-5,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2017-05,RankGAN model in \'Adversarial Ranking for Language Generation\',0.463,BLEU-5,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2017-09,LeakGAN model in \'Long Text Generation via Adversarial Training with Leaked Information\',0.498,BLEU-5,pos
Natural language generation,Text generation,Text generation,Text generation,EMNLP2017 WMT,2020-06,PPOGAN model in \'Improving GAN Training with Probability Ratio Clipping and Sample Reweighting\',2.265,NLLgen,pos
Natural language generation,Text generation,Text generation,Text generation,Czech restaurant information,2021-02,"TGen++ model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.167,METEOR,pos
Natural language generation,Text generation,Text generation,Text generation,DailyDialog,2018-08,AEM+Attention model in \'An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation\',14.17,BLEU-1,pos
Natural language generation,Text generation,Text generation,Text generation,DailyDialog,2018-08,AEM+Attention model in \'An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation\',2.84,BLEU-4,pos
Natural language generation,Text generation,Text generation,Text generation,DailyDialog,2018-08,AEM+Attention model in \'An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation\',5.69,BLEU-2,pos
Natural language generation,Text generation,Text generation,Text generation,DailyDialog,2018-08,AEM+Attention model in \'An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation\',3.78,BLEU-3,pos
Natural language generation,Text generation,Text generation,Text generation,Yahoo Questions,2017-02,CNN-VAE model in \'Improved Variational Autoencoders for Text Modeling using Dilated Convolutions\',10.0,KL,pos
Natural language generation,Text generation,Text generation,Text generation,Yahoo Questions,2017-02,CNN-VAE model in \'Improved Variational Autoencoders for Text Modeling using Dilated Convolutions\',332.1,NLL,pos
Natural language generation,Text generation,Text generation,Text generation,Yahoo Questions,2017-02,CNN-VAE model in \'Improved Variational Autoencoders for Text Modeling using Dilated Convolutions\',-63.9,Perplexity,neg
Natural language generation,Text generation,Text generation,Text generation,Yahoo Questions,2018-02,SA-VAE model in \'Semi-Amortized Variational Autoencoders\',-60.4,Perplexity,neg
Natural language generation,Text generation,Text generation,Text generation,Yahoo Questions,2019-01,Aggressive VAE model in \'Lagging Inference Networks and Posterior Collapse in Variational Autoencoders\',-59.7,Perplexity,neg
Natural language generation,Text generation,Text style transfer,Text style transfer,Yelp Review Dataset (Small),2017-05,CAE model in \'Style Transfer from Non-Parallel Text by Cross-Alignment\',38.66,"G-Score (BLEU, Accuracy)",pos
Natural language generation,Text generation,Text style transfer,Text style transfer,Yelp Review Dataset (Small),2017-11,MultiDecoder model in \'Style Transfer in Text: Exploration and Evaluation\',45.02,"G-Score (BLEU, Accuracy)",pos
Natural language generation,Text generation,Text style transfer,Text style transfer,Yelp Review Dataset (Small),2018-04,"DeleteAndRetrieve model in \'Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer\'",54.64,"G-Score (BLEU, Accuracy)",pos
Natural language generation,Text generation,Text style transfer,Text style transfer,Yelp Review Dataset (Small),2019-08,"SAE+Discriminator model in \'Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites\'",74.56,"G-Score (BLEU, Accuracy)",pos
Natural language generation,Text generation,Text style transfer,Text style transfer,Yelp Review Dataset (Large),2021-05,StyleEmb model in \'A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations\',30.0,BLEU,pos
Natural language generation,Text generation,Text style transfer,Text style transfer,Yelp Review Dataset (Large),2020-04,SentiInc model in \'SentiInc: Incorporating Sentiment Information into Sentiment Transfer Without Parallel Data\',59.17,"G-Score (BLEU, Accuracy)",pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2020-05,MUSS (BART+ACCESS Supervised) model in \'MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases\',72.98,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2012-07,PBMT-R model in \'Sentence Simplification by Monolingual Machine Translation\',34.63,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2017-03,Dress-LS model in \'Sentence Simplification with Deep Reinforcement Learning\',36.59,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2018-10,DMASS-DCSS model in \'Integrating Transformer and Paraphrase Rules for Sentence Simplification\',38.67,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2019-10,ACCESS model in \'Controllable Sentence Simplification\',40.13,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2020-05,MUSS (BART+ACCESS Supervised) model in \'MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases\',44.15,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2021-02,"T5 model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.581,METEOR,pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2020-05,MUSS (BART+ACCESS Unsupervised) model in \'MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases\',8.23,FKGL,pos
Natural language generation,Text simplification,Text simplification,Text simplification,ASSET,2021-10,Control Prefixes (BART) model in \'Control Prefixes for Parameter-Efficient Text Generation\',0.64,"QuestEval (Reference-less, BERTScore)",pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2017-03,Dress-LS model in \'Sentence Simplification with Deep Reinforcement Learning\',80.12,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2017-07,NTS-SARI model in \'Exploring Neural Text Simplification Models\',80.69,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2018-04,NSELSTM-B model in \'Sentence Simplification with Memory-Augmented Neural Networks\',92.02,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2012-07,PBMT-R model in \'Sentence Simplification by Monolingual Machine Translation\',38.04,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2016-01,SBMT-SARI model in \'Optimizing Statistical Machine Translation for Text Simplification\',39.56,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2018-10,DMASS-DCSS model in \'Integrating Transformer and Paraphrase Rules for Sentence Simplification\',40.45,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2019-10,ACCESS model in \'Controllable Sentence Simplification\',41.38,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2020-05,MUSS (BART+ACCESS Supervised) model in \'MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases\',42.53,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2021-02,"T5 model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.649,METEOR,pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2020-05,MUSS (BART+ACCESS Unsupervised) model in \'MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases\',8.79,FKGL,pos
Natural language generation,Text simplification,Text simplification,Text simplification,TurkCorpus,2021-10,Control Prefixes (BART) model in \'Control Prefixes for Parameter-Efficient Text Generation\',0.66,"QuestEval (Reference-less, BERTScore)",pos
Natural language generation,Text simplification,Text simplification,Text simplification,Newsela,2017-03,DRESS-LS model in \'Sentence Simplification with Deep Reinforcement Learning\',24.3,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,Newsela,2018-04,NSELSTM-B  model in \'Sentence Simplification with Memory-Augmented Neural Networks\',26.31,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,Newsela,2017-03,DRESS model in \'Sentence Simplification with Deep Reinforcement Learning\',27.37,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,Newsela,2018-04,NSELSTM-S  model in \'Sentence Simplification with Memory-Augmented Neural Networks\',29.58,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,Newsela,2018-06,Pointer + Multi-task Entailment and Paraphrase Generation model in \'Dynamic Multi-Level Multi-Task Learning for Sentence Simplification\',33.22,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,Newsela,2020-05,CRF Alignment + Transformer model in \'Neural CRF Model for Sentence Alignment in Text Simplification\',36.6,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,PWKP / WikiSmall,2015-07,UNSUP model in \'Unsupervised Sentence Simplification Using Deep Semantics\',38.47,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,PWKP / WikiSmall,2018-04,NSELSTM-B model in \'Sentence Simplification with Memory-Augmented Neural Networks\',53.42,BLEU,pos
Natural language generation,Text simplification,Text simplification,Text simplification,PWKP / WikiSmall,2017-03,DRESS model in \'Sentence Simplification with Deep Reinforcement Learning\',27.48,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,PWKP / WikiSmall,2018-04,NSELSTM-S model in \'Sentence Simplification with Memory-Augmented Neural Networks\',29.75,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,PWKP / WikiSmall,2019-06,EditNTS model in \'EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing\',32.35,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,PWKP / WikiSmall,2021-03,TST model in \'Text Simplification by Tagging\',44.67,SARI,pos
Natural language generation,Text simplification,Text simplification,Text simplification,PWKP / WikiSmall,2021-03,TST model in \'Text Simplification by Tagging\',44.67,SARI (EASSE>=0.2.1),pos
Natural language generation,Text simplification,Text simplification,Text simplification,EurekaAlert,2020-07,HTSS model in \'HTSS: A Novel Hybrid Text Summarisation and Simplification Architecture\',21.94,Rouge1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,AESLC,2019-06,Multi-Stage Extractor/Abstractor model in \'This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation\',23.44,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,AESLC,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',36.51,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,AESLC,2019-06,Multi-Stage Extractor/Abstractor model in \'This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation\',23.67,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,AESLC,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',37.68,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,AESLC,2019-06,Multi-Stage Extractor/Abstractor model in \'This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation\',10.29,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,AESLC,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',21.25,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,DaNewsroom,2020-05,DanSum model in \'DaNewsroom: A Large-scale Danish Summarisation Dataset\',71.41,Avg. Test BERTScore,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,DaNewsroom,2020-05,DanSum model in \'DaNewsroom: A Large-scale Danish Summarisation Dataset\',23.1,Avg. Test Rouge1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,DaNewsroom,2020-05,DanSum model in \'DaNewsroom: A Large-scale Danish Summarisation Dataset\',7.53,Avg. Test Rouge2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,DaNewsroom,2020-05,DanSum model in \'DaNewsroom: A Large-scale Danish Summarisation Dataset\',18.52,Avg. Test RougeL,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,MLSUM de,2021-02,"mBART model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.437,METEOR,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2016-02,LEAD-3 model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',36.67,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2018-03,DCA model in \'Deep Communicating Agents for Abstractive Summarization\',37.92,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2018-05,rnn-ext + abs + RL + rerank model in \'Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\',38.54,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-02,Two-Stage + RL model in \'Pretraining-Based Natural Language Generation for Text Summarization\',38.79,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',40.34,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-10,"BART model in \'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\'",40.9,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',41.11,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2020-01,ERNIE-GENLARGE (large-scale text corpora) model in \'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\',41.6,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2022-03,BRIO model in \'BRIO: Bringing Order to Abstractive Summarization\',44.57,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2016-02,LEAD-3 model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',40.42,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2018-03,DCA model in \'Deep Communicating Agents for Abstractive Summarization\',41.69,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-02,Two-Stage + RL model in \'Pretraining-Based Natural Language Generation for Text Summarization\',41.71,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',43.08,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-10,"BART model in \'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\'",44.16,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',44.17,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2020-01,ERNIE-GENLARGE (large-scale text corpora) model in \'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\',44.31,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2020-08,BART+R3F model in \'Better Fine-Tuning by Reducing Representational Collapse\',44.38,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2021-01,MUPPET BART Large model in \'Muppet: Massive Multi-task Representations with Pre-Finetuning\',44.45,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2021-03,GLM-XXLarge model in \'GLM: General Language Model Pretraining with Autoregressive Blank Infilling\',44.7,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2022-03,BRIO model in \'BRIO: Bringing Order to Abstractive Summarization\',47.78,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2016-02,LEAD-3 model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',17.62,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2018-03,DCA model in \'Deep Communicating Agents for Abstractive Summarization\',19.47,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-02,Two-Stage + RL model in \'Pretraining-Based Natural Language Generation for Text Summarization\',19.49,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',20.43,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2019-10,T5 model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',21.55,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2021-06,BART + R-Drop model in \'R-Drop: Regularized Dropout for Neural Networks\',21.58,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,CNN / Daily Mail,2022-03,BRIO model in \'BRIO: Bringing Order to Abstractive Summarization\',23.55,ROUGE-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,MLSum-it,2022-04,mBART model in \'Two New Datasets for Italian-Language Abstractive Text Summarization\',19.35,rouge1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,Abstractive Text Summarization from Fanpage,2022-04,mBART model in \'Two New Datasets for Italian-Language Abstractive Text Summarization\',36.5,rouge1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,vietnews,2021-09,BARTpho model in \'BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese\',40.15,Rouge-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,vietnews,2022-05,ViT5 large model in \'ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation\',43.55,Rouge-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,vietnews,2021-09,BARTpho model in \'BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese\',61.14,Rouge-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,vietnews,2022-05,ViT5 large model in \'ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation\',63.37,Rouge-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,vietnews,2021-09,BARTpho model in \'BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese\',30.31,Rouge-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,vietnews,2022-05,ViT5 large model in \'ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation\',34.24,Rouge-2,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,Abstractive Text Summarization from Il Post,2022-04,mBART model in \'Two New Datasets for Italian-Language Abstractive Text Summarization\',38.91,rouge1,pos
Natural language generation,Text summarization,Abstractive text summarization,Abstractive text summarization,MLSUM es,2021-02,"mBART model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.21,METEOR,pos
Natural language generation,Text summarization,Abstractive text summarization,Multimodal abstractive text summarization,How2 300h,2020-10,MAST model in \'MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention\',43.23,ROUGE-L,pos
Natural language generation,Text summarization,Abstractive text summarization,Reader-aware summarization,RASG,2018-12,RASG model in \'Abstractive Text Summarization by Incorporating Reader Comments\',30.33,ROUGE-1,pos
Natural language generation,Text summarization,Abstractive text summarization,Timeline summarization,MTS,2019-08,MTS model in \'Learning towards Abstractive Timeline Summarization\',39.78,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,BBC XSum,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',38.8,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,BBC XSum,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',47.12,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,BBC XSum,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',24.05,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,arXiv,2021-11,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',47.15,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,HowSumm-Step,2021-10,LexRank (query: step title) model in \'HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles\',39.6,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,arXiv Summarization Dataset,2021-11,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',19.99,Rouge-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2017-04,Lead-3 model in \'Get To The Point: Summarization with Pointer-Generator Networks\',36.57,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2017-05,"ML + RL (Paulus et al., 2017) model in \'A Deep Reinforced Model for Abstractive Summarization\'",36.9,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2018-08,Bottom-Up Sum model in \'Bottom-Up Abstractive Summarization\',38.34,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-03,BERTSUM+Transformer model in \'Fine-tune BERT for Extractive Summarization\',39.63,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-05,UniLM (Abstractive Summarization) model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',40.34,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',40.69,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',40.74,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2021-03,GLM-XXLarge model in \'GLM: General Language Model Pretraining with Autoregressive Blank Infilling\',41.4,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2021-04,HAT-BART model in \'Hierarchical Learning for Generation with Long Source Sequences\',41.52,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2022-03,PEGASUS + SummaReranker model in \'SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization\',43.87,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2017-09,C2F + ALTERNATE model in \'Coarse-to-Fine Attention Models for Document Summarization\',-23.6,PPL,neg
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2017-04,Lead-3 model in \'Get To The Point: Summarization with Pointer-Generator Networks\',40.34,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2018-08,Bottom-Up Sum model in \'Bottom-Up Abstractive Summarization\',41.22,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-03,BERTSUM+Transformer model in \'Fine-tune BERT for Extractive Summarization\',43.25,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-08,BertSumExt model in \'Text Summarization with Pretrained Encoders\',43.85,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2020-04,MatchSum (RoBERTa-base) model in \'Extractive Summarization as Text Matching\',44.41,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2021-03,GLM-XXLarge model in \'GLM: General Language Model Pretraining with Autoregressive Blank Infilling\',44.7,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2022-03,PEGASUS + SummaReranker model in \'SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization\',47.16,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2017-04,Lead-3 model in \'Get To The Point: Summarization with Pointer-Generator Networks\',17.7,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2018-08,Bottom-Up Sum model in \'Bottom-Up Abstractive Summarization\',18.68,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-03,BERTSUM+Transformer model in \'Fine-tune BERT for Extractive Summarization\',20.24,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-05,UniLM (Abstractive Summarization) model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',20.43,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',21.55,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,CNN / Daily Mail,2022-03,PEGASUS + SummaReranker model in \'SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization\',22.55,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Document summarization,HowSumm-Method,2021-10,LexRank (query: method + article + steps titles) model in \'HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles\',53.5,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (long),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',32.4,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (long),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',45.98,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (long),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',15.49,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (long),2021-07,SemiSuptogether model in \'EmailSum: Abstractive Email Thread Summarization\',32.3,BertS,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (long),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',42.14,RLsum,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (short),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',30.17,ROUGE-L,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (short),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',39.04,ROUGE-1,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (short),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',12.47,ROUGE-2,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (short),2021-07,SemiSuptogether model in \'EmailSum: Abstractive Email Thread Summarization\',33.91,BertS,pos
Natural language generation,Text summarization,Document summarization,Email Thread Summarization,EmailSum (short),2021-07,Oracle model in \'EmailSum: Abstractive Email Thread Summarization\',35.61,RLsum,pos
Natural language generation,Text summarization,Extractive document summarization,Reader-aware summarization,RASG,2018-12,RASG model in \'Abstractive Text Summarization by Incorporating Reader Comments\',30.33,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2017-04,Lead-3 baseline model in \'Get To The Point: Summarization with Pointer-Generator Networks\',36.57,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2018-02,REFRESH model in \'Ranking Sentences for Extractive Summarization with Reinforcement Learning\',36.6,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2018-07,NeuSUM model in \'Neural Document Summarization by Jointly Learning to Score and Select Sentences\',37.98,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2019-05,HIBERT model in \'HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization\',38.83,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2019-07,PNBERT model in \'Searching for Effective Neural Extractive Summarization: What Works and What\'s Next\',38.85,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2019-09,BERT-ext + RL model in \'Summary Level Training of Sentence Rewriting for Abstractive Summarization\',39.11,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',40.55,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2017-04,Lead-3 baseline model in \'Get To The Point: Summarization with Pointer-Generator Networks\',40.34,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2018-07,NeuSUM model in \'Neural Document Summarization by Jointly Learning to Score and Select Sentences\',41.59,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2019-05,HIBERT model in \'HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization\',42.37,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2019-07,PNBERT model in \'Searching for Effective Neural Extractive Summarization: What Works and What\'s Next\',42.69,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2019-09,BERT-ext + RL model in \'Summary Level Training of Sentence Rewriting for Abstractive Summarization\',42.76,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',44.41,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2017-04,Lead-3 baseline model in \'Get To The Point: Summarization with Pointer-Generator Networks\',17.7,ROUGE-2,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2018-02,REFRESH model in \'Ranking Sentences for Extractive Summarization with Reinforcement Learning\',18.2,ROUGE-2,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2018-07,NeuSUM model in \'Neural Document Summarization by Jointly Learning to Score and Select Sentences\',19.01,ROUGE-2,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2019-05,HIBERT model in \'HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization\',19.95,ROUGE-2,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,CNN / Daily Mail,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',20.86,ROUGE-2,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,DebateSum,2020-11,Longformer-Base model in \'DebateSum: A large-scale argument mining and summarization dataset\',57.21,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,DUC 2004 Task 1,2015-09,Abs model in \'A Neural Attention Model for Abstractive Sentence Summarization\',22.05,ROUGE-L,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,DUC 2004 Task 1,2015-09,Abs model in \'A Neural Attention Model for Abstractive Sentence Summarization\',26.55,ROUGE-1,pos
Natural language generation,Text summarization,Extractive text summarization,Extractive text summarization,DUC 2004 Task 1,2015-09,Abs model in \'A Neural Attention Model for Abstractive Sentence Summarization\',7.06,ROUGE-2,pos
Natural language generation,Text summarization,Extractive text summarization,Reader-aware summarization,RASG,2018-12,RASG model in \'Abstractive Text Summarization by Incorporating Reader Comments\',30.33,ROUGE-1,pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,XSum,2021-02,"PEGASUS model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",0.216,METEOR,pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,GEM-XSum,2021-05,ByT5 model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',15.3,BLEU score,pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,GEM-XSum,2021-02,"PEGASUS model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",568000000.0,Parameters,pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,GEM-XSum,2021-02,"PEGASUS model in \'The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics\'",23.2,ROUGE-2,pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,TLDR9+,2021-10,ORACLE-EXT model in \'TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts\',30.26,RG-1(%),pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,TLDR9+,2021-10,ORACLE-EXT model in \'TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts\',9.74,RG-2(%),pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,TLDR9+,2021-10,ORACLE-EXT model in \'TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts\',20.6,RG-L(%),pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,CiteSum,2022-05,EXT-ORACLE model in \'CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation\',38.32,ROUGE-L,pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,CiteSum,2022-05,EXT-ORACLE model in \'CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation\',44.17,ROUGE-1,pos
Natural language generation,Text summarization,Extreme Summarization,Extreme Summarization,CiteSum,2022-05,EXT-ORACLE model in \'CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation\',27.22,ROUGE-2,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,DUC 2004,2017-06,GCN: Personalized Discourse Graph model in \'Graph-based Neural Multi-Document Summarization\',38.23,ROUGE-1,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,WCEP,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',37.9,ROUGE-L,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,WCEP,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',46.1,ROUGE-1,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,WCEP,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',25.2,ROUGE-2,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',25.9,ROUGE-L,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2018-08,CopyTransformer model in \'Bottom-Up Abstractive Summarization\',43.57,ROUGE-1,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2021-07,CTF+DPP model in \'Multi-Document Summarization withDeterminantal Point Process Attention\',45.84,ROUGE-1,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',49.9,ROUGE-1,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2018-08,PG-BRNN model in \'Bottom-Up Abstractive Summarization\',14.19,ROUGE-2,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2019-06,Hi-MAP model in \'Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\',14.89,ROUGE-2,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2021-07,CTF+DPP model in \'Multi-Document Summarization withDeterminantal Point Process Attention\',15.94,ROUGE-2,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',21.1,ROUGE-2,pos
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,Multi-News,2018-08,PG-BRNN model in \'Bottom-Up Abstractive Summarization\',-16.75,ROUGE-SU4,neg
Natural language generation,Text summarization,Multi-document summarization,Multi-document summarization,review,2018-12,solar model in \'Solar Cell Surface Defect Inspection Based on Multispectral Convolutional Neural Network\',100.0,1-of-100 Accuracy,pos
Natural language generation,Text summarization,Query-based extractive summarization,Query-based extractive summarization,Debatepedia,2017-04,SD2 model in \'Diversity driven Attention Model for Query-based Abstractive Summarization\',41.26,ROUGE-1,pos
Natural language generation,Text summarization,Query-based extractive summarization,Query-based extractive summarization,Debatepedia,2018-01,"RSA Word Count model in \'Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models\'",53.09,ROUGE-1,pos
Natural language generation,Text summarization,Reasoning,Anachronisms,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",56.4,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Anachronisms,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",69.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Analogical Similarity,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",17.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Analogical Similarity,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",38.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Answer Generation,CICERO,2022-03,T5-large model in \'CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues\',0.2947,ROUGE,pos
Natural language generation,Text summarization,Reasoning,Arithmetic Reasoning,GSM8K,2022-03,PaLM 540B maj1@40  model in \'Self-Consistency Improves Chain of Thought Reasoning in Language Models\',74.4,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Arithmetic Reasoning,GSM8K,2022-06,Minerva 540B-maj1@k model in \'Solving Quantitative Reasoning Problems with Language Models\',78.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Arithmetic Reasoning,MultiArith,2022-05,Text-davinci-002 (175B)(zero-shot-cot) model in \'Large Language Models are Zero-Shot Reasoners\',78.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,HolStep (Conditional),2017-03,Siamese 1D CNN-LSTM model in \'HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\',0.83,Classification Accuracy,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,HolStep (Conditional),2017-09,FormulaNet model in \'Premise Selection for Theorem Proving by Deep Graph Embedding\',0.903,Classification Accuracy,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,HolStep (Conditional),2019-11,MPNN-DagLSTM model in \'Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling\',0.916,Classification Accuracy,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,CompCert,2020-05,Proverbot9001 model in \'Generating Correctness Proofs with Neural Networks\',19.36,Percentage correct,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,Metamath set.mm,2016-08,Holophrasm (\'16) model in \'Holophrasm: a neural Automated Theorem Prover for higher-order logic\',14.26,Percentage correct,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-test,2021-08,Lean GPT-f model in \'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics\',24.6,Pass@1,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-test,2022-02,Lean Expert Iteration model in \'Formal Mathematics Statement Curriculum Learning\',29.6,Pass@1,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-test,2022-05,Thor + expert iteration on autoformalised theorems model in \'Autoformalization with Large Language Models\',35.2,Pass@1,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-test,2021-08,Lean GPT-f model in \'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics\',29.2,Pass@8,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-test,2022-02,Lean Expert Iteration model in \'Formal Mathematics Statement Curriculum Learning\',34.5,Pass@8,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-test,2022-02,Lean Expert Iteration model in \'Formal Mathematics Statement Curriculum Learning\',36.6,Pass@64,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-test,2022-05,Evariste model in \'HyperTree Proof Search for Neural Theorem Proving\',41.0,Pass@64,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,CoqGym,2019-05,ASTactic model in \'Learning to Prove Theorems via Interacting with Proof Assistants\',12.2,Percentage correct,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-valid,2021-08,Lean GPT-f model in \'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics\',23.9,Pass@1,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,miniF2F-valid,2021-08,Lean GPT-f model in \'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics\',29.3,Pass@8,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,HolStep (Unconditional),2017-03,1D CNN-LSTM model in \'HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving\',0.83,Classification Accuracy,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,HolStep (Unconditional),2017-09,FormulaNet model in \'Premise Selection for Theorem Proving by Deep Graph Embedding\',0.9,Classification Accuracy,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,HOList benchmark,2019-04,Tactic Dependent Loop model in \'HOList: An Environment for Machine Learning of Higher-Order Theorem Proving\',38.88,Percentage correct,pos
Natural language generation,Text summarization,Reasoning,Automated theorem proving,HOList benchmark,2019-05,"4-hop GNN, sub-expression sharing model in \'Graph Representations for Higher-Order Logic and Theorem Proving\'",49.95,Percentage correct,pos
Natural language generation,Text summarization,Reasoning,Causal Judgment,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",50.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Causal Judgment,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",57.4,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Event2Mind dev,2018-05,"ConvNet model in \'Event2Mind: Commonsense Inference on Events, Intents, and Reactions\'",4.44,Average Cross-Ent,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,WSC273,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',80.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Winograd Schema Challenge,2018-06,Ensemble of 14 LMs model in \'A Simple Method for Commonsense Reasoning\',63.7,Score,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Winograd Schema Challenge,2019-02,GPT-2 model in \'Language Models are Unsupervised Multitask Learners\',70.7,Score,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Winograd Schema Challenge,2019-05,BERTWiki-WSCR model in \'A Surprisingly Robust Trick for Winograd Schema Challenge\',72.2,Score,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Winograd Schema Challenge,2019-07,HNN model in \'A Hybrid Neural Network Model for Commonsense Reasoning\',75.1,Score,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,PARus,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.982,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ARC (Easy),2020-05,GPT-3 175B (1 shot) model in \'Language Models are Few-Shot Learners\',71.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ARC (Easy),2021-12,GLaM (64B/64E) (5-shot) model in \'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\',74.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ARC (Easy),2022-02,ST-MoE-32B model in \'ST-MoE: Designing Stable and Transferable Sparse Expert Models\',95.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ARC (Challenge),2020-05,GPT-3 175B (1 shot) model in \'Language Models are Few-Shot Learners\',53.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ARC (Challenge),2022-02,ST-MoE-32B model in \'ST-MoE: Designing Stable and Transferable Sparse Expert Models\',86.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Event2Mind test,2018-05,"ConvNet model in \'Event2Mind: Commonsense Inference on Events, Intents, and Reactions\'",4.4,Average Cross-Ent,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Russian Event2Mind,2020-06,araneum word2vec (skipgram) + GRU model in \'Event2Mind for Russian: Understanding Emotions and Intents in Texts. Corpus and Model for Evaluation\',0.827,recall@10,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,SWAG,2018-08,ESIM + ELMo model in \'SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\',59.2,Test,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,SWAG,2018-10,BERT Large model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',86.3,Test,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,SWAG,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',89.9,Test,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,SWAG,2020-06,DeBERTalarge model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',90.8,Test,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,SWAG,2018-08,ESIM + ELMo model in \'SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\',59.1,Dev,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,SWAG,2018-10,BERT Large model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',86.6,Dev,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Visual Dialog  v0.9,2019-09,PDUN model in \'Probabilistic framework for solving Visual Dialog\',81.0,1 in 10 R-at-5,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Visual Dialog  v0.9,2019-09,PDUN model in \'Probabilistic framework for solving Visual Dialog\',90.5,Recall-at-10,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,RuCoS,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.93,Average F1,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,RuCoS,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.89,EM,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,Visual Dialog v0.9,2018-09,NMN [kottur2018visual] model in \'Visual Coreference Resolution in Visual Dialog using Neural Module Networks\',80.1,1 in 10 R-at-5,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,CommonsenseQA,2018-11,BERT-LARGE model in \'CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge\',55.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,CommonsenseQA,2019-06,CAGE-reasoning model in \'Explain Yourself! Leveraging Language Models for Commonsense Reasoning\',64.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,CommonsenseQA,2019-07,RoBERTa Liu et al. (2019) model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',72.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,CommonsenseQA,2019-09,Albert Lan et al. (2020) (ensemble) model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',76.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,CommonsenseQA,2020-05,UnifiedQA* Khashabi et al. (2020) model in \'UnifiedQA: Crossing Format Boundaries With a Single QA System\',79.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,CommonsenseQA,2020-12,DEKCOR model in \'Fusing Context Into Knowledge Graph for Commonsense Question Answering\',83.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,CommonsenseQA,2021-12,KEAR model in \'Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention\',89.4,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,RWSD,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.84,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2021-09,FLAN 137B zero-shot model in \'Finetuned Language Models Are Zero-Shot Learners\',72.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2018-10,BERT-Base (single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',56.065,F1,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',94.1,F1,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',94.5,F1,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2022-04,PaLM 540B (finetuned)  model in \'PaLM: Scaling Language Modeling with Pathways\',94.6,F1,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2018-10,BERT-Base (single model) model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',54.04,EM,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',93.4,EM,pos
Natural language generation,Text summarization,Reasoning,Common sense reasoning,ReCoRD,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',94.1,EM,pos
Natural language generation,Text summarization,Reasoning,Crash Blossom,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",63.6,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Crass AI,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",56.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Crass AI,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",75.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI,2020-10,Decoupled model in \'Rethinking embedding coupling in pre-trained language models\',71.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',83.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2017-05,X-BiLSTM model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',68.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',74.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',85.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-French,2017-05,X-BiLSTM model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',67.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-French,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',84.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2017-05,X-CBOW model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',61.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',70.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',84.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Crowdsourced text aggregation,CrowdSpeech test-clean,2021-07,ROVER model in \'CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription\',-7.29,Word Error Rate (WER),neg
Natural language generation,Text summarization,Reasoning,Crowdsourced text aggregation,CrowdSpeech test-other,2021-07,ROVER model in \'CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription\',-13.41,Word Error Rate (WER),neg
Natural language generation,Text summarization,Reasoning,Disambiguation Q,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",45.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Disambiguation Q,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",54.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Discourse Marker Prediction,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",11.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Discourse Marker Prediction,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",13.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Empirical Judgments,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",52.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Empirical Judgments,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",67.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,General reinforcement learning,Obstacle Tower (Weak Gen) fixed,2019-02,"PPO model in \'Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning\'",1.2,Score,pos
Natural language generation,Text summarization,Reasoning,General reinforcement learning,Obstacle Tower (No Gen) fixed,2019-02,"RNB model in \'Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning\'",7.0,Score,pos
Natural language generation,Text summarization,Reasoning,General reinforcement learning,Obstacle Tower (Strong Gen) fixed,2019-02,"RNB model in \'Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning\'",0.6,Score,pos
Natural language generation,Text summarization,Reasoning,General reinforcement learning,Obstacle Tower (No Gen) varied,2019-02,"RNB model in \'Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning\'",4.8,Score,pos
Natural language generation,Text summarization,Reasoning,General reinforcement learning,Obstacle Tower (Strong Gen) varied,2019-02,"RNB model in \'Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning\'",0.8,Score,pos
Natural language generation,Text summarization,Reasoning,General reinforcement learning,Obstacle Tower (Weak Gen) varied,2019-02,"RNB model in \'Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning\'",3.4,Score,pos
Natural language generation,Text summarization,Reasoning,HellaSwag,BIG-bench,2021-12,"Gopher-280B (zero-shot) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",79.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Human Judgment Classification,Pascal-50S,2021-04,RefCLIP-S model in \'CLIPScore: A Reference-free Evaluation Metric for Image Captioning\',83.1,Mean Accuracy,pos
Natural language generation,Text summarization,Reasoning,Human Judgment Classification,Pascal-50S,2022-05,MID model in \'Mutual Information Divergence: A Unified Metric for Multimodal Generative Models\',85.2,Mean Accuracy,pos
Natural language generation,Text summarization,Reasoning,Human Judgment Correlation,Flickr8k-CF,2021-04,RefCLIP-S model in \'CLIPScore: A Reference-free Evaluation Metric for Image Captioning\',36.4,Kendall's Tau-b,pos
Natural language generation,Text summarization,Reasoning,Human Judgment Correlation,Flickr8k-CF,2022-05,MID model in \'Mutual Information Divergence: A Unified Metric for Multimodal Generative Models\',37.3,Kendall's Tau-b,pos
Natural language generation,Text summarization,Reasoning,Human Judgment Correlation,Flickr8k-Expert,2021-04,RefCLIP-S model in \'CLIPScore: A Reference-free Evaluation Metric for Image Captioning\',53.0,Kendall's Tau-c,pos
Natural language generation,Text summarization,Reasoning,Human Judgment Correlation,Flickr8k-Expert,2022-05,MID model in \'Mutual Information Divergence: A Unified Metric for Multimodal Generative Models\',54.9,Kendall's Tau-c,pos
Natural language generation,Text summarization,Reasoning,Identify Odd Metapor,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",38.6,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Irony Identification,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",69.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Irony Identification,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",73.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,Math23K,2019-07,GROUP-ATT model in \'Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions\',66.9,Accuracy (5-fold),pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,Math23K,2019-08,GTS model in \'A Goal-Driven Tree-Structured Neural Model for Math Word Problems\',74.3,Accuracy (5-fold),pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,Math23K,2020-07,Graph2Tree model in \'Graph-to-Tree Learning for Solving Math Word Problems\',75.5,Accuracy (5-fold),pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,Math23K,2019-07,GROUP-ATT model in \'Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions\',69.5,Accuracy (training-test),pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,Math23K,2020-07,Graph2Tree model in \'Graph-to-Tree Learning for Solving Math Word Problems\',77.4,Accuracy (training-test),pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,Math23K,2020-12,LBF model in \'Learning by Fixing: Solving Math Word Problems with Weak Supervision\',59.8,weakly-supervised,pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,MATH,2021-03,GPT-2 (1.5B) model in \'Measuring Mathematical Problem Solving With the MATH Dataset\',6.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,MATH,2022-06,Minerva 540B-maj1@k model in \'Solving Quantitative Reasoning Problems with Language Models\',50.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,MATH,2021-03,GPT-3-175B (few-shot) model in \'Measuring Mathematical Problem Solving With the MATH Dataset\',175.0,Parameters (Billions),pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,MATH,2022-06,PaLM 540B model in \'Solving Quantitative Reasoning Problems with Language Models\',540.0,Parameters (Billions),pos
Natural language generation,Text summarization,Reasoning,Math word problem solving,SVAMP,2021-03,Graph2Tree with RoBERTa model in \'Are NLP Models really able to Solve Simple Math Word Problems?\',43.8,Execution Accuracy,pos
Natural language generation,Text summarization,Reasoning,Mathematical question answering,Geometry3K,2021-05,Human Expert model in \'Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning\',90.9,Accuracy (%),pos
Natural language generation,Text summarization,Reasoning,Mathematical question answering,GeoS,2015-09,GEOS model in \'Solving Geometry Problems: Combining Text and Diagram Interpretation\',49.0,Accuracy (%),pos
Natural language generation,Text summarization,Reasoning,Mathematical question answering,GeoS,2021-05,Inter-GPS model in \'Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning\',67.0,Accuracy (%),pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SciTail,2017-12,"CAFE model in \'Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference\'",83.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SciTail,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',88.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SciTail,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',92.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SciTail,2019-01,MT-DNN model in \'Multi-Task Deep Neural Networks for Natural Language Understanding\',94.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SciTail,2019-11,SMART-MT-DNN model in \'SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\',96.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SciTail,2020-09,CA-MTL model in \'Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data\',96.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI Chinese,2019-04,ERNIE model in \'ERNIE: Enhanced Representation through Knowledge Integration\',78.4,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI Chinese,2019-07,ERNIE 2.0 Large model in \'ERNIE 2.0: A Continual Pre-training Framework for Language Understanding\',81.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,TERRa,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.92,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI Dev,2019-09,TinyBERT (M=6;d\'=768;d\'i=3072) model in \'TinyBERT: Distilling BERT for Natural Language Understanding\',84.5,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI Dev,2019-09,TinyBERT (M=6;d\'=768;d\'i=3072) model in \'TinyBERT: Distilling BERT for Natural Language Understanding\',84.5,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MRPC,2021-11,DeBERTaV3large model in \'DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\',92.2,Acc,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI French,2018-09,BiLSTM-max model in \'XNLI: Evaluating Cross-lingual Sentence Representations\',68.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI French,2019-01,XLM (MLM+TLM) model in \'Cross-lingual Language Model Pretraining\',80.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI French,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',81.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI French,2019-12,FlauBERT (large) model in \'FlauBERT: Unsupervised Language Model Pre-training for French\',83.4,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SICK,2021-05,NeuralLog model in \'NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning\',0.903,1:1 Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,V-SNLI,2018-06,V-BiMPM model in \'Grounded Textual Entailment\',86.99,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,V-SNLI,2019-09,MMBT model in \'Supervised Multimodal Bitransformers for Classifying Images and Text\',90.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MED,2021-05,NeuralLog model in \'NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning\',0.934,1:1 Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-03,GenSen model in \'Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\',71.4,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-04,Multi-task BiLSTM + Attn model in \'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\',72.2,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',82.1,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-10,Snorkel MeTaL (ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',87.6,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-04,MT-DNN-ensemble model in \'Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding\',87.9,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',90.8,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-08,Adv-RoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',91.1,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-09,ALBERT model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',91.3,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',92.0,Matched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-03,GenSen model in \'Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\',71.3,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-04,Multi-task BiLSTM + Attn model in \'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\',72.1,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',81.4,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2018-10,Snorkel MeTaL (ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',87.2,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-04,MT-DNN-ensemble model in \'Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding\',87.4,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',90.2,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-08,Adv-RoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',90.7,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MultiNLI,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',91.7,Mismatched,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,KUAKE-QQR,2021-06,BERT-base model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',84.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RCB,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.702,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RCB,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.68,Average F1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,FarsTail,2020-09,mBERT model in \'FarsTail: A Persian Natural Language Inference Dataset\',83.38,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,WNLI,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',92.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,WNLI,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',93.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,WNLI,2020-06,DeBERTa model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',94.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,Quora Question Pairs,2018-12,aESIM model in \'Attention Boosted Sequential Inference Model\',88.01,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MRPC Dev,2020-06,DeBERTa (large) model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',92.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,CommitmentBank,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',96.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,CommitmentBank,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',97.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,CommitmentBank,2022-04,PaLM 540B (finetuned) model in \'PaLM: Scaling Language Modeling with Pathways\',100.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,CommitmentBank,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',93.9,F1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,CommitmentBank,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',94.9,F1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,CommitmentBank,2022-04,PaLM 540B (finetuned) model in \'PaLM: Scaling Language Modeling with Pathways\',100.0,F1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',68.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',85.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',88.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2019-08,Adv-RoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',88.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2019-09,ALBERT model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',89.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',92.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',93.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,RTE,2022-04,PaLM 540B (finetuned)  model in \'PaLM: Scaling Language Modeling with Pathways\',95.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,LiDiRus,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.626,MCC,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XWINO,2022-04,mGPT model in \'mGPT: Few-Shot Learners Go Multilingual\',0.562,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,QNLI,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',91.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,QNLI,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',94.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,QNLI,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',98.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,QNLI,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',99.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,KUAKE-QTR,2021-06,MacBERT-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',62.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2014-04,DCNN [[Blunsom et al.2014]] model in \'A Convolutional Neural Network for Modelling Sentences\',86.8,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2014-08,CNN-MC [[Kim2014]] model in \'Convolutional Neural Networks for Sentence Classification\',88.1,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2016-09,600D ESIM + 300D Syntactic TreeLSTM model in \'Enhanced LSTM for Natural Language Inference\',88.6,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2017-02,BiMPM Ensemble model in \'Bilateral Multi-Perspective Matching for Natural Language Sentences\',88.8,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2017-09,"448D Densely Interactive Inference Network (DIIN, code) Ensemble model in \'Natural Language Inference over Interaction Space\'",88.9,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2017-11,KIM Ensemble model in \'Neural Natural Language Inference Models Enhanced with External Knowledge\',89.1,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2017-12,"300D CAFE Ensemble model in \'Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference\'",89.3,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble model in \'Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information\',90.1,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2018-09,SJRC (BERT-Large +SRL) model in \'Explicit Contextual Semantics for Text Comprehension\',91.3,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2019-01,MT-DNN model in \'Multi-Task Deep Neural Networks for Natural Language Understanding\',91.6,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2019-09,SemBERT model in \'Semantics-aware BERT for Language Understanding\',91.9,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2020-09,CA-MTL model in \'Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data\',92.1,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2020-12,RoBERTa-large+Self-Explaining model in \'Self-Explaining Structures Improve NLP Models\',92.3,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2021-04,EFL model in \'Entailment as Few-Shot Learner\',93.1,% Test Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2015-08,+ Unigram and bigram features model in \'A large annotated corpus for learning natural language inference\',99.7,% Train Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2015-08,100D LSTM encoders model in \'A large annotated corpus for learning natural language inference\',220000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2015-09,100D LSTMs w/ word-by-word attention model in \'Reasoning about Entailment with Neural Attention\',250000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2015-11,1024D GRU encoders w/ unsupervised \'skip-thoughts\' pre-training model in \'Order-Embeddings of Images and Language\',15000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2017-05,4096D BiLSTM with max-pooling model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',40000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2017-11,KIM Ensemble model in \'Neural Natural Language Inference Models Enhanced with External Knowledge\',43000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2018-02,450D DR-BiLSTM Ensemble model in \'DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference\',45000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble model in \'Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information\',53300000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2018-06,Fine-Tuned LM-Pretrained Transformer model in \'Improving Language Understanding by Generative Pre-Training\',85000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2018-09,SJRC (BERT-Large +SRL) model in \'Explicit Contextual Semantics for Text Comprehension\',308000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2019-01,MT-DNN model in \'Multi-Task Deep Neural Networks for Natural Language Understanding\',330000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2019-09,SemBERT model in \'Semantics-aware BERT for Language Understanding\',339000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,SNLI,2020-09,CA-MTL model in \'Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data\',340000000.0,Parameters,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MedNLI,2019-08,BioBERT-MIMIC model in \'Saama Research at MEDIQA 2019: Pre-trained BioBERT with Attention Visualisation for Medical Natural Language Inference\',83.45,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MedNLI,2020-10,"CharacterBERT (base, medical) model in \'CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters\'",84.95,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MedNLI,2021-05,SciFive-large model in \'SciFive: a text-to-text transformer model for biomedical literature\',86.57,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,MedNLI,2019-06,NCBI_BERT(base) (P+M) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',84.0,F1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI Chinese Dev,2019-04,ERNIE model in \'ERNIE: Enhanced Representation through Knowledge Integration\',79.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,XNLI Chinese Dev,2019-07,ERNIE 2.0 Large model in \'ERNIE 2.0: A Continual Pre-training Framework for Language Understanding\',82.6,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2019-06,XLNet (Large) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',70.3,A1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2019-07,RoBERTa (Large) model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',72.4,A1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2020-10,InfoBERT (RoBERTa) model in \'InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective\',75.0,A1,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2019-06,XLNet (Large) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',50.9,A2,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2020-04,ALUM (RoBERTa-LARGE) model in \'Adversarial Training for Large Neural Language Models\',52.1,A2,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2019-06,XLNet (Large) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',49.4,A3,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2020-04,ALUM (RoBERTa-LARGE) model in \'Adversarial Training for Large Neural Language Models\',57.0,ANLI,pos
Natural language generation,Text summarization,Reasoning,Natural language inference,ANLI test,2020-10,InfoBERT (RoBERTa) model in \'InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective\',58.3,ANLI,pos
Natural language generation,Text summarization,Reasoning,Odd One Out,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",32.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Odd One Out,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",70.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Offline RL,Walker2d,2021-09,ParPI model in \'Particle Based Stochastic Policy Optimization\',151.4,D4RL Normalized Score,pos
Natural language generation,Text summarization,Reasoning,PIQA,BIG-bench,2021-12,"Gopher-280B (zero-shot) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",81.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Physical Commonsense Reasoning,Physical Audiovisual CommonSense,2022-03,Majority model in \'PACS: A Dataset for Physical Audiovisual CommonSense Reasoning\',50.4,With Audio (Acc %),pos
Natural language generation,Text summarization,Reasoning,Physical Commonsense Reasoning,Physical Audiovisual CommonSense,2022-03,Majority model in \'PACS: A Dataset for Physical Audiovisual CommonSense Reasoning\',50.4,Without Audio (Acc %),pos
Natural language generation,Text summarization,Reasoning,Program repair,DeepFix,2018-01,RLAssist model in \'Deep Reinforcement Learning for Programming Language Correction\',26.6,Average Success Rate,pos
Natural language generation,Text summarization,Reasoning,Program repair,DeepFix,2019-06,SampleFix model in \'SampleFix: Learning to Generate Functionally Diverse Fixes\',45.3,Average Success Rate,pos
Natural language generation,Text summarization,Reasoning,Program repair,DeepFix,2020-05,"DrRepair model in \'Graph-based, Self-Supervised Program Repair from Diagnostic Feedback\'",68.2,Average Success Rate,pos
Natural language generation,Text summarization,Reasoning,Program repair,DeepFix,2021-06,DrRepair + BIFI model in \'Break-It-Fix-It: Unsupervised Learning for Program Repair\',71.7,Average Success Rate,pos
Natural language generation,Text summarization,Reasoning,Program repair,GitHub-Python,2021-06,Transformer + BIFI model in \'Break-It-Fix-It: Unsupervised Learning for Program Repair\',90.5,Accuracy (%),pos
Natural language generation,Text summarization,Reasoning,Program synthesis,HumanEval,2022-03,CodeGen-16B-mono model in \'A Conversational Paradigm for Program Synthesis\',29.28,Pass@1,pos
Natural language generation,Text summarization,Reasoning,Program synthesis,SPoC TestW,2019-06,Multiclass localizer model in \'SPoC: Search-based Pseudocode to Code\',53.7,Success rate @budget 100,pos
Natural language generation,Text summarization,Reasoning,Program synthesis,SPoC TestW,2020-05,"DrRepair model in \'Graph-based, Self-Supervised Program Repair from Diagnostic Feedback\'",57.0,Success rate @budget 100,pos
Natural language generation,Text summarization,Reasoning,Program synthesis,AlgoLisp,2021-04,CodeTrans-MT-TF-Small model in \'CodeTrans: Towards Cracking the Language of Silicon\'s Code Through Self-Supervised Deep Learning and High Performance Computing\',90.31,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Program synthesis,SPoC TestP,2019-06,Multiclass localizer model in \'SPoC: Search-based Pseudocode to Code\',34.2,Success rate @budget 100,pos
Natural language generation,Text summarization,Reasoning,Program synthesis,SPoC TestP,2020-05,"DrRepair model in \'Graph-based, Self-Supervised Program Repair from Diagnostic Feedback\'",38.5,Success rate @budget 100,pos
Natural language generation,Text summarization,Reasoning,Riddle Sense,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",68.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Riddle Sense,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",85.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,SIQA,BIG-bench,2021-12,"Gopher-280B (zero-shot) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",50.6,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Timedial,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",50.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Timedial,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",68.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Type prediction,Py150,2020-03,DFSud model in \'Code Prediction by Feeding Trees to Transformers\',98.7,MRR,pos
Natural language generation,Text summarization,Reasoning,Type prediction,DeepTyper,2020-07,ContraCode model in \'Contrastive Code Representation Learning\',84.6,Accuracy@5,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',57.52,Average Accuracy,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',59.84,Average Accuracy,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-02,CodeBERT model in \'CodeBERT: A Pre-Trained Model for Programming and Natural Languages\',61.72,Average Accuracy,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-09,GraphCodeBERT model in \'GraphCodeBERT: Pre-training Code Representations with Data Flow\',62.51,Average Accuracy,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',54.18,Average Precision,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',57.45,Average Precision,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-02,CodeBERT model in \'CodeBERT: A Pre-Trained Model for Programming and Natural Languages\',59.34,Average Precision,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-09,GraphCodeBERT model in \'GraphCodeBERT: Pre-training Code Representations with Data Flow\',60.06,Average Precision,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',54.02,Average Recall,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',57.62,Average Recall,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-02,CodeBERT model in \'CodeBERT: A Pre-Trained Model for Programming and Natural Languages\',59.8,Average Recall,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-09,GraphCodeBERT model in \'GraphCodeBERT: Pre-training Code Representations with Data Flow\',61.08,Average Recall,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',54.1,Average F1,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',57.54,Average F1,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-02,CodeBERT model in \'CodeBERT: A Pre-Trained Model for Programming and Natural Languages\',59.57,Average F1,pos
Natural language generation,Text summarization,Reasoning,Type prediction,ManyTypes4TypeScript,2020-09,GraphCodeBERT model in \'GraphCodeBERT: Pre-training Code Representations with Data Flow\',60.57,Average F1,pos
Natural language generation,Text summarization,Reasoning,Understanding Fables,BIG-bench,2021-12,"Gopher-280B (few-shot, k=5) model in \'Scaling Language Models: Methods, Analysis & Insights from Training Gopher\'",39.6,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Understanding Fables,BIG-bench,2022-03,"Chinchilla-70B (few-shot, k=5) model in \'Training Compute-Optimal Large Language Models\'",60.3,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Value prediction,Py150,2020-03,DFSud model in \'Code Prediction by Feeding Trees to Transformers\',73.6,MRR,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE val,2019-01,EVE-ROI* model in \'Visual Entailment: A Novel Task for Fine-Grained Image Understanding\',70.81,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE val,2019-09,UNITER model in \'UNITER: UNiversal Image-TExt Representation Learning\',78.98,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE val,2020-06,VILLA-LARGE model in \'Large-Scale Adversarial Training for Vision-and-Language Representation Learning\',80.18,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE val,2021-04,SOHO model in \'Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\',85.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE val,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',86.21,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE val,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",91.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE test,2019-01,EVE-ROI* model in \'Visual Entailment: A Novel Task for Fine-Grained Image Understanding\',70.47,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE test,2019-09,UNITER (Large) model in \'UNITER: UNiversal Image-TExt Representation Learning\',78.98,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE test,2021-04,SOHO model in \'Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\',84.95,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE test,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',86.32,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual Entailment,SNLI-VE test,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",91.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,VCR (QA-R) dev,2022-05,PEVL model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',76.4,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,VCR (Q-A) test,2022-05,PEVL model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',76.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,VCR (Q-A) dev,2022-05,PEVL model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',75.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,VCR (Q-AR) dev,2022-05,PEVL model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',57.8,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,VCR (Q-AR) test,2022-05,PEVL model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',58.6,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,VCR (QA-R) test,2022-05,PEVL model in \'PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models\',76.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,GD-VCR,2021-09,Human model in \'Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning\',88.84,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual commonsense reasoning,GD-VCR,2021-09,ViLBERT model in \'Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning\',-7.28,Gap (West),pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',67.4,Accuracy (Dev),pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',67.0,Accuracy (Test-P),pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR,2019-08,VisualBERT model in \'VisualBERT: A Simple and Performant Baseline for Vision and Language\',67.3,Accuracy (Test-U),pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,PHYRE-1B-Within,2019-08,DQN model in \'PHYRE: A New Benchmark for Physical Reasoning\',77.6,AUCCESS,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,PHYRE-1B-Within,2020-06,Dec[Joint]1f  model in \'Forward Prediction for Physical Reasoning\',80.0,AUCCESS,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,PHYRE-1B-Within,2020-08,RPIN model in \'Learning Long-term Visual Dynamics with Region Proposal Interaction Networks\',85.2,AUCCESS,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Test,2019-08,LXMERT model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\',76.2,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Test,2019-09,UNITER (Large) model in \'UNITER: UNiversal Image-TExt Representation Learning\',79.5,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Test,2021-07,ALBEF (14M) model in \'Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\',82.55,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Test,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',85.15,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Test,2021-11,VLMo model in \'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts\',86.86,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Test,2022-05,CoCa model in \'CoCa: Contrastive Captioners are Image-Text Foundation Models\',87.0,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Dev,2019-08,LXMERT (Pre-train + scratch) model in \'LXMERT: Learning Cross-Modality Encoder Representations from Transformers\',74.9,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Dev,2021-02,ViLT-B/32 model in \'ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\',75.7,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Dev,2021-04,SOHO model in \'Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\',76.37,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Dev,2021-07,ALBEF (14M) model in \'Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\',83.14,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Dev,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',84.53,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Dev,2021-11,VLMo model in \'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts\',85.64,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,NLVR2 Dev,2022-05,CoCa model in \'CoCa: Contrastive Captioners are Image-Text Foundation Models\',86.1,Accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,VSR,2022-04,LXMERT model in \'Visual Spatial Reasoning\',72.5,accuracy,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,PHYRE-1B-Cross,2019-08,DQN model in \'PHYRE: A New Benchmark for Physical Reasoning\',36.8,AUCCESS,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,PHYRE-1B-Cross,2020-06,Dec[Joint]1f model in \'Forward Prediction for Physical Reasoning\',40.3,AUCCESS,pos
Natural language generation,Text summarization,Reasoning,Visual reasoning,PHYRE-1B-Cross,2020-08,RPIN model in \'Learning Long-term Visual Dynamics with Region Proposal Interaction Networks\',42.2,AUCCESS,pos
Natural language generation,Text summarization,Sentence summarization,Sentence compression,Google Dataset,2015-09,LSTM model in \'Sentence Compression by Deletion with LSTMs\',0.82,F1,pos
Natural language generation,Text summarization,Sentence summarization,Sentence compression,Google Dataset,2018-06,Higher-Order Syntactic Attention Network model in \'Higher-Order Syntactic Attention Network for Longer Sentence Compression\',0.835,F1,pos
Natural language generation,Text summarization,Sentence summarization,Sentence compression,Google Dataset,2018-07,BiRNN + LM Evaluator model in \'A Language Model based Evaluator for Sentence Compression\',0.851,F1,pos
Natural language generation,Text summarization,Sentence summarization,Sentence compression,Google Dataset,2020-02,SLAHAN (LSTM+syntactic-information) model in \'Syntactically Look-Ahead Attention Network for Sentence Compression\',0.855,F1,pos
Natural language generation,Text summarization,Sentence summarization,Sentence compression,Google Dataset,2015-09,LSTM model in \'Sentence Compression by Deletion with LSTMs\',0.38,CR,pos
Natural language generation,Text summarization,Sentence summarization,Sentence compression,Google Dataset,2017-07,BiLSTM model in \'Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains\',0.43,CR,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BigPatent,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',50.01,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BigPatent,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',70.76,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BigPatent,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',60.64,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BigPatent,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',76.87,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BigPatent,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',42.46,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BigPatent,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',66.06,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BillSum,2019-10,Longformer Encoder Decoder model in \'BillSum: A Corpus for Automatic Summarization of US Legislation\',38.65,rouge1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2015-09,Abs+ model in \'A Neural Attention Model for Abstractive Sentence Summarization\',23.81,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2016-02,words-lvt5k-1sent model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',25.24,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2016-12,EndDec+WFE model in \'Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization\',27.8,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble model in \'Positional Encoding to Control Output Sequence Length\',28.52,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2015-09,Abs+ model in \'A Neural Attention Model for Abstractive Sentence Summarization\',28.18,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2016-02,words-lvt5k-1sent model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',28.61,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2016-06,RAS-Elman model in \'Abstractive Sentence Summarization with Attentive Recurrent Neural Networks\',28.97,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2016-12,EndDec+WFE model in \'Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization\',32.28,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble model in \'Positional Encoding to Control Output Sequence Length\',32.85,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2021-04,Transformer+WDrop model in \'Rethinking Perturbations in Encoder-Decoders for Fast Training\',33.06,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2015-09,Abs+ model in \'A Neural Attention Model for Abstractive Sentence Summarization\',8.49,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2016-02,words-lvt5k-1sent model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',9.42,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2016-12,EndDec+WFE model in \'Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization\',10.54,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2017-08,DRGD model in \'Deep Recurrent Generative Decoder for Abstractive Text Summarization\',10.75,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2018-05,Reinforced-Topic-ConvS2S model in \'A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization\',10.85,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble model in \'Positional Encoding to Control Output Sequence Length\',11.78,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,DUC 2004 Task 1,2019-05,Transformer LM model in \'Sample Efficient Text Summarization Using a Single Pre-Trained Transformer\',17.74,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Webis-Snippet-20 Corpus,2020-02,Anchor-context + Query biased model in \'Abstractive Snippet Generation\',20.1,Rouge-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Webis-Snippet-20 Corpus,2020-02,Anchor-context + Query biased model in \'Abstractive Snippet Generation\',25.7,Rouge-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Webis-Snippet-20 Corpus,2020-02,Anchor-context + Query biased model in \'Abstractive Snippet Generation\',5.2,Rouge-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2018-10,Pointer-generator + coverage model in \'WikiHow: A Large Scale Text Summarization Dataset\',26.54,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2020-04,MatchSum (BERT-base) model in \'Extractive Summarization as Text Matching\',29.58,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2020-08,BertSum model in \'Abstractive Summarization of Spoken andWritten Instructions with BERT\',34.82,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2018-10,Pointer-generator + coverage model in \'WikiHow: A Large Scale Text Summarization Dataset\',28.53,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2020-04,MatchSum (BERT-base) model in \'Extractive Summarization as Text Matching\',31.85,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2020-08,BertSum model in \'Abstractive Summarization of Spoken andWritten Instructions with BERT\',35.91,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2018-10,Pointer-generator + coverage model in \'WikiHow: A Large Scale Text Summarization Dataset\',9.23,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2020-08,BertSum model in \'Abstractive Summarization of Spoken andWritten Instructions with BERT\',13.9,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,WikiHow,2020-08,BertSum model in \'Abstractive Summarization of Spoken andWritten Instructions with BERT\',29.8,Content F1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2016-02,words-lvt2k-temp-att model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',32.65,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2016-11,Lead-3 baseline model in \'SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\',35.5,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2017-05,"ML+RL, with intra-attention model in \'A Deep Reinforced Model for Abstractive Summarization\'",36.9,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2018-04,RNES w/o coherence model in \'Learning to Extract Coherent Summary via Deep Reinforcement Learning\',37.75,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2016-02,words-lvt2k-temp-att model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',35.46,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2016-11,SummaRuNNer model in \'SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\',39.6,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2017-05,"ML+RL, with intra-attention model in \'A Deep Reinforced Model for Abstractive Summarization\'",39.87,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2017-11,GAN model in \'Generative Adversarial Network for Abstractive Text Summarization\',39.92,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2018-04,RNES w/o coherence model in \'Learning to Extract Coherent Summary via Deep Reinforcement Learning\',41.25,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2018-05,HSSAS model in \'A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS)\',42.3,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2016-02,words-lvt2k-temp-att model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',13.3,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2016-11,SummaRuNNer model in \'SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents\',16.2,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2017-11,GAN model in \'Generative Adversarial Network for Abstractive Text Summarization\',17.65,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CNN / Daily Mail (Anonymized),2018-04,RNES w/o coherence model in \'Learning to Extract Coherent Summary via Deep Reinforcement Learning\',18.87,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2022-03,PEGASUS + SummaReranker model in \'SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization\',40.0,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2018-08,"T-ConvS2S model in \'Don\'t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization\'",31.89,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-08,BertSumExtAbs model in \'Text Summarization with Pretrained Encoders\',38.81,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-10,"BART model in \'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\'",45.14,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-12,PEGASUSLARGE model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',47.21,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2022-03,BRIO model in \'BRIO: Bringing Order to Abstractive Summarization\',49.07,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2018-08,"T-ConvS2S model in \'Don\'t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization\'",11.54,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-08,BertSumExtAbs model in \'Text Summarization with Pretrained Encoders\',16.5,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-10,"BART model in \'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\'",22.27,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-12,PEGASUSLARGE model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',24.56,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2022-03,BRIO model in \'BRIO: Bringing Order to Abstractive Summarization\',25.59,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2018-08,"T-ConvS2S model in \'Don\'t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization\'",25.75,ROUGE-3,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-08,BertSumExtAbs model in \'Text Summarization with Pretrained Encoders\',31.27,ROUGE-3,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2019-10,"BART model in \'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\'",37.25,ROUGE-3,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2022-03,BRIO model in \'BRIO: Bringing Order to Abstractive Summarization\',40.4,ROUGE-3,pos
Natural language generation,Text summarization,Text summarization,Text summarization,X-Sum,2021-10,Control Prefixes (BART) model in \'Control Prefixes for Parameter-Efficient Text Generation\',0.51,"Human Overall (GENIE, External)",pos
Natural language generation,Text summarization,Text summarization,Text summarization,OrangeSum,2020-10,mBARThez (OrangeSum abstract) model in \'BARThez: a Skilled Pretrained French Sequence-to-Sequence Model\',32.67,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2020-04,DANCER PEGASUS model in \'A Divide-and-Conquer Approach to the Summarization of Long Documents\',40.56,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',41.77,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-04,HAT-BART model in \'Hierarchical Learning for Generation with Long Source Sequences\',42.2,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',44.27,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2022-03,Top Down Transformer (AdaPool) (464M) model in \'Long Document Summarization with Top-down and Bottom-up Inference\',45.61,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2017-04,Pntr-Gen-Seq2Seq model in \'Get To The Point: Summarization with Pointer-Generator Networks\',32.06,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2018-04,Discourse model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',35.8,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2019-09,ExtSum-LG model in \'Extractive Summarization of Long Documents by Combining Global and Local Context\',43.58,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',44.67,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2020-04,DANCER PEGASUS model in \'A Divide-and-Conquer Approach to the Summarization of Long Documents\',45.01,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',46.63,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-04,HAT-BART model in \'Hierarchical Learning for Generation with Long Source Sequences\',46.74,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-11,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',47.15,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',48.35,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2022-03,Top Down Transformer (AdaPool) (464M) model in \'Long Document Summarization with Top-down and Bottom-up Inference\',50.95,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2019-09,ExtSum-LG model in \'Extractive Summarization of Long Documents by Combining Global and Local Context\',17.37,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2020-04,DANCER PEGASUS model in \'A Divide-and-Conquer Approach to the Summarization of Long Documents\',17.6,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',19.02,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-04,HAT-BART model in \'Hierarchical Learning for Generation with Long Source Sequences\',19.19,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-11,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',19.99,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',21.92,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv,2022-03,Top Down Transformer (AdaPool) (464M) model in \'Long Document Summarization with Top-down and Bottom-up Inference\',21.93,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2020-04,DANCER PEGASUS model in \'A Divide-and-Conquer Approach to the Summarization of Long Documents\',42.42,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',46.67,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2017-04,Pntr-Gen-Seq2Seq model in \'Get To The Point: Summarization with Pointer-Generator Networks\',35.86,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2018-04,Discourse model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',38.93,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2019-09,Sent-CLF model in \'On Extractive and Abstractive Neural Document Summarization with Transformer Language Models\',45.01,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',45.09,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2020-04,DANCER PEGASUS model in \'A Divide-and-Conquer Approach to the Summarization of Long Documents\',46.34,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2020-09,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',47.81,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2021-04,HAT-BART model in \'Hierarchical Learning for Generation with Long Source Sequences\',48.25,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',50.23,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2022-03,Top Down Transformer (AdaPool) (464M) model in \'Long Document Summarization with Top-down and Bottom-up Inference\',51.05,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2019-09,ExtSum-LG model in \'Extractive Summarization of Long Documents by Combining Global and Local Context\',19.74,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2020-04,DANCER PEGASUS model in \'A Divide-and-Conquer Approach to the Summarization of Long Documents\',19.97,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2020-07,BigBird-Pegasus model in \'Big Bird: Transformers for Longer Sequences\',20.65,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2020-09,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',21.14,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2021-04,HAT-BART model in \'Hierarchical Learning for Generation with Long Source Sequences\',21.35,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Pubmed,2021-12,LongT5 model in \'LongT5: Efficient Text-To-Text Transformer for Long Sequences\',24.76,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',20.13,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2020-08,BART+R3F model in \'Better Fine-Tuning by Reducing Representational Collapse\',24.74,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2021-01,MUPPET BART Large model in \'Muppet: Massive Multi-task Representations with Pre-Finetuning\',24.92,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',25.09,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2020-08,BART+R3F model in \'Better Fine-Tuning by Reducing Representational Collapse\',30.31,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',6.17,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2020-08,BART+R3F model in \'Better Fine-Tuning by Reducing Representational Collapse\',10.98,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Reddit TIFU,2021-01,MUPPET BART Large model in \'Muppet: Massive Multi-task Representations with Pre-Finetuning\',11.25,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Gazeta,2020-06,Finetuned mBART model in \'Dataset for Automatic Summarization of Russian News\',12.4,BLEU,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Gazeta,2020-06,Finetuned mBART model in \'Dataset for Automatic Summarization of Russian News\',27.9,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Gazeta,2020-06,Finetuned mBART model in \'Dataset for Automatic Summarization of Russian News\',32.1,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Gazeta,2020-06,Finetuned mBART model in \'Dataset for Automatic Summarization of Russian News\',14.2,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Gazeta,2020-06,Finetuned mBART model in \'Dataset for Automatic Summarization of Russian News\',25.7,Meteor,pos
Natural language generation,Text summarization,Text summarization,Text summarization,SAMSum Corpus,2021-04,HAT-CNNDM RL model in \'Hierarchical Learning for Generation with Long Source Sequences\',48.84,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,SAMSum Corpus,2021-04,HAT-CNNDM model in \'Hierarchical Learning for Generation with Long Source Sequences\',53.01,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,SAMSum Corpus,2021-09,ConDigSum model in \'Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization\',54.3,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,SAMSum Corpus,2021-04,HAT-CNNDM model in \'Hierarchical Learning for Generation with Long Source Sequences\',28.27,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,SAMSum Corpus,2021-09,ConDigSum model in \'Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization\',29.3,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,SAMSum Corpus,2021-09,ConDigSum model in \'Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization\',54.0,BertScoreF1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,AMI,2021-04,HAT-CNNDM model in \'Hierarchical Learning for Generation with Long Source Sequences\',50.57,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,AMI,2021-04,HAT-CNNDM model in \'Hierarchical Learning for Generation with Long Source Sequences\',52.27,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,AMI,2021-04,HAT-CNNDM model in \'Hierarchical Learning for Generation with Long Source Sequences\',20.15,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,How2,2019-06,Ground-truth transcript + Action with Hierarchical Attn model in \'Multimodal Abstractive Summarization for How2 Videos\',54.9,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,How2,2020-08,BertSum model in \'Abstractive Summarization of Spoken andWritten Instructions with BERT\',48.26,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,How2,2019-06,Ground-truth transcript + Action with Hierarchical Attn model in \'Multimodal Abstractive Summarization for How2 Videos\',48.9,Content F1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv Summarization Dataset,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',42.6,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv Summarization Dataset,2020-09,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',47.15,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv Summarization Dataset,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',47.6,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv Summarization Dataset,2020-09,DeepPyramidion model in \'Sparsifying Transformer Models with Trainable Representation Pooling\',19.99,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,arXiv Summarization Dataset,2021-10,PRIMER model in \'PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization\',20.8,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BBC XSum,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',18.41,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BBC XSum,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',24.86,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,BBC XSum,2020-04,MatchSum model in \'Extractive Summarization as Text Matching\',4.66,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord-10k,2020-01,ERNIE-GENLARGE (large-scale text corpora) model in \'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\',33.23,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord-10k,2020-01,ERNIE-GENLARGE (large-scale text corpora) model in \'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\',35.51,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord-10k,2020-01,ERNIE-GENLARGE (large-scale text corpora) model in \'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\',16.79,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2016-02,words-lvt5k-1sent model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',33.71,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2016-12,EndDec+WFE model in \'Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization\',33.88,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2017-06,Transformer model in \'Attention Is All You Need\',34.69,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2018-06,Seq2seq + E2T_cnn model in \'Entity Commonsense Representation for Neural Abstractive Summarization\',34.93,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',36.0,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2019-06,BiSET model in \'BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization\',36.87,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2021-04,Transformer+Rep(Uni) model in \'Rethinking Perturbations in Encoder-Decoders for Fast Training\',36.93,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",37.11,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2015-09,Abs+ model in \'A Neural Attention Model for Abstractive Sentence Summarization\',31.0,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2016-02,words-lvt5k-1sent model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',36.4,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2017-06,Transformer model in \'Attention Is All You Need\',37.57,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',38.9,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2019-06,BiSET model in \'BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization\',39.11,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2019-12,PEGASUS model in \'PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\',39.12,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2020-01,ProphetNet model in \'ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training\',39.51,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2020-08,BART-RXF model in \'Better Fine-Tuning by Reducing Representational Collapse\',40.45,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2016-02,words-lvt5k-1sent model in \'Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\',17.7,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2017-06,Transformer model in \'Attention Is All You Need\',18.9,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2018-07,"Re^3 Sum model in \'Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization\'",19.03,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2019-05,UniLM model in \'Unified Language Model Pre-training for Natural Language Understanding and Generation\',20.05,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2019-11,ControlCopying model in \'Controlling the Amount of Verbatim Copying in Abstractive Summarization\',20.47,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,GigaWord,2020-08,BART-RXF model in \'Better Fine-Tuning by Reducing Representational Collapse\',20.69,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,CL-SciSumm,2019-09,GCN Hybrid model in \'ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks\',33.88,ROUGE-2,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Klexikon,2022-01,Lead-k model in \'Klexikon: A German Dataset for Joint Summarization and Simplification\',12.1,ROUGE-L,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Klexikon,2022-01,Luhn\'s algorithm (25 sentences) model in \'Klexikon: A German Dataset for Joint Summarization and Simplification\',32.0,ROUGE-1,pos
Natural language generation,Text summarization,Text summarization,Text summarization,Klexikon,2022-01,Luhn\'s algorithm (25 sentences) model in \'Klexikon: A German Dataset for Joint Summarization and Simplification\',5.63,ROUGE-2,pos
Natural language generation,Text summarization,Unsupervised Opinion Summarization,Unsupervised Opinion Summarization,Yelp,2021-04,BiMeanVAE - Coop model in \'Convex Aggregation for Opinion Summarization\',19.94,ROUGE-L,pos
Natural language generation,Text summarization,Unsupervised Opinion Summarization,Unsupervised Opinion Summarization,Yelp,2021-04,BiMeanVAE - Coop model in \'Convex Aggregation for Opinion Summarization\',35.37,ROUGE-1,pos
Natural language generation,Text summarization,Unsupervised Opinion Summarization,Unsupervised Opinion Summarization,Yelp,2021-04,BiMeanVAE - Coop model in \'Convex Aggregation for Opinion Summarization\',7.35,ROUGE-2,pos
Natural language generation,Text summarization,Unsupervised Opinion Summarization,Unsupervised Opinion Summarization,Amazon,2021-04,BiMeanVAE - Coop model in \'Convex Aggregation for Opinion Summarization\',21.24,ROUGE-L,pos
Natural language generation,Text summarization,Unsupervised Opinion Summarization,Unsupervised Opinion Summarization,Amazon,2021-04,BiMeanVAE - Coop model in \'Convex Aggregation for Opinion Summarization\',36.57,ROUGE-1,pos
Natural language generation,Text summarization,Unsupervised Opinion Summarization,Unsupervised Opinion Summarization,Amazon,2021-04,BiMeanVAE - Coop model in \'Convex Aggregation for Opinion Summarization\',7.23,ROUGE-2,pos
Natural language generation,Text-to-speech synthesis,Prosody prediction,Prosody prediction,Helsinki Prosody Corpus,2019-08,BERT model in \'Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations\',83.2,Accuracy,pos
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,LJSpeech,2018-09,Transformer TTS (Mel + WaveGlow) model in \'Neural Speech Synthesis with Transformer Network\',3.88,Audio Quality MOS,pos
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,LJSpeech,2020-05,Glow-TTS + HiFiGAN model in \'Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search\',4.34,Audio Quality MOS,pos
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,LJSpeech,2021-05,Grad-TTS + HiFiGAN (1000 steps) model in \'Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech\',4.37,Audio Quality MOS,pos
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,LJSpeech,2022-05,NaturalSpeech model in \'NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality\',4.56,Audio Quality MOS,pos
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,LJSpeech,2020-05,Flowtron model in \'Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis\',3.665,Pleasantness MOS,pos
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,20000 utterances,2020-10,Mia model in \'MIA-Prognosis: A Deep Learning Framework to Predict Therapy Response\',16.0,10-keyword Speech Commands dataset,pos
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,CMUDict 0.7b,2019-04,Token-Level Ensemble Distillation model in \'Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion\',-19.88,Word Error Rate (WER),neg
Natural language generation,Text-to-speech synthesis,Text-to-speech synthesis,Text-to-speech synthesis,CMUDict 0.7b,2019-04,Token-Level Ensemble Distillation model in \'Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion\',4.6,Phoneme Error Rate,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Crackling Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.93,Accuracy,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Crackling Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.93,F1 Score,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,VOXLINGUA107,2020-11,Noisy model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-7.1,Average,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,VOXLINGUA107,2020-11,Noisy model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-12.3,0..5sec,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,VOXLINGUA107,2020-11,Noisy model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-6.1,5..20sec,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,VoxForge Commonwealth,2019-10,2D ConvNet(MixUp=YES) model in \'Spoken Language Identification using ConvNets\',95.4,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (White Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.91,Accuracy,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (White Noise),2021-10,CRNN model in \'Is Attention always needed? A Case Study on Language Identification from Speech\',0.912,Accuracy,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (White Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.91,F1 Score,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (No Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.96,Accuracy,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (No Noise),2021-10,CRNN model in \'Is Attention always needed? A Case Study on Language Identification from Speech\',0.967,Accuracy,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (No Noise),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.96,F1 Score,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,VoxForge European,2019-10,2D ConvNet(MixUp=YES) model in \'Spoken Language Identification using ConvNets\',96.3,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,CNN-LDE model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-4.0,Average,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,CNN-LDE model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-8.25,3 sec,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,Kaldi i-vector model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',11.93,10 sec,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,LRE07,2020-11,Kaldi i-vector model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',4.52,30 sec,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.041,PC,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.022,EC,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.058,EO,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,KALAKA-3,2020-11,Model on the automatically filtered (cleaned) data model in \'VOXLINGUA107: A DATASET FOR SPOKEN LANGUAGE RECOGNITION\',-0.056,PO,neg
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,Untranscribed mixed-speech dataset,2015-09,SVM model in \'Automatic Dialect Detection in Arabic Broadcast Speech\',45.2,ACC,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,Untranscribed mixed-speech dataset,2015-09,SVM model in \'Automatic Dialect Detection in Arabic Broadcast Speech\',44.8,PRC,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,Untranscribed mixed-speech dataset,2015-09,Naive Bayes model in \'Automatic Dialect Detection in Arabic Broadcast Speech\',50.2,RCL,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Background Music),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.89,Accuracy,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,YouTube News dataset (Background Music),2017-08,Inception-v3 CRNN model in \'Language Identification Using Deep Convolutional Recurrent Neural Networks\',0.89,F1 Score,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language identification,IndicTTS,2021-10,CRNN model in \'Is Attention always needed? A Case Study on Language Identification from Speech\',0.987,Classification Accuracy,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Spoken-SQuAD,2018-04,Baseline model in \'Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension\',58.71,F1 score,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Spoken-SQuAD,2019-04,QANet + GAN model in \'Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation\',63.11,F1 score,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Spoken-SQuAD,2019-10,SpeechBERT model in \'SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering\',71.75,F1 score,pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartLights,2018-10,Snips model in \'Spoken Language Understanding on the Edge\',84.2,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartLights,2020-12,AT-AT model in \'Exploring Transfer Learning For End-to-End Spoken Language Understanding\',84.9,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartLights,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',88.0,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Timers and Such,2021-04,Baseline model in \'Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers\',81.6,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Timers and Such,2021-06,"wav2vec 2.0 \""960 Hr\"" model in \'SpeechBrain: A General-Purpose Speech Toolkit\'",94.0,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Timers and Such,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',95.4,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2018-10,Snips model in \'Spoken Language Understanding on the Edge\',68.7,Accuracy-EN (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',80.4,Accuracy-EN (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2018-10,Snips model in \'Spoken Language Understanding on the Edge\',75.1,Accuracy-FR (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Snips-SmartSpeaker,2022-06,Finstreder (Conformer) model in \'Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models\',78.3,Accuracy-FR (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Fluent Speech Commands,2019-04,Pooling classifier pre-trained using force-aligned phoneme and word labels on LibriSpeech model in \'Speech Model Pre-training for End-to-End Spoken Language Understanding\',98.8,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Fluent Speech Commands,2020-08,Reptile model in \'Improving End-to-End Speech-to-Intent Classification with Reptile\',99.2,Accuracy (%),pos
Natural language understanding,Dialog understanding,Spoken language understanding,Spoken language understanding,Fluent Speech Commands,2020-10,textual-kd-slu model in \'Two-stage Textual Knowledge Distillation for End-to-End Spoken Language Understanding\',99.7,Accuracy (%),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2009-08,HILDA Parser model in \'A Novel Discourse Parser Based on Support Vector Machine Classification\',83.0,RST-Parseval (Span),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2014-06,Bottom-up Linear-chain CRF-based Parser model in \'A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing\',85.7,RST-Parseval (Span),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2017-07,Two-stage Parser model in \'A Two-Stage Parsing Method for Text-Level Discourse Analysis\',86.0,RST-Parseval (Span),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2020-04,Top-down Span-based Parser model in \'Top-Down RST Parsing Utilizing Granularity Levels in Documents\',87.0,RST-Parseval (Span),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2021-06,Top-down Span-based Parser with Silver Agreement Subtrees (ensemble) model in \'Improving Neural RST Parsing Model with Silver Agreement Subtrees\',87.1,RST-Parseval (Span),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2009-08,HILDA Parser model in \'A Novel Discourse Parser Based on Support Vector Machine Classification\',68.4,RST-Parseval (Nuclearity),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2014-06,Bottom-up Linear-chain CRF-based Parser model in \'A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing\',71.0,RST-Parseval (Nuclearity),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2017-07,Two-stage Parser model in \'A Two-Stage Parsing Method for Text-Level Discourse Analysis\',72.4,RST-Parseval (Nuclearity),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2018-08,Transition-based Parser with Implicit Syntax Features model in \'Transition-based Neural RST Parsing with Implicit Syntax Features\',73.1,RST-Parseval (Nuclearity),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2020-04,Top-down Span-based Parser model in \'Top-Down RST Parsing Utilizing Granularity Levels in Documents\',74.6,RST-Parseval (Nuclearity),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2021-06,Top-down Span-based Parser with Silver Agreement Subtrees (ensemble) model in \'Improving Neural RST Parsing Model with Silver Agreement Subtrees\',75.0,RST-Parseval (Nuclearity),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2009-08,HILDA Parser model in \'A Novel Discourse Parser Based on Support Vector Machine Classification\',55.3,RST-Parseval (Relation),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2014-06,Bottom-up Linear-chain CRF-based Parser model in \'A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing\',58.2,RST-Parseval (Relation),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2017-07,Two-stage Parser model in \'A Two-Stage Parsing Method for Text-Level Discourse Analysis\',59.7,RST-Parseval (Relation),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2018-08,Transition-based Parser with Implicit Syntax Features model in \'Transition-based Neural RST Parsing with Implicit Syntax Features\',60.2,RST-Parseval (Relation),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2021-06,Top-down Span-based Parser with Silver Agreement Subtrees (ensemble) model in \'Improving Neural RST Parsing Model with Silver Agreement Subtrees\',63.2,RST-Parseval (Relation),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2009-08,HILDA Parser model in \'A Novel Discourse Parser Based on Support Vector Machine Classification\',54.8,RST-Parseval (Full),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2018-08,Transition-based Parser with Implicit Syntax Features model in \'Transition-based Neural RST Parsing with Implicit Syntax Features\',59.9,RST-Parseval (Full),pos
Natural language understanding,Discourse parsing,Discourse parsing,Discourse parsing,RST-DT,2021-06,Top-down Span-based Parser with Silver Agreement Subtrees (ensemble) model in \'Improving Neural RST Parsing Model with Silver Agreement Subtrees\',62.6,RST-Parseval (Full),pos
Natural language understanding,Inference and reasoning,Natural language inference,Answer Generation,CICERO,2022-03,T5-large model in \'CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues\',0.2947,ROUGE,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI,2020-10,Decoupled model in \'Rethinking embedding coupling in pre-trained language models\',71.3,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI,2021-05,ByT5 XXL model in \'ByT5: Towards a token-free future with pre-trained byte-to-byte models\',83.7,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2017-05,X-BiLSTM model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',68.7,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',74.3,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-Spanish,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',85.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-French,2017-05,X-BiLSTM model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',67.7,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-French,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',84.7,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2017-05,X-CBOW model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',61.0,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',70.5,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Cross-lingual natural language inference,XNLI Zero-Shot English-to-German,2020-08,XLM-R R4F model in \'Better Fine-Tuning by Reducing Representational Collapse\',84.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Crowdsourced text aggregation,CrowdSpeech test-clean,2021-07,ROVER model in \'CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription\',-7.29,Word Error Rate (WER),neg
Natural language understanding,Inference and reasoning,Natural language inference,Crowdsourced text aggregation,CrowdSpeech test-other,2021-07,ROVER model in \'CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription\',-13.41,Word Error Rate (WER),neg
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SciTail,2017-12,"CAFE model in \'Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference\'",83.3,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SciTail,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',88.3,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SciTail,2018-10,BERT model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',92.0,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SciTail,2019-01,MT-DNN model in \'Multi-Task Deep Neural Networks for Natural Language Understanding\',94.1,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SciTail,2019-11,SMART-MT-DNN model in \'SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\',96.1,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SciTail,2020-09,CA-MTL model in \'Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data\',96.8,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI Chinese,2019-04,ERNIE model in \'ERNIE: Enhanced Representation through Knowledge Integration\',78.4,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI Chinese,2019-07,ERNIE 2.0 Large model in \'ERNIE 2.0: A Continual Pre-training Framework for Language Understanding\',81.0,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,TERRa,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.92,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI Dev,2019-09,TinyBERT (M=6;d\'=768;d\'i=3072) model in \'TinyBERT: Distilling BERT for Natural Language Understanding\',84.5,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI Dev,2019-09,TinyBERT (M=6;d\'=768;d\'i=3072) model in \'TinyBERT: Distilling BERT for Natural Language Understanding\',84.5,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MRPC,2021-11,DeBERTaV3large model in \'DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\',92.2,Acc,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI French,2018-09,BiLSTM-max model in \'XNLI: Evaluating Cross-lingual Sentence Representations\',68.3,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI French,2019-01,XLM (MLM+TLM) model in \'Cross-lingual Language Model Pretraining\',80.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI French,2019-11,CamemBERT model in \'CamemBERT: a Tasty French Language Model\',81.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI French,2019-12,FlauBERT (large) model in \'FlauBERT: Unsupervised Language Model Pre-training for French\',83.4,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SICK,2021-05,NeuralLog model in \'NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning\',0.903,1:1 Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,V-SNLI,2018-06,V-BiMPM model in \'Grounded Textual Entailment\',86.99,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,V-SNLI,2019-09,MMBT model in \'Supervised Multimodal Bitransformers for Classifying Images and Text\',90.5,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MED,2021-05,NeuralLog model in \'NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning\',0.934,1:1 Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-03,GenSen model in \'Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\',71.4,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-04,Multi-task BiLSTM + Attn model in \'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\',72.2,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',82.1,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-10,Snorkel MeTaL (ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',87.6,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-04,MT-DNN-ensemble model in \'Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding\',87.9,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',90.8,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-08,Adv-RoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',91.1,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-09,ALBERT model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',91.3,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',92.0,Matched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-03,GenSen model in \'Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\',71.3,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-04,Multi-task BiLSTM + Attn model in \'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\',72.1,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-06,Finetuned Transformer LM model in \'Improving Language Understanding by Generative Pre-Training\',81.4,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2018-10,Snorkel MeTaL (ensemble) model in \'Training Complex Models with Multi-Task Weak Supervision\',87.2,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-04,MT-DNN-ensemble model in \'Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding\',87.4,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',90.2,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-08,Adv-RoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',90.7,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MultiNLI,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',91.7,Mismatched,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,KUAKE-QQR,2021-06,BERT-base model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',84.7,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RCB,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.702,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RCB,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.68,Average F1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,FarsTail,2020-09,mBERT model in \'FarsTail: A Persian Natural Language Inference Dataset\',83.38,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,WNLI,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',92.5,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,WNLI,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',93.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,WNLI,2020-06,DeBERTa model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',94.5,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,Quora Question Pairs,2018-12,aESIM model in \'Attention Boosted Sequential Inference Model\',88.01,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MRPC Dev,2020-06,DeBERTa (large) model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',92.5,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,CommitmentBank,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',96.8,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,CommitmentBank,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',97.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,CommitmentBank,2022-04,PaLM 540B (finetuned) model in \'PaLM: Scaling Language Modeling with Pathways\',100.0,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,CommitmentBank,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',93.9,F1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,CommitmentBank,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',94.9,F1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,CommitmentBank,2022-04,PaLM 540B (finetuned) model in \'PaLM: Scaling Language Modeling with Pathways\',100.0,F1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',68.8,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',85.9,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',88.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2019-08,Adv-RoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',88.7,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2019-09,ALBERT model in \'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\',89.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2019-10,T5-11B model in \'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\',92.5,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2020-06,DeBERTa-1.5B model in \'DeBERTa: Decoding-enhanced BERT with Disentangled Attention\',93.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,RTE,2022-04,PaLM 540B (finetuned)  model in \'PaLM: Scaling Language Modeling with Pathways\',95.7,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,LiDiRus,2020-10,Human Benchmark model in \'RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark\',0.626,MCC,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XWINO,2022-04,mGPT model in \'mGPT: Few-Shot Learners Go Multilingual\',0.562,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,QNLI,2019-05,ERNIE model in \'ERNIE: Enhanced Language Representation with Informative Entities\',91.3,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,QNLI,2019-06,XLNet (single model) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',94.9,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,QNLI,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',98.9,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,QNLI,2019-08,StructBERTRoBERTa ensemble model in \'StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\',99.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,KUAKE-QTR,2021-06,MacBERT-large model in \'CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\',62.9,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2014-04,DCNN [[Blunsom et al.2014]] model in \'A Convolutional Neural Network for Modelling Sentences\',86.8,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2014-08,CNN-MC [[Kim2014]] model in \'Convolutional Neural Networks for Sentence Classification\',88.1,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2016-09,600D ESIM + 300D Syntactic TreeLSTM model in \'Enhanced LSTM for Natural Language Inference\',88.6,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2017-02,BiMPM Ensemble model in \'Bilateral Multi-Perspective Matching for Natural Language Sentences\',88.8,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2017-09,"448D Densely Interactive Inference Network (DIIN, code) Ensemble model in \'Natural Language Inference over Interaction Space\'",88.9,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2017-11,KIM Ensemble model in \'Neural Natural Language Inference Models Enhanced with External Knowledge\',89.1,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2017-12,"300D CAFE Ensemble model in \'Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference\'",89.3,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble model in \'Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information\',90.1,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2018-09,SJRC (BERT-Large +SRL) model in \'Explicit Contextual Semantics for Text Comprehension\',91.3,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2019-01,MT-DNN model in \'Multi-Task Deep Neural Networks for Natural Language Understanding\',91.6,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2019-09,SemBERT model in \'Semantics-aware BERT for Language Understanding\',91.9,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2020-09,CA-MTL model in \'Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data\',92.1,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2020-12,RoBERTa-large+Self-Explaining model in \'Self-Explaining Structures Improve NLP Models\',92.3,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2021-04,EFL model in \'Entailment as Few-Shot Learner\',93.1,% Test Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2015-08,+ Unigram and bigram features model in \'A large annotated corpus for learning natural language inference\',99.7,% Train Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2015-08,100D LSTM encoders model in \'A large annotated corpus for learning natural language inference\',220000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2015-09,100D LSTMs w/ word-by-word attention model in \'Reasoning about Entailment with Neural Attention\',250000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2015-11,1024D GRU encoders w/ unsupervised \'skip-thoughts\' pre-training model in \'Order-Embeddings of Images and Language\',15000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2017-05,4096D BiLSTM with max-pooling model in \'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\',40000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2017-11,KIM Ensemble model in \'Neural Natural Language Inference Models Enhanced with External Knowledge\',43000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2018-02,450D DR-BiLSTM Ensemble model in \'DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference\',45000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble model in \'Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information\',53300000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2018-06,Fine-Tuned LM-Pretrained Transformer model in \'Improving Language Understanding by Generative Pre-Training\',85000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2018-09,SJRC (BERT-Large +SRL) model in \'Explicit Contextual Semantics for Text Comprehension\',308000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2019-01,MT-DNN model in \'Multi-Task Deep Neural Networks for Natural Language Understanding\',330000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2019-09,SemBERT model in \'Semantics-aware BERT for Language Understanding\',339000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,SNLI,2020-09,CA-MTL model in \'Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data\',340000000.0,Parameters,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MedNLI,2019-08,BioBERT-MIMIC model in \'Saama Research at MEDIQA 2019: Pre-trained BioBERT with Attention Visualisation for Medical Natural Language Inference\',83.45,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MedNLI,2020-10,"CharacterBERT (base, medical) model in \'CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters\'",84.95,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MedNLI,2021-05,SciFive-large model in \'SciFive: a text-to-text transformer model for biomedical literature\',86.57,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,MedNLI,2019-06,NCBI_BERT(base) (P+M) model in \'Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets\',84.0,F1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI Chinese Dev,2019-04,ERNIE model in \'ERNIE: Enhanced Representation through Knowledge Integration\',79.9,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,XNLI Chinese Dev,2019-07,ERNIE 2.0 Large model in \'ERNIE 2.0: A Continual Pre-training Framework for Language Understanding\',82.6,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2019-06,XLNet (Large) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',70.3,A1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2019-07,RoBERTa (Large) model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',72.4,A1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2020-10,InfoBERT (RoBERTa) model in \'InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective\',75.0,A1,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2019-06,XLNet (Large) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',50.9,A2,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2020-04,ALUM (RoBERTa-LARGE) model in \'Adversarial Training for Large Neural Language Models\',52.1,A2,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2019-06,XLNet (Large) model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',49.4,A3,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2020-04,ALUM (RoBERTa-LARGE) model in \'Adversarial Training for Large Neural Language Models\',57.0,ANLI,pos
Natural language understanding,Inference and reasoning,Natural language inference,Natural language inference,ANLI test,2020-10,InfoBERT (RoBERTa) model in \'InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective\',58.3,ANLI,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE val,2019-01,EVE-ROI* model in \'Visual Entailment: A Novel Task for Fine-Grained Image Understanding\',70.81,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE val,2019-09,UNITER model in \'UNITER: UNiversal Image-TExt Representation Learning\',78.98,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE val,2020-06,VILLA-LARGE model in \'Large-Scale Adversarial Training for Vision-and-Language Representation Learning\',80.18,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE val,2021-04,SOHO model in \'Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\',85.0,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE val,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',86.21,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE val,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",91.0,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE test,2019-01,EVE-ROI* model in \'Visual Entailment: A Novel Task for Fine-Grained Image Understanding\',70.47,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE test,2019-09,UNITER (Large) model in \'UNITER: UNiversal Image-TExt Representation Learning\',78.98,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE test,2021-04,SOHO model in \'Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\',84.95,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE test,2021-08,SimVLM model in \'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\',86.32,Accuracy,pos
Natural language understanding,Inference and reasoning,Natural language inference,Visual Entailment,SNLI-VE test,2022-02,"OFA model in \'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework\'",91.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Machine reading comprehension,ReClor,2020-02,XLNet-large model in \'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\',56.0,Accuracy,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Machine reading comprehension,ReClor,2020-02,XLNet-large model in \'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\',75.7,Accuracy (easy),pos
Natural language understanding,Inference and reasoning,Reading comprehension,Machine reading comprehension,ReClor,2020-02,XLNet-large model in \'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\',40.5,Accuracy (hard),pos
Natural language understanding,Inference and reasoning,Reading comprehension,Machine reading comprehension,BIOMRC,2022-06,MLP-based-weighting (on BIOMRC Lite) model in \'Contextual embedding and model weighting by fusing domain knowledge on Biomedical Question Answering\',88.0,Acc,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Machine reading comprehension,UQuAD,2021-11,XLM-RoBERTa model in \'UQuAD1.0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension\',66.0,F1,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Machine reading comprehension,UQuAD,2021-11,BERT model in \'UQuAD1.0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension\',66.0,Exact Match,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2019-07,RoBERTa model in \'RoBERTa: A Robustly Optimized BERT Pretraining Approach\',83.2,Accuracy,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2019-09,Megatron-BERT (ensemble) model in \'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\',90.9,Accuracy,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2020-11,ALBERT (Ensemble) model in \'Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning\',91.4,Accuracy,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',84.0,Accuracy (High),pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2019-09,Megatron-BERT (ensemble) model in \'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\',90.0,Accuracy (High),pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2020-01,ALBERTxxlarge+DUMA(ensemble) model in \'DUMA: Reading Comprehension with Transposition Thinking\',92.6,Accuracy (High),pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2019-06,XLNet model in \'XLNet: Generalized Autoregressive Pretraining for Language Understanding\',88.6,Accuracy (Middle),pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,RACE,2019-09,Megatron-BERT (ensemble) model in \'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\',93.1,Accuracy (Middle),pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,ReClor,2020-02,XLNet-large model in \'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning\',56.0,Test,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,ReClor,2021-03,NAACL 2021 model in \'DAGN: Discourse-Aware Graph Network for Logical Reasoning\',58.2,Test,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,ReClor,2021-05,LReasoner ensemble model in \'Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text\',76.1,Test,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,ReClor,2022-03,MERIt(MERIt-deberta-v2-xxlarge ) model in \'MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning\',79.3,Test,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,AdversarialQA,2020-02,RoBERTa-Large model in \'Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension\',64.4,Overall: F1,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,AdversarialQA,2020-02,RoBERTa-Large model in \'Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension\',74.1,D(BiDAF): F1,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,AdversarialQA,2020-02,RoBERTa-Large model in \'Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension\',65.5,D(BERT): F1,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,AdversarialQA,2020-02,BERT-Large model in \'Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension\',54.4,D(RoBERTa): F1,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,CrowdSource QA,2020-02,BERT model in \'Predicting Subjective Features of Questions of QA Websites using BERT\',-0.046,MSE,neg
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,MuSeRC,2020-10,MT5 Large model in \'mT5: A massively multilingual pre-trained text-to-text transformer\',0.844,Average F1,pos
Natural language understanding,Inference and reasoning,Reading comprehension,Reading comprehension,MuSeRC,2020-10,MT5 Large model in \'mT5: A massively multilingual pre-trained text-to-text transformer\',0.543,EM,pos
Natural language understanding,Inference and reasoning,Relational reasoning,Relational reasoning,CLUTRR (k=3),2020-07,CTP A model in \'Learning Reasoning Strategies in End-to-End Differentiable Proving\',0.99,4 Hops,pos
Natural language understanding,Inference and reasoning,Relational reasoning,Relational reasoning,CLUTRR (k=3),2020-07,CTP A model in \'Learning Reasoning Strategies in End-to-End Differentiable Proving\',0.99,5 Hops,pos
Natural language understanding,Inference and reasoning,Relational reasoning,Relational reasoning,CLUTRR (k=3),2020-07,CTP A model in \'Learning Reasoning Strategies in End-to-End Differentiable Proving\',0.99,6 Hops,pos
Natural language understanding,Inference and reasoning,Relational reasoning,Relational reasoning,CLUTRR (k=3),2020-07,CTP A model in \'Learning Reasoning Strategies in End-to-End Differentiable Proving\',0.96,7 Hops,pos
Natural language understanding,Inference and reasoning,Relational reasoning,Relational reasoning,CLUTRR (k=3),2020-07,CTP A model in \'Learning Reasoning Strategies in End-to-End Differentiable Proving\',0.94,8 Hops,pos
Natural language understanding,Inference and reasoning,Relational reasoning,Relational reasoning,CLUTRR (k=3),2020-07,CTP A model in \'Learning Reasoning Strategies in End-to-End Differentiable Proving\',0.89,9 Hops,pos
Natural language understanding,Inference and reasoning,Relational reasoning,Relational reasoning,CLUTRR (k=3),2020-07,CTP A model in \'Learning Reasoning Strategies in End-to-End Differentiable Proving\',0.9,10 Hops,pos
Natural language understanding,Inference and reasoning,Table-based fact verification,Table-based fact verification,TabFact,2019-09,Table-BERT-Horizontal-T+F-Template model in \'TabFact: A Large-scale Dataset for Table-based Fact Verification\',66.1,Val,pos
Natural language understanding,Inference and reasoning,Table-based fact verification,Table-based fact verification,TabFact,2020-10,TAPAS-Large classifier with Counterfactual + Synthetic pre-training model in \'Understanding tables with intermediate pre-training\',81.0,Val,pos
Natural language understanding,Inference and reasoning,Table-based fact verification,Table-based fact verification,TabFact,2021-07,TAPEX-Large model in \'TAPEX: Table Pre-training via Learning a Neural SQL Executor\',84.6,Val,pos
Natural language understanding,Inference and reasoning,Table-based fact verification,Table-based fact verification,TabFact,2019-09,Table-BERT-Horizontal-T+F-Template model in \'TabFact: A Large-scale Dataset for Table-based Fact Verification\',65.12,Test,pos
Natural language understanding,Inference and reasoning,Table-based fact verification,Table-based fact verification,TabFact,2020-10,TAPAS-Large classifier with Counterfactual + Synthetic pre-training model in \'Understanding tables with intermediate pre-training\',81.0,Test,pos
Natural language understanding,Inference and reasoning,Table-based fact verification,Table-based fact verification,TabFact,2021-07,TAPEX-Large model in \'TAPEX: Table Pre-training via Learning a Neural SQL Executor\',84.2,Test,pos
Natural language understanding,Natural language understanding,Natural language understanding,Natural language understanding,WNLI,2018-10,BERTLARGE model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',65.1,Accuracy,pos
Natural language understanding,Natural language understanding,Natural language understanding,Natural language understanding,WNLI,2019-05,BERTWiki-WSCR model in \'A Surprisingly Robust Trick for Winograd Schema Challenge\',71.9,Accuracy,pos
Natural language understanding,Natural language understanding,Natural language understanding,Natural language understanding,WNLI,2019-07,HNN model in \'A Hybrid Neural Network Model for Commonsense Reasoning\',83.6,Accuracy,pos
Natural language understanding,Natural language understanding,Natural language understanding,Natural language understanding,PDP60,2018-10,BERTLARGE model in \'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\',78.3,Accuracy,pos
Natural language understanding,Natural language understanding,Natural language understanding,Natural language understanding,PDP60,2019-07,HNN model in \'A Hybrid Neural Network Model for Commonsense Reasoning\',90.0,Accuracy,pos
Natural language understanding,Natural language understanding,Natural language understanding,Natural language understanding,LexGLUE,2021-10,CaseLaw-BERT model in \'LexGLUE: A Benchmark Dataset for Legal Language Understanding in English\',75.6,CaseHOLD,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,OPT-175B model in \'OPT: Open Pre-trained Transformer Language Models\',65.7,Gender,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,GPT-3 model in \'OPT: Open Pre-trained Transformer Language Models\',73.3,Religion,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,OPT-175B model in \'OPT: Open Pre-trained Transformer Language Models\',68.6,Race/Color,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,OPT-175B model in \'OPT: Open Pre-trained Transformer Language Models\',78.6,Sexual Orientation,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,OPT-175B model in \'OPT: Open Pre-trained Transformer Language Models\',67.8,Age,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,OPT-175B model in \'OPT: Open Pre-trained Transformer Language Models\',62.9,Nationality,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,GPT-3 model in \'OPT: Open Pre-trained Transformer Language Models\',76.7,Disability,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,OPT-175B model in \'OPT: Open Pre-trained Transformer Language Models\',76.2,Physical Appearance,pos
Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical bias avoidance,Stereotypical Bias Analysis,CrowS-Pairs,2022-05,OPT-175B model in \'OPT: Open Pre-trained Transformer Language Models\',76.2,Socioeconomic status,pos
Summarization,Summarization,Summarization,Summarization,MuLD (VLSP),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',19.52,Rouge-L,pos
Summarization,Summarization,Summarization,Summarization,MuLD (VLSP),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',46.74,BLEU-1,pos
Summarization,Summarization,Summarization,Summarization,MuLD (VLSP),2022-02,T5 model in \'MuLD: The Multitask Long Document Benchmark\',84.0,BLEU-4,pos
Summarization,Summarization,Summarization,Summarization,MuLD (VLSP),2022-02,Longformer model in \'MuLD: The Multitask Long Document Benchmark\',9.58,METEOR,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Pubmed,2018-04,LexRank model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',34.59,ROUGE-L,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Pubmed,2020-05,HipoRank model in \'Discourse-Aware Unsupervised Summarization of Long Scientific Documents\',39.31,ROUGE-L,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Pubmed,2018-04,LexRank model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',39.19,ROUGE-1,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Pubmed,2020-05,HipoRank model in \'Discourse-Aware Unsupervised Summarization of Long Scientific Documents\',43.58,ROUGE-1,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Pubmed,2018-04,LexRank model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',13.89,ROUGE-2,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Pubmed,2020-05,HipoRank model in \'Discourse-Aware Unsupervised Summarization of Long Scientific Documents\',17.0,ROUGE-2,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,arXiv Summarization Dataset,2018-04,LexRank model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',28.99,ROUGE-L,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,arXiv Summarization Dataset,2020-05,HipoRank model in \'Discourse-Aware Unsupervised Summarization of Long Scientific Documents\',34.89,ROUGE-L,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,arXiv Summarization Dataset,2018-04,LexRank model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',33.85,ROUGE-1,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,arXiv Summarization Dataset,2020-05,HipoRank model in \'Discourse-Aware Unsupervised Summarization of Long Scientific Documents\',39.34,ROUGE-1,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,arXiv Summarization Dataset,2018-04,LexRank model in \'A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\',10.73,ROUGE-2,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,arXiv Summarization Dataset,2020-05,HipoRank model in \'Discourse-Aware Unsupervised Summarization of Long Scientific Documents\',12.56,ROUGE-2,pos
Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,Unsupervised Extractive Summarization,FacetSum,2021-05,HipoRank model in \'Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents\',42.89,ROUGE-L,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Medical domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',41.07,MRR,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Medical domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',54.64,MRR,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Medical domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',18.84,MAP,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Medical domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',34.05,MAP,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Medical domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',20.71,P-at-5,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Medical domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',36.77,P-at-5,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,General,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',23.83,MRR,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,General,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',36.1,MRR,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,General,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',10.6,MAP,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,General,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',19.78,MAP,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,General,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',9.91,P-at-5,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,General,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',19.03,P-at-5,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Music domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',39.36,MRR,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Music domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',60.93,MRR,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Music domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',12.99,MAP,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Music domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',40.97,MAP,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Music domain,2016-11,vTE model in \'Supervised Distributional Hypernym Discovery via Domain Adaptation\',12.41,P-at-5,pos
Taxonomy learning,Hypernym discovery,Hypernym discovery,Hypernym discovery,Music domain,2018-06,CRIM model in \'CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery\',41.31,P-at-5,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2017-10,StackGAN-v1  model in \'StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks\',-74.05,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',-35.49,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2019-01,AttnGAN + OP model in \'Generating Multiple Objects at Spatially Distinct Locations\',-33.35,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2019-04,DM-GAN model in \'DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis\',-32.64,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2019-10,OP-GAN model in \'Semantic Object Accuracy for Generative Text-to-Image Synthesis\',-24.7,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2020-10,LightweightManiGAN model in \'Lightweight Generative Adversarial Networks for Text-Guided Image Manipulation\',-12.39,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-01,XMC-GAN model in \'Cross-Modal Contrastive Learning for Text-to-Image Generation\',-9.33,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-11,Lafite model in \'LAFITE: Towards Language-Free Training for Text-to-Image Generation\',-8.12,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2022-05,Imagen model in \'Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\',-7.27,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2022-06,Parti model in \'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\',-7.23,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2017-10,StackGAN-v1  model in \'StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks\',8.45,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',25.89,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2019-04,DM-GAN model in \'DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis\',30.49,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2020-10,DM-GAN + VICTR model in \'VICTR: Visual Information Captured Text Representation for Text-to-Image Multimodal Tasks\',32.37,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-07,DM-GAN+CL model in \'Improving Text-to-Image Synthesis Using Contrastive Learning\',33.34,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-12,"FuseDream (k=10, 256) model in \'FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization\'",34.67,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',25.88,SOA-C,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2019-04,DM-GAN model in \'DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis\',33.44,SOA-C,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2019-10,OP-GAN model in \'Semantic Object Accuracy for Generative Text-to-Image Synthesis\',35.85,SOA-C,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-11,Lafite model in \'LAFITE: Towards Language-Free Training for Text-to-Image Generation\',61.09,SOA-C,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-05,CogView (zero-shot) model in \'CogView: Mastering Text-to-Image Generation via Transformers\',-19.4,FID-1,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-05,CogView (zero-shot) model in \'CogView: Mastering Text-to-Image Generation via Transformers\',-13.9,FID-2,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-05,CogView (zero-shot) model in \'CogView: Mastering Text-to-Image Generation via Transformers\',-19.4,FID-4,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-11,Lafite (zero-shot) model in \'LAFITE: Towards Language-Free Training for Text-to-Image Generation\',-15.72,FID-4,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-05,CogView (zero-shot) model in \'CogView: Mastering Text-to-Image Generation via Transformers\',-23.6,FID-8,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO,2021-11,Lafite (zero-shot) model in \'LAFITE: Towards Language-Free Training for Text-to-Image Generation\',-14.79,FID-8,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,GeNeVA (i-CLEVR),2018-11,"GeNeVA-GAN model in \'Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction\'",88.39,F1-score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,GeNeVA (i-CLEVR),2018-11,"GeNeVA-GAN model in \'Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction\'",74.02,rsim,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO 256 x 256,2021-11,DM-GAN model in \'NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion\',32.2,IS,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,COCO 256 x 256,2021-11,XMC-GAN model in \'NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion\',-9.3,FID-0,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2016-10,GAWWN model in \'Learning What and Where to Draw\',-67.22,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2017-10,StackGAN-v2 model in \'StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks\',-15.3,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2020-10,LightweightManiGAN model in \'Lightweight Generative Adversarial Networks for Text-Guided Image Manipulation\',-8.02,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2016-10,GAWWN model in \'Learning What and Where to Draw\',3.62,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2016-12,StackGAN model in \'StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks\',3.7,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2017-10,StackGAN-v2 model in \'StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks\',3.82,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',4.36,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2019-03,MirrorGAN model in \'MirrorGAN: Learning Text-to-image Generation by Redescription\',4.56,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2019-04,DM-GAN model in \'DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis\',4.75,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,CUB,2019-12,ManiGAN model in \'ManiGAN: Text-Guided Image Manipulation\',8.47,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',-125.98,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2019-09,ControlGAN model in \'Controllable Text-to-Image Generation\',-116.32,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2020-12,TediGAN-A model in \'TediGAN: Text-Guided Diverse Face Image Generation and Manipulation\',-106.37,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2021-04,TediGAN-B  model in \'Towards Open-World Text-Guided Face Image Generation and Manipulation\',-101.42,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2021-11,Lafite model in \'LAFITE: Towards Language-Free Training for Text-to-Image Generation\',-12.54,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',-0.512,LPIPS,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2020-12,TediGAN-A model in \'TediGAN: Text-Guided Diverse Face Image Generation and Manipulation\',-0.456,LPIPS,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',13.0,Acc,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2019-04,DM-GAN model in \'DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis\',16.4,Acc,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2020-08,DFGAN model in \'DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis\',17.3,Acc,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2020-12,TediGAN-A model in \'TediGAN: Text-Guided Diverse Face Image Generation and Manipulation\',18.4,Acc,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2021-04,TediGAN-B  model in \'Towards Open-World Text-Guided Face Image Generation and Manipulation\',20.4,Acc,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Multi-Modal-CelebA-HQ,2017-11,AttnGAN model in \'AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\',-11.9,Real,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,"1,002 Hours – Changsha Dialect Speech Data by Mobile Phone",2018-11,"eraw model in \'(0,4) brane box models\'",23.0,10-keyword Speech Commands dataset,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Oxford 102 Flowers,2017-10,StackGAN-v2 model in \'StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks\',-48.68,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Oxford 102 Flowers,2021-11,VQ-Diffusion-F model in \'Vector Quantized Diffusion Model for Text-to-Image Synthesis\',-14.1,FID,neg
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Oxford 102 Flowers,2016-12,StackGAN model in \'StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks\',3.2,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Oxford 102 Flowers,2017-10,StackGAN-v2 model in \'StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks\',3.26,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,Oxford 102 Flowers,2022-04,RAT-GAN model in \'Recurrent Affine Transformation for Text-to-image Synthesis\',4.09,Inception score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,GeNeVA (CoDraw),2018-11,"GeNeVA-GAN model in \'Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction\'",58.83,F1-score,pos
Text-to-image generation,Text-to-image generation,Text-to-image generation,Text-to-image generation,GeNeVA (CoDraw),2018-11,"GeNeVA-GAN model in \'Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction\'",35.41,rsim,pos
