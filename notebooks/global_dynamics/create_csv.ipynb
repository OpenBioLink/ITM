{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:44:42.836077Z",
     "iopub.status.busy": "2022-08-10T09:44:42.836077Z",
     "iopub.status.idle": "2022-08-10T09:44:42.874081Z",
     "shell.execute_reply": "2022-08-10T09:44:42.873083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://193.171.177.138:9999/blazegraph/namespace/ito/sparql'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define here the end point  (i.e. where the blazergraph instance is running)\n",
    "#current one\n",
    "import yaml\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "endpoint = config['blazegraph']['endpoint']\n",
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:44:42.879092Z",
     "iopub.status.busy": "2022-08-10T09:44:42.879092Z",
     "iopub.status.idle": "2022-08-10T09:44:44.746357Z",
     "shell.execute_reply": "2022-08-10T09:44:44.745323Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "#import some modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "from SPARQLWrapper import SPARQLWrapper, N3, JSON\n",
    "from rdflib import Graph\n",
    "from scipy import stats\n",
    "\n",
    "prefixes = \"\"\"\n",
    "PREFIX edam: <http://edamontology.org/>\n",
    "PREFIX obo:  <http://purl.obolibrary.org/obo/>\n",
    "PREFIX ito:  <https://identifiers.org/ito#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Use this function to escape some desired_benchmark names that might contain special chars causing crashes.\n",
    "def escape(s):\n",
    "    return s.translate(str.maketrans({  \"'\":   r\"\\'\",\n",
    "                                        '\"':   r'\\\"',\n",
    "                                        \"\\\\\":  r\"\\\\\",\n",
    "                                        \"\\r\":  r\"\\r\",\n",
    "                                        \"\\n\":  r\"\\n\"}))\n",
    "\n",
    "\n",
    "def query(service, query, numeric_cols = [], date_cols = []):\n",
    "    \"\"\"\n",
    "    Helper function to convert SPARQL results into a Pandas data frame.\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(service)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    result = sparql.query()\n",
    "    processed_results = json.load(result.response)\n",
    "    cols = processed_results['head']['vars']\n",
    "    out = []\n",
    "    for row in processed_results['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        out.append(item)\n",
    "        \n",
    "    df = pd.DataFrame(out, columns=cols)\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ito_to_df(endpoint, ito):\n",
    "    q = \"\"\"\n",
    "        PREFIX edam: <http://edamontology.org/>\n",
    "        PREFIX obo:  <http://purl.obolibrary.org/obo/>\n",
    "        PREFIX ito:  <https://identifiers.org/ito:>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "        PREFIX oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "\n",
    "        SELECT DISTINCT *\n",
    "        WHERE {\n",
    "                ?paper a edam:data_0971 . \n",
    "                ?paper rdfs:label ?paper_label. \n",
    "                ?paper oboInOwl:date ?date. \n",
    "                ?model  rdfs:seeAlso ?paper ;\n",
    "                        rdfs:label ?model_label ;\n",
    "                        a ?dataset . # this will create a place holder for the rdfs:type results that contains the information about the individual\n",
    "                \n",
    "                BIND(ito:\"\"\" + ito + \"\"\" AS ?top_level_class)\n",
    "                ?top_level_class rdfs:label ?top_level_class_label .\n",
    "\n",
    "                ?dataset rdfs:label ?dataset_label ;\n",
    "                        rdfs:subClassOf* ?top_level_class .\n",
    "\n",
    "                ?metric rdfs:subPropertyOf* ito:performance_measure .\n",
    "                ?metric rdfs:label ?metric_label .\n",
    "                ?model ?metric ?result\n",
    "\n",
    "                FILTER(?top_level_class != ito:Benchmarking) \n",
    "                FILTER(?top_level_class != ito:ITO_01524) \n",
    "            } ORDER by ?date\n",
    "    \"\"\"\n",
    "\n",
    "    all = query(endpoint, q, numeric_cols = [\"result\"], date_cols = [\"date\"])\n",
    "    \n",
    "    all[['dataset_label', 'task_label']] = all['dataset_label'].str.rsplit(' - ', 1, expand=True)\n",
    "    all[\"task_label\"] = all[\"task_label\"].str.replace(\" benchmarking\",\"\")\n",
    "\n",
    "    # reorder columns (more hierarchical)\n",
    "    all = all.reindex(columns=['date', 'top_level_class', 'top_level_class_label', 'task_label', 'dataset', 'dataset_label', 'paper', 'paper_label', 'model', 'model_label', 'metric', 'metric_label', 'result'])\n",
    "    return all\n",
    "\n",
    "def fix_polarities(df):\n",
    "    # merge with polarities df and change polarities of negative polar results\n",
    "    # this file can be found here and is extracted from supplementary tables (See google sheets)\n",
    "    # https://github.com/OpenBioLink/ITO/blob/master/notebooks/barbosa-silva-etal-2022/data/polarities.csv\n",
    "    polarities = pd.read_csv(\"data/polarities.csv\", sep=\";\").drop_duplicates()\n",
    "    pol_all = df.merge(polarities.rename(columns={\"metric\": \"metric_label\"}), how=\"left\", on=\"metric_label\")\n",
    "    pol_all.loc[pol_all[\"polarity\"] == \"neg\", \"result\"] = pol_all[pol_all[\"polarity\"] == \"neg\"][\"result\"] * -1\n",
    "    pol_all[\"polarity\"] = pol_all[\"polarity\"].fillna(\"pos\")\n",
    "    return pol_all\n",
    "\n",
    "def add_katrin_groups(df, ito):\n",
    "    #get grouping: this tries to add the annotations by Kathrin, we need a merge here...\n",
    "    grouping_table = pd.read_csv(\"data/grouping_\"+ito+\".csv\")\n",
    "\n",
    "    # fix dirty data\n",
    "    grouping_table[\"Suggested_label\"] = grouping_table[\"Suggested_label\"].str.replace(\"Semantic segmenation\", \"Semantic segmentation\")\n",
    "    # grouping_table[\"Suggested_label\"] = grouping_table[\"Suggested_label\"].str.replace(\"Text summarization\", \"Natural language generation\")\n",
    "\n",
    "    grouping_table = grouping_table.rename(columns={'Class_Label': 'task_label_lower', 'Suggested_label': 'suggested_task_label'})\n",
    "    \n",
    "    grouping_table[\"task_label_lower\"] = grouping_table[\"task_label_lower\"].str.lower()\n",
    "    df[\"task_label_lower\"] = df[\"task_label\"].str.lower()\n",
    "    \n",
    "    group_all = df.merge(grouping_table, on = 'task_label_lower', how = 'left')\n",
    "    group_all = group_all.drop(['id', 'Superclass_id','Superclass_label', \"task_label_lower\"], axis = 1)\n",
    "    return group_all\n",
    "\n",
    "def extract_sota(df):\n",
    "    # extract sota trajectories\n",
    "    def agg_(ex):\n",
    "        \n",
    "        # convert to monthly\n",
    "        ex[\"date\"] = ex[\"date\"].dt.strftime(\"%Y-%m\").astype(\"string\")\n",
    "\n",
    "        # Sort rows first by date ascending\n",
    "        # then by value descending (this is needed to keep only the best result per date, see drop_duplicates below)\n",
    "        ex = ex.sort_values(by=[\"date\", \"result\"], ascending=[True, False])\n",
    "\n",
    "        # keeps only best result of a specific date\n",
    "        ex = ex.drop_duplicates(subset=[\"date\"])\n",
    "\n",
    "        # calculates sota\n",
    "        # see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cummax.html\n",
    "        ex[\"result\"] = ex[\"result\"].cummax()\n",
    "\n",
    "        # keeps only best results so far\n",
    "        ex = ex.drop_duplicates(subset=[\"result\"])\n",
    "\n",
    "        return ex\n",
    "\n",
    "    # key for grouping\n",
    "    grp = [\"top_level_class\", \"top_level_class_label\", \"l1_label\", \"l2_label\", \"l3_label\", \"task_label\", \"dataset\", \"dataset_label\", \"metric\"]\n",
    "    # aggregate groups\n",
    "    sota = df.groupby(grp)[[x for x in df.columns if x not in grp]].apply(agg_).reset_index()\n",
    "    return sota\n",
    "\n",
    "def add_superclasses(endpoint, ito, df, up_to_lvl):\n",
    "\n",
    "    # appearantly i am too stupid to write a better sparql :(\n",
    "\n",
    "    prefix = \"\"\"\n",
    "    PREFIX edam: <http://edamontology.org/>\n",
    "    PREFIX obo:  <http://purl.obolibrary.org/obo/>\n",
    "    PREFIX ito:  <https://identifiers.org/ito:>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "    \"\"\"\n",
    "\n",
    "    lvl1 = \"\"\"\n",
    "    SELECT DISTINCT ?dataset ?l1_label\n",
    "    WHERE {\n",
    "    BIND(ito:\"\"\" + ito + \"\"\" AS ?top_level_class)\n",
    "    ?dataset rdfs:subClassOf+ ?l1 .\n",
    "    ?l1 rdfs:subClassOf ?top_level_class .\n",
    "    ?l1 rdfs:label ?l1_label .\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    lvl2 = \"\"\"\n",
    "    SELECT DISTINCT ?dataset ?l1_label ?l2_label\n",
    "    WHERE {\n",
    "    BIND(ito:\"\"\" + ito + \"\"\" AS ?top_level_class)\n",
    "    ?dataset rdfs:subClassOf+ ?l2 .\n",
    "    ?l2 rdfs:subClassOf ?l1 .\n",
    "    ?l1 rdfs:subClassOf ?top_level_class .\n",
    "    ?l2 rdfs:label ?l2_label .\n",
    "    ?l1 rdfs:label ?l1_label .\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    lvl3 = \"\"\"\n",
    "    SELECT DISTINCT ?dataset ?l1_label ?l2_label ?l3_label\n",
    "    WHERE {\n",
    "    BIND(ito:\"\"\" + ito + \"\"\" AS ?top_level_class)\n",
    "    ?dataset rdfs:subClassOf+ ?l3 .\n",
    "    ?l3 rdfs:subClassOf ?l2 .\n",
    "    ?l2 rdfs:subClassOf ?l1 .\n",
    "    ?l1 rdfs:subClassOf ?top_level_class .\n",
    "    ?l3 rdfs:label ?l3_label .\n",
    "    ?l2 rdfs:label ?l2_label .\n",
    "    ?l1 rdfs:label ?l1_label .\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    classes_lvl1 = query(endpoint, prefix + lvl1)\n",
    "    classes_lvl2 = query(endpoint, prefix + lvl2)\n",
    "    classes_lvl3 = query(endpoint, prefix + lvl3)\n",
    "    classes = classes_lvl1\n",
    "    classes = pd.merge(classes, classes_lvl2, how=\"left\", on=list(classes.columns))\n",
    "    classes = pd.merge(classes, classes_lvl3, how=\"left\", on=list(classes.columns))\n",
    "    df = pd.merge(df, classes, how=\"left\", on=\"dataset\")\n",
    "\n",
    "    df.loc[df[\"l2_label\"].isna(), \"l2_label\"] = df[df[\"l2_label\"].isna()][\"l1_label\"]\n",
    "    df.loc[df[\"l3_label\"].isna(), \"l3_label\"] = df[df[\"l3_label\"].isna()][\"l2_label\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-10T09:44:44.751356Z",
     "iopub.status.busy": "2022-08-10T09:44:44.750345Z",
     "iopub.status.idle": "2022-08-10T09:45:33.680314Z",
     "shell.execute_reply": "2022-08-10T09:45:33.678337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sota curves for ITO_00101\n",
      "Saved sota curve to data/sota_ITO_00101.csv\n",
      "Creating sota curves for ITO_00141\n",
      "Saved sota curve to data/sota_ITO_00141.csv\n"
     ]
    }
   ],
   "source": [
    "for ito in [\"ITO_00101\", \"ITO_00141\"]:\n",
    "    print(f\"Creating sota curves for {ito}\")\n",
    "    df = ito_to_df(endpoint, ito)\n",
    "    df = fix_polarities(df)\n",
    "    df = add_superclasses(endpoint, ito, df, 3)\n",
    "    df.to_csv(f\"data/all_{ito}.csv\", index=None)\n",
    "    df = extract_sota(df)\n",
    "\n",
    "    sota_papers = df[[\"dataset_label\", \"paper\", \"paper_label\"]].drop_duplicates().to_csv(f\"data/sota_papers_{ito}.csv\", index=None)\n",
    "\n",
    "    out = df[[\"l1_label\", \"l2_label\", \"l3_label\", \"task_label\", \"dataset_label\", \"date\", \"model_label\", \"result\", \"metric_label\", \"polarity\"]]\n",
    "    out = out.rename(columns={\n",
    "        \"l1_label\": \"l1\", \n",
    "        \"l2_label\": \"l2\", \n",
    "        \"l3_label\": \"l3\", \n",
    "        \"task_label\": \"task\", \n",
    "        \"dataset_label\": \"dataset\",\n",
    "        \"model_label\": \"model\",\n",
    "        \"metric_label\": \"metric\"\n",
    "    })\n",
    "    out.to_csv(f\"data/sota_{ito}.csv\", index=None)\n",
    "    print(f\"Saved sota curve to data/sota_{ito}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarks with ≥ 1 reported result: 2447\n",
      "Benchmarks with ≥ 3 results at different time points (% of above): 1274 52.063751532488766\n",
      "AI tasks with ≥ 1 reported result: 601\n",
      "AI tasks with ≥ 3 results at different time points (% of above): 386 64.22628951747087\n"
     ]
    }
   ],
   "source": [
    "ito = \"ITO_00101\"\n",
    "all = pd.read_csv(f\"data/all_{ito}.csv\")\n",
    "all = all[[\"date\", \"task_label\", \"dataset_label\", \"metric_label\", \"model_label\", \"result\"]].drop_duplicates()\n",
    "\n",
    "len_ds = len(all[\"dataset_label\"].unique())\n",
    "len_ds_ge_3 = len(all.groupby([\"task_label\", \"dataset_label\", \"metric_label\"]).filter(lambda x: len(x) >= 3).reset_index()[\"dataset_label\"].unique())\n",
    "\n",
    "print(\"Benchmarks with ≥ 1 reported result:\", len_ds)\n",
    "print(\"Benchmarks with ≥ 3 results at different time points (% of above):\", len_ds_ge_3, len_ds_ge_3/len_ds * 100)\n",
    "\n",
    "len_t = len(all[\"task_label\"].unique())\n",
    "len_t_ge_3 = len(all.groupby([\"task_label\", \"dataset_label\", \"metric_label\"]).filter(lambda x: len(x) >= 3).reset_index()[\"task_label\"].unique())\n",
    "\n",
    "print(\"AI tasks with ≥ 1 reported result:\", len_t)\n",
    "print(\"AI tasks with ≥ 3 results at different time points (% of above):\", len_t_ge_3, len_t_ge_3/len_t * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd80215105c70ce5df714f5d86018d2a6da9d2221533fcee9ade4479e45eda6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
