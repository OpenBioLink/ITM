{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import types\n",
    "import datetime\n",
    "import decimal\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint = \"http://localhost:9999/blazegraph/sparql\"  # SPARQL endpoint hosting previous version of ITO.owl\n",
    "prefixes = \"\"\"\n",
    "prefix owl: <http://www.w3.org/2002/07/owl#>\n",
    "prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "prefix ito: <https://identifiers.org/ito:>\n",
    "prefix edam: <http://edamontology.org/>\n",
    "prefix obo: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def query(query, return_format = JSON):\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.method = 'POST'\n",
    "    sparql.setReturnFormat(return_format)\n",
    "    sparql.setQuery(prefixes + query)\n",
    "    results = sparql.query().convert()\n",
    "    return results\n",
    "\n",
    "\n",
    "def query_df(query, numeric_cols = []):\n",
    "    # Run SPARQL query, and convert results to Pandas dataframe\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.method = 'POST'\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setQuery(prefixes + query)\n",
    "    results = sparql.query()\n",
    "    processed_results = json.load(results.response)\n",
    "    cols = processed_results['head']['vars']\n",
    "\n",
    "    out = []\n",
    "    for row in processed_results['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        out.append(item)\n",
    "        \n",
    "    df = pd.DataFrame(out, columns=cols)\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def query_if_iri_exists(iri):\n",
    "    q = \"\"\"ASK FROM <tag:ITO> {\n",
    "    VALUES (?r) { (<\"\"\" + iri + \"\"\">) }\n",
    "        { ?r ?p ?o }\n",
    "        UNION\n",
    "        { ?s ?r ?o }\n",
    "        UNION\n",
    "        { ?s ?p ?r }\n",
    "    } \"\"\"\n",
    "    return query(q)['boolean']\n",
    "\n",
    "\n",
    "def query_entites_with_pwc_label(pwc_label):\n",
    "    q = 'SELECT ?x FROM <tag:ITO> WHERE { ?x ito:papers_with_code_id \"' + escape(pwc_label.strip()) + '\" }'  #.strip() added because some part of process (WebProtege?) seem to strip whitespace without user intent\n",
    "    r = query(q)\n",
    "    if len(r['results']['bindings']) > 1:\n",
    "        raise Exception(\n",
    "            f\"query_entites_with_pwc_label: It seems like there are multiple existing entities for {pwc_label}!\")\n",
    "    elif len(r['results']['bindings']) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return r['results']['bindings'][0]['x']['value']\n",
    "\n",
    "\n",
    "def query_label(iri):\n",
    "    q = 'SELECT ?label FROM <tag:ITO> WHERE { <' + iri + '> rdfs:label ?label }'\n",
    "    r = query(q)\n",
    "    if len(r['results']['bindings']) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return r['results']['bindings'][0]['label']['value']\n",
    "\n",
    "\n",
    "def add_triple(raw_sparql_fragment):\n",
    "    query(\"INSERT DATA { GRAPH <tag:ITO> { \" + raw_sparql_fragment + \" }  } \")\n",
    "\n",
    "\n",
    "def find_new_iri_fragment(prefix, number_format='{:05}'):\n",
    "    # Find new unique IRI fragment (e.g., 'ITO_00101'), start trying with global increment variable i\n",
    "    global iri_increment\n",
    "    while True:\n",
    "        candidate_iri = prefix + number_format.format(iri_increment)\n",
    "        if query_if_iri_exists(candidate_iri) == True:\n",
    "            iri_increment += 1\n",
    "        else:\n",
    "            break\n",
    "    return candidate_iri\n",
    "\n",
    "\n",
    "def clean_metric_value(metric_value):\n",
    "    if metric_value == None:\n",
    "        raise ValueError(\"Metric value None\")\n",
    "    if len(metric_value) == 0:\n",
    "        raise ValueError(\"Metric value empty\")\n",
    "    metric_value = metric_value.strip()\n",
    "    # remove % signs\n",
    "    metric_value = metric_value.replace(\"%\", \"\")\n",
    "    # convert m/M postfixes to numbers (e.g. 22M)\n",
    "    if metric_value[-1] in ['m', 'M']:\n",
    "        metric_value = metric_value[:-1]\n",
    "        metric_value = decimal.Decimal(\n",
    "            metric_value) * decimal.Decimal('1000000')\n",
    "    # convert k/K postfixes to numbers (e.g. 22k)\n",
    "    elif metric_value[-1] in ['k', 'K']:\n",
    "        metric_value = metric_value[:-1]\n",
    "        metric_value = decimal.Decimal(metric_value) * decimal.Decimal('1000')\n",
    "    metric_value = float(metric_value)\n",
    "    return metric_value\n",
    "\n",
    "\n",
    "def clean_ontology():\n",
    "    query(\"\"\"\n",
    "        WITH <tag:ITO>\n",
    "        DELETE { ?class rdfs:subClassOf ?redundant_superclass . }\n",
    "        WHERE {\n",
    "            ?class rdfs:subClassOf ?redundant_superclass .\n",
    "            ?class rdfs:subClassOf ?intermediate_class .\n",
    "            ?intermediate_class rdfs:subClassOf+ ?redundant_superclass .\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "    query(\"\"\"\n",
    "        WITH <tag:ITO>\n",
    "        DELETE { ?class rdfs:subClassOf owl:Thing . }\n",
    "        WHERE {\n",
    "            ?class rdfs:subClassOf ?someClass .\n",
    "            ?class rdfs:subClassOf owl:Thing .\n",
    "            FILTER (!sameTerm(?someClass, owl:Thing))\n",
    "        }\"\"\")\n",
    "\n",
    "    query(\"\"\"\n",
    "        WITH <tag:ITO>\n",
    "        DELETE { ?class rdfs:subClassOf ?superclass . }\n",
    "        INSERT { ?class rdfs:subClassOf ito:ITO_00492 . }\n",
    "        WHERE {\n",
    "        ?class rdfs:subClassOf+ ito:ITO_01625 . \n",
    "        ?class rdfs:subClassOf ?superclass . \n",
    "        FILTER NOT EXISTS { \n",
    "            ?subclass rdfs:subClassOf* ?class .\n",
    "            ?subclass rdfs:subClassOf ito:Benchmarking . } \n",
    "        }\"\"\")\n",
    "\n",
    "    # Merge entities with the same papers_with_code_id value.\n",
    "\n",
    "    df = query_df('''\n",
    "        select (GROUP_CONCAT(?subject1) as ?subject1_concat) ?object from <tag:ITO> where { \n",
    "        ?subject1 ito:papers_with_code_id ?object .\n",
    "        ?subject2 ito:papers_with_code_id ?object .\n",
    "        filter ( ?subject1 != ?subject2 )\n",
    "        }\n",
    "        GROUP BY ?object\n",
    "\n",
    "        ''')\n",
    "    for index, row in df.iterrows():\n",
    "        iris = row['subject1_concat'].split(' ')\n",
    "        iris = sorted(set(iris))\n",
    "        primary_iri = iris[0]\n",
    "        other_iris = iris[1:]\n",
    "\n",
    "        for other_iri in other_iris: \n",
    "            query(\"\"\"\n",
    "                WITH <tag:ITO>\n",
    "                DELETE {<\"\"\" + other_iri + \"\"\"> ?p ?o}\n",
    "                INSERT {<\"\"\" + primary_iri + \"\"\"> ?p ?o}\n",
    "                WHERE  {<\"\"\" + other_iri + \"\"\"> ?p ?o}\n",
    "                \"\"\")\n",
    "\n",
    "            query(\"\"\"\n",
    "                WITH <tag:ITO>\n",
    "                DELETE { ?s ?p <\"\"\" + other_iri + \"\"\">}\n",
    "                INSERT { ?s ?p <\"\"\" + primary_iri + \"\"\">}\n",
    "                WHERE  { ?s ?p <\"\"\" + other_iri + \"\"\">}\n",
    "                \"\"\")\n",
    "\n",
    "            query(\"\"\"\n",
    "                WITH <tag:ITO>\n",
    "                DELETE { ?s <\"\"\" + other_iri + \"\"\"> ?o}\n",
    "                INSERT { ?s <\"\"\" + primary_iri + \"\"\"> ?o}\n",
    "                WHERE  { ?s <\"\"\" + other_iri + \"\"\"> ?o}\n",
    "                \"\"\")\n",
    "\n",
    "\n",
    "def escape(s):\n",
    "    return s.translate(str.maketrans({  \"'\":   r\"\\'\",\n",
    "                                        '\"':   r'\\\"',\n",
    "                                        \"\\\\\":  r\"\\\\\",\n",
    "                                        \"\\r\":  r\"\\r\",\n",
    "                                        \"\\n\":  r\"\\n\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_class_from_paperswithcode(class_pwc_label, label=None, superclass=\"http://www.w3.org/2002/07/owl#Thing\", entity_type=\"class\", random_iri=False, flag_as_imported=True):\n",
    "    # Create a new class from paperswithcode input data, only if the class has not yet been added to the ontology\n",
    "\n",
    "    # Quick fix for rare cases where entities contain '*' wildcard character (historic, was required for Owlready2)\n",
    "    class_pwc_label = class_pwc_label.replace(\"*\", \"_\")\n",
    "\n",
    "    existing_matching_classes = query_entites_with_pwc_label(class_pwc_label)\n",
    "\n",
    "    if existing_matching_classes == None:\n",
    "\n",
    "        if random_iri:\n",
    "            iri = 'ITO_i' + \\\n",
    "                  ''.join(random.choices(string.ascii_letters + string.digits, k=16))\n",
    "        else:\n",
    "            iri = find_new_iri_fragment('https://identifiers.org/ito:ITO_')\n",
    "        \n",
    "        logfile.write(\n",
    "            f\"INFO: Creating an new entity for {class_pwc_label} ({iri}) \\n\")\n",
    "\n",
    "        if entity_type == \"individual\":\n",
    "            add_triple(f\"<{iri}> a <{superclass}>\")\n",
    "\n",
    "        elif entity_type == \"property\":\n",
    "            add_triple(f\"<{iri}> rdfs:subPropertyOf  <{superclass}>\")\n",
    "            add_triple(f\"<{iri}> a owl:DatatypeProperty\")\n",
    "            if flag_as_imported:\n",
    "                add_triple(f\"<{iri}> rdfs:subPropertyOf ito:Data_property_requiring_curation\")\n",
    "\n",
    "        elif entity_type == \"class\":\n",
    "            add_triple(f\"<{iri}> rdfs:subClassOf  <{superclass}>\")\n",
    "            add_triple(f\"<{iri}> a owl:Class\")\n",
    "            if flag_as_imported:\n",
    "                add_triple(f\"<{iri}> rdfs:subClassOf ito:Class_requiring_curation\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Illegal entity_type argument')  \n",
    "\n",
    "        if flag_as_imported:\n",
    "            add_triple(f\"<{iri}> edam:refactor_comment 'IMPORTED'\")\n",
    "\n",
    "        add_triple(f\"<{iri}> obo:creation_date '{str(datetime.datetime.now().isoformat())}'^^xsd:dateTime\")\n",
    "\n",
    "        logfile.write(\n",
    "            f\"INFO: Created new entity for {class_pwc_label} ({iri})\\n\")\n",
    "\n",
    "        if label:\n",
    "            add_triple(f\"<{iri}> rdfs:label '{escape(label.strip())}'\")\n",
    "        else:\n",
    "            add_triple(f\"<{iri}> rdfs:label '{escape(class_pwc_label.strip())}'\")\n",
    "\n",
    "        add_triple(f\"<{iri}> ito:papers_with_code_id '{escape(class_pwc_label)}'\")\n",
    "\n",
    "        add_triple(f\"<{iri}> dc:source ito:Papers_with_code\")\n",
    "\n",
    "        return iri\n",
    "\n",
    "    else:\n",
    "        # logfile.write(f\"INFO: An entity for {class_pwc_label} already exists ({existing_matching_classes}) -- skipping entity creation\\n\")\n",
    "        return existing_matching_classes\n",
    "\n",
    "\n",
    "def add_superclass(myclass, superclass):\n",
    "    if not myclass == superclass:\n",
    "        add_triple(f\"<{myclass}> rdfs:subClassOf <{superclass}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"DROP GRAPH <tag:ITO>\")\n",
    "####\n",
    "#query(\"\"\"\n",
    "#LOAD <file:///C:/Users/samwa/Documents/Dev/ITO/ITO.owl> INTO GRAPH <tag:ITO>\n",
    "#\"\"\")\n",
    "####\n",
    "query(\"\"\"\n",
    "LOAD <file:///C:/Users/samwa/Downloads/ITO-new-2.owl> INTO GRAPH <tag:ITO>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query(\"\"\"\n",
    "#LOAD <file:///D:/Documents/Database/AI-KG/aikg.ttl> INTO GRAPH <tag:AIKG>\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total triple count before purge\n",
    "\n",
    "query_df(\"SELECT (count(*) as ?triple_count) FROM <tag:ITO> WHERE {?s ?p ?o}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purge existing benchmark result data / pre-July 2021 entities that became obsolete through new modeling\n",
    "\n",
    "query(\"\"\"\n",
    "WITH <tag:ITO>\n",
    "DELETE { ?s ?p ?o }\n",
    "WHERE {\n",
    "?s a/rdfs:subClassOf+ ito:Software .\n",
    "?s ?p ?o .\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "query(\"\"\"\n",
    "WITH <tag:ITO>\n",
    "DELETE { ?s ?p ?o }\n",
    "WHERE {\n",
    "?s rdfs:subClassOf+ ito:Software .\n",
    "?s ?p ?o .\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "query(\"\"\"\n",
    "WITH <tag:ITO>\n",
    "DELETE { ?s ?p ?o }\n",
    "WHERE {\n",
    "?s a/rdfs:subClassOf+ ito:Benchmarking .\n",
    "?s ?p ?o .\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ontology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total triple count after purge\n",
    "\n",
    "query_df(\"SELECT (count(*) as ?triple_count) FROM <tag:ITO> WHERE {?s ?p ?o}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and read PWC data dump\n",
    "'''\n",
    "filename = './evaluation-tables.json.gz'\n",
    "if not os.path.exists(filename):\n",
    "    url = 'https://paperswithcode.com/media/about/evaluation-tables.json.gz'\n",
    "    myfile = requests.get(url)\n",
    "    open(filename, 'wb').write(myfile.content)\n",
    "    with gzip.open(filename, 'rb') as f_in:\n",
    "        with open('./evaluation-tables.json', 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "'''\n",
    "with open('C:\\\\Users\\\\samwa\\\\Documents\\\\Database\\\\paperswithcode\\\\evaluation-tables.json') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this values based on highest IRI in current ontology to save some startup time\n",
    "iri_increment = 17000\n",
    "logfile = open(\"logfile.txt\", \"w\", encoding=\"utf-8\")\n",
    "failed_rows = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_process_class = \"https://identifiers.org/ito:ITO_01625\"\n",
    "\n",
    "def create_task_class(task, depth=0, superclass=None):\n",
    "    logfile.write(\n",
    "        f\"INFO: Processing task to create class hierarchy: {task['task']}\\n\")\n",
    "\n",
    "    # Create class\n",
    "\n",
    "    task_class = create_new_class_from_paperswithcode(task['task'])\n",
    "\n",
    "    for category in task['categories']:\n",
    "        categories_class = create_new_class_from_paperswithcode(category, superclass = ai_process_class)\n",
    "        # add_superclass(categories_class, ai_process_class)\n",
    "        if not categories_class == task_class:   # in some cases category name and task name are equal; this would create a cycle that we need to avoid\n",
    "            add_superclass(task_class, categories_class)\n",
    "    if superclass:\n",
    "        add_superclass(task_class, superclass)\n",
    "\n",
    "    for subtask in task['subtasks']: \n",
    "        create_task_class(subtask, depth + 1, superclass = task_class)\n",
    "\n",
    "\n",
    "for task in data:\n",
    "    #logfile.write(f\"INFO: Creating task class for  {task['task']}\\n\")\n",
    "    create_task_class(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ontology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_result_rows = set()   # PWC benchmark data contains duplicates, we make a set of hashes of results to avoid adding the same result more than once.\n",
    "\n",
    "article_class = \"http://edamontology.org/data_0971\"\n",
    "\n",
    "def process_task(task, depth=0, superclass=None):\n",
    "    global failed_rows\n",
    "\n",
    "    task_class = create_new_class_from_paperswithcode(task['task'])\n",
    "\n",
    "    # Add process definitions -- TODO: THIS PROBABLY LEADS TO DUPLICATES IF MINOR CHANGES IN PWC DESCRIPTIONS WERE INTRODUCED\n",
    "\n",
    "    description = str(task['description']).split('<span')[0].strip()  # remove image attributions\n",
    "    description = re.sub('<[^<]+?>', '', description)  # strip HTML\n",
    "    if description and len(description) < 800:\n",
    "        description += \" (Source: paperswithcode.com)\"\n",
    "        add_triple(f\"<{task_class}> obo:hasDefinition '{escape(description)}'\")\n",
    "\n",
    "    # Convert benchmark results data\n",
    "\n",
    "    for dataset in task['datasets']:\n",
    "        #dataset['dataset'] = dataset['dataset'].encode('utf-8').decode('utf-8')\n",
    "        #print('decoded: ',dataset['dataset'])\n",
    "        \n",
    "        dataset_entity = create_new_class_from_paperswithcode(\n",
    "            dataset['dataset'] + ' dataset', superclass=\"https://identifiers.org/ito:Benchmark_dataset\", entity_type=\"individual\")\n",
    "        logfile.write(f\"Dataset_entity: {dataset_entity}\\n\")\n",
    "\n",
    "        benchmark_pwc_label = dataset['dataset'] + ' - ' + query_label(task_class) + ' benchmarking'\n",
    "\n",
    "        logfile.write(f\"benchmark_pwc_label: {benchmark_pwc_label}\\n\")\n",
    "\n",
    "        benchmark_class = create_new_class_from_paperswithcode(benchmark_pwc_label, superclass = \"https://identifiers.org/ito:Benchmarking\", flag_as_imported = False)\n",
    "        if not benchmark_class == task_class:    # in rare cases the benchmark and task class end up the same - TODO: Deal with this better\n",
    "            add_superclass(benchmark_class, task_class)\n",
    "        #add_superclass(benchmark_class, \"https://identifiers.org/ito:Benchmarking\")\n",
    "\n",
    "        for metric in dataset['sota']['metrics']:\n",
    "            metric_property = create_new_class_from_paperswithcode(\n",
    "                metric + \" property\", metric, superclass=\"https://identifiers.org/ito:performance_measure\", entity_type=\"property\")\n",
    "\n",
    "            add_triple(f\"<{benchmark_class}> rdfs:seeAlso <{metric_property}>\")\n",
    "\n",
    "        for row in dataset['sota']['rows']:\n",
    "            #row_entity = benchmark_class(0) ############\n",
    "            \n",
    "            '''\n",
    "            if not row['model_name'] == 'RCU_8':\n",
    "                continue\n",
    "            print(str(row))\n",
    "            \n",
    "            hashed_row = hash(str(row))\n",
    "            print(hashed_row)\n",
    "            continue\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            hashed_row = hash(str(json.dumps(row, sort_keys=True)))\n",
    "            \n",
    "            if hashed_row in hashed_result_rows:\n",
    "                logfile.write(f\"INFO: Skipping duplicate result row, hash {hashed_row}\\n\")\n",
    "            else:\n",
    "                hashed_result_rows.add(hashed_row)\n",
    "\n",
    "                row_entity = 'https://identifiers.org/ito:ITO_i' + ''.join(random.choices(string.ascii_letters + string.digits, k=16)) # create random IRI\n",
    "                add_triple(f\"<{row_entity}> a <{benchmark_class}>\")\n",
    "\n",
    "                if not row['model_name']:\n",
    "                    logfile.write(\"WARNING: Model name missing, skipping row\\n\")\n",
    "                    failed_rows += 1\n",
    "                    continue\n",
    "\n",
    "                for metric, value in row['metrics'].items():\n",
    "                    cleaned_metric_value = None\n",
    "                    try:\n",
    "                        cleaned_metric_value = clean_metric_value(value)\n",
    "                    except Exception as e:\n",
    "                        logfile.write(\n",
    "                            f\"ERROR: Could not convert metric value: {metric} {value}\\n\")\n",
    "\n",
    "                    if cleaned_metric_value:\n",
    "                        metric_property = create_new_class_from_paperswithcode(\n",
    "                            metric + \" property\", metric, superclass=\"https://identifiers.org/ito:performance_measure\", entity_type=\"property\")\n",
    "                        add_triple(f\"<{row_entity}> <{metric_property}> {cleaned_metric_value}\")\n",
    "\n",
    "                \n",
    "                software_entity = create_new_class_from_paperswithcode(row['model_name'] + \" software\", row['model_name'], superclass=\"https://identifiers.org/ito:Software\", entity_type=\"individual\")\n",
    "\n",
    "                add_triple(f\"<{row_entity}> rdfs:seeAlso <{software_entity}>\")\n",
    "                add_triple(f\"<{row_entity}> ito:has_input <{dataset_entity}>\")\n",
    "\n",
    "                if row['paper_title']:\n",
    "                    row_entity_label = row['model_name'] + \" model in '\" + row['paper_title'] + \"'\"        \n",
    "                    row_entity_label = escape(row_entity_label.strip()) \n",
    "\n",
    "                    add_triple(f\"<{row_entity}> rdfs:label '{escape(row_entity_label)}'\")\n",
    "\n",
    "                    article_entity = create_new_class_from_paperswithcode(row['paper_url'], row['paper_title'], superclass=article_class, entity_type=\"individual\")\n",
    "\n",
    "                    add_triple(f\"<{article_entity}> foaf:page <{row['paper_url']}>\")\n",
    "\n",
    "                    add_triple(f\"<{row_entity}> rdfs:seeAlso <{article_entity}>\")\n",
    "                    add_triple(f\"<{software_entity}> rdfs:seeAlso <{article_entity}>\")\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        paper_date = datetime.datetime.strptime(row['paper_date'], '%Y-%m-%d').date()\n",
    "                        \n",
    "                        add_triple(f\"<{row_entity}> obo:date '{paper_date}'^^xsd:date\")\n",
    "                        add_triple(f\"<{article_entity}> obo:date '{paper_date}'^^xsd:date\")\n",
    "\n",
    "                    except TypeError:\n",
    "                        logfile.write(\"ERROR: No valid date found\\n\")\n",
    "                else:\n",
    "                    add_triple(f\"<{row_entity}> rdfs:label '{escape(row['model_name'])}'\")\n",
    "\n",
    "                for code_link in row['code_links']:\n",
    "                    add_triple(f\"<{row_entity}> rdfs:seeAlso '{code_link['url']}'\")\n",
    "\n",
    "    # Recursively iterate through subtasks\n",
    "\n",
    "    for subtask in task['subtasks']:\n",
    "        process_task(subtask, depth + 1, superclass=task_class)\n",
    "\n",
    "\n",
    "#logfile.write(f\"INFO: Failed rows: {failed_rows}\\n\")\n",
    "#logfile.write(f\"INFO: Final iri_increment value: {iri_increment}\\n\")\n",
    "\n",
    "for task in data:\n",
    "    logfile.write(f\"INFO: Processing benchmark results for {task['task']}\\n\")\n",
    "    process_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ontology()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7d1ce6b838bce0d0a1611437c6759461d23eef7b1262337623bc7843e4bab38"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
