,ds_count,task,ds,date,model_label,value,percent_of_max_sota,gain,ratio,max_sota,percent_of_max_metric,merge
0,1,Sentiment Analysis,1B Words,2012-06,BilSTM,93.0,100.0,93,1.0,93,1.0,1\\ in\\ 10\\ R\\-at\\-1
0,1,Citation Intent Classification,ACL-ARC,2013-06,SVM,41.0,60.38,41.0,0.6,67.9,0.42,F1
1,1,Citation Intent Classification,ACL-ARC,2016-06,BiLSTM-Attention,51.8,76.29,10.8,0.16,67.9,0.53,F1
2,1,Citation Intent Classification,ACL-ARC,2018-01,Feature-rich Random Forest,53.0,78.06,1.2,0.02,67.9,0.54,F1
3,1,Citation Intent Classification,ACL-ARC,2018-02,BiLSTM-Attention + ELMo,54.6,80.41,1.6,0.02,67.9,0.55,F1
4,1,Citation Intent Classification,ACL-ARC,2019-03,SciBERT,65.8,96.91,11.2,0.16,67.9,0.67,F1
5,1,Citation Intent Classification,ACL-ARC,2019-04,Structural-scaffolds,67.9,100.0,2.1,0.03,67.9,0.69,F1
6,1,Semantic Textual Similarity,MRPC,2013-10,TF-KLD,85.9,92.86,85.9,0.93,92.5,0.87,F1
7,1,Semantic Textual Similarity,MRPC,2019-10,T5-Large,92.4,99.89,6.5,0.07,92.5,0.94,F1
8,1,Semantic Textual Similarity,MRPC,2019-10,T5-3B,92.5,100.0,0.1,0.0,92.5,0.94,F1
9,1,Question Answering,WebQuestions,2014-04,Weakly Supervised Embeddings,29.7,70.38,29.7,0.7,42.2,0.3,F1
10,1,Question Answering,WebQuestions,2014-06,Subgraph embeddings,39.2,92.89,9.5,0.23,42.2,0.4,F1
11,1,Question Answering,WebQuestions,2015-06,Memory Networks (ensemble),42.2,100.0,3.0,0.07,42.2,0.43,F1
12,1,Named Entity Recognition,SciERC,2014-09,SciBERT (Base Vocab),65.12,92.59,65.12,0.93,70.33,0.66,F1
13,1,Named Entity Recognition,SciERC,2019-03,SciBERT (SciVocab),65.5,93.13,0.4,0.01,70.33,0.67,F1
14,1,Named Entity Recognition,SciERC,2019-09,SpERT,70.33,100.0,4.8,0.07,70.33,0.71,F1
15,1,Word Sense Disambiguation,SensEval 3 Lexical Sample,2015-05,IMS + adapted CW,73.4,91.61,73.4,0.92,80.12,0.75,F1
16,1,Word Sense Disambiguation,SensEval 3 Lexical Sample,2019-09,kNN-BERT,80.12,100.0,6.7,0.08,80.12,0.81,F1
17,1,Word Sense Disambiguation,SensEval 2 Lexical Sample,2015-05,IMS + adapted CW,66.2,86.51,66.2,0.87,76.52,0.67,F1
18,1,Word Sense Disambiguation,SensEval 2 Lexical Sample,2016-06,BiLSTM with GloVe,66.9,87.43,0.7,0.01,76.52,0.68,F1
19,1,Word Sense Disambiguation,SensEval 2 Lexical Sample,2019-09,kNN-BERT,76.52,100.0,9.6,0.13,76.52,0.78,F1
20,1,Question Answering,SimpleQuestions,2015-06,Memory Networks (ensemble),63.9,100.0,63.9,1.0,63.9,0.65,F1
21,1,Chinese Word Segmentation,MSRA,2015-09,Pre-trained+bigram+ LSTM+CRF,97.4,100.0,97.4,1.0,97.4,0.99,F1
22,1,Sentence Compression,Google Dataset,2015-09,LSTM,0.82,96.36,0.82,0.96,0.851,0.01,F1
23,1,Sentence Compression,Google Dataset,2018-07,BiRNN + LM Evaluator,0.851,100.0,0.0,0.0,0.851,0.01,F1
24,1,Word Sense Disambiguation,SemEval 2007 Task 7,2016-03,"LSTMLP (T:SemCor, U:OMSTI)",84.3,93.25,84.3,0.93,90.4,0.86,F1
25,1,Word Sense Disambiguation,SemEval 2007 Task 7,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",86.02,95.15,1.7,0.02,90.4,0.87,F1
26,1,Word Sense Disambiguation,SemEval 2007 Task 7,2019-05,"SemCor+WNGC, hypernyms",90.4,100.0,4.4,0.05,90.4,0.92,F1
27,1,Word Sense Disambiguation,SensEval 3 Task 1,2016-03,LSTM (T:SemCor),69.2,88.95,69.2,0.89,77.8,0.7,F1
28,1,Word Sense Disambiguation,SensEval 3 Task 1,2016-03,"LSTMLP (T:SemCor, U:1K)",71.8,92.29,2.6,0.03,77.8,0.73,F1
29,1,Word Sense Disambiguation,SensEval 3 Task 1,2019-05,"SemCor+WNGC, hypernyms",77.8,100.0,6.0,0.08,77.8,0.79,F1
30,1,Word Sense Disambiguation,SemEval 2013 Task 12,2016-03,LSTM (T:SemCor),67.0,85.13,67.0,0.85,78.7,0.68,F1
31,1,Word Sense Disambiguation,SemEval 2013 Task 12,2016-03,"LSTMLP (T:SemCor, U:1K)",69.5,88.31,2.5,0.03,78.7,0.71,F1
32,1,Word Sense Disambiguation,SemEval 2013 Task 12,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",72.63,92.29,3.1,0.04,78.7,0.74,F1
33,1,Word Sense Disambiguation,SemEval 2013 Task 12,2019-05,"SemCor+WNGC, hypernyms",78.7,100.0,6.1,0.08,78.7,0.8,F1
34,1,Word Sense Disambiguation,SemEval 2007 Task 17,2016-03,LSTM (T:SemCor),64.2,87.47,64.2,0.87,73.4,0.65,F1
35,1,Word Sense Disambiguation,SemEval 2007 Task 17,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",66.81,91.02,2.6,0.04,73.4,0.68,F1
36,1,Word Sense Disambiguation,SemEval 2007 Task 17,2019-05,"SemCor+WNGC, hypernyms",73.4,100.0,6.6,0.09,73.4,0.75,F1
37,1,Word Sense Disambiguation,SensEval 2,2016-03,"LSTMLP (T:OMSTI, U:1K)",74.4,93.35,74.4,0.93,79.7,0.76,F1
38,1,Word Sense Disambiguation,SensEval 2,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",75.15,94.29,0.8,0.01,79.7,0.76,F1
39,1,Word Sense Disambiguation,SensEval 2,2019-05,"SemCor+WNGC, hypernyms",79.7,100.0,4.5,0.06,79.7,0.81,F1
40,1,Coreference Resolution,OntoNotes,2016-04,Global,64.21,80.67,64.21,0.81,79.6,0.65,F1
41,1,Coreference Resolution,OntoNotes,2016-06,NN Cluster Ranker,65.29,82.02,1.1,0.01,79.6,0.66,F1
42,1,Coreference Resolution,OntoNotes,2016-09,Reward Rescaling,65.73,82.58,0.4,0.01,79.6,0.67,F1
43,1,Coreference Resolution,OntoNotes,2017-07,e2e-coref,67.2,84.42,1.5,0.02,79.6,0.68,F1
44,1,Coreference Resolution,OntoNotes,2018-02,e2e-coref + ELMo,70.4,88.44,3.2,0.04,79.6,0.72,F1
45,1,Coreference Resolution,OntoNotes,2018-04,c2f-coref,73.0,91.71,2.6,0.03,79.6,0.74,F1
46,1,Coreference Resolution,OntoNotes,2019-07,BERT + EE,76.61,96.24,3.6,0.05,79.6,0.78,F1
47,1,Coreference Resolution,OntoNotes,2019-07,SpanBERT,79.6,100.0,3.0,0.04,79.6,0.81,F1
48,1,Relation Extraction,SemEval-2010 Task 8,2016-08,Att-Pooling-CNN,88.0,96.7,88.0,0.97,91.0,0.89,F1
49,1,Relation Extraction,SemEval-2010 Task 8,2019-02,Entity-Aware BERT,89.0,97.8,1.0,0.01,91.0,0.9,F1
50,1,Relation Extraction,SemEval-2010 Task 8,2019-05,R-BERT,89.25,98.08,0.2,0.0,91.0,0.91,F1
51,1,Relation Extraction,SemEval-2010 Task 8,2019-06,BERTEM+MTB,89.5,98.35,0.2,0.0,91.0,0.91,F1
52,1,Relation Extraction,SemEval-2010 Task 8,2019-11,EPGNN,90.2,99.12,0.7,0.01,91.0,0.92,F1
53,1,Relation Extraction,SemEval-2010 Task 8,2020-04,REDN,91.0,100.0,0.8,0.01,91.0,0.92,F1
54,1,Question Answering,SQuAD1.1 dev,2016-08,Match-LSTM with Bi-Ans-Ptr (Boundary+Search+b) ,64.7,67.56,64.7,0.68,95.77,0.66,F1
55,1,Question Answering,SQuAD1.1 dev,2016-10,DCR,71.2,74.34,6.5,0.07,95.77,0.72,F1
56,1,Question Answering,SQuAD1.1 dev,2016-11,RASOR,74.9,78.21,3.7,0.04,95.77,0.76,F1
57,1,Question Answering,SQuAD1.1 dev,2016-11,BIDAF (single),77.3,80.71,2.4,0.03,95.77,0.79,F1
58,1,Question Answering,SQuAD1.1 dev,2017-03,SEDT-LSTM,77.42,80.84,0.1,0.0,95.77,0.79,F1
59,1,Question Answering,SQuAD1.1 dev,2017-03,FastQAExt (beam-size 5),78.5,81.97,1.1,0.01,95.77,0.8,F1
60,1,Question Answering,SQuAD1.1 dev,2017-03,DrQA (Document Reader only),78.8,82.28,0.3,0.0,95.77,0.8,F1
61,1,Question Answering,SQuAD1.1 dev,2017-04,Ruminating Reader,79.5,83.01,0.7,0.01,95.77,0.81,F1
62,1,Question Answering,SQuAD1.1 dev,2017-05,R.M-Reader (single),86.3,90.11,6.8,0.07,95.77,0.88,F1
63,1,Question Answering,SQuAD1.1 dev,2018-10,BERT large (+TriviaQA),91.1,95.12,4.8,0.05,95.77,0.93,F1
64,1,Question Answering,SQuAD1.1 dev,2019-06,XLNet (single model),95.1,99.3,4.0,0.04,95.77,0.97,F1
65,1,Question Answering,SQuAD1.1 dev,2019-10,T5-11B,95.64,99.86,0.5,0.01,95.77,0.97,F1
66,1,Question Answering,SQuAD1.1 dev,2019-11,XLNet+DSC,95.77,100.0,0.1,0.0,95.77,0.97,F1
67,1,Question Answering,SQuAD1.1,2016-08,Match-LSTM with Ans-Ptr (Boundary) (ensemble),77.022,81.01,77.022,0.81,95.08,0.78,F1
68,1,Question Answering,SQuAD1.1,2016-09,ReasoNet (single model),79.364,83.47,2.3,0.02,95.08,0.81,F1
69,1,Question Answering,SQuAD1.1,2016-09,ReasoNet (ensemble),82.552,86.82,3.2,0.03,95.08,0.84,F1
70,1,Question Answering,SQuAD1.1,2017-05,Reinforced Mnemonic Reader (ensemble model),88.533,93.11,6.0,0.06,95.08,0.9,F1
71,1,Question Answering,SQuAD1.1,2018-10,BERT (single model),91.835,96.59,3.3,0.03,95.08,0.93,F1
72,1,Question Answering,SQuAD1.1,2018-10,BERT (ensemble),93.16,97.98,1.3,0.01,95.08,0.95,F1
73,1,Question Answering,SQuAD1.1,2019-06,XLNet (single model),95.08,100.0,1.9,0.02,95.08,0.97,F1
74,1,Question Answering,NewsQA,2017-03,FastQAExt,56.1,76.22,56.1,0.76,73.6,0.57,F1
75,1,Question Answering,NewsQA,2018-01,AMANDA,63.7,86.55,7.6,0.1,73.6,0.65,F1
76,1,Question Answering,NewsQA,2018-11,DecaProp,66.3,90.08,2.6,0.04,73.6,0.67,F1
77,1,Question Answering,NewsQA,2019-07,SpanBERT,73.6,100.0,7.3,0.1,73.6,0.75,F1
78,1,Question Answering,TriviaQA,2017-05,Mnemonic Reader,52.85,63.22,52.85,0.63,83.6,0.54,F1
79,1,Question Answering,TriviaQA,2017-06,Reading Twice for NLU,56.73,67.86,3.9,0.05,83.6,0.58,F1
80,1,Question Answering,TriviaQA,2017-10,S-Norm,71.32,85.31,14.6,0.17,83.6,0.72,F1
81,1,Question Answering,TriviaQA,2018-10,MemoReader,73.26,87.63,1.9,0.02,83.6,0.74,F1
82,1,Question Answering,TriviaQA,2019-07,SpanBERT,83.6,100.0,10.3,0.12,83.6,0.85,F1
83,1,Relation Extraction,NYT,2017-06,NovelTagging,42.0,46.77,42.0,0.47,89.8,0.43,F1
84,1,Relation Extraction,NYT,2018-07,CopyRE MultiDecoder,58.7,65.37,16.7,0.19,89.8,0.6,F1
85,1,Relation Extraction,NYT,2019-07,GraphRel2p,61.9,68.93,3.2,0.04,89.8,0.63,F1
86,1,Relation Extraction,NYT,2019-09,CASREL,89.6,99.78,27.7,0.31,89.8,0.91,F1
87,1,Relation Extraction,NYT,2020-04,REDN,89.8,100.0,0.2,0.0,89.8,0.91,F1
88,1,Relation Extraction,NYT-single,2017-06,NovelTagging,49.5,83.33,49.5,0.83,59.4,0.5,F1
89,1,Relation Extraction,NYT-single,2019-07,PA-LSTM,53.8,90.57,4.3,0.07,59.4,0.55,F1
90,1,Relation Extraction,NYT-single,2019-09,ETL-Span,59.4,100.0,5.6,0.09,59.4,0.6,F1
91,1,Relation Extraction,WebNLG,2017-06,NovelTagging,28.3,29.39,28.3,0.29,96.3,0.29,F1
92,1,Relation Extraction,WebNLG,2018-07,CopyRE MultiDecoder,37.1,38.53,8.8,0.09,96.3,0.38,F1
93,1,Relation Extraction,WebNLG,2019-07,GraphRel2p,42.9,44.55,5.8,0.06,96.3,0.44,F1
94,1,Relation Extraction,WebNLG,2019-09,CASREL,91.8,95.33,48.9,0.51,96.3,0.93,F1
95,1,Relation Extraction,WebNLG,2020-04,REDN,96.3,100.0,4.5,0.05,96.3,0.98,F1
96,1,Emotion Recognition in Conversation,IEMOCAP,2017-07,bc-LSTM+Att,56.19,87.55,56.19,0.88,64.18,0.57,F1
97,1,Emotion Recognition in Conversation,IEMOCAP,2018-10,ICON,58.54,91.21,2.4,0.04,64.18,0.59,F1
98,1,Emotion Recognition in Conversation,IEMOCAP,2019-05,DialogueRNN,62.75,97.77,4.2,0.07,64.18,0.64,F1
99,1,Emotion Recognition in Conversation,IEMOCAP,2019-08,DialogueGCN,64.18,100.0,1.4,0.02,64.18,0.65,F1
100,1,Predicate Detection,CoNLL 2005,2017-07,DeepSRL,96.4,97.97,96.4,0.98,98.4,0.98,F1
101,1,Predicate Detection,CoNLL 2005,2018-04,LISA,98.4,100.0,2.0,0.02,98.4,1.0,F1
102,1,Semantic Role Labeling,OntoNotes,2017-07,He et al.,81.7,93.91,81.7,0.94,87.0,0.83,F1
103,1,Semantic Role Labeling,OntoNotes,2017-12,Tan et al.,82.7,95.06,1.0,0.01,87.0,0.84,F1
104,1,Semantic Role Labeling,OntoNotes,2018-02,"(He et al., 2017) + ELMo",84.6,97.24,1.9,0.02,87.0,0.86,F1
105,1,Semantic Role Labeling,OntoNotes,2018-07,"He et al.,",85.5,98.28,0.9,0.01,87.0,0.87,F1
106,1,Semantic Role Labeling,OntoNotes,2018-10,BiLSTM-Span (Ensemble),87.0,100.0,1.5,0.02,87.0,0.88,F1
107,1,Question Answering,COMPLEXQUESTIONS,2017-07,WebQA,53.6,100.0,53.6,1.0,53.6,0.54,F1
108,1,Sarcasm Detection,SCv1,2017-08,DeepMoji,0.69,100.0,0.69,1.0,0.69,0.01,F1
109,1,Relation Extraction,TACRED,2017-09,PA-LSTM,65.1,91.05,65.1,0.91,71.5,0.66,F1
110,1,Relation Extraction,TACRED,2018-09,GCN + PA-LSTM,67.1,93.85,2.0,0.03,71.5,0.68,F1
111,1,Relation Extraction,TACRED,2018-09,C-GCN + PA-LSTM,68.2,95.38,1.1,0.02,71.5,0.69,F1
112,1,Relation Extraction,TACRED,2019-05,R-BERT,69.4,97.06,1.2,0.02,71.5,0.71,F1
113,1,Relation Extraction,TACRED,2019-06,BERTEM+MTB,71.5,100.0,2.1,0.03,71.5,0.73,F1
114,1,Named Entity Recognition,Long-tail emerging entities,2017-09,SpinningBytes,40.78,81.24,40.78,0.81,50.2,0.41,F1
115,1,Named Entity Recognition,Long-tail emerging entities,2018-06,Aguilar et al.,45.55,90.74,4.8,0.1,50.2,0.46,F1
116,1,Named Entity Recognition,Long-tail emerging entities,2018-08,Flair embeddings,50.2,100.0,4.7,0.09,50.2,0.51,F1
117,1,Question Answering,SQuAD2.0,2017-11,FusionNet++ (ensemble),72.484,77.96,72.484,0.78,92.978,0.74,F1
118,1,Question Answering,SQuAD2.0,2017-12,SAN (ensemble model),73.704,79.27,1.2,0.01,92.978,0.75,F1
119,1,Question Answering,SQuAD2.0,2018-08,Reinforced Mnemonic Reader + Answer Verifier (single model),74.295,79.91,0.6,0.01,92.978,0.76,F1
120,1,Question Answering,SQuAD2.0,2018-10,BERT (single model),83.061,89.33,8.8,0.09,92.978,0.84,F1
121,1,Question Answering,SQuAD2.0,2019-06,XLNet (single model),90.689,97.54,7.6,0.08,92.978,0.92,F1
122,1,Question Answering,SQuAD2.0,2019-08,XLNet + SG-Net Verifier (ensemble),90.702,97.55,0.0,0.0,92.978,0.92,F1
123,1,Question Answering,SQuAD2.0,2019-09,ALBERT (ensemble model),92.215,99.18,1.5,0.02,92.978,0.94,F1
124,1,Question Answering,SQuAD2.0,2020-01,Retro-Reader (ensemble),92.978,100.0,0.8,0.01,92.978,0.94,F1
125,1,Sentence Classification,SciCite,2018-01,Feature-Rich Random Forest,79.6,93.76,79.6,0.94,84.9,0.81,F1
126,1,Sentence Classification,SciCite,2018-10,BERT,84.4,99.41,4.8,0.06,84.9,0.86,F1
127,1,Sentence Classification,SciCite,2019-03,SciBERT,84.9,100.0,0.5,0.01,84.9,0.86,F1
128,1,Citation Intent Classification,SciCite,2018-01,Feature-Rich Random Forest,79.6,93.66,79.6,0.94,84.99,0.81,F1
129,1,Citation Intent Classification,SciCite,2019-03,SciBERT,84.99,100.0,5.4,0.06,84.99,0.86,F1
130,1,Sentence Classification,ACL-ARC,2018-01,Random Forest,53.0,78.06,53.0,0.78,67.9,0.54,F1
131,1,Sentence Classification,ACL-ARC,2019-03,SciBERT (Base Vocab),65.79,96.89,12.8,0.19,67.9,0.67,F1
132,1,Sentence Classification,ACL-ARC,2019-04,Structural-scaffolds,67.9,100.0,2.1,0.03,67.9,0.69,F1
133,1,Entity Linking,WebQSP-WD,2018-04,VCG,0.73,100.0,0.73,1.0,0.73,0.01,F1
134,1,Predicate Detection,CoNLL 2012,2018-04,LISA,97.2,100.0,97.2,1.0,97.2,0.99,F1
135,1,Semantic Role Labeling,CoNLL 2005,2018-04,LISA,86.04,97.22,86.04,0.97,88.5,0.87,F1
136,1,Semantic Role Labeling,CoNLL 2005,2018-10,BiLSTM-Span (Ensemble),88.5,100.0,2.5,0.03,88.5,0.9,F1
137,1,Chinese Named Entity Recognition,OntoNotes 4,2018-05,Lattice,73.88,87.46,73.88,0.87,84.47,0.75,F1
138,1,Chinese Named Entity Recognition,OntoNotes 4,2019-10,BERT-MRC,82.11,97.21,8.2,0.1,84.47,0.83,F1
139,1,Chinese Named Entity Recognition,OntoNotes 4,2019-11,BERT-MRC+DSC,84.47,100.0,2.4,0.03,84.47,0.86,F1
140,1,Chinese Named Entity Recognition,MSRA,2018-05,Lattice,93.18,96.34,93.18,0.96,96.72,0.95,F1
141,1,Chinese Named Entity Recognition,MSRA,2019-01,Glyce + BERT,95.54,98.78,2.4,0.02,96.72,0.97,F1
142,1,Chinese Named Entity Recognition,MSRA,2019-10,BERT-MRC,95.75,99.0,0.2,0.0,96.72,0.97,F1
143,1,Chinese Named Entity Recognition,MSRA,2019-11,BERT-MRC+DSC,96.72,100.0,1.0,0.01,96.72,0.98,F1
144,1,Chinese Named Entity Recognition,Weibo NER,2018-05,Lattice,58.79,86.97,58.79,0.87,67.6,0.6,F1
145,1,Chinese Named Entity Recognition,Weibo NER,2019-01,Glyce + BERT,67.6,100.0,8.8,0.13,67.6,0.69,F1
146,1,Chinese Named Entity Recognition,Resume NER,2018-05,Lattice,94.46,97.85,94.46,0.98,96.54,0.96,F1
147,1,Chinese Named Entity Recognition,Resume NER,2019-01,Glyce + BERT,96.54,100.0,2.1,0.02,96.54,0.98,F1
148,1,Word Sense Disambiguation,SemEval 2015 Task 13,2018-05,GASext (Concatenation),72.6,87.89,72.6,0.88,82.6,0.74,F1
149,1,Word Sense Disambiguation,SemEval 2015 Task 13,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",74.46,90.15,1.9,0.02,82.6,0.76,F1
150,1,Word Sense Disambiguation,SemEval 2015 Task 13,2019-05,"SemCor+WNGC, hypernyms",82.6,100.0,8.1,0.1,82.6,0.84,F1
151,1,Named Entity Recognition,CoNLL 2000,2018-05,SWEM-CRF,90.34,100.0,90.34,1.0,90.34,0.92,F1
152,1,Paraphrase Identification,MSRP,2018-05,SWEM-concat,81.3,100.0,81.3,1.0,81.3,0.83,F1
153,1,Question Answering,CliCR,2018-06,Gated-Attention Reader,33.9,100.0,33.9,1.0,33.9,0.34,F1
154,1,Named Entity Recognition,GENIA,2018-06,Neural layered model,74.7,89.19,74.7,0.89,83.75,0.76,F1
155,1,Named Entity Recognition,GENIA,2018-10,Neural segmental hypergraphs,75.1,89.67,0.4,0.0,83.75,0.76,F1
156,1,Named Entity Recognition,GENIA,2019-07,seq2seq+BERT+Flair,78.31,93.5,3.2,0.04,83.75,0.8,F1
157,1,Named Entity Recognition,GENIA,2019-10,BERT-MRC,83.75,100.0,5.4,0.06,83.75,0.85,F1
158,1,Named Entity Recognition,ACE 2005,2018-06,Neural layered model,72.2,83.1,72.2,0.83,86.88,0.73,F1
159,1,Named Entity Recognition,ACE 2005,2018-10,Neural segmental hypergraphs,74.5,85.75,2.3,0.03,86.88,0.76,F1
160,1,Named Entity Recognition,ACE 2005,2019-06,Anchor-Region Networks,74.9,86.21,0.4,0.0,86.88,0.76,F1
161,1,Named Entity Recognition,ACE 2005,2019-06,MGNER,78.2,90.01,3.3,0.04,86.88,0.79,F1
162,1,Named Entity Recognition,ACE 2005,2019-06,Merge and Label,82.4,94.84,4.2,0.05,86.88,0.84,F1
163,1,Named Entity Recognition,ACE 2005,2019-07,seq2seq+BERT+Flair,84.33,97.06,1.9,0.02,86.88,0.86,F1
164,1,Named Entity Recognition,ACE 2005,2019-10,BERT-MRC,86.88,100.0,2.5,0.03,86.88,0.88,F1
165,1,Nested Mention Recognition,ACE 2005,2018-06,Neural layered model,72.2,83.1,72.2,0.83,86.88,0.73,F1
166,1,Nested Mention Recognition,ACE 2005,2018-10,Neural segmental hypergraphs,74.5,85.75,2.3,0.03,86.88,0.76,F1
167,1,Nested Mention Recognition,ACE 2005,2019-06,Anchor-Region Networks,74.9,86.21,0.4,0.0,86.88,0.76,F1
168,1,Nested Mention Recognition,ACE 2005,2019-06,MGNER,78.2,90.01,3.3,0.04,86.88,0.79,F1
169,1,Nested Mention Recognition,ACE 2005,2019-06,Merge and Label,82.4,94.84,4.2,0.05,86.88,0.84,F1
170,1,Nested Mention Recognition,ACE 2005,2019-08,seq2seq+BERT+Flair,84.33,97.06,1.9,0.02,86.88,0.86,F1
171,1,Nested Mention Recognition,ACE 2005,2019-10,BERT-MRC,86.88,100.0,2.5,0.03,86.88,0.88,F1
172,1,Nested Named Entity Recognition,GENIA,2018-06,Neural layered model,74.7,89.19,74.7,0.89,83.75,0.76,F1
173,1,Nested Named Entity Recognition,GENIA,2018-10,Neural segmental hypergraphs,75.1,89.67,0.4,0.0,83.75,0.76,F1
174,1,Nested Named Entity Recognition,GENIA,2019-08,seq2seq+BERT+Flair,78.31,93.5,3.2,0.04,83.75,0.8,F1
175,1,Nested Named Entity Recognition,GENIA,2019-10,BERT-MRC,83.75,100.0,5.4,0.06,83.75,0.85,F1
176,1,Multimodal Emotion Recognition,IEMOCAP,2018-06,CHFusion (A+T+V),0.768,100.0,0.768,1.0,0.768,0.01,F1
177,1,Open-Domain Question Answering,SearchQA,2018-07,Denoising QA,64.5,76.06,64.5,0.76,84.8,0.66,F1
178,1,Open-Domain Question Answering,SearchQA,2019-07,SpanBERT,84.8,100.0,20.3,0.24,84.8,0.86,F1
179,1,Dependency Parsing,GENIA,2018-08,BiLSTM-CRF,91.92,100.0,91.92,1.0,91.92,0.93,F1
180,1,Dependency Parsing,GENIA,2018-08,BiLSTM-CRF,92.84,100.0,92.84,1.0,92.84,0.94,F1
181,1,Question Answering,SQuAD2.0 dev,2018-08,RMR + ELMo (Model-III),74.8,82.56,74.8,0.83,90.6,0.76,F1
182,1,Question Answering,SQuAD2.0 dev,2018-10,BERT large,81.9,90.4,7.1,0.08,90.6,0.83,F1
183,1,Question Answering,SQuAD2.0 dev,2019-06,XLNet (single model),90.6,100.0,8.7,0.1,90.6,0.92,F1
184,1,Sentence Classification,PubMed 20k RCT,2018-08,Hierarchical Neural Networks,92.6,100.0,92.6,1.0,92.6,0.94,F1
185,1,Named Entity Recognition,JNLPBA,2018-09,CollaboNet,78.58,100.0,78.58,1.0,78.58,0.8,F1
186,1,Named Entity Recognition,BC5CDR,2018-09,CollaboNet,87.12,96.88,87.12,0.97,89.93,0.89,F1
187,1,Named Entity Recognition,BC5CDR,2019-03,SciBERT (SciVocab),88.94,98.9,1.8,0.02,89.93,0.9,F1
188,1,Named Entity Recognition,BC5CDR,2019-08,BioFLAIR,89.42,99.43,0.5,0.01,89.93,0.91,F1
189,1,Named Entity Recognition,BC5CDR,2019-11,NER+PA+RL (PubMed),89.93,100.0,0.5,0.01,89.93,0.91,F1
190,1,Chinese Named Entity Recognition,SighanNER,2018-10,BiLSTM+CRF+adversarial+self-attention,90.64,100.0,90.64,1.0,90.64,0.92,F1
191,1,Nested Named Entity Recognition,ACE 2004,2018-10,Neural segmental hypergraphs,75.1,87.35,75.1,0.87,85.98,0.76,F1
192,1,Nested Named Entity Recognition,ACE 2004,2019-06,MGNER,79.5,92.46,4.4,0.05,85.98,0.81,F1
193,1,Nested Named Entity Recognition,ACE 2004,2019-08,seq2seq+BERT+Flair,84.4,98.16,4.9,0.06,85.98,0.86,F1
194,1,Nested Named Entity Recognition,ACE 2004,2019-09,Second-best learning and decoding,84.97,98.83,0.6,0.01,85.98,0.86,F1
195,1,Nested Named Entity Recognition,ACE 2004,2019-10,BERT-MRC,85.98,100.0,1.0,0.01,85.98,0.87,F1
196,1,Named Entity Recognition,ACE 2004,2018-10,Neural segmental hypergraphs,75.1,87.35,75.1,0.87,85.98,0.76,F1
197,1,Named Entity Recognition,ACE 2004,2019-06,MGNER,79.5,92.46,4.4,0.05,85.98,0.81,F1
198,1,Named Entity Recognition,ACE 2004,2019-07,seq2seq+BERT+Flair,84.4,98.16,4.9,0.06,85.98,0.86,F1
199,1,Named Entity Recognition,ACE 2004,2019-09,Second-best learning and decoding,84.97,98.83,0.6,0.01,85.98,0.86,F1
200,1,Named Entity Recognition,ACE 2004,2019-10,BERT-MRC,85.98,100.0,1.0,0.01,85.98,0.87,F1
201,1,Nested Mention Recognition,ACE 2004,2018-10,Neural segmental hypergraphs,75.1,87.35,75.1,0.87,85.98,0.76,F1
202,1,Nested Mention Recognition,ACE 2004,2019-06,MGNER,79.5,92.46,4.4,0.05,85.98,0.81,F1
203,1,Nested Mention Recognition,ACE 2004,2019-08,seq2seq+BERT+Flair,84.4,98.16,4.9,0.06,85.98,0.86,F1
204,1,Nested Mention Recognition,ACE 2004,2019-09,Second-best learning and decoding,84.97,98.83,0.6,0.01,85.98,0.86,F1
205,1,Nested Mention Recognition,ACE 2004,2019-10,BERT-MRC,85.98,100.0,1.0,0.01,85.98,0.87,F1
206,1,Question Answering,QuAC,2018-10,FlowQA (single model),64.1,100.0,64.1,1.0,64.1,0.65,F1
207,1,Slot Filling,SNIPS,2018-12,Capsule-NLU,0.918,99.53,0.918,1.0,0.9223,0.01,F1
208,1,Slot Filling,SNIPS,2019-06,SF-ID,0.9223,100.0,0.0,0.0,0.9223,0.01,F1
209,1,Slot Filling,ATIS,2018-12,Capsule-NLU,0.952,99.37,0.952,0.99,0.958,0.01,F1
210,1,Slot Filling,ATIS,2019-06,SF-ID,0.958,100.0,0.0,0.0,0.958,0.01,F1
211,1,Relation Extraction,ChemProt,2019-01,BioBERT,76.46,100.0,76.46,1.0,76.46,0.78,F1
212,1,Named Entity Recognition,NCBI-disease,2019-01,BioBERT,89.71,100.0,89.71,1.0,89.71,0.91,F1
213,1,Chinese Word Segmentation,MSR,2019-01,Glyce + BERT,98.3,99.95,98.3,1.0,98.35,1.0,F1
214,1,Chinese Word Segmentation,MSR,2019-11,ZEN (Init with Chinese BERT),98.35,100.0,0.0,0.0,98.35,1.0,F1
215,1,Chinese Word Segmentation,AS,2019-01,Glyce + BERT,96.7,100.0,96.7,1.0,96.7,0.98,F1
216,1,Chinese Word Segmentation,PKU,2019-01,Glyce + BERT,96.7,100.0,96.7,1.0,96.7,0.98,F1
217,1,Chinese Word Segmentation,CITYU,2019-01,Glyce + BERT,97.9,100.0,97.9,1.0,97.9,0.99,F1
218,1,Chinese Named Entity Recognition,OntoNotes,2019-01,Glyce + BERT,80.62,100.0,80.62,1.0,80.62,0.82,F1
219,1,Document Classification,Reuters-21578,2019-02,VLAWE,89.3,100.0,89.3,1.0,89.3,0.91,F1
220,1,Sentence Classification,ScienceCite,2019-03,SciBERT (SciVocab),84.99,100.0,84.99,1.0,84.99,0.86,F1
221,1,Relation Extraction,SciERC,2019-03,SciBERT (SciVocab),74.64,100.0,74.64,1.0,74.64,0.76,F1
222,1,Sentence Classification,Paper Field,2019-03,SciBERT (SciVocab),64.07,100.0,64.07,1.0,64.07,0.65,F1
223,1,Named Entity Recognition,WetLab,2019-04,BiLSTM-CRF with ELMo,79.62,100.0,79.62,1.0,79.62,0.81,F1
224,1,Speech Emotion Recognition,IEMOCAP,2019-04,Ensemble (Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression) / (A+T),0.718,100.0,0.718,1.0,0.718,0.01,F1
225,1,Document Classification,AAPD,2019-04,KD-LSTMreg,72.9,100.0,72.9,1.0,72.9,0.74,F1
226,1,Chinese Named Entity Recognition,MSRA Dev,2019-04,ERNIE,95.0,98.65,95.0,0.99,96.3,0.97,F1
227,1,Chinese Named Entity Recognition,MSRA Dev,2019-07,ERNIE 2.0 Large,96.3,100.0,1.3,0.01,96.3,0.98,F1
228,1,Relation Extraction,FewRel,2019-05,ERNIE,88.32,100.0,88.32,1.0,88.32,0.9,F1
229,1,Entity Typing,Open Entity,2019-05,ERNIE,75.56,100.0,75.56,1.0,75.56,0.77,F1
230,1,Relation Extraction,DocRED,2019-06,Context-Aware,50.7,84.63,50.7,0.85,59.91,0.52,F1
231,1,Relation Extraction,DocRED,2019-06,BiLSTM,51.06,85.23,0.4,0.01,59.91,0.52,F1
232,1,Relation Extraction,DocRED,2019-09,BERT-Two-Step,53.92,90.0,2.9,0.05,59.91,0.55,F1
233,1,Relation Extraction,DocRED,2020-03,HIN-BERT,55.6,92.81,1.7,0.03,59.91,0.57,F1
234,1,Relation Extraction,DocRED,2020-04,CorefRoBERTaLarge,59.91,100.0,4.3,0.07,59.91,0.61,F1
235,1,Sentiment Analysis,ChnSentiCorp Dev,2019-06,RoBERTa-wwm-ext-large,95.8,100.0,95.8,1.0,95.8,0.97,F1
236,1,Sentiment Analysis,ChnSentiCorp,2019-06,RoBERTa-wwm-ext-large,95.8,100.0,95.8,1.0,95.8,0.97,F1
237,1,Question Answering,NaturalQA,2019-07,SpanBERT,82.5,100.0,82.5,1.0,82.5,0.84,F1
238,1,Intent Detection,ATIS,2019-08,DELTA (BLSTM-CRF),0.952,100.0,0.952,1.0,0.952,0.01,F1
239,1,Named Entity Recognition,LINNAEUS,2019-08,BioFLAIR,87.02,100.0,87.02,1.0,87.02,0.88,F1
240,1,Named Entity Recognition,Species-800,2019-08,BioFLAIR,82.44,100.0,82.44,1.0,82.44,0.84,F1
241,1,Named Entity Recognition,Code-Switching English-Spanish NER,2019-09,HME (word + BPE + char),69.17,100.0,69.17,1.0,69.17,0.7,F1
242,1,Named Entity Recognition,ontontoes chinese v5,2019-09,DGLSTM-CRF,79.92,100.0,79.92,1.0,79.92,0.81,F1
243,1,Question Answering,ReCoRD,2019-10,T5-11B,93.3,100.0,93.3,1.0,93.3,0.95,F1
244,1,Natural Language Inference,CommitmentBank,2019-10,T5-11B,93.0,100.0,93,1.0,93,0.95,F1
245,1,Named Entity Recognition,French Treebank,2019-11,CamemBERT (subword masking),87.93,100.0,87.93,1.0,87.93,0.89,F1
246,1,Negation Scope Resolution,SFU Review Corpus,2019-11,NegBERT,90.95,99.67,90.95,1.0,91.25,0.92,F1
247,1,Negation Scope Resolution,SFU Review Corpus,2020-01,XLNet,91.25,100.0,0.3,0.0,91.25,0.93,F1
248,1,Negation Scope Resolution,_sem 2012 Shared Task: Sherlock Dataset,2019-11,NegBERT,92.36,100.0,92.36,1.0,92.36,0.94,F1
249,1,Negation Scope Resolution,BioScope : Abstracts,2019-11,NegBERT,95.68,99.94,95.68,1.0,95.74,0.97,F1
250,1,Negation Scope Resolution,BioScope : Abstracts,2020-01,XLNet,95.74,100.0,0.1,0.0,95.74,0.97,F1
251,1,Negation Scope Resolution,BioScope : Full Papers,2019-11,NegBERT,91.24,96.65,91.24,0.97,94.4,0.93,F1
252,1,Negation Scope Resolution,BioScope : Full Papers,2020-01,XLNet,94.4,100.0,3.2,0.03,94.4,0.96,F1
253,1,Intent Detection,ASOS.com user intent,2019-12,plain-LSTM,0.887,100.0,0.887,1.0,0.887,0.01,F1
254,1,Speculation Scope Resolution,BioScope : Full Papers,2020-01,XLNet,96.91,100.0,96.91,1.0,96.91,0.98,F1
255,1,Speculation Scope Resolution,SFU Review Corpus,2020-01,XLNet,91.0,100.0,91,1.0,91,0.92,F1
256,1,Speculation Scope Resolution,BioScope : Abstracts,2020-01,XLNet,97.87,100.0,97.87,1.0,97.87,0.99,F1
257,1,Sentiment Analysis,DBRD,2020-01,RobBERT,94.422,100.0,94.422,1.0,94.422,0.96,F1
258,1,Question Answering,FQuAD,2020-02,CamemBERTQA,88.0,100.0,88,1.0,88,0.89,F1
259,1,Named Entity Recognition,SoSciSoCi,2020-03,Bi-LSTM-CRF (SSC->GSC),0.82,100.0,0.82,1.0,0.82,0.01,F1
0,1,Sentiment Analysis,SST-2 Binary classification,2013-10,RNTN,85.4,87.68,85.4,0.88,97.4,0.86,Accuracy
1,1,Sentiment Analysis,SST-2 Binary classification,2014-08,CNN-MC [kim:13],88.1,90.45,2.7,0.03,97.4,0.88,Accuracy
2,1,Sentiment Analysis,SST-2 Binary classification,2015-06,DMN [ankit16],88.6,90.97,0.5,0.01,97.4,0.89,Accuracy
3,1,Sentiment Analysis,SST-2 Binary classification,2016-03,CNN + Logic rules,89.3,91.68,0.7,0.01,97.4,0.9,Accuracy
4,1,Sentiment Analysis,SST-2 Binary classification,2016-07,Neural Semantic Encoder,89.7,92.09,0.4,0.0,97.4,0.9,Accuracy
5,1,Sentiment Analysis,SST-2 Binary classification,2017-04,bmLSTM,91.8,94.25,2.1,0.02,97.4,0.92,Accuracy
6,1,Sentiment Analysis,SST-2 Binary classification,2017-12,Block-sparse LSTM,93.2,95.69,1.4,0.01,97.4,0.94,Accuracy
7,1,Sentiment Analysis,SST-2 Binary classification,2019-01,MT-DNN,95.6,98.15,2.4,0.02,97.4,0.96,Accuracy
8,1,Sentiment Analysis,SST-2 Binary classification,2019-06,XLNet (single model),97.0,99.59,1.4,0.01,97.4,0.97,Accuracy
9,1,Sentiment Analysis,SST-2 Binary classification,2019-09,ALBERT,97.1,99.69,0.1,0.0,97.4,0.97,Accuracy
10,1,Sentiment Analysis,SST-2 Binary classification,2019-10,T5-3B,97.4,100.0,0.3,0.0,97.4,0.98,Accuracy
11,1,Sentiment Analysis,SST-5 Fine-grained classification,2013-10,RNTN,45.7,82.34,45.7,0.82,55.5,0.46,Accuracy
12,1,Sentiment Analysis,SST-5 Fine-grained classification,2014-06,Epic,49.6,89.37,3.9,0.07,55.5,0.5,Accuracy
13,1,Sentiment Analysis,SST-5 Fine-grained classification,2015-02,Constituency Tree-LSTM,51.0,91.89,1.4,0.03,55.5,0.51,Accuracy
14,1,Sentiment Analysis,SST-5 Fine-grained classification,2017-08,BCN+Char+CoVe,53.7,96.76,2.7,0.05,55.5,0.54,Accuracy
15,1,Sentiment Analysis,SST-5 Fine-grained classification,2018-02,BCN+ELMo,54.7,98.56,1.0,0.02,55.5,0.55,Accuracy
16,1,Sentiment Analysis,SST-5 Fine-grained classification,2019-10,BERT Large,55.5,100.0,0.8,0.01,55.5,0.56,Accuracy
17,1,Semantic Textual Similarity,MRPC,2013-10,TF-KLD,80.4,86.08,80.4,0.86,93.4,0.81,Accuracy
18,1,Semantic Textual Similarity,MRPC,2019-05,ERNIE,88.2,94.43,7.8,0.08,93.4,0.89,Accuracy
19,1,Semantic Textual Similarity,MRPC,2019-06,XLNet (single model),90.8,97.22,2.6,0.03,93.4,0.91,Accuracy
20,1,Semantic Textual Similarity,MRPC,2019-07,SpanBERT,90.9,97.32,0.1,0.0,93.4,0.91,Accuracy
21,1,Semantic Textual Similarity,MRPC,2019-07,RoBERTa,92.3,98.82,1.4,0.01,93.4,0.93,Accuracy
22,1,Semantic Textual Similarity,MRPC,2019-09,ALBERT,93.4,100.0,1.1,0.01,93.4,0.94,Accuracy
23,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 English-to-German,2013-12,biCVM+,86.2,92.99,86.2,0.93,92.7,0.87,Accuracy
24,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 English-to-German,2014-04,Bi+,88.1,95.04,1.9,0.02,92.7,0.88,Accuracy
25,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 English-to-German,2014-12,Biinclusion (Euro500kReuters),92.7,100.0,4.6,0.05,92.7,0.93,Accuracy
26,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 German-to-English,2013-12,biCVM+,76.9,91.11,76.9,0.91,84.4,0.77,Accuracy
27,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 German-to-English,2014-04,Bi+,79.2,93.84,2.3,0.03,84.4,0.8,Accuracy
28,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 German-to-English,2014-12,Biinclusion (Euro500kReuters),84.4,100.0,5.2,0.06,84.4,0.85,Accuracy
29,1,Document Classification,Cora,2014-03,DeepWalk,67.2,80.48,67.2,0.8,83.5,0.67,Accuracy
30,1,Document Classification,Cora,2016-03,Planetoid*,75.7,90.66,8.5,0.1,83.5,0.76,Accuracy
31,1,Document Classification,Cora,2016-09,Graph-CNN,81.5,97.6,5.8,0.07,83.5,0.82,Accuracy
32,1,Document Classification,Cora,2016-11,MoNet,81.7,97.84,0.2,0.0,83.5,0.82,Accuracy
33,1,Document Classification,Cora,2017-10,GAT,83.0,99.4,1.3,0.02,83.5,0.83,Accuracy
34,1,Document Classification,Cora,2018-08,LGCN,83.3,99.76,0.3,0.0,83.5,0.84,Accuracy
35,1,Document Classification,Cora,2019-04,ACNet,83.5,100.0,0.2,0.0,83.5,0.84,Accuracy
36,1,Question Answering,Reverb,2014-04,Weakly Supervised Embeddings,73.0,100.0,73,1.0,73,0.73,Accuracy
37,1,Text Classification,IMDb,2014-05,Paragraph Vectors Le & Mikolov (2014),92.58,95.64,92.58,0.96,96.8,0.93,Accuracy
38,1,Text Classification,IMDb,2019-01,HAHNN (CNN),95.17,98.32,2.6,0.03,96.8,0.96,Accuracy
39,1,Text Classification,IMDb,2019-04,BERT Finetune + UDA,95.8,98.97,0.6,0.01,96.8,0.96,Accuracy
40,1,Text Classification,IMDb,2019-06,XLNet,96.8,100.0,1.0,0.01,96.8,0.97,Accuracy
41,1,Document Classification,Reuters En-De,2014-10,BilBOWA,86.5,100.0,86.5,1.0,86.5,0.87,Accuracy
42,1,Document Classification,Reuters De-En,2014-10,BilBOWA,75.0,100.0,75,1.0,75,0.75,Accuracy
43,1,Semantic Parsing,ATIS,2014-11,"ZH15 (Zhao and Huang, 2015)",84.2,97.68,84.2,0.98,86.2,0.85,Accuracy
44,1,Semantic Parsing,ATIS,2017-04,"ASN (Rabinovich et al., 2017)",85.3,98.96,1.1,0.01,86.2,0.86,Accuracy
45,1,Semantic Parsing,ATIS,2018-10,Tranx,86.2,100.0,0.9,0.01,86.2,0.87,Accuracy
46,1,Sentiment Analysis,IMDb,2014-12,seq2-bown-CNN,92.33,94.79,92.33,0.95,97.4,0.93,Accuracy
47,1,Sentiment Analysis,IMDb,2016-02,oh-LSTM,94.1,96.61,1.8,0.02,97.4,0.94,Accuracy
48,1,Sentiment Analysis,IMDb,2017-12,Block-sparse LSTM,94.99,97.53,0.9,0.01,97.4,0.95,Accuracy
49,1,Sentiment Analysis,IMDb,2018-01,ULMFiT,95.4,97.95,0.4,0.0,97.4,0.96,Accuracy
50,1,Sentiment Analysis,IMDb,2019-02,L MIXED,95.68,98.23,0.3,0.0,97.4,0.96,Accuracy
51,1,Sentiment Analysis,IMDb,2019-04,BERT large finetune UDA,95.8,98.36,0.1,0.0,97.4,0.96,Accuracy
52,1,Sentiment Analysis,IMDb,2019-06,GraphStar,96.0,98.56,0.2,0.0,97.4,0.96,Accuracy
53,1,Sentiment Analysis,IMDb,2019-07,NB-weighted-BON + dv-cosine,97.4,100.0,1.4,0.01,97.4,0.98,Accuracy
54,1,Subjectivity Analysis,SUBJ,2015-04,AdaSent,95.5,100.0,95.5,1.0,95.5,0.96,Accuracy
55,1,Part-Of-Speech Tagging,Penn Treebank,2015-08,Bi-LSTM,97.36,99.39,97.36,0.99,97.96,0.98,Accuracy
56,1,Part-Of-Speech Tagging,Penn Treebank,2015-08,Char Bi-LSTM,97.78,99.82,0.4,0.0,97.96,0.98,Accuracy
57,1,Part-Of-Speech Tagging,Penn Treebank,2018-05,Meta BiLSTM,97.96,100.0,0.2,0.0,97.96,0.98,Accuracy
58,1,Machine Translation,20NEWS,2015-08,12,1.0,100.0,1,1.0,1,0.01,Accuracy
59,1,Entity Linking,AIDA-CoNLL,2016-01,Wikipedia2Vec-GBRT,93.1,98.31,93.1,0.98,94.7,0.93,Accuracy
60,1,Entity Linking,AIDA-CoNLL,2017-05,NTEE,94.7,100.0,1.6,0.02,94.7,0.95,Accuracy
61,1,Text Classification,RCV1,2016-02,One-hot CNN+ Johnson & Zhang ([2016b]),94.13,100.0,94.13,1.0,94.13,0.95,Accuracy
62,1,Dialog Act Classification,Switchboard corpus,2016-03,CNN[[Lee and Dernoncourt2016]],73.1,89.91,73.1,0.9,81.3,0.73,Accuracy
63,1,Dialog Act Classification,Switchboard corpus,2017-09,Bi-LSTM-CRF,79.2,97.42,6.1,0.08,81.3,0.8,Accuracy
64,1,Dialog Act Classification,Switchboard corpus,2017-11,CRF-ASN,81.3,100.0,2.1,0.03,81.3,0.82,Accuracy
65,1,Question Answering,MCTest-160,2016-03,"syntax, frame, coreference, and word embedding features",75.27,100.0,75.27,1.0,75.27,0.76,Accuracy
66,1,Question Answering,MCTest-500,2016-03,Parallel-Hierarchical,71.0,100.0,71.0,1.0,71.0,0.71,Accuracy
67,1,Question Answering,Story Cloze Test,2016-06,Memory chains and semantic supervision,78.7,100.0,78.7,1.0,78.7,0.79,Accuracy
68,1,Phrase Grounding,Flickr30k Entities Test,2016-06,MCB,48.69,100.0,48.69,1.0,48.69,0.49,Accuracy
69,1,Phrase Grounding,ReferIt,2016-06,MCB,28.91,100.0,28.91,1.0,28.91,0.29,Accuracy
70,1,Sentiment Analysis,Amazon Review Polarity,2016-07,FastText,94.6,97.16,94.6,0.97,97.37,0.95,Accuracy
71,1,Sentiment Analysis,Amazon Review Polarity,2017-07,DPCNN,96.68,99.29,2.1,0.02,97.37,0.97,Accuracy
72,1,Sentiment Analysis,Amazon Review Polarity,2019-04,BERT large,97.37,100.0,0.7,0.01,97.37,0.98,Accuracy
73,1,Sentiment Analysis,Sogou News,2016-07,"fastText, h=10, bigram",96.8,100.0,96.8,1.0,96.8,0.97,Accuracy
74,1,Text Classification,Yahoo! Answers,2016-07,FastText,72.3,93.15,72.3,0.93,77.62,0.73,Accuracy
75,1,Text Classification,Yahoo! Answers,2018-05,SWEM-concat,73.53,94.73,1.2,0.02,77.62,0.74,Accuracy
76,1,Text Classification,Yahoo! Answers,2018-07,DRNN,76.26,98.25,2.7,0.03,77.62,0.77,Accuracy
77,1,Text Classification,Yahoo! Answers,2019-05,BERT-ITPT-FiT,77.62,100.0,1.4,0.02,77.62,0.78,Accuracy
78,1,Sentiment Analysis,Amazon Review Full,2016-07,FastText,60.2,91.45,60.2,0.91,65.83,0.6,Accuracy
79,1,Sentiment Analysis,Amazon Review Full,2017-07,DPCNN,65.19,99.03,5.0,0.08,65.83,0.65,Accuracy
80,1,Sentiment Analysis,Amazon Review Full,2019-04,BERT large,65.83,100.0,0.6,0.01,65.83,0.66,Accuracy
81,1,Sentiment Analysis,MR,2017-02,GRU-RNN-WORD2VEC,78.26,90.16,78.26,0.9,86.8,0.79,Accuracy
82,1,Sentiment Analysis,MR,2018-02,RNN-Capsule,83.8,96.54,5.5,0.06,86.8,0.84,Accuracy
83,1,Sentiment Analysis,MR,2018-05,byte mLSTM7,86.8,100.0,3.0,0.03,86.8,0.87,Accuracy
84,1,Paraphrase Identification,Quora Question Pairs,2017-02,BiMPM,88.17,98.4,88.17,0.98,89.6,0.89,Accuracy
85,1,Paraphrase Identification,Quora Question Pairs,2017-04,pt-DecAtt,88.4,98.66,0.2,0.0,89.6,0.89,Accuracy
86,1,Paraphrase Identification,Quora Question Pairs,2017-09,DIIN,89.06,99.4,0.7,0.01,89.6,0.89,Accuracy
87,1,Paraphrase Identification,Quora Question Pairs,2019-01,MT-DNN,89.6,100.0,0.5,0.01,89.6,0.9,Accuracy
88,1,Stance Detection,RumourEval,2017-04,Kochkina et al. 2017,0.784,100.0,0.784,1.0,0.784,0.01,Accuracy
89,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-German,2017-05,X-BiLSTM,67.7,96.03,67.7,0.96,70.5,0.68,Accuracy
90,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-German,2018-10,BERT,70.5,100.0,2.8,0.04,70.5,0.71,Accuracy
91,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-French,2017-05,X-BiLSTM,67.7,100.0,67.7,1.0,67.7,0.68,Accuracy
92,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-Spanish,2017-05,X-BiLSTM,68.7,92.46,68.7,0.92,74.3,0.69,Accuracy
93,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-Spanish,2018-10,BERT,74.3,100.0,5.6,0.08,74.3,0.75,Accuracy
94,1,Multimodal Emotion Recognition,Monologue,2017-07,bc-LSTM,74.1,100.0,74.1,1.0,74.1,0.74,Accuracy
95,1,Multimodal Sentiment Analysis,MOSI,2017-07,bc-LSTM,80.3,97.56,80.3,0.98,82.31,0.81,Accuracy
96,1,Multimodal Sentiment Analysis,MOSI,2018-10,MMMU-BA,82.31,100.0,2.0,0.02,82.31,0.83,Accuracy
97,1,Text Classification,Ohsumed,2017-07,CNN+Lowercased,36.2,52.85,36.2,0.53,68.5,0.36,Accuracy
98,1,Text Classification,Ohsumed,2018-09,Text GCN,68.36,99.8,32.2,0.47,68.5,0.69,Accuracy
99,1,Text Classification,Ohsumed,2019-02,SGCN,68.5,100.0,0.1,0.0,68.5,0.69,Accuracy
100,1,Document Classification,WOS-5736,2017-09,HDLTex,90.93,97.18,90.93,0.97,93.57,0.91,Accuracy
101,1,Document Classification,WOS-5736,2018-05,RMDL (30 RDLs),93.57,100.0,2.6,0.03,93.57,0.94,Accuracy
102,1,Document Classification,WOS-11967,2017-09,HDLTex,86.07,93.97,86.07,0.94,91.59,0.86,Accuracy
103,1,Document Classification,WOS-11967,2018-05,RMDL (30 RDLs),91.59,100.0,5.5,0.06,91.59,0.92,Accuracy
104,1,Document Classification,WOS-46985,2017-09,HDLTex,76.58,92.91,76.58,0.93,82.42,0.77,Accuracy
105,1,Document Classification,WOS-46985,2018-05,RMDL (30 RDLs),82.42,100.0,5.8,0.07,82.42,0.83,Accuracy
106,1,Lexical Normalization,LexNorm,2017-10,MoNoise,87.63,100.0,87.63,1.0,87.63,0.88,Accuracy
107,1,Sentiment Analysis,CR,2017-12,Block-sparse LSTM,92.2,100.0,92.2,1.0,92.2,0.93,Accuracy
108,1,Natural Language Inference,SciTail,2017-12,CAFE,83.3,88.52,83.3,0.89,94.1,0.84,Accuracy
109,1,Natural Language Inference,SciTail,2018-08,Hierarchical BiLSTM Max Pooling,86.0,91.39,2.7,0.03,94.1,0.86,Accuracy
110,1,Natural Language Inference,SciTail,2018-10,BERT,92.0,97.77,6.0,0.06,94.1,0.92,Accuracy
111,1,Natural Language Inference,SciTail,2019-01,MT-DNN,94.1,100.0,2.1,0.02,94.1,0.94,Accuracy
112,1,Sentiment Analysis,MPQA,2018-03,USE_T+DAN (w2v w.e.) ,88.14,98.12,88.14,0.98,89.83,0.89,Accuracy
113,1,Sentiment Analysis,MPQA,2018-05,byte mLSTM7,88.8,98.85,0.7,0.01,89.83,0.89,Accuracy
114,1,Sentiment Analysis,MPQA,2019-05,STM+TSED+PT+2L,89.83,100.0,1.0,0.01,89.83,0.9,Accuracy
115,1,Text Classification,20NEWS,2018-05,RMDL (15 RDLs),87.91,99.33,87.91,0.99,88.5,0.88,Accuracy
116,1,Text Classification,20NEWS,2019-02,SGC,88.5,100.0,0.6,0.01,88.5,0.89,Accuracy
117,1,Document Classification,Reuters-21578,2018-05,RMDL (30 RDLs),90.69,93.07,90.69,0.93,97.44,0.91,Accuracy
118,1,Document Classification,Reuters-21578,2019-04,ApproxRepSet,97.17,99.72,6.5,0.07,97.44,0.98,Accuracy
119,1,Document Classification,Reuters-21578,2019-08,MPAD-path,97.44,100.0,0.3,0.0,97.44,0.98,Accuracy
120,1,Semantic Parsing,Geo,2018-05,coarse2fine,88.2,100.0,88.2,1.0,88.2,0.89,Accuracy
121,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German,2018-05,MultiCCA + CNN,81.2,83.75,81.2,0.84,96.95,0.82,Accuracy
122,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German,2018-12,Massively Multilingual Sentence Embeddings,84.78,87.45,3.6,0.04,96.95,0.85,Accuracy
123,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German,2019-09,"MultiFiT, pseudo",91.62,94.5,6.8,0.07,96.95,0.92,Accuracy
124,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German,2019-09,XLMft UDA,96.95,100.0,5.3,0.05,96.95,0.97,Accuracy
125,1,Paraphrase Identification,MSRP,2018-05,SWEM-concat,71.5,100.0,71.5,1.0,71.5,0.72,Accuracy
126,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Chinese,2018-05,MultiCCA + CNN,74.73,80.08,74.73,0.8,93.32,0.75,Accuracy
127,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Chinese,2019-09,"MultiFiT, pseudo",82.48,88.38,7.8,0.08,93.32,0.83,Accuracy
128,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Chinese,2019-09,XLMft UDA,93.32,100.0,10.8,0.12,93.32,0.94,Accuracy
129,1,Cross-Lingual Document Classification,MLDoc Zero-Shot German-to-French,2018-05,BiLSTM (Europarl),75.45,100.0,75.45,1.0,75.45,0.76,Accuracy
130,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish,2018-05,MultiCCA + CNN,72.5,74.9,72.5,0.75,96.8,0.73,Accuracy
131,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish,2018-12,Massively Multilingual Sentence Embeddings,77.33,79.89,4.8,0.05,96.8,0.78,Accuracy
132,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish,2019-09,"MultiFiT, pseudo",79.1,81.71,1.8,0.02,96.8,0.79,Accuracy
133,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish,2019-09,XLMft UDA,96.8,100.0,17.7,0.18,96.8,0.97,Accuracy
134,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French,2018-05,BiLSTM (UN),74.52,77.58,74.52,0.78,96.05,0.75,Accuracy
135,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French,2018-12,Massively Multilingual Sentence Embeddings,77.95,81.16,3.4,0.04,96.05,0.78,Accuracy
136,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French,2019-09,"MultiFiT, pseudo",89.42,93.1,11.5,0.12,96.05,0.9,Accuracy
137,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French,2019-09,XLMft UDA,96.05,100.0,6.6,0.07,96.05,0.96,Accuracy
138,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian,2018-05,BiLSTM (UN),61.42,68.47,61.42,0.68,89.7,0.62,Accuracy
139,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian,2018-12,Massively Multilingual Sentence Embeddings,67.78,75.56,6.4,0.07,89.7,0.68,Accuracy
140,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian,2019-09,"MultiFiT, pseudo",67.83,75.62,0.0,0.0,89.7,0.68,Accuracy
141,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian,2019-09,XLMft UDA,89.7,100.0,21.9,0.24,89.7,0.9,Accuracy
142,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Japanese,2018-05,MultiCCA + CNN,67.63,97.21,67.63,0.97,69.57,0.68,Accuracy
143,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Japanese,2019-09,"MultiFiT, pseudo",69.57,100.0,1.9,0.03,69.57,0.7,Accuracy
144,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Italian,2018-05,MultiCCA + CNN,69.38,91.27,69.38,0.91,76.02,0.7,Accuracy
145,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Italian,2018-12,Massively Multilingual Sentence Embeddings,69.43,91.33,0.1,0.0,76.02,0.7,Accuracy
146,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Italian,2019-09,"MultiFiT, pseudo",76.02,100.0,6.6,0.09,76.02,0.76,Accuracy
147,1,Question Answering,Quora Question Pairs,2018-05,SWEM-concat,83.03,89.96,83.03,0.9,92.3,0.83,Accuracy
148,1,Question Answering,Quora Question Pairs,2019-06,XLNet (single model),92.3,100.0,9.3,0.1,92.3,0.93,Accuracy
149,1,Memex Question Answering,MemexQA,2018-06,FVTA,0.357,100.0,0.357,1.0,0.357,0.0,Accuracy
150,1,Entity Typing,Freebase FIGER,2018-06,TextEnt-full,37.4,100.0,37.4,1.0,37.4,0.38,Accuracy
151,1,Text Classification,R8,2018-06,TextEnt-full,96.7,98.77,96.7,0.99,97.9,0.97,Accuracy
152,1,Text Classification,R8,2018-09,Text GCN,97.07,99.15,0.4,0.0,97.9,0.97,Accuracy
153,1,Text Classification,R8,2019-02,SGC,97.2,99.28,0.1,0.0,97.9,0.98,Accuracy
154,1,Text Classification,R8,2019-06,GraphStar,97.4,99.49,0.2,0.0,97.9,0.98,Accuracy
155,1,Text Classification,R8,2019-09,NABoE-full,97.9,100.0,0.5,0.01,97.9,0.98,Accuracy
156,1,Text-To-Speech Synthesis,LJSpeech,2018-06,tacotron,12.0,100.0,12,1.0,12,0.12,Accuracy
157,1,Natural Language Inference,V-SNLI,2018-06,V-BiMPM,86.99,100.0,86.99,1.0,86.99,0.87,Accuracy
158,1,Natural Language Inference,MultiNLI,2018-06,MQAN,72.8,100.0,72.8,1.0,72.8,0.73,Accuracy
159,1,Multimodal Sentiment Analysis,CMU-MOSEI,2018-07,Graph-MFN,76.9,93.67,76.9,0.94,82.1,0.77,Accuracy
160,1,Multimodal Sentiment Analysis,CMU-MOSEI,2020-02,Multilogue-Net,82.1,100.0,5.2,0.06,82.1,0.82,Accuracy
161,1,Question Answering,SWAG,2018-08,BERT Large,86.3,100.0,86.3,1.0,86.3,0.87,Accuracy
162,1,Query Wellformedness,Query Wellformedness,2018-08,"word-1, 2 POS-1, 2, 3",70.7,100.0,70.7,1.0,70.7,0.71,Accuracy
163,1,Natural Language Inference,XNLI French,2018-09,BiLSTM-max,68.3,84.11,68.3,0.84,81.2,0.69,Accuracy
164,1,Natural Language Inference,XNLI French,2019-01,XLM (MLM+TLM),80.2,98.77,11.9,0.15,81.2,0.81,Accuracy
165,1,Natural Language Inference,XNLI French,2019-11,CamemBERT,81.2,100.0,1.0,0.01,81.2,0.82,Accuracy
166,1,Text Classification,R52,2018-09,Text GCN,93.56,98.48,93.56,0.98,95.0,0.94,Accuracy
167,1,Text Classification,R52,2019-02,SGC,94.0,98.95,0.4,0.0,95.0,0.94,Accuracy
168,1,Text Classification,R52,2019-06,GraphStar,95.0,100.0,1.0,0.01,95.0,0.95,Accuracy
169,1,Semantic Parsing,spider,2018-09,Exact Set Matching,19.7,100.0,19.7,1.0,19.7,0.2,Accuracy
170,1,Text Classification,Sogou News,2018-10,CCCapsNet,97.25,99.16,97.25,0.99,98.07,0.98,Accuracy
171,1,Text Classification,Sogou News,2019-05,BERT-ITPT-FiT,98.07,100.0,0.8,0.01,98.07,0.98,Accuracy
172,1,Natural Language Inference,Quora Question Pairs,2018-12,aESIM,88.01,100.0,88.01,1.0,88.01,0.88,Accuracy
173,1,Hate Speech Detection,Automatic Misogynistic Identification,2018-12,Logistic Regression,0.704,100.0,0.704,1.0,0.704,0.01,Accuracy
174,1,Intent Detection,SNIPS,2018-12,Capsule-NLU,0.977,100.0,0.977,1.0,0.977,0.01,Accuracy
175,1,Intent Detection,ATIS,2018-12,Capsule-NLU,0.95,97.18,0.95,0.97,0.9776,0.01,Accuracy
176,1,Intent Detection,ATIS,2019-06,SF-ID,0.9776,100.0,0.0,0.0,0.9776,0.01,Accuracy
177,1,Text Classification,Yelp-5,2019-01,HAHNN (CNN),73.28,100.0,73.28,1.0,73.28,0.74,Accuracy
178,1,Linguistic Acceptability,CoLA,2019-01,MT-DNN,68.4,96.61,68.4,0.97,70.8,0.69,Accuracy
179,1,Linguistic Acceptability,CoLA,2019-06,XLNet (single model),69.0,97.46,0.6,0.01,70.8,0.69,Accuracy
180,1,Linguistic Acceptability,CoLA,2019-09,ALBERT,69.1,97.6,0.1,0.0,70.8,0.69,Accuracy
181,1,Linguistic Acceptability,CoLA,2019-10,T5-11B,70.8,100.0,1.7,0.02,70.8,0.71,Accuracy
182,1,Sentiment Analysis,Twitter,2019-02,AEN-BERT,74.71,100.0,74.71,1.0,74.71,0.75,Accuracy
183,1,Document Classification,BBCSport,2019-04,ApproxRepSet,95.73,96.12,95.73,0.96,99.59,0.96,Accuracy
184,1,Document Classification,BBCSport,2019-08,MPAD-path,99.59,100.0,3.9,0.04,99.59,1.0,Accuracy
185,1,Document Classification,Classic,2019-04,ApproxRepSet,96.24,99.37,96.24,0.99,96.85,0.97,Accuracy
186,1,Document Classification,Classic,2019-12,REL-RWMD k-NN,96.85,100.0,0.6,0.01,96.85,0.97,Accuracy
187,1,Document Classification,Twitter,2019-04,ApproxRepSet,72.6,100.0,72.6,1.0,72.6,0.73,Accuracy
188,1,Document Classification,Amazon,2019-04,ApproxRepSet,94.31,100.0,94.31,1.0,94.31,0.95,Accuracy
189,1,Document Classification,Recipe,2019-04,ApproxRepSet,59.06,100.0,59.06,1.0,59.06,0.59,Accuracy
190,1,Question Answering,CODAH,2019-04,BERT Large,69.6,100.0,69.6,1.0,69.6,0.7,Accuracy
191,1,Document Classification,Yelp-14,2019-04,KD-LSTMreg,69.4,100.0,69.4,1.0,69.4,0.7,Accuracy
192,1,Natural Language Inference,XNLI Chinese,2019-04,ERNIE,78.4,96.79,78.4,0.97,81.0,0.79,Accuracy
193,1,Natural Language Inference,XNLI Chinese,2019-07,ERNIE 2.0 Large,81.0,100.0,2.6,0.03,81.0,0.81,Accuracy
194,1,Natural Language Inference,XNLI Chinese Dev,2019-04,ERNIE,79.9,96.73,79.9,0.97,82.6,0.8,Accuracy
195,1,Natural Language Inference,XNLI Chinese Dev,2019-07,ERNIE 2.0 Large,82.6,100.0,2.7,0.03,82.6,0.83,Accuracy
196,1,Text Classification,Yelp-2,2019-04,BERT Finetune + UDA,97.95,99.31,97.95,0.99,98.63,0.98,Accuracy
197,1,Text Classification,Yelp-2,2019-05,BERT-ITPT-FiT,98.08,99.44,0.1,0.0,98.63,0.98,Accuracy
198,1,Text Classification,Yelp-2,2019-06,XLNet,98.63,100.0,0.5,0.01,98.63,0.99,Accuracy
199,1,Emotion Recognition in Conversation,IEMOCAP,2019-05,DialogueRNN,63.4,97.16,63.4,0.97,65.25,0.64,Accuracy
200,1,Emotion Recognition in Conversation,IEMOCAP,2019-08,DialogueGCN,65.25,100.0,1.9,0.03,65.25,0.66,Accuracy
201,1,Emotion Recognition,MPED,2019-05,BiHDM,40.34,100.0,40.34,1.0,40.34,0.41,Accuracy
202,1,Entity Linking,FIGER,2019-05,ERNIE,57.19,100.0,57.19,1.0,57.19,0.57,Accuracy
203,1,Natural Language Inference,RTE,2019-05,ERNIE,68.8,74.38,68.8,0.74,92.5,0.69,Accuracy
204,1,Natural Language Inference,RTE,2019-06,XLNet (single model),85.9,92.86,17.1,0.18,92.5,0.86,Accuracy
205,1,Natural Language Inference,RTE,2019-07,RoBERTa,88.2,95.35,2.3,0.02,92.5,0.89,Accuracy
206,1,Natural Language Inference,RTE,2019-09,ALBERT,89.2,96.43,1.0,0.01,92.5,0.9,Accuracy
207,1,Natural Language Inference,RTE,2019-10,T5-11B,92.5,100.0,3.3,0.04,92.5,0.93,Accuracy
208,1,Natural Language Inference,QNLI,2019-05,ERNIE,91.3,92.04,91.3,0.92,99.2,0.92,Accuracy
209,1,Natural Language Inference,QNLI,2019-06,XLNet (single model),94.9,95.67,3.6,0.04,99.2,0.95,Accuracy
210,1,Natural Language Inference,QNLI,2019-07,RoBERTa,98.9,99.7,4.0,0.04,99.2,0.99,Accuracy
211,1,Natural Language Inference,QNLI,2019-09,ALBERT,99.2,100.0,0.3,0.0,99.2,1.0,Accuracy
212,1,Document Classification,IMDb-M,2019-06,LSTM-reg (single model),52.8,100.0,52.8,1.0,52.8,0.53,Accuracy
213,1,Reading Comprehension,RACE,2019-06,XLNet,85.4,100.0,85.4,1.0,85.4,0.86,Accuracy
214,1,Emotion Recognition,SEED-IV,2019-07,RGNN,79.37,100.0,79.37,1.0,79.37,0.8,Accuracy
215,1,Natural Language Inference,WNLI,2019-07,RoBERTa,89.0,95.49,89.0,0.95,93.2,0.89,Accuracy
216,1,Natural Language Inference,WNLI,2019-09,ALBERT,91.8,98.5,2.8,0.03,93.2,0.92,Accuracy
217,1,Natural Language Inference,WNLI,2019-10,T5-11B,93.2,100.0,1.4,0.02,93.2,0.94,Accuracy
218,1,Prosody Prediction,Helsinki Prosody Corpus,2019-08,BERT,83.2,100.0,83.2,1.0,83.2,0.84,Accuracy
219,1,Document Classification,MPQA,2019-08,MPAD-path,89.81,100.0,89.81,1.0,89.81,0.9,Accuracy
220,1,Sentiment Analysis,Financial PhraseBank,2019-08,FinBERT,86.0,100.0,86,1.0,86,0.86,Accuracy
221,1,Linguistic Acceptability,CoLA Dev,2019-09,TinyBERT (M=6;d' =768;d'i=3072),54.0,100.0,54,1.0,54,0.54,Accuracy
222,1,Semantic Textual Similarity,MRPC Dev,2019-09,TinyBERT (M=6;d'=768;d'i=3072),86.3,100.0,86.3,1.0,86.3,0.87,Accuracy
223,1,Semantic Parsing,WikiSQL,2019-10,NL2SQL-BERT,89.0,100.0,89,1.0,89,0.89,Accuracy
224,1,Word Sense Disambiguation,Words in Context,2019-10,T5-11B,76.1,100.0,76.1,1.0,76.1,0.76,Accuracy
225,1,Question Answering,BoolQ,2019-10,T5-11B,91.0,100.0,91,1.0,91,0.91,Accuracy
226,1,Question Answering,COPA,2019-10,T5-11B,94.8,100.0,94.8,1.0,94.8,0.95,Accuracy
227,1,Entity Linking,CoNLL-Aida,2020-01,RELIC + CoNLL-Aida tuning,94.9,100.0,94.9,1.0,94.9,0.95,Accuracy
228,1,Entity Linking,TAC-KBP 2010,2020-01,RELIC + CoNLL-Aida tuning,89.8,100.0,89.8,1.0,89.8,0.9,Accuracy
229,1,Sentiment Analysis,DBRD,2020-01,RobBERT,94.422,100.0,94.422,1.0,94.422,0.95,Accuracy
230,1,Sentiment Analysis,AJGT,2020-02,AraBERTv1,93.8,100.0,93.8,1.0,93.8,0.94,Accuracy
231,1,Sentiment Analysis,HARD,2020-02,AraBERTv1,96.1,100.0,96.1,1.0,96.1,0.96,Accuracy
0,1,Language Modelling,One Billion Word,2013-12,RNN-1024 + 9 Gram,51.3,96.98,51.3,0.97,52.9,0.06,PPL
1,1,Language Modelling,One Billion Word,2014-12,Sparse Non-Negative,52.9,100.0,1.6,0.03,52.9,0.06,PPL
2,1,Document Summarization,CNN / Daily Mail,2017-09,C2F + ALTERNATE,23.6,72.06,23.6,0.72,32.75,0.03,PPL
3,1,Document Summarization,CNN / Daily Mail,2018-08,Bottom-Up Sum,32.75,100.0,9.1,0.28,32.75,0.04,PPL
4,1,Topic Models,20NEWS,2019-08,Bayesian SMM,851.0,100.0,851,1.0,851,1.0,PPL
0,1,Natural Language Inference,SNLI,2014-04,DCNN [[Blunsom et al.2014]],86.8,94.45,86.8,0.94,91.9,0.94,%\\ Test\\ Accuracy
1,1,Natural Language Inference,SNLI,2014-08,CNN-MC [[Kim2014]],88.1,95.87,1.3,0.01,91.9,0.96,%\\ Test\\ Accuracy
2,1,Natural Language Inference,SNLI,2016-09,600D ESIM + 300D Syntactic TreeLSTM,88.6,96.41,0.5,0.01,91.9,0.96,%\\ Test\\ Accuracy
3,1,Natural Language Inference,SNLI,2017-02,BiMPM Ensemble,88.8,96.63,0.2,0.0,91.9,0.97,%\\ Test\\ Accuracy
4,1,Natural Language Inference,SNLI,2017-09,"448D Densely Interactive Inference Network (DIIN, code) Ensemble",88.9,96.74,0.1,0.0,91.9,0.97,%\\ Test\\ Accuracy
5,1,Natural Language Inference,SNLI,2017-11,KIM Ensemble,89.1,96.95,0.2,0.0,91.9,0.97,%\\ Test\\ Accuracy
6,1,Natural Language Inference,SNLI,2017-12,300D CAFE Ensemble,89.3,97.17,0.2,0.0,91.9,0.97,%\\ Test\\ Accuracy
7,1,Natural Language Inference,SNLI,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble,90.1,98.04,0.8,0.01,91.9,0.98,%\\ Test\\ Accuracy
8,1,Natural Language Inference,SNLI,2018-09,SJRC (BERT-Large +SRL),91.3,99.35,1.2,0.01,91.9,0.99,%\\ Test\\ Accuracy
9,1,Natural Language Inference,SNLI,2019-01,MT-DNN,91.6,99.67,0.3,0.0,91.9,1.0,%\\ Test\\ Accuracy
10,1,Natural Language Inference,SNLI,2019-09,SemBERT,91.9,100.0,0.3,0.0,91.9,1.0,%\\ Test\\ Accuracy
0,1,Question Answering,QASent,2014-05,Paragraph vector (lexical overlap + dist output),0.7514,92.57,0.7514,0.93,0.8117,0.01,MRR
1,1,Question Answering,QASent,2014-12,Bigram-CNN (lexical overlap + dist output),0.7846,96.66,0.0,0.0,0.8117,0.01,MRR
2,1,Question Answering,QASent,2015-11,Attentive LSTM,0.8117,100.0,0.0,0.0,0.8117,0.01,MRR
3,1,Question Answering,WikiQA,2014-05,Paragraph vector (lexical overlap + dist output),0.6058,64.93,0.6058,0.65,0.933,0.01,MRR
4,1,Question Answering,WikiQA,2014-12,Bigram-CNN (lexical overlap + dist output),0.6652,71.3,0.1,0.11,0.933,0.01,MRR
5,1,Question Answering,WikiQA,2015-11,LSTM (lexical overlap + dist output),0.6988,74.9,0.0,0.0,0.933,0.01,MRR
6,1,Question Answering,WikiQA,2015-11,Attentive LSTM,0.7069,75.77,0.0,0.0,0.933,0.01,MRR
7,1,Question Answering,WikiQA,2016-02,LDC,0.7226,77.45,0.0,0.0,0.933,0.01,MRR
8,1,Question Answering,WikiQA,2016-06,PWIM,0.7234,77.53,0.0,0.0,0.933,0.01,MRR
9,1,Question Answering,WikiQA,2016-06,Key-Value Memory Network,0.7265,77.87,0.0,0.0,0.933,0.01,MRR
10,1,Question Answering,WikiQA,2017-07,HyperQA,0.727,77.92,0.0,0.0,0.933,0.01,MRR
11,1,Question Answering,WikiQA,2019-05,Comp-Clip + LM + LC,0.784,84.03,0.1,0.11,0.933,0.01,MRR
12,1,Question Answering,WikiQA,2019-11,"TANDA-RoBERTa (ASNQ, WikiQA)",0.933,100.0,0.1,0.11,0.933,0.01,MRR
13,1,Question Answering,TrecQA,2014-12,CNN,0.785,80.6,0.785,0.81,0.974,0.01,MRR
14,1,Question Answering,TrecQA,2016-06,PWIN,0.8219,84.38,0.0,0.0,0.974,0.01,MRR
15,1,Question Answering,TrecQA,2017-07,HyperQA,0.825,84.7,0.0,0.0,0.974,0.01,MRR
16,1,Question Answering,TrecQA,2019-05,Comp-Clip + LM + LC,0.928,95.28,0.1,0.1,0.974,0.01,MRR
17,1,Question Answering,TrecQA,2019-11,"TANDA-RoBERTa (ASNQ, TREC-QA)",0.974,100.0,0.0,0.0,0.974,0.01,MRR
18,1,Question Answering,YahooCQA,2016-02,AP-BiLSTM,0.731,84.9,0.731,0.85,0.861,0.01,MRR
19,1,Question Answering,YahooCQA,2017-07,HyperQA,0.801,93.03,0.1,0.12,0.861,0.01,MRR
20,1,Question Answering,YahooCQA,2020-02,sMIM (1024) +,0.861,100.0,0.1,0.12,0.861,0.01,MRR
21,1,Hypernym Discovery,Medical domain,2016-11,vTE,41.07,75.16,41.07,0.75,54.64,0.59,MRR
22,1,Hypernym Discovery,Medical domain,2018-06,CRIM,54.64,100.0,13.6,0.25,54.64,0.79,MRR
23,1,Hypernym Discovery,Music domain,2016-11,vTE,39.36,64.6,39.36,0.65,60.93,0.57,MRR
24,1,Hypernym Discovery,Music domain,2018-06,CRIM,60.93,100.0,21.6,0.35,60.93,0.88,MRR
25,1,Hypernym Discovery,General,2016-11,vTE,23.83,66.01,23.83,0.66,36.1,0.34,MRR
26,1,Hypernym Discovery,General,2018-06,CRIM,36.1,100.0,12.3,0.34,36.1,0.52,MRR
27,1,Visual Dialog,VisDial v0.9 val,2017-09,AMEM Seo et al. (2017),62.27,90.35,62.27,0.9,68.92,0.9,MRR
28,1,Visual Dialog,VisDial v0.9 val,2017-11,CoAtt Wu et al. (2018),63.98,92.83,1.7,0.02,68.92,0.92,MRR
29,1,Visual Dialog,VisDial v0.9 val,2019-02,DAN,66.38,96.31,2.4,0.03,68.92,0.96,MRR
30,1,Visual Dialog,VisDial v0.9 val,2019-04,9xFGA (VGG),68.92,100.0,2.5,0.04,68.92,0.99,MRR
31,1,Visual Dialog,VisDial v1.0 test-std,2018-09,CorefNMN Kottur et al. (2018),61.5,88.74,61.5,0.89,69.3,0.89,MRR
32,1,Visual Dialog,VisDial v1.0 test-std,2019-02,DAN,63.2,91.2,1.7,0.02,69.3,0.91,MRR
33,1,Visual Dialog,VisDial v1.0 test-std,2019-04,5xFGA (F-RCNNx101),69.3,100.0,6.1,0.09,69.3,1.0,MRR
34,1,Passage Re-Ranking,MS MARCO,2019-01,BERT + Small Training,0.359,97.55,0.359,0.98,0.368,0.01,MRR
35,1,Passage Re-Ranking,MS MARCO,2019-04,BERT + Doc2query,0.368,100.0,0.0,0.0,0.368,0.01,MRR
36,1,Visual Dialog,Visual Dialog v1.0,2019-02,HACAN,64.22,99.04,64.22,0.99,64.84,0.93,MRR
37,1,Visual Dialog,Visual Dialog v1.0,2020-04,MVAN,64.84,100.0,0.6,0.01,64.84,0.94,MRR
0,1,Question Answering,WikiQA,2014-05,Paragraph vector (lexical overlap + dist output),0.5976,64.96,0.5976,0.65,0.92,0.01,MAP
1,1,Question Answering,WikiQA,2014-12,Bigram-CNN (lexical overlap + dist output),0.652,70.87,0.1,0.11,0.92,0.02,MAP
2,1,Question Answering,WikiQA,2015-11,LSTM (lexical overlap + dist output),0.682,74.13,0.0,0.0,0.92,0.02,MAP
3,1,Question Answering,WikiQA,2015-11,Attentive LSTM,0.6886,74.85,0.0,0.0,0.92,0.02,MAP
4,1,Question Answering,WikiQA,2016-02,LDC,0.7058,76.72,0.0,0.0,0.92,0.02,MAP
5,1,Question Answering,WikiQA,2016-06,PWIM,0.709,77.07,0.0,0.0,0.92,0.02,MAP
6,1,Question Answering,WikiQA,2017-07,HyperQA,0.712,77.39,0.0,0.0,0.92,0.02,MAP
7,1,Question Answering,WikiQA,2019-05,Comp-Clip + LM + LC,0.764,83.04,0.1,0.11,0.92,0.02,MAP
8,1,Question Answering,WikiQA,2019-11,"TANDA-RoBERTa (ASNQ, WikiQA)",0.92,100.0,0.2,0.22,0.92,0.02,MAP
9,1,Question Answering,QASent,2014-05,Paragraph vector (lexical overlap + dist output),0.6762,92.14,0.6762,0.92,0.7339,0.02,MAP
10,1,Question Answering,QASent,2014-12,Bigram-CNN (lexical overlap + dist output),0.7113,96.92,0.0,0.0,0.7339,0.02,MAP
11,1,Question Answering,QASent,2015-11,Attentive LSTM,0.7339,100.0,0.0,0.0,0.7339,0.02,MAP
12,1,Question Answering,TrecQA,2014-12,CNN,0.711,75.4,0.711,0.75,0.943,0.02,MAP
13,1,Question Answering,TrecQA,2016-06,PWIN,0.7588,80.47,0.0,0.0,0.943,0.02,MAP
14,1,Question Answering,TrecQA,2017-07,HyperQA,0.77,81.65,0.0,0.0,0.943,0.02,MAP
15,1,Question Answering,TrecQA,2019-05,Comp-Clip + LM + LC,0.868,92.05,0.1,0.11,0.943,0.02,MAP
16,1,Question Answering,TrecQA,2019-11,"TANDA-RoBERTa (ASNQ, TREC-QA)",0.943,100.0,0.1,0.11,0.943,0.02,MAP
17,1,Question Answering,SemEvalCQA,2015-03,ARC-II,0.78,98.11,0.78,0.98,0.795,0.02,MAP
18,1,Question Answering,SemEvalCQA,2016-06,Kelp,0.792,99.62,0.0,0.0,0.795,0.02,MAP
19,1,Question Answering,SemEvalCQA,2017-07,HyperQA,0.795,100.0,0.0,0.0,0.795,0.02,MAP
20,1,Hypernym Discovery,Music domain,2016-11,vTE,12.99,31.71,12.99,0.32,40.97,0.32,MAP
21,1,Hypernym Discovery,Music domain,2018-06,CRIM,40.97,100.0,28.0,0.68,40.97,1.0,MAP
22,1,Hypernym Discovery,General,2016-11,vTE,10.6,53.59,10.6,0.54,19.78,0.26,MAP
23,1,Hypernym Discovery,General,2018-06,CRIM,19.78,100.0,9.2,0.47,19.78,0.48,MAP
24,1,Hypernym Discovery,Medical domain,2016-11,vTE,18.84,55.33,18.84,0.55,34.05,0.46,MAP
25,1,Hypernym Discovery,Medical domain,2018-06,CRIM,34.05,100.0,15.2,0.45,34.05,0.83,MAP
26,1,Ad-Hoc Information Retrieval,TREC Robust04,2017-04,FNRM-RankProb_Embed,0.2837,86.55,0.2837,0.87,0.3278,0.01,MAP
27,1,Ad-Hoc Information Retrieval,TREC Robust04,2018-10,SNRM,0.2856,87.13,0.0,0.0,0.3278,0.01,MAP
28,1,Ad-Hoc Information Retrieval,TREC Robust04,2018-10,SNRM-PRF,0.2971,90.63,0.0,0.0,0.3278,0.01,MAP
29,1,Ad-Hoc Information Retrieval,TREC Robust04,2018-12,Anserini BM25+RM3,0.302,92.13,0.0,0.0,0.3278,0.01,MAP
30,1,Ad-Hoc Information Retrieval,TREC Robust04,2019-03,BERT FT(Microblog),0.3278,100.0,0.0,0.0,0.3278,0.01,MAP
0,1,Relation Extraction,ACE 2005,2014-06,Joint w/ Global,52.1,82.44,52.1,0.82,63.2,0.73,Relation\\ F1
1,1,Relation Extraction,ACE 2005,2016-01,SPTree,55.6,87.97,3.5,0.06,63.2,0.78,Relation\\ F1
2,1,Relation Extraction,ACE 2005,2017-07,Attention,55.9,88.45,0.3,0.0,63.2,0.78,Relation\\ F1
3,1,Relation Extraction,ACE 2005,2017-09,Global,57.5,90.98,1.6,0.03,63.2,0.8,Relation\\ F1
4,1,Relation Extraction,ACE 2005,2018-10,MRT,59.6,94.3,2.1,0.03,63.2,0.83,Relation\\ F1
5,1,Relation Extraction,ACE 2005,2019-04,DYGIE,63.2,100.0,3.6,0.06,63.2,0.88,Relation\\ F1
6,1,Relation Extraction,CoNLL04,2014-10,Table Representation,61.0,85.35,61.0,0.85,71.47,0.85,Relation\\ F1
7,1,Relation Extraction,CoNLL04,2014-10,Table Representation,62.8,87.87,1.8,0.03,71.47,0.88,Relation\\ F1
8,1,Relation Extraction,CoNLL04,2017-09,Global,67.8,94.86,5.0,0.07,71.47,0.95,Relation\\ F1
9,1,Relation Extraction,CoNLL04,2019-05,Multi-turn QA,68.9,96.4,1.1,0.02,71.47,0.96,Relation\\ F1
10,1,Relation Extraction,CoNLL04,2019-09,SpERT,71.47,100.0,2.6,0.04,71.47,1.0,Relation\\ F1
11,1,Joint Entity and Relation Extraction,SciERC,2018-08,SciIE,39.3,77.3,39.3,0.77,50.84,0.55,Relation\\ F1
12,1,Joint Entity and Relation Extraction,SciERC,2019-04,DyGIE,41.6,81.83,2.3,0.05,50.84,0.58,Relation\\ F1
13,1,Joint Entity and Relation Extraction,SciERC,2019-09,DyGIE++,48.4,95.2,6.8,0.13,50.84,0.68,Relation\\ F1
14,1,Joint Entity and Relation Extraction,SciERC,2019-09,SpERT,50.84,100.0,2.4,0.05,50.84,0.71,Relation\\ F1
0,1,Relation Extraction,ACE 2004,2014-06,Joint w/ Global,45.3,75.88,45.3,0.76,59.7,0.57,Entity\\+Relation\\ F1
1,1,Relation Extraction,ACE 2004,2016-01,SPTree,48.4,81.07,3.1,0.05,59.7,0.61,Entity\\+Relation\\ F1
2,1,Relation Extraction,ACE 2004,2019-04,DYGIE,59.7,100.0,11.3,0.19,59.7,0.76,Entity\\+Relation\\ F1
3,1,Relation Extraction,ADE Corpus,2018-04,multi-head,74.58,94.6,74.58,0.95,78.84,0.95,Entity\\+Relation\\ F1
4,1,Relation Extraction,ADE Corpus,2018-08,multi-head + AT,75.52,95.79,0.9,0.01,78.84,0.96,Entity\\+Relation\\ F1
5,1,Relation Extraction,ADE Corpus,2019-05,Relation-Metric,77.19,97.91,1.7,0.02,78.84,0.98,Entity\\+Relation\\ F1
6,1,Relation Extraction,ADE Corpus,2019-09,SpERT,78.84,100.0,1.7,0.02,78.84,1.0,Entity\\+Relation\\ F1
0,1,Relation Extraction,ACE 2004,2014-06,Joint w/ Global,79.7,91.19,79.7,0.91,87.4,0.86,Entity\\ F1
1,1,Relation Extraction,ACE 2004,2016-01,SPTree,81.8,93.59,2.1,0.02,87.4,0.88,Entity\\ F1
2,1,Relation Extraction,ACE 2004,2019-04,DYGIE,87.4,100.0,5.6,0.06,87.4,0.94,Entity\\ F1
3,1,Relation Extraction,ACE 2005,2014-06,Joint w/ Global,80.8,91.4,80.8,0.91,88.4,0.87,Entity\\ F1
4,1,Relation Extraction,ACE 2005,2016-01,SPTree,83.4,94.34,2.6,0.03,88.4,0.9,Entity\\ F1
5,1,Relation Extraction,ACE 2005,2017-09,Global,83.6,94.57,0.2,0.0,88.4,0.9,Entity\\ F1
6,1,Relation Extraction,ACE 2005,2019-04,DYGIE,88.4,100.0,4.8,0.05,88.4,0.96,Entity\\ F1
7,1,Relation Extraction,CoNLL04,2014-10,Table Representation,80.7,90.74,80.7,0.91,88.94,0.87,Entity\\ F1
8,1,Relation Extraction,CoNLL04,2017-09,Global,85.6,96.24,4.9,0.06,88.94,0.93,Entity\\ F1
9,1,Relation Extraction,CoNLL04,2018-12,Biaffine attention,86.2,96.92,0.6,0.01,88.94,0.93,Entity\\ F1
10,1,Relation Extraction,CoNLL04,2019-05,Multi-turn QA,87.8,98.72,1.6,0.02,88.94,0.95,Entity\\ F1
11,1,Relation Extraction,CoNLL04,2019-09,SpERT,88.94,100.0,1.1,0.01,88.94,0.96,Entity\\ F1
12,1,Relation Extraction,ADE Corpus,2018-04,multi-head,86.4,96.77,86.4,0.97,89.28,0.93,Entity\\ F1
13,1,Relation Extraction,ADE Corpus,2018-08,multi-head + AT,86.73,97.14,0.3,0.0,89.28,0.94,Entity\\ F1
14,1,Relation Extraction,ADE Corpus,2019-05,Relation-Metric,87.02,97.47,0.3,0.0,89.28,0.94,Entity\\ F1
15,1,Relation Extraction,ADE Corpus,2019-09,SpERT,89.28,100.0,2.3,0.03,89.28,0.97,Entity\\ F1
16,1,Joint Entity and Relation Extraction,SciERC,2018-08,SciIE,64.2,91.28,64.2,0.91,70.33,0.69,Entity\\ F1
17,1,Joint Entity and Relation Extraction,SciERC,2019-04,DyGIE,65.2,92.71,1.0,0.01,70.33,0.7,Entity\\ F1
18,1,Joint Entity and Relation Extraction,SciERC,2019-09,DyGIE++,67.5,95.98,2.3,0.03,70.33,0.73,Entity\\ F1
19,1,Joint Entity and Relation Extraction,SciERC,2019-09,SpERT,70.33,100.0,2.8,0.04,70.33,0.76,Entity\\ F1
20,1,Coreference Resolution,GAP,2019-08,ProBERT,92.5,100.0,92.5,1.0,92.5,1.0,Entity\\ F1
0,1,Machine Translation,WMT2014 English-French,2014-06,CSLM + RNN + WP,34.54,75.75,34.54,0.76,45.6,0.76,BLEU\\ score
1,1,Machine Translation,WMT2014 English-French,2014-09,RNN-search50*,36.2,79.39,1.7,0.04,45.6,0.79,BLEU\\ score
2,1,Machine Translation,WMT2014 English-French,2014-09,SMT+LSTM5,36.5,80.04,0.3,0.01,45.6,0.8,BLEU\\ score
3,1,Machine Translation,WMT2014 English-French,2014-10,LSTM6 + PosUnk,37.5,82.24,1.0,0.02,45.6,0.82,BLEU\\ score
4,1,Machine Translation,WMT2014 English-French,2016-06,Deep-Att + PosUnk,39.2,85.96,1.7,0.04,45.6,0.86,BLEU\\ score
5,1,Machine Translation,WMT2014 English-French,2016-09,GNMT+RL,39.9,87.5,0.7,0.02,45.6,0.87,BLEU\\ score
6,1,Machine Translation,WMT2014 English-French,2017-01,MoE,40.56,88.95,0.7,0.02,45.6,0.89,BLEU\\ score
7,1,Machine Translation,WMT2014 English-French,2017-05,ConvS2S (ensemble),41.3,90.57,0.7,0.02,45.6,0.91,BLEU\\ score
8,1,Machine Translation,WMT2014 English-French,2017-11,Weighted Transformer (large),41.4,90.79,0.1,0.0,45.6,0.91,BLEU\\ score
9,1,Machine Translation,WMT2014 English-French,2018-03,Transformer (big) + Relative Position Representations,41.5,91.01,0.1,0.0,45.6,0.91,BLEU\\ score
10,1,Machine Translation,WMT2014 English-French,2018-06,Transformer Big,43.2,94.74,1.7,0.04,45.6,0.95,BLEU\\ score
11,1,Machine Translation,WMT2014 English-French,2018-08,Transformer Big + BT,45.6,100.0,2.4,0.05,45.6,1.0,BLEU\\ score
12,1,Machine Translation,IWSLT2015 German-English,2014-09,Bi-GRU (MLE+SLE),28.53,80.37,28.53,0.8,35.5,0.63,BLEU\\ score
13,1,Machine Translation,IWSLT2015 German-English,2016-07,RNNsearch,29.98,84.45,1.4,0.04,35.5,0.66,BLEU\\ score
14,1,Machine Translation,IWSLT2015 German-English,2016-11,Conv-LSTM (deep+pos),30.4,85.63,0.4,0.01,35.5,0.67,BLEU\\ score
15,1,Machine Translation,IWSLT2015 German-English,2017-05,ConvS2S,32.31,91.01,1.9,0.05,35.5,0.71,BLEU\\ score
16,1,Machine Translation,IWSLT2015 German-English,2017-06,Transformer,34.44,97.01,2.1,0.06,35.5,0.76,BLEU\\ score
17,1,Machine Translation,IWSLT2015 German-English,2019-06,Transformer Base + adversarial MLE,35.18,99.1,0.7,0.02,35.5,0.77,BLEU\\ score
18,1,Machine Translation,IWSLT2015 German-English,2020-02,TaLK Convolutions,35.5,100.0,0.3,0.01,35.5,0.78,BLEU\\ score
19,1,Machine Translation,WMT2015 English-German,2015-08,BPE word segmentation,22.8,86.69,22.8,0.87,26.3,0.5,BLEU\\ score
20,1,Machine Translation,WMT2015 English-German,2016-03,Enc-Dec Att (char),23.5,89.35,0.7,0.03,26.3,0.52,BLEU\\ score
21,1,Machine Translation,WMT2015 English-German,2016-10,ByteNet,26.3,100.0,2.8,0.11,26.3,0.58,BLEU\\ score
22,1,Machine Translation,WMT2015 English-Russian,2015-08,C2-50k Segmentation,20.9,100.0,20.9,1.0,20.9,0.46,BLEU\\ score
23,1,Machine Translation,WMT2016 Romanian-English,2016-06,Attentional encoder-decoder + BPE,33.3,94.33,33.3,0.94,35.3,0.73,BLEU\\ score
24,1,Machine Translation,WMT2016 Romanian-English,2019-01,MLM pretraining,35.3,100.0,2.0,0.06,35.3,0.77,BLEU\\ score
25,1,Machine Translation,WMT2016 Czech-English,2016-06,Attentional encoder-decoder + BPE,31.4,100.0,31.4,1.0,31.4,0.69,BLEU\\ score
26,1,Machine Translation,WMT2016 English-German,2016-06,Attentional encoder-decoder + BPE,34.2,84.07,34.2,0.84,40.68,0.75,BLEU\\ score
27,1,Machine Translation,WMT2016 English-German,2019-05,MADL,40.68,100.0,6.5,0.16,40.68,0.89,BLEU\\ score
28,1,Machine Translation,WMT2016 English-Romanian,2016-06,BiGRU,28.1,86.86,28.1,0.87,32.35,0.62,BLEU\\ score
29,1,Machine Translation,WMT2016 English-Romanian,2016-08,GRU BPE90k,28.9,89.34,0.8,0.02,32.35,0.63,BLEU\\ score
30,1,Machine Translation,WMT2016 English-Romanian,2017-05,ConvS2S BPE40k,29.9,92.43,1.0,0.03,32.35,0.66,BLEU\\ score
31,1,Machine Translation,WMT2016 English-Romanian,2019-09,FlowSeq-large (NPD n = 30),32.35,100.0,2.5,0.08,32.35,0.71,BLEU\\ score
32,1,Machine Translation,WMT2016 English-Russian,2016-06,Attentional encoder-decoder + BPE,26.0,100.0,26.0,1.0,26.0,0.57,BLEU\\ score
33,1,Machine Translation,WMT2016 English-Czech,2016-06,Attentional encoder-decoder + BPE,25.8,100.0,25.8,1.0,25.8,0.57,BLEU\\ score
34,1,Machine Translation,WMT2016 German-English,2016-06,Attentional encoder-decoder + BPE,38.6,100.0,38.6,1.0,38.6,0.85,BLEU\\ score
35,1,Machine Translation,WMT2016 Russian-English,2016-06,Attentional encoder-decoder + BPE,28.0,100.0,28,1.0,28,0.61,BLEU\\ score
36,1,Machine Translation,WMT2014 English-German,2016-06,Deep-Att,20.7,59.14,20.7,0.59,35.0,0.45,BLEU\\ score
37,1,Machine Translation,WMT2014 English-German,2016-09,GNMT+RL,26.3,75.14,5.6,0.16,35.0,0.58,BLEU\\ score
38,1,Machine Translation,WMT2014 English-German,2017-05,ConvS2S (ensemble),26.4,75.43,0.1,0.0,35.0,0.58,BLEU\\ score
39,1,Machine Translation,WMT2014 English-German,2017-06,Transformer Big,28.4,81.14,2.0,0.06,35.0,0.62,BLEU\\ score
40,1,Machine Translation,WMT2014 English-German,2017-11,Weighted Transformer (large),28.9,82.57,0.5,0.01,35.0,0.63,BLEU\\ score
41,1,Machine Translation,WMT2014 English-German,2018-03,Transformer (big) + Relative Position Representations,29.2,83.43,0.3,0.01,35.0,0.64,BLEU\\ score
42,1,Machine Translation,WMT2014 English-German,2018-06,Transformer Big,29.3,83.71,0.1,0.0,35.0,0.64,BLEU\\ score
43,1,Machine Translation,WMT2014 English-German,2018-08,Transformer Big + BT,35.0,100.0,5.7,0.16,35.0,0.77,BLEU\\ score
44,1,Machine Translation,IWSLT2015 Thai-English,2016-06,Seq-KD + Seq-Inter + Word-KD,14.2,100.0,14.2,1.0,14.2,0.31,BLEU\\ score
45,1,Machine Translation,IWSLT2014 German-English,2016-07,Actor-Critic [Bahdanau2017],28.53,78.6,28.53,0.79,36.3,0.63,BLEU\\ score
46,1,Machine Translation,IWSLT2014 German-English,2017-06,Neural PBMT + LM [Huang2018],30.08,82.87,1.5,0.04,36.3,0.66,BLEU\\ score
47,1,Machine Translation,IWSLT2014 German-English,2017-11,Minimum Risk Training [Edunov2017],32.84,90.47,2.8,0.08,36.3,0.72,BLEU\\ score
48,1,Machine Translation,IWSLT2014 German-English,2018-07,Variational Attention,33.1,91.18,0.3,0.01,36.3,0.73,BLEU\\ score
49,1,Machine Translation,IWSLT2014 German-English,2019-01,DynamicConv,35.2,96.97,2.1,0.06,36.3,0.77,BLEU\\ score
50,1,Machine Translation,IWSLT2014 German-English,2019-05,Local Joint Self-attention,35.7,98.35,0.5,0.01,36.3,0.78,BLEU\\ score
51,1,Machine Translation,IWSLT2014 German-English,2020-01,MUSE(Parallel Multi-scale Attention),36.3,100.0,0.6,0.02,36.3,0.8,BLEU\\ score
52,1,Machine Translation,IWSLT2015 English-German,2016-07,RNNsearch,25.04,88.7,25.04,0.89,28.23,0.55,BLEU\\ score
53,1,Machine Translation,IWSLT2015 English-German,2017-05,ConvS2S,26.73,94.69,1.7,0.06,28.23,0.59,BLEU\\ score
54,1,Machine Translation,IWSLT2015 English-German,2017-06,Transformer,28.23,100.0,1.5,0.05,28.23,0.62,BLEU\\ score
55,1,Machine Translation,WMT2014 German-English,2017-11,NAT +FT + NPD,23.2,82.01,23.2,0.82,28.29,0.51,BLEU\\ score
56,1,Machine Translation,WMT2014 German-English,2018-02,Denoising autoencoders (non-autoregressive),25.43,89.89,2.2,0.08,28.29,0.56,BLEU\\ score
57,1,Machine Translation,WMT2014 German-English,2019-09,FlowSeq-large (NPD n = 30),28.29,100.0,2.9,0.1,28.29,0.62,BLEU\\ score
58,1,Machine Translation,WMT 2017 English-Chinese,2018-03,Hassan et al. (2018),24.2,99.18,24.2,0.99,24.4,0.53,BLEU\\ score
59,1,Machine Translation,WMT 2017 English-Chinese,2019-01,DynamicConv,24.4,100.0,0.2,0.01,24.4,0.54,BLEU\\ score
60,1,Machine Translation,WMT2014 French-English,2018-09,SMT + iterative backtranslation (unsupervised),25.87,100.0,25.87,1.0,25.87,0.57,BLEU\\ score
61,1,Machine Translation,WMT2014 English-Czech,2019-01,Evolved Transformer Big,28.2,100.0,28.2,1.0,28.2,0.62,BLEU\\ score
62,1,Machine Translation,WMT2019 English-German,2019-07,Facebook-FAIR (ensemble),43.1,100.0,43.1,1.0,43.1,0.95,BLEU\\ score
0,1,Language Modelling,WikiText-2,2016-11,Inan et al. (2016),87.0,87.61,87.0,0.88,99.3,0.88,Test\\ perplexity
1,1,Language Modelling,WikiText-2,2016-11,Inan et al. (2016),87.7,88.32,0.7,0.01,99.3,0.88,Test\\ perplexity
2,1,Language Modelling,WikiText-2,2016-12,Grave et al. (2016),99.3,100.0,11.6,0.12,99.3,1.0,Test\\ perplexity
3,1,Language Modelling,WikiText-103,2016-12,LSTM,48.7,100.0,48.7,1.0,48.7,0.49,Test\\ perplexity
4,1,Language Modelling,Yahoo! Answers,2020-02,sMIM (1024) +,12.62,69.46,12.62,0.69,18.17,0.13,Test\\ perplexity
5,1,Language Modelling,Yahoo! Answers,2020-02,sMIM (1024),18.17,100.0,5.6,0.31,18.17,0.18,Test\\ perplexity
6,1,Language Modelling,Yelp15,2020-02,sMIM (1024) +,8.19,82.06,8.19,0.82,9.98,0.08,Test\\ perplexity
7,1,Language Modelling,Yelp15,2020-02,sMIM (1024),9.98,100.0,1.8,0.18,9.98,0.1,Test\\ perplexity
0,1,Language Modelling,WikiText-2,2016-11,Inan et al. (2016),91.5,99.13,91.5,0.99,92.3,0.99,Validation\\ perplexity
1,1,Language Modelling,WikiText-2,2016-11,Inan et al. (2016),92.3,100.0,0.8,0.01,92.3,1.0,Validation\\ perplexity
2,1,Language Modelling,WikiText-103,2018-03,4 layer QRNN,32.0,88.89,32.0,0.89,36.0,0.35,Validation\\ perplexity
3,1,Language Modelling,WikiText-103,2018-03,LSTM (Hebbian),34.1,94.72,2.1,0.06,36.0,0.37,Validation\\ perplexity
4,1,Language Modelling,WikiText-103,2018-03,LSTM,36.0,100.0,1.9,0.05,36.0,0.39,Validation\\ perplexity
5,1,Language Modelling,One Billion Word,2018-09,Adaptive Input Very Large,22.92,96.18,22.92,0.96,23.83,0.25,Validation\\ perplexity
6,1,Language Modelling,One Billion Word,2018-09,Adaptive Input Large,23.83,100.0,0.9,0.04,23.83,0.26,Validation\\ perplexity
0,1,Question Answering,MS MARCO,2016-11,BiDaF Baseline,10.64,19.47,10.64,0.19,54.64,0.17,BLEU\\-1
1,1,Question Answering,MS MARCO,2018-05,VNET,54.37,99.51,43.7,0.8,54.64,0.85,BLEU\\-1
2,1,Question Answering,MS MARCO,2018-11,Deep Cascade QA,54.64,100.0,0.3,0.01,54.64,0.85,BLEU\\-1
3,1,Question Answering,NarrativeQA,2016-11,BiDAF,33.45,61.82,33.45,0.62,54.11,0.52,BLEU\\-1
4,1,Question Answering,NarrativeQA,2018-09,MHPGM + NOIC,43.63,80.63,10.2,0.19,54.11,0.68,BLEU\\-1
5,1,Question Answering,NarrativeQA,2018-11,DecaProp,44.35,81.96,0.7,0.01,54.11,0.69,BLEU\\-1
6,1,Question Answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO),54.11,100.0,9.8,0.18,54.11,0.84,BLEU\\-1
7,1,Paraphrase Generation,quora,2017-09,VAE-SVG-eq,22.9,50.11,22.9,0.5,45.7,0.36,BLEU\\-1
8,1,Paraphrase Generation,quora,2018-06,EDD-LG,45.7,100.0,22.8,0.5,45.7,0.71,BLEU\\-1
9,1,Question Generation,Visual Question Generation,2018-08,MDN,36.0,100.0,36,1.0,36,0.56,BLEU\\-1
10,1,Text Generation,DailyDialog,2018-08,AEM+Attention,14.17,100.0,14.17,1.0,14.17,0.22,BLEU\\-1
11,1,Image Captioning,COCO,2019-05,"NIC (ResNet-50, CutMix)",64.2,100.0,64.2,1.0,64.2,1.0,BLEU\\-1
0,1,Constituency Parsing,Penn Treebank,2014-12,Semi-supervised LSTM,92.1,96.34,92.1,0.96,95.6,0.95,F1\\ score
1,1,Constituency Parsing,Penn Treebank,2016-11,Semi-supervised LSTM-LM,93.8,98.12,1.7,0.02,95.6,0.97,F1\\ score
2,1,Constituency Parsing,Penn Treebank,2017-01,In-order,94.2,98.54,0.4,0.0,95.6,0.97,F1\\ score
3,1,Constituency Parsing,Penn Treebank,2017-07,Model combination,94.66,99.02,0.5,0.01,95.6,0.98,F1\\ score
4,1,Constituency Parsing,Penn Treebank,2018-05,Self-attentive encoder + ELMo,95.13,99.51,0.5,0.01,95.6,0.98,F1\\ score
5,1,Constituency Parsing,Penn Treebank,2019-03,CNN Large + fine-tune,95.6,100.0,0.5,0.01,95.6,0.99,F1\\ score
6,1,Cross-Lingual Bitext Mining,BUCC German-to-English,2015-11,Monolingual training data,76.9,79.95,76.9,0.8,96.19,0.8,F1\\ score
7,1,Cross-Lingual Bitext Mining,BUCC German-to-English,2018-11,Multilingual Sentence Embeddings,95.58,99.37,18.7,0.19,96.19,0.99,F1\\ score
8,1,Cross-Lingual Bitext Mining,BUCC German-to-English,2018-12,Massively Multilingual Sentence Embeddings,96.19,100.0,0.6,0.01,96.19,0.99,F1\\ score
9,1,Cross-Lingual Bitext Mining,BUCC French-to-English,2015-11,Monolingual training data,75.8,80.72,75.8,0.81,93.91,0.78,F1\\ score
10,1,Cross-Lingual Bitext Mining,BUCC French-to-English,2018-11,Multilingual Sentence Embeddings,92.89,98.91,17.1,0.18,93.91,0.96,F1\\ score
11,1,Cross-Lingual Bitext Mining,BUCC French-to-English,2018-12,Massively Multilingual Sentence Embeddings,93.91,100.0,1.0,0.01,93.91,0.97,F1\\ score
12,1,Chunking,Penn Treebank,2016-08,Low supervision,95.57,98.81,95.57,0.99,96.72,0.99,F1\\ score
13,1,Chunking,Penn Treebank,2016-11,JMT,95.77,99.02,0.2,0.0,96.72,0.99,F1\\ score
14,1,Chunking,Penn Treebank,2018-08,Flair embeddings,96.72,100.0,1.0,0.01,96.72,1.0,F1\\ score
15,1,Temporal Information Extraction,TimeBank,2016-12,Catena,0.511,100.0,0.511,1.0,0.511,0.01,F1\\ score
16,1,Extract Aspect,SemEval 2015 Task 12,2017-10,EliXa,0.7,100.0,0.7,1.0,0.7,0.01,F1\\ score
17,1,Extract aspect-polarity tuple,SemEval 2015 Task 12,2017-10,Syntactic Grammer Model,0.51,100.0,0.51,1.0,0.51,0.01,F1\\ score
18,1,Cross-Lingual Bitext Mining,BUCC Russian-to-English,2018-12,Massively Multilingual Sentence Embeddings,93.3,100.0,93.3,1.0,93.3,0.96,F1\\ score
19,1,Cross-Lingual Bitext Mining,BUCC Chinese-to-English,2018-12,Massively Multilingual Sentence Embeddings,92.27,100.0,92.27,1.0,92.27,0.95,F1\\ score
20,1,Low Resource Named Entity Recognition,Uyghur Unsequestered Set,2019-02,Low Resource Named Entity Recognition using Contextual Word Representation and Neural Cross-Lingual Knowledge Transfer,42.88,100.0,42.88,1.0,42.88,0.44,F1\\ score
21,1,Low Resource Named Entity Recognition,CONLL 2003 German,2019-02,Low Resource Named Entity Recognition using Contextual Word Representation and Neural Cross-Lingual Knowledge Transfer,58.63,89.87,58.63,0.9,65.24,0.61,F1\\ score
22,1,Low Resource Named Entity Recognition,CONLL 2003 German,2019-11,Zero-Resource Transfer From CoNLL-2003 English dataset.,65.24,100.0,6.6,0.1,65.24,0.67,F1\\ score
23,1,Low Resource Named Entity Recognition,Conll 2003 Spanish,2019-02,Low Resource Named Entity Recognition using Contextual Word Representation and Neural Cross-Lingual Knowledge Transfer,75.34,99.22,75.34,0.99,75.93,0.78,F1\\ score
24,1,Low Resource Named Entity Recognition,Conll 2003 Spanish,2019-11,Zero-Resource Cross-lingual Transfer From CoNLL-2003 English dataset.,75.93,100.0,0.6,0.01,75.93,0.79,F1\\ score
25,1,Low Resource Named Entity Recognition,CONLL 2003 Dutch,2019-02,Low Resource Named Entity Recognition using Contextual Word Representation and Neural Cross-Lingual Knowledge Transfer,75.1,100.0,75.1,1.0,75.1,0.78,F1\\ score
26,1,Counterspeech Detection,Youtube counterspeech dataset,2019-07,XGBoost,0.715,100.0,0.715,1.0,0.715,0.01,F1\\ score
27,1,Sentiment Analysis,Financial PhraseBank,2019-08,FinBERT,84.0,100.0,84,1.0,84,0.87,F1\\ score
28,1,Question Similarity,Q2Q Arabic Benchmark,2019-12,Tha3aroon,0.94848,98.88,0.94848,0.99,0.95924,0.01,F1\\ score
29,1,Question Similarity,Q2Q Arabic Benchmark,2020-04,Ensemble multilingual BERT model,0.95924,100.0,0.0,0.0,0.95924,0.01,F1\\ score
0,1,Semantic Similarity Estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015)",0.8676,100.0,0.8676,1.0,0.8676,0.94,Pearson\\ Correlation
1,1,Semantic Textual Similarity,STS Benchmark,2018-03,USE_T,0.782,84.54,0.782,0.85,0.925,0.85,Pearson\\ Correlation
2,1,Semantic Textual Similarity,STS Benchmark,2019-05,ERNIE,0.832,89.95,0.0,0.0,0.925,0.9,Pearson\\ Correlation
3,1,Semantic Textual Similarity,STS Benchmark,2019-06,XLNet (single model),0.925,100.0,0.1,0.11,0.925,1.0,Pearson\\ Correlation
0,1,Semantic Similarity Estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015)",0.8083,100.0,0.8083,1.0,0.8083,0.88,Spearman\\ Correlation
1,1,Semantic Textual Similarity,STS Benchmark,2019-10,T5-11B,0.921,100.0,0.921,1.0,0.921,1.0,Spearman\\ Correlation
0,1,Semantic Similarity Estimation,SICK,2015-02,"Dependency Tree-LSTM (Tai et al., 2015)",0.2532,82.93,0.2532,0.83,0.3053,0.83,MSE
1,1,Semantic Similarity Estimation,SICK,2015-02,"Bidirectional LSTM (Tai et al., 2015)",0.2736,89.62,0.0,0.0,0.3053,0.9,MSE
2,1,Semantic Similarity Estimation,SICK,2015-02,"LSTM (Tai et al., 2015)",0.2831,92.73,0.0,0.0,0.3053,0.93,MSE
3,1,Semantic Similarity Estimation,SICK,2017-07,Doc2VecC,0.3053,100.0,0.0,0.0,0.3053,1.0,MSE
4,1,Sentiment Analysis,FiQA,2019-08,FinBERT,0.07,100.0,0.07,1.0,0.07,0.23,MSE
5,1,Reading Comprehension,CrowdSource QA,2020-02,BERT,0.046,100.0,0.046,1.0,0.046,0.15,MSE
6,1,Community Question Answering,CrowdSource QA,2020-02,BERT,0.046,100.0,0.046,1.0,0.046,0.15,MSE
7,1,Question Quality Assessment,CrowdSource QA,2020-02,BERT,0.046,100.0,0.046,1.0,0.046,0.15,MSE
0,1,Question Answering,SemEvalCQA,2015-03,ARC-II,0.753,93.08,0.753,0.93,0.809,0.01,P\\-at\\-1
1,1,Question Answering,SemEvalCQA,2016-02,AP-CNN,0.755,93.33,0.0,0.0,0.809,0.01,P\\-at\\-1
2,1,Question Answering,SemEvalCQA,2017-07,HyperQA,0.809,100.0,0.1,0.12,0.809,0.01,P\\-at\\-1
3,1,Question Answering,YahooCQA,2016-02,AP-BiLSTM,0.568,75.43,0.568,0.75,0.753,0.01,P\\-at\\-1
4,1,Question Answering,YahooCQA,2017-07,HyperQA,0.683,90.7,0.1,0.13,0.753,0.01,P\\-at\\-1
5,1,Question Answering,YahooCQA,2020-02,sMIM (1024) +,0.753,100.0,0.1,0.13,0.753,0.01,P\\-at\\-1
6,1,Question Answering,AI2 Kaggle Dataset,2017-08,IR Baseline,47.2,87.41,47.2,0.87,54.0,0.49,P\\-at\\-1
7,1,Question Answering,AI2 Kaggle Dataset,2017-08,Our Approach w/o IR,50.54,93.59,3.3,0.06,54.0,0.52,P\\-at\\-1
8,1,Question Answering,AI2 Kaggle Dataset,2017-08,IR++,50.7,93.89,0.2,0.0,54.0,0.52,P\\-at\\-1
9,1,Question Answering,AI2 Kaggle Dataset,2017-08,OUR APPROACH,54.0,100.0,3.3,0.06,54.0,0.56,P\\-at\\-1
10,1,Word Alignment,en-es,2017-10,Adv,81.7,100.0,81.7,1.0,81.7,0.84,P\\-at\\-1
11,1,Word Alignment,en-fr,2017-10,Adv,82.3,100.0,82.3,1.0,82.3,0.85,P\\-at\\-1
12,1,Word Alignment,fr-en,2017-10,Adv,82.1,100.0,82.1,1.0,82.1,0.85,P\\-at\\-1
13,1,Word Alignment,es-en,2017-10,Adv,83.3,100.0,83.3,1.0,83.3,0.86,P\\-at\\-1
14,1,Entity Typing,Freebase FIGER,2018-06,TextEnt-full,93.2,100.0,93.2,1.0,93.2,0.96,P\\-at\\-1
15,1,Multi-Label Text Classification,EUR-Lex,2019-05,LAHA,74.95,93.45,74.95,0.93,80.2,0.77,P\\-at\\-1
16,1,Multi-Label Text Classification,EUR-Lex,2019-06,NLP-Cap,80.2,100.0,5.2,0.06,80.2,0.83,P\\-at\\-1
17,1,Multi-Label Text Classification,Amazon-12K,2019-05,LAHA,94.87,100.0,94.87,1.0,94.87,0.98,P\\-at\\-1
18,1,Multi-Label Text Classification,Kan-Shan Cup,2019-05,LAHA,54.38,100.0,54.38,1.0,54.38,0.56,P\\-at\\-1
19,1,Multi-Label Text Classification,Wiki-30K,2019-05,LAHA,84.18,100.0,84.18,1.0,84.18,0.87,P\\-at\\-1
20,1,Multi-Label Text Classification,AAPD,2019-05,LAHA,84.48,100.0,84.48,1.0,84.48,0.87,P\\-at\\-1
21,1,Text Classification,RCV1,2019-06,NLP-Cap,97.05,100.0,97.05,1.0,97.05,1.0,P\\-at\\-1
0,1,Question Answering,bAbi,2015-03,End-To-End Memory Networks,7.5,26.13,7.5,0.26,28.7,0.26,Mean\\ Error\\ Rate
1,1,Question Answering,bAbi,2016-10,LSTM,28.7,100.0,21.2,0.74,28.7,1.0,Mean\\ Error\\ Rate
0,1,Question Answering,bAbi,2015-03,End-To-End Memory Networks,93.4,93.54,93.4,0.94,99.85,0.94,Accuracy\\ \\(trained\\ on\\ 10k\\)
1,1,Question Answering,bAbi,2016-06,QRN,99.7,99.85,6.3,0.06,99.85,1.0,Accuracy\\ \\(trained\\ on\\ 10k\\)
2,1,Question Answering,bAbi,2020-02,STM,99.85,100.0,0.1,0.0,99.85,1.0,Accuracy\\ \\(trained\\ on\\ 10k\\)
0,1,Question Answering,bAbi,2015-03,End-To-End Memory Networks,86.1,95.56,86.1,0.96,90.1,0.96,Accuracy\\ \\(trained\\ on\\ 1k\\)
1,1,Question Answering,bAbi,2016-06,QRN,90.1,100.0,4.0,0.04,90.1,1.0,Accuracy\\ \\(trained\\ on\\ 1k\\)
0,1,Text Classification,TREC-6,2015-04,TBCNN,4.0,41.67,4.0,0.42,9.6,0.09,Error
1,1,Text Classification,TREC-6,2015-11,C-LSTM,5.4,56.25,1.4,0.15,9.6,0.12,Error
2,1,Text Classification,TREC-6,2017-02,GRU-RNN-GLOVE,7.0,72.92,1.6,0.17,9.6,0.15,Error
3,1,Text Classification,TREC-6,2018-03,Capsule-B,7.2,75.0,0.2,0.02,9.6,0.15,Error
4,1,Text Classification,TREC-6,2018-05,byte mLSTM7,9.6,100.0,2.4,0.25,9.6,0.21,Error
5,1,Text Classification,AG News,2015-09,Char-level CNN,9.51,67.93,9.51,0.68,14.0,0.2,Error
6,1,Text Classification,AG News,2018-05,Seq2CNN with GWS(50),9.64,68.86,0.1,0.01,14.0,0.21,Error
7,1,Text Classification,AG News,2018-08,ToWE-SG,14.0,100.0,4.4,0.31,14.0,0.3,Error
8,1,Sentiment Analysis,Yelp Fine-grained classification,2015-09,Char-level CNN,37.95,81.09,37.95,0.81,46.8,0.81,Error
9,1,Sentiment Analysis,Yelp Fine-grained classification,2019-01,SVDCNN,46.8,100.0,8.8,0.19,46.8,1.0,Error
10,1,Sentiment Analysis,Yelp Binary classification,2015-09,Char-level CNN,4.88,100.0,4.88,1.0,4.88,0.1,Error
11,1,Text Classification,DBpedia,2015-09,Char-level CNN,1.55,55.96,1.55,0.56,2.77,0.03,Error
12,1,Text Classification,DBpedia,2018-05,Seq2CNN(50),2.77,100.0,1.2,0.43,2.77,0.06,Error
13,1,Text Classification,TREC-50,2016-12,Rules,2.8,100.0,2.8,1.0,2.8,0.06,Error
14,1,Text Classification,Amazon-2,2019-04,BERT Finetune + UDA,3.5,89.74,3.5,0.9,3.9,0.07,Error
15,1,Text Classification,Amazon-2,2019-09,ULMFiT (Small data),3.9,100.0,0.4,0.1,3.9,0.08,Error
16,1,Text Classification,Amazon-5,2019-04,BERT Finetune + UDA,37.12,100.0,37.12,1.0,37.12,0.79,Error
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-05,DANN,76.26,91.55,76.26,0.92,83.3,0.92,Average
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE,78.36,94.07,2.1,0.03,83.3,0.94,Average
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2017-02,Asymmetric tri-training,78.39,94.11,0.0,0.0,83.3,0.94,Average
3,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-04,Multi-task tri-training,79.15,95.02,0.8,0.01,83.3,0.95,Average
4,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-10,Distributional Correspondence Indexing,83.3,100.0,4.1,0.05,83.3,1.0,Average
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-05,DANN,75.4,93.09,75.4,0.93,81.0,0.93,DVD
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE,76.57,94.53,1.2,0.01,81.0,0.95,DVD
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-04,Multi-task tri-training,78.14,96.47,1.6,0.02,81.0,0.96,DVD
3,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-10,Distributional Correspondence Indexing,81.0,100.0,2.9,0.04,81.0,1.0,DVD
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-05,DANN,71.43,87.75,71.43,0.88,81.4,0.88,Books
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE,73.4,90.17,2.0,0.02,81.4,0.9,Books
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-04,Multi-task tri-training,74.86,91.97,1.5,0.02,81.4,0.92,Books
3,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-10,Distributional Correspondence Indexing,81.4,100.0,6.5,0.08,81.4,1.0,Books
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-05,DANN,77.67,95.36,77.67,0.95,81.45,0.95,Electronics
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE,80.53,98.87,2.9,0.04,81.45,0.99,Electronics
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-04,Multi-task tri-training,81.45,100.0,0.9,0.01,81.45,1.0,Electronics
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-05,DANN,80.53,93.75,80.53,0.94,85.9,0.94,Kitchen
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2015-11,VFAE,82.93,96.54,2.4,0.03,85.9,0.97,Kitchen
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2017-02,Asymmetric tri-training,83.97,97.75,1.0,0.01,85.9,0.98,Kitchen
3,1,Sentiment Analysis,Multi-Domain Sentiment Dataset,2018-10,Distributional Correspondence Indexing,85.9,100.0,1.9,0.02,85.9,1.0,Kitchen
0,1,Question Answering,CNN / Daily Mail,2015-06,MemNNs (ensemble),69.4,88.3,69.4,0.88,78.6,0.88,CNN
1,1,Question Answering,CNN / Daily Mail,2016-03,AS Reader (single model),69.5,88.42,0.1,0.0,78.6,0.88,CNN
2,1,Question Answering,CNN / Daily Mail,2016-03,AS Reader (ensemble model),75.4,95.93,5.9,0.08,78.6,0.96,CNN
3,1,Question Answering,CNN / Daily Mail,2016-06,GA Reader,77.9,99.11,2.5,0.03,78.6,0.99,CNN
4,1,Question Answering,CNN / Daily Mail,2017-03,GA+MAGE (32),78.6,100.0,0.7,0.01,78.6,1.0,CNN
0,1,Question Answering,CNN / Daily Mail,2015-06,Impatient Reader,68.0,84.05,68.0,0.84,80.9,0.84,Daily\\ Mail
1,1,Question Answering,CNN / Daily Mail,2015-06,Attentive Reader,69.0,85.29,1.0,0.01,80.9,0.85,Daily\\ Mail
2,1,Question Answering,CNN / Daily Mail,2016-03,AS Reader (single model),73.9,91.35,4.9,0.06,80.9,0.91,Daily\\ Mail
3,1,Question Answering,CNN / Daily Mail,2016-03,AS Reader (ensemble model),77.7,96.04,3.8,0.05,80.9,0.96,Daily\\ Mail
4,1,Question Answering,CNN / Daily Mail,2016-06,GA Reader,80.9,100.0,3.2,0.04,80.9,1.0,Daily\\ Mail
0,1,Dependency Parsing,Penn Treebank,2015-06,Weiss et al.,92.05,96.87,92.05,0.97,95.02,0.97,LAS
1,1,Dependency Parsing,Penn Treebank,2016-03,Andor et al.,92.79,97.65,0.7,0.01,95.02,0.98,LAS
2,1,Dependency Parsing,Penn Treebank,2016-11,Deep Biaffine,93.76,98.67,1.0,0.01,95.02,0.99,LAS
3,1,Dependency Parsing,Penn Treebank,2018-05,Stack-Pointer Network,94.19,99.13,0.4,0.0,95.02,0.99,LAS
4,1,Dependency Parsing,Penn Treebank,2018-09,CVT + Multi-Task,95.02,100.0,0.8,0.01,95.02,1.0,LAS
5,1,Dependency Parsing,Sequoia Treebank,2019-11,CamemBERT,94.39,100.0,94.39,1.0,94.39,0.99,LAS
6,1,Dependency Parsing,Spoken Corpus,2019-11,CamemBERT,80.07,100.0,80.07,1.0,80.07,0.84,LAS
7,1,Dependency Parsing,French GSD,2019-11,CamemBERT,92.47,100.0,92.47,1.0,92.47,0.97,LAS
8,1,Dependency Parsing,ParTUT,2019-11,CamemBERT,92.9,100.0,92.9,1.0,92.9,0.98,LAS
0,1,Dependency Parsing,Penn Treebank,2015-06,Weiss et al.,93.99,97.29,93.99,0.97,96.61,0.97,UAS
1,1,Dependency Parsing,Penn Treebank,2016-03,Andor et al.,94.61,97.93,0.6,0.01,96.61,0.98,UAS
2,1,Dependency Parsing,Penn Treebank,2016-11,Deep Biaffine,95.44,98.79,0.8,0.01,96.61,0.99,UAS
3,1,Dependency Parsing,Penn Treebank,2018-05,Stack-Pointer Network,95.87,99.23,0.4,0.0,96.61,0.99,UAS
4,1,Dependency Parsing,Penn Treebank,2018-09,CVT + Multi-Task,96.61,100.0,0.7,0.01,96.61,1.0,UAS
5,1,Dependency Grammar Induction,WSJ10,2019-07,D-NDMV,75.6,100.0,75.6,1.0,75.6,0.78,UAS
6,1,Dependency Parsing,Sequoia Treebank,2019-11,CamemBERT,95.56,100.0,95.56,1.0,95.56,0.99,UAS
7,1,Dependency Parsing,Spoken Corpus,2019-11,CamemBERT,86.05,100.0,86.05,1.0,86.05,0.89,UAS
8,1,Dependency Parsing,French GSD,2019-11,CamemBERT,94.82,100.0,94.82,1.0,94.82,0.98,UAS
9,1,Dependency Parsing,ParTUT,2019-11,CamemBERT,95.21,100.0,95.21,1.0,95.21,0.99,UAS
0,1,Dependency Parsing,Penn Treebank,2015-06,Weiss et al.,97.44,99.46,97.44,0.99,97.97,0.99,POS
1,1,Dependency Parsing,Penn Treebank,2018-07,jPTDP,97.97,100.0,0.5,0.01,97.97,1.0,POS
0,1,Natural Language Inference,SNLI,2015-08,+ Unigram and bigram features,99.7,100.0,99.7,1.0,99.7,1.0,%\\ Train\\ Accuracy
0,1,Natural Language Inference,SNLI,2015-08,100D LSTM encoders,220000.0,0.06,220000,0.0,339000000,0.0,Parameters
1,1,Natural Language Inference,SNLI,2015-09,100D LSTMs w/ word-by-word attention,250000.0,0.07,30000,0.0,339000000,0.0,Parameters
2,1,Natural Language Inference,SNLI,2015-11,1024D GRU encoders w/ unsupervised 'skip-thoughts' pre-training,15000000.0,4.42,14750000,0.04,339000000,0.04,Parameters
3,1,Natural Language Inference,SNLI,2017-05,4096D BiLSTM with max-pooling,40000000.0,11.8,25000000,0.07,339000000,0.12,Parameters
4,1,Natural Language Inference,SNLI,2017-11,KIM Ensemble,43000000.0,12.68,3000000,0.01,339000000,0.13,Parameters
5,1,Natural Language Inference,SNLI,2018-02,450D DR-BiLSTM Ensemble,45000000.0,13.27,2000000,0.01,339000000,0.13,Parameters
6,1,Natural Language Inference,SNLI,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble,53300000.0,15.72,8300000,0.02,339000000,0.16,Parameters
7,1,Natural Language Inference,SNLI,2018-06,Fine-Tuned LM-Pretrained Transformer,85000000.0,25.07,31700000,0.09,339000000,0.25,Parameters
8,1,Natural Language Inference,SNLI,2018-09,SJRC (BERT-Large +SRL),308000000.0,90.86,223000000,0.66,339000000,0.91,Parameters
9,1,Natural Language Inference,SNLI,2019-01,MT-DNN,330000000.0,97.35,22000000,0.06,339000000,0.97,Parameters
10,1,Natural Language Inference,SNLI,2019-09,SemBERT,339000000.0,100.0,9000000,0.03,339000000,1.0,Parameters
0,1,Sentence Compression,Google Dataset,2015-09,LSTM,0.38,88.37,0.38,0.88,0.43,0.88,CR
1,1,Sentence Compression,Google Dataset,2017-07,BiLSTM,0.43,100.0,0.0,0.0,0.43,1.0,CR
0,1,Text Summarization,DUC 2004 Task 1,2015-09,Abs+,8.49,72.07,8.49,0.72,11.78,0.39,ROUGE\\-2
1,1,Text Summarization,DUC 2004 Task 1,2016-02,words-lvt5k-1sent,9.42,79.97,0.9,0.08,11.78,0.44,ROUGE\\-2
2,1,Text Summarization,DUC 2004 Task 1,2017-04,EndDec+WFE,10.54,89.47,1.1,0.09,11.78,0.49,ROUGE\\-2
3,1,Text Summarization,DUC 2004 Task 1,2017-09,DRGD,10.75,91.26,0.2,0.02,11.78,0.5,ROUGE\\-2
4,1,Text Summarization,DUC 2004 Task 1,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble,11.78,100.0,1.0,0.08,11.78,0.55,ROUGE\\-2
5,1,Text Summarization,GigaWord,2016-02,words-lvt5k-1sent,17.7,86.68,17.7,0.87,20.42,0.82,ROUGE\\-2
6,1,Text Summarization,GigaWord,2018-07,CGU,18.0,88.15,0.3,0.01,20.42,0.84,ROUGE\\-2
7,1,Text Summarization,GigaWord,2018-07,Re^3 Sum,19.03,93.19,1.0,0.05,20.42,0.88,ROUGE\\-2
8,1,Text Summarization,GigaWord,2019-05,MASS,19.71,96.52,0.7,0.03,20.42,0.91,ROUGE\\-2
9,1,Text Summarization,GigaWord,2019-05,UniLM,20.05,98.19,0.3,0.01,20.42,0.93,ROUGE\\-2
10,1,Text Summarization,GigaWord,2020-01,ProphetNet,20.42,100.0,0.4,0.02,20.42,0.95,ROUGE\\-2
11,1,Abstractive Text Summarization,CNN / Daily Mail,2017-04,Pointer-Generator + Coverage,17.28,80.48,17.28,0.8,21.47,0.8,ROUGE\\-2
12,1,Abstractive Text Summarization,CNN / Daily Mail,2019-05,UniLM,20.43,95.16,3.1,0.14,21.47,0.95,ROUGE\\-2
13,1,Abstractive Text Summarization,CNN / Daily Mail,2019-12,PEGASUS,21.47,100.0,1.0,0.05,21.47,1.0,ROUGE\\-2
14,1,Document Summarization,CNN / Daily Mail,2017-05,"ML + Intra-Attention (Paulus et al., 2017)",14.81,68.72,14.81,0.69,21.55,0.69,ROUGE\\-2
15,1,Document Summarization,CNN / Daily Mail,2017-05,"ML + RL (Paulus et al., 2017)",15.82,73.41,1.0,0.05,21.55,0.73,ROUGE\\-2
16,1,Document Summarization,CNN / Daily Mail,2018-08,Bottom-Up Sum,18.68,86.68,2.9,0.13,21.55,0.87,ROUGE\\-2
17,1,Document Summarization,CNN / Daily Mail,2019-03,BERTSUM+Transformer,20.24,93.92,1.6,0.07,21.55,0.94,ROUGE\\-2
18,1,Document Summarization,CNN / Daily Mail,2019-05,UniLM (Abstractive Summarization),20.43,94.8,0.2,0.01,21.55,0.95,ROUGE\\-2
19,1,Document Summarization,CNN / Daily Mail,2019-10,T5-11B,21.55,100.0,1.1,0.05,21.55,1.0,ROUGE\\-2
20,1,Extractive Document Summarization,CNN / Daily Mail,2018-09,ITS,12.6,61.95,12.6,0.62,20.34,0.58,ROUGE\\-2
21,1,Extractive Document Summarization,CNN / Daily Mail,2019-03,BERTSUM,20.24,99.51,7.6,0.37,20.34,0.94,ROUGE\\-2
22,1,Extractive Document Summarization,CNN / Daily Mail,2019-08,BertSumExt,20.34,100.0,0.1,0.0,20.34,0.94,ROUGE\\-2
23,1,Sentence Summarization,GigaWord,2019-07,Contextual Match,10.05,100.0,10.05,1.0,10.05,0.47,ROUGE\\-2
24,1,Unsupervised Sentence Summarization,GigaWord,2019-07,Contextual Match,10.05,100.0,10.05,1.0,10.05,0.47,ROUGE\\-2
0,1,Text Summarization,DUC 2004 Task 1,2015-09,Abs+,23.81,83.49,23.81,0.83,28.52,0.34,ROUGE\\-L
1,1,Text Summarization,DUC 2004 Task 1,2016-02,words-lvt5k-1sent,25.24,88.5,1.4,0.05,28.52,0.36,ROUGE\\-L
2,1,Text Summarization,DUC 2004 Task 1,2017-04,EndDec+WFE,27.8,97.48,2.6,0.09,28.52,0.39,ROUGE\\-L
3,1,Text Summarization,DUC 2004 Task 1,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble,28.52,100.0,0.7,0.02,28.52,0.4,ROUGE\\-L
4,1,Text Summarization,GigaWord,2016-02,words-lvt5k-1sent,33.71,91.88,33.71,0.92,36.69,0.48,ROUGE\\-L
5,1,Text Summarization,GigaWord,2017-04,EndDec+WFE,33.88,92.34,0.2,0.01,36.69,0.48,ROUGE\\-L
6,1,Text Summarization,GigaWord,2017-11,FTSum_g,34.24,93.32,0.4,0.01,36.69,0.48,ROUGE\\-L
7,1,Text Summarization,GigaWord,2018-06,Seq2seq + E2T_cnn,34.93,95.2,0.7,0.02,36.69,0.49,ROUGE\\-L
8,1,Text Summarization,GigaWord,2019-05,MASS,35.96,98.01,1.0,0.03,36.69,0.51,ROUGE\\-L
9,1,Text Summarization,GigaWord,2019-05,UniLM,36.0,98.12,0.0,0.0,36.69,0.51,ROUGE\\-L
10,1,Text Summarization,GigaWord,2019-12,PEGASUS,36.24,98.77,0.2,0.01,36.69,0.51,ROUGE\\-L
11,1,Text Summarization,GigaWord,2020-01,ProphetNet,36.69,100.0,0.4,0.01,36.69,0.52,ROUGE\\-L
12,1,Document Summarization,CNN / Daily Mail,2017-05,"ML + Intra-Attention (Paulus et al., 2017)",35.49,87.22,35.49,0.87,40.69,0.5,ROUGE\\-L
13,1,Document Summarization,CNN / Daily Mail,2017-05,"ML + RL (Paulus et al., 2017)",36.9,90.69,1.4,0.03,40.69,0.52,ROUGE\\-L
14,1,Document Summarization,CNN / Daily Mail,2018-08,Bottom-Up Sum,38.34,94.22,1.4,0.03,40.69,0.54,ROUGE\\-L
15,1,Document Summarization,CNN / Daily Mail,2019-03,BERTSUM+Transformer,39.63,97.39,1.3,0.03,40.69,0.56,ROUGE\\-L
16,1,Document Summarization,CNN / Daily Mail,2019-05,UniLM (Abstractive Summarization),40.34,99.14,0.7,0.02,40.69,0.57,ROUGE\\-L
17,1,Document Summarization,CNN / Daily Mail,2019-10,T5-11B,40.69,100.0,0.3,0.01,40.69,0.57,ROUGE\\-L
18,1,Data-to-Text Generation,E2E NLG Challenge,2017-12,Gong,66.45,93.82,66.45,0.94,70.83,0.94,ROUGE\\-L
19,1,Data-to-Text Generation,E2E NLG Challenge,2018-03,Zhang,70.83,100.0,4.4,0.06,70.83,1.0,ROUGE\\-L
20,1,Paper generation,ACL Title and Abstract Dataset,2018-05,Writing-editing Network,20.3,100.0,20.3,1.0,20.3,0.29,ROUGE\\-L
21,1,Extractive Document Summarization,CNN / Daily Mail,2019-03,BERTSUM,39.63,99.32,39.63,0.99,39.9,0.56,ROUGE\\-L
22,1,Extractive Document Summarization,CNN / Daily Mail,2019-08,BertSumExt,39.9,100.0,0.3,0.01,39.9,0.56,ROUGE\\-L
23,1,Abstractive Text Summarization,CNN / Daily Mail,2019-05,UniLM,40.34,97.46,40.34,0.97,41.39,0.57,ROUGE\\-L
24,1,Abstractive Text Summarization,CNN / Daily Mail,2019-12,PEGASUS,41.11,99.32,0.8,0.02,41.39,0.58,ROUGE\\-L
25,1,Abstractive Text Summarization,CNN / Daily Mail,2020-01,ProphetNet,41.39,100.0,0.3,0.01,41.39,0.58,ROUGE\\-L
26,1,Sentence Summarization,GigaWord,2019-07,Contextual Match,24.41,100.0,24.41,1.0,24.41,0.34,ROUGE\\-L
27,1,Unsupervised Sentence Summarization,GigaWord,2019-07,Contextual Match,24.41,100.0,24.41,1.0,24.41,0.34,ROUGE\\-L
0,1,Text Summarization,DUC 2004 Task 1,2015-09,Abs+,28.18,85.78,28.18,0.86,32.85,0.53,ROUGE\\-1
1,1,Text Summarization,DUC 2004 Task 1,2016-02,words-lvt5k-1sent,28.61,87.09,0.4,0.01,32.85,0.54,ROUGE\\-1
2,1,Text Summarization,DUC 2004 Task 1,2016-06,RAS-Elman,28.97,88.19,0.4,0.01,32.85,0.55,ROUGE\\-1
3,1,Text Summarization,DUC 2004 Task 1,2017-04,EndDec+WFE,32.28,98.26,3.3,0.1,32.85,0.61,ROUGE\\-1
4,1,Text Summarization,DUC 2004 Task 1,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble,32.85,100.0,0.6,0.02,32.85,0.62,ROUGE\\-1
5,1,Text Summarization,GigaWord,2016-02,words-lvt5k-1sent,36.4,92.13,36.4,0.92,39.51,0.69,ROUGE\\-1
6,1,Text Summarization,GigaWord,2017-11,FTSum_g,37.27,94.33,0.9,0.02,39.51,0.7,ROUGE\\-1
7,1,Text Summarization,GigaWord,2019-05,MASS,38.73,98.03,1.5,0.04,39.51,0.73,ROUGE\\-1
8,1,Text Summarization,GigaWord,2019-05,UniLM,38.9,98.46,0.2,0.01,39.51,0.73,ROUGE\\-1
9,1,Text Summarization,GigaWord,2019-12,PEGASUS,39.12,99.01,0.2,0.01,39.51,0.74,ROUGE\\-1
10,1,Text Summarization,GigaWord,2020-01,ProphetNet,39.51,100.0,0.4,0.01,39.51,0.74,ROUGE\\-1
11,1,Abstractive Text Summarization,CNN / Daily Mail,2017-04,Pointer-Generator + Coverage,39.53,89.43,39.53,0.89,44.2,0.74,ROUGE\\-1
12,1,Abstractive Text Summarization,CNN / Daily Mail,2019-05,UniLM,43.08,97.47,3.5,0.08,44.2,0.81,ROUGE\\-1
13,1,Abstractive Text Summarization,CNN / Daily Mail,2019-12,PEGASUS,44.17,99.93,1.1,0.02,44.2,0.83,ROUGE\\-1
14,1,Abstractive Text Summarization,CNN / Daily Mail,2020-01,ProphetNet,44.2,100.0,0.0,0.0,44.2,0.83,ROUGE\\-1
15,1,Query-Based Extractive Summarization,Debatepedia,2017-04,SD2,41.26,77.72,41.26,0.78,53.09,0.78,ROUGE\\-1
16,1,Query-Based Extractive Summarization,Debatepedia,2018-01,RSA Word Count,53.09,100.0,11.8,0.22,53.09,1.0,ROUGE\\-1
17,1,Document Summarization,CNN / Daily Mail,2017-05,"ML + Intra-Attention (Paulus et al., 2017)",38.3,87.34,38.3,0.87,43.85,0.72,ROUGE\\-1
18,1,Document Summarization,CNN / Daily Mail,2017-05,"ML + RL (Paulus et al., 2017)",39.87,90.92,1.6,0.04,43.85,0.75,ROUGE\\-1
19,1,Document Summarization,CNN / Daily Mail,2018-08,Bottom-Up Sum,41.22,94.0,1.4,0.03,43.85,0.78,ROUGE\\-1
20,1,Document Summarization,CNN / Daily Mail,2019-03,BERTSUM+Transformer,43.25,98.63,2.0,0.05,43.85,0.81,ROUGE\\-1
21,1,Document Summarization,CNN / Daily Mail,2019-08,BertSumExt,43.85,100.0,0.6,0.01,43.85,0.83,ROUGE\\-1
22,1,Extractive Document Summarization,CNN / Daily Mail,2018-09,ITS,30.8,70.24,30.8,0.7,43.85,0.58,ROUGE\\-1
23,1,Extractive Document Summarization,CNN / Daily Mail,2019-03,BERTSUM,43.25,98.63,12.4,0.28,43.85,0.81,ROUGE\\-1
24,1,Extractive Document Summarization,CNN / Daily Mail,2019-08,BertSumExt,43.85,100.0,0.6,0.01,43.85,0.83,ROUGE\\-1
25,1,Reader-Aware Summarization,RASG,2018-12,RASG,30.33,100.0,30.33,1.0,30.33,0.57,ROUGE\\-1
26,1,Sentence Summarization,GigaWord,2019-07,Contextual Match,26.48,100.0,26.48,1.0,26.48,0.5,ROUGE\\-1
27,1,Unsupervised Sentence Summarization,GigaWord,2019-07,Contextual Match,26.48,100.0,26.48,1.0,26.48,0.5,ROUGE\\-1
28,1,Timeline Summarization,MTS,2019-08,MTS,39.78,100.0,39.78,1.0,39.78,0.75,ROUGE\\-1
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2015-12,TD-LSTM,71.88,83.35,71.88,0.83,86.24,0.83,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
1,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2016-05,MemNet,76.58,88.8,4.7,0.05,86.24,0.89,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
2,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2017-09,RAM,77.36,89.7,0.8,0.01,86.24,0.9,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
3,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-02,LCR-Rot,78.29,90.78,0.9,0.01,86.24,0.91,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
4,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-04,SA-LSTM-P,78.35,90.85,0.1,0.0,86.24,0.91,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
5,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-05,TNet-LF,78.4,90.91,0.1,0.0,86.24,0.91,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
6,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-10,HAPN,79.75,92.47,1.3,0.02,86.24,0.92,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
7,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-02,AEN-BERT,81.53,94.54,1.8,0.02,86.24,0.95,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
8,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-02,BERT-SPC,81.73,94.77,0.2,0.0,86.24,0.95,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
9,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-06,SDGCN-BERT,82.46,95.62,0.7,0.01,86.24,0.96,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
10,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-08,BERT-ADA,84.06,97.47,1.6,0.02,86.24,0.97,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
11,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-12,LCF-ATEPC,86.24,100.0,2.2,0.03,86.24,1.0,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2015-12,TD-LSTM,75.63,83.87,75.63,0.84,90.18,0.84,Restaurant\\ \\(Acc\\)
1,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2016-05,MemNet,80.95,89.76,5.3,0.06,90.18,0.9,Restaurant\\ \\(Acc\\)
2,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-02,LCR-Rot,81.34,90.2,0.4,0.0,90.18,0.9,Restaurant\\ \\(Acc\\)
3,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-04,SA-LSTM-P,81.6,90.49,0.3,0.0,90.18,0.9,Restaurant\\ \\(Acc\\)
4,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-10,HAPN,82.23,91.18,0.6,0.01,90.18,0.91,Restaurant\\ \\(Acc\\)
5,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-02,BERT-SPC,84.46,93.66,2.2,0.02,90.18,0.94,Restaurant\\ \\(Acc\\)
6,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-04,BERT-PT,84.95,94.2,0.5,0.01,90.18,0.94,Restaurant\\ \\(Acc\\)
7,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-08,BERT-ADA,87.89,97.46,2.9,0.03,90.18,0.97,Restaurant\\ \\(Acc\\)
8,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-12,LCF-ATEPC,90.18,100.0,2.3,0.03,90.18,1.0,Restaurant\\ \\(Acc\\)
9,1,Aspect-Based Sentiment Analysis,SemEval-2016 Task 5 Subtask 1,2019-03,HAABSA,88.0,100.0,88,1.0,88,0.98,Restaurant\\ \\(Acc\\)
10,1,Aspect-Based Sentiment Analysis,SemEval 2015 Task 12,2019-03,HAABSA,80.6,98.65,80.6,0.99,81.7,0.89,Restaurant\\ \\(Acc\\)
11,1,Aspect-Based Sentiment Analysis,SemEval 2015 Task 12,2020-04,HAABSA++,81.7,100.0,1.1,0.01,81.7,0.91,Restaurant\\ \\(Acc\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2015-12,TD-LSTM,68.13,82.79,68.13,0.83,82.29,0.83,Laptop\\ \\(Acc\\)
1,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2016-05,MemNet,72.21,87.75,4.1,0.05,82.29,0.88,Laptop\\ \\(Acc\\)
2,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2017-09,RAM,74.49,90.52,2.3,0.03,82.29,0.91,Laptop\\ \\(Acc\\)
3,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-02,LCR-Rot,75.24,91.43,0.8,0.01,82.29,0.91,Laptop\\ \\(Acc\\)
4,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-05,TNet-LF,76.01,92.37,0.8,0.01,82.29,0.92,Laptop\\ \\(Acc\\)
5,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2018-10,HAPN,77.27,93.9,1.3,0.02,82.29,0.94,Laptop\\ \\(Acc\\)
6,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-02,AEN-BERT,79.93,97.13,2.7,0.03,82.29,0.97,Laptop\\ \\(Acc\\)
7,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-06,SDGCN-BERT,81.35,98.86,1.4,0.02,82.29,0.99,Laptop\\ \\(Acc\\)
8,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2,2019-12,LCF-ATEPC,82.29,100.0,0.9,0.01,82.29,1.0,Laptop\\ \\(Acc\\)
0,1,Machine Translation,IWSLT2015 English-Vietnamese,2015-12,LSTM+Attention+Ensemble,26.4,79.35,26.4,0.79,33.27,0.38,BLEU
1,1,Machine Translation,IWSLT2015 English-Vietnamese,2018-06,DeconvDec,28.47,85.57,2.1,0.06,33.27,0.42,BLEU
2,1,Machine Translation,IWSLT2015 English-Vietnamese,2018-08,Self-Adaptive Control of Temperature,29.12,87.53,0.7,0.02,33.27,0.42,BLEU
3,1,Machine Translation,IWSLT2015 English-Vietnamese,2018-09,CVT,29.6,88.97,0.5,0.02,33.27,0.43,BLEU
4,1,Machine Translation,IWSLT2015 English-Vietnamese,2019-10,Transformer+BPE+FixNorm+ScaleNorm,32.8,98.59,3.2,0.1,33.27,0.48,BLEU
5,1,Machine Translation,IWSLT2015 English-Vietnamese,2019-10,Transformer+BPE-dropout,33.27,100.0,0.5,0.02,33.27,0.48,BLEU
6,1,Table-to-Text Generation,WikiBio,2016-03,Table NLM,34.7,77.3,34.7,0.77,44.89,0.51,BLEU
7,1,Table-to-Text Generation,WikiBio,2017-11,Field-gating Seq2seq + dual attention,44.89,100.0,10.2,0.23,44.89,0.65,BLEU
8,1,Data-to-Text Generation,RotoWire,2017-07,Encoder-decoder + conditional copy,14.19,81.09,14.19,0.81,17.5,0.21,BLEU
9,1,Data-to-Text Generation,RotoWire,2018-09,Neural Content Planning + conditional copy,16.5,94.29,2.3,0.13,17.5,0.24,BLEU
10,1,Data-to-Text Generation,RotoWire,2019-12,Hierarchical transformer encoder + conditional copy,17.5,100.0,1.0,0.06,17.5,0.26,BLEU
11,1,Machine Translation,WMT 2017 Latvian-English,2017-09,mLSTM with factored data,20.8,85.35,20.8,0.85,24.37,0.3,BLEU
12,1,Machine Translation,WMT 2017 Latvian-English,2018-10,Transformer trained on highly filtered data,24.37,100.0,3.6,0.15,24.37,0.36,BLEU
13,1,Data-to-Text Generation,E2E NLG Challenge,2017-12,Gong,64.22,93.62,64.22,0.94,68.6,0.94,BLEU
14,1,Data-to-Text Generation,E2E NLG Challenge,2018-03,Zhang,65.45,95.41,1.2,0.02,68.6,0.95,BLEU
15,1,Data-to-Text Generation,E2E NLG Challenge,2018-04,Sys1-Primary,65.61,95.64,0.2,0.0,68.6,0.96,BLEU
16,1,Data-to-Text Generation,E2E NLG Challenge,2018-05,Slug,66.19,96.49,0.6,0.01,68.6,0.96,BLEU
17,1,Data-to-Text Generation,E2E NLG Challenge,2019-04,S_1^R,68.6,100.0,2.4,0.03,68.6,1.0,BLEU
18,1,Unsupervised Machine Translation,WMT2014 French-English,2018-04,PBSMT + NMT,27.7,79.37,27.7,0.79,34.9,0.4,BLEU
19,1,Unsupervised Machine Translation,WMT2014 French-English,2019-01,SMT as posterior regularization,28.9,82.81,1.2,0.03,34.9,0.42,BLEU
20,1,Unsupervised Machine Translation,WMT2014 French-English,2019-01,MLM pretraining for encoder and decoder,33.3,95.42,4.4,0.13,34.9,0.49,BLEU
21,1,Unsupervised Machine Translation,WMT2014 French-English,2019-02,SMT + NMT (tuning and joint refinement),33.5,95.99,0.2,0.01,34.9,0.49,BLEU
22,1,Unsupervised Machine Translation,WMT2014 French-English,2019-05,MASS (6-layer Transformer),34.9,100.0,1.4,0.04,34.9,0.51,BLEU
23,1,Unsupervised Machine Translation,WMT2016 German-English,2018-04,PBSMT,25.2,71.59,25.2,0.72,35.2,0.37,BLEU
24,1,Unsupervised Machine Translation,WMT2016 German-English,2018-10,Synthetic bilingual data init,26.7,75.85,1.5,0.04,35.2,0.39,BLEU
25,1,Unsupervised Machine Translation,WMT2016 German-English,2019-01,MLM pretraining for encoder and decoder,34.3,97.44,7.6,0.22,35.2,0.5,BLEU
26,1,Unsupervised Machine Translation,WMT2016 German-English,2019-02,SMT + NMT (tuning and joint refinement),34.4,97.73,0.1,0.0,35.2,0.5,BLEU
27,1,Unsupervised Machine Translation,WMT2016 German-English,2019-05,MASS (6-layer Transformer),35.2,100.0,0.8,0.02,35.2,0.51,BLEU
28,1,Unsupervised Machine Translation,WMT2016 English-German,2018-04,PBSMT + NMT,20.2,71.38,20.2,0.71,28.3,0.29,BLEU
29,1,Unsupervised Machine Translation,WMT2016 English-German,2019-01,SMT as posterior regularization,21.7,76.68,1.5,0.05,28.3,0.32,BLEU
30,1,Unsupervised Machine Translation,WMT2016 English-German,2019-01,MLM pretraining for encoder and decoder,26.4,93.29,4.7,0.17,28.3,0.38,BLEU
31,1,Unsupervised Machine Translation,WMT2016 English-German,2019-02,SMT + NMT (tuning and joint refinement),26.9,95.05,0.5,0.02,28.3,0.39,BLEU
32,1,Unsupervised Machine Translation,WMT2016 English-German,2019-05,MASS (6-layer Transformer),28.3,100.0,1.4,0.05,28.3,0.41,BLEU
33,1,Unsupervised Machine Translation,WMT2014 English-French,2018-04,PBSMT + NMT,27.6,73.6,27.6,0.74,37.5,0.4,BLEU
34,1,Unsupervised Machine Translation,WMT2014 English-French,2019-01,SMT as posterior regularization,29.5,78.67,1.9,0.05,37.5,0.43,BLEU
35,1,Unsupervised Machine Translation,WMT2014 English-French,2019-01,MLM pretraining for encoder and decoder,33.4,89.07,3.9,0.1,37.5,0.49,BLEU
36,1,Unsupervised Machine Translation,WMT2014 English-French,2019-02,SMT + NMT (tuning and joint refinement),36.2,96.53,2.8,0.07,37.5,0.53,BLEU
37,1,Unsupervised Machine Translation,WMT2014 English-French,2019-05,MASS (6-layer Transformer),37.5,100.0,1.3,0.03,37.5,0.55,BLEU
38,1,Machine Translation,ACCURAT balanced test corpus for under resourced languages Estonian-Russian,2018-05,Multilingual Transformer,19.18,100.0,19.18,1.0,19.18,0.28,BLEU
39,1,Machine Translation,ACCURAT balanced test corpus for under resourced languages Russian-Estonian,2018-05,Multilingual Transformer,18.03,100.0,18.03,1.0,18.03,0.26,BLEU
40,1,Text Generation,LDC2016E25,2018-05,Graph2Seq,22.0,100.0,22,1.0,22,0.32,BLEU
41,1,Graph-to-Sequence,LDC2015E86:,2018-05,GRN,33.6,100.0,33.6,1.0,33.6,0.49,BLEU
42,1,Table-to-Text Generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model,23.2,100.0,23.2,1.0,23.2,0.34,BLEU
43,1,KB-to-Language Generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model,23.2,100.0,23.2,1.0,23.2,0.34,BLEU
44,1,Machine Translation,WMT 2018 Estonian-English,2018-10,Multi-pass backtranslated adapted transformer,29.0,100.0,29,1.0,29,0.42,BLEU
45,1,Machine Translation,WMT 2018 English-Estonian,2018-10,Multi-pass backtranslated adapted transformer,24.1,100.0,24.1,1.0,24.1,0.35,BLEU
46,1,Machine Translation,WMT 2018 English-Finnish,2018-10,Transformer trained on highly filtered data,17.4,100.0,17.4,1.0,17.4,0.25,BLEU
47,1,Machine Translation,WMT 2018 Finnish-English,2018-10,Transformer trained on highly filtered data,24.0,90.57,24.0,0.91,26.5,0.35,BLEU
48,1,Machine Translation,WMT 2018 Finnish-English,2019-06,CT+B/S construction,26.5,100.0,2.5,0.09,26.5,0.39,BLEU
49,1,Machine Translation,WMT 2017 English-Latvian,2018-10,Transformer trained on highly filtered data,22.89,100.0,22.89,1.0,22.89,0.33,BLEU
50,1,Data-to-Text Generation,WebNLG,2018-10,GCN EC,0.559,100.0,0.559,1.0,0.559,0.01,BLEU
51,1,Data-to-Text Generation,SR11Deep,2018-10,GCN + feat,0.666,100.0,0.666,1.0,0.666,0.01,BLEU
52,1,Unsupervised Machine Translation,WMT2014 English-German,2019-01,SMT as posterior regularization,17.0,75.56,17.0,0.76,22.5,0.25,BLEU
53,1,Unsupervised Machine Translation,WMT2014 English-German,2019-02,SMT + NMT (tuning and joint refinement),22.5,100.0,5.5,0.24,22.5,0.33,BLEU
54,1,Unsupervised Machine Translation,WMT2014 German-English,2019-01,SMT as posterior regularization,20.4,75.56,20.4,0.76,27.0,0.3,BLEU
55,1,Unsupervised Machine Translation,WMT2014 German-English,2019-02,SMT + NMT (tuning and joint refinement),27.0,100.0,6.6,0.24,27.0,0.39,BLEU
56,1,Unsupervised Machine Translation,WMT2016 English-Romanian,2019-01,MLM pretraining for encoder and decoder,33.3,94.6,33.3,0.95,35.2,0.49,BLEU
57,1,Unsupervised Machine Translation,WMT2016 English-Romanian,2019-05,MASS (6-layer Transformer),35.2,100.0,1.9,0.05,35.2,0.51,BLEU
58,1,Unsupervised Machine Translation,WMT2016 English--Romanian,2019-01,MLM pretraining for encoder and decoder,33.3,100.0,33.3,1.0,33.3,0.49,BLEU
59,1,Unsupervised Machine Translation,WMT2016 Romanian-English,2019-01,MLM pretraining for encoder and decoder,31.8,96.07,31.8,0.96,33.1,0.46,BLEU
60,1,Unsupervised Machine Translation,WMT2016 Romanian-English,2019-05,MASS (6-layer Transformer),33.1,100.0,1.3,0.04,33.1,0.48,BLEU
61,1,Question Answering,JD Product Question Answer,2019-01,PAAG,2.0189,100.0,2.0189,1.0,2.0189,0.03,BLEU
62,1,Machine Translation,WMT2017 Finnish-English,2019-06,CT+B/S construction,35.5,100.0,35.5,1.0,35.5,0.52,BLEU
63,1,Machine Translation,WMT2016 Finnish-English,2019-06,CT+B/S construction,32.4,100.0,32.4,1.0,32.4,0.47,BLEU
64,1,Machine Translation,WMT2019 Finnish-English,2019-06,CT+B/S construction,34.1,100.0,34.1,1.0,34.1,0.5,BLEU
65,1,Data-to-Text Generation,WebNLG Full,2019-08,Transformer (Pipeline),51.68,97.69,51.68,0.98,52.9,0.75,BLEU
66,1,Data-to-Text Generation,WebNLG Full,2020-04,DATATUNER_NO_FC,52.9,100.0,1.2,0.02,52.9,0.77,BLEU
67,1,Data-to-Text Generation,LDC2017T10,2019-09,DualGraph,28.26,74.96,28.26,0.75,37.7,0.41,BLEU
68,1,Data-to-Text Generation,LDC2017T10,2020-04,DataTuner_FC,37.7,100.0,9.4,0.25,37.7,0.55,BLEU
69,1,Data-to-Text Generation,ViGGO,2019-10,Bo3,52.1,97.2,52.1,0.97,53.6,0.76,BLEU
70,1,Data-to-Text Generation,ViGGO,2020-04,DataTuner_FC,53.6,100.0,1.5,0.03,53.6,0.78,BLEU
71,1,Data-to-Text Generation,Cleaned E2E NLG Challenge,2019-11,TGen,40.73,93.42,40.73,0.93,43.6,0.59,BLEU
72,1,Data-to-Text Generation,Cleaned E2E NLG Challenge,2020-04,DataTuner_FC,43.6,100.0,2.9,0.07,43.6,0.64,BLEU
73,1,Machine Translation,IWSLT2015 Chinese-English,2019-11,BP-Transformer,19.84,100.0,19.84,1.0,19.84,0.29,BLEU
0,1,Entity Disambiguation,TAC2010,2016-01,Wikipedia2Vec,85.2,97.15,85.2,0.97,87.7,0.88,Micro\\-F1
1,1,Entity Disambiguation,TAC2010,2017-05,NTEE,87.7,100.0,2.5,0.03,87.7,0.91,Micro\\-F1
2,1,Entity Disambiguation,AIDA-CoNLL,2016-01,Wikipedia2Vec-GBRT,93.1,98.0,93.1,0.98,95.0,0.97,Micro\\-F1
3,1,Entity Disambiguation,AIDA-CoNLL,2017-05,NTEE,94.7,99.68,1.6,0.02,95.0,0.98,Micro\\-F1
4,1,Entity Disambiguation,AIDA-CoNLL,2019-09,confidence-order,95.0,100.0,0.3,0.0,95.0,0.99,Micro\\-F1
5,1,Entity Disambiguation,ACE2004,2017-04,Global,88.5,96.3,88.5,0.96,91.9,0.92,Micro\\-F1
6,1,Entity Disambiguation,ACE2004,2019-09,confidence-order,91.9,100.0,3.4,0.04,91.9,0.95,Micro\\-F1
7,1,Entity Disambiguation,MSNBC,2017-04,Global,93.7,97.3,93.7,0.97,96.3,0.97,Micro\\-F1
8,1,Entity Disambiguation,MSNBC,2019-09,confidence-order,96.3,100.0,2.6,0.03,96.3,1.0,Micro\\-F1
9,1,Entity Disambiguation,AQUAINT,2017-04,Global,88.5,94.45,88.5,0.94,93.7,0.92,Micro\\-F1
10,1,Entity Disambiguation,AQUAINT,2019-09,MEP + pseudo entities,93.7,100.0,5.2,0.06,93.7,0.97,Micro\\-F1
11,1,Entity Disambiguation,WNED-CWEB,2017-04,Global,77.9,98.73,77.9,0.99,78.9,0.81,Micro\\-F1
12,1,Entity Disambiguation,WNED-CWEB,2019-09,confidence-order,78.9,100.0,1.0,0.01,78.9,0.82,Micro\\-F1
13,1,Entity Disambiguation,WNED-WIKI,2017-04,Glonal,77.5,86.98,77.5,0.87,89.1,0.8,Micro\\-F1
14,1,Entity Disambiguation,WNED-WIKI,2019-09,confidence-order,89.1,100.0,11.6,0.13,89.1,0.93,Micro\\-F1
15,1,Entity Linking,OKE-2015,2018-08,E2E,66.9,100.0,66.9,1.0,66.9,0.69,Micro\\-F1
16,1,Entity Linking,OKE-2016,2018-08,E2E,58.4,100.0,58.4,1.0,58.4,0.61,Micro\\-F1
17,1,Entity Linking,MSNBC,2018-08,E2E,72.4,100.0,72.4,1.0,72.4,0.75,Micro\\-F1
18,1,Entity Linking,N3-Reuters-128,2018-08,E2E,54.6,100.0,54.6,1.0,54.6,0.57,Micro\\-F1
19,1,Entity Linking,Derczynski,2018-08,E2E,42.3,100.0,42.3,1.0,42.3,0.44,Micro\\-F1
20,1,Emotion Recognition in Conversation,EC,2019-03,HRLCE + BERT,0.7709,99.28,0.7709,0.99,0.7765,0.01,Micro\\-F1
21,1,Emotion Recognition in Conversation,EC,2019-04,NELEC,0.7765,100.0,0.0,0.0,0.7765,0.01,Micro\\-F1
22,1,Multi-Label Text Classification,RCV1-v2,2020-02,MAGNET,88.5,100.0,88.5,1.0,88.5,0.92,Micro\\-F1
23,1,Multi-Label Text Classification,Reuters-21578,2020-02,MAGNET,89.9,100.0,89.9,1.0,89.9,0.93,Micro\\-F1
24,1,Multi-Label Text Classification,Slashdot,2020-02,MAGNET,56.8,100.0,56.8,1.0,56.8,0.59,Micro\\-F1
0,1,Language Modelling,Text8,2016-02,"td-LSTM (Zhang et al., 2016)",1.63,100.0,1.63,1.0,1.63,1.0,Bit\\ per\\ Character\\ \\(BPC\\)
1,1,Language Modelling,enwiki8,2016-07,Recurrent highway networks,1.27,94.78,1.27,0.95,1.34,0.78,Bit\\ per\\ Character\\ \\(BPC\\)
2,1,Language Modelling,enwiki8,2016-09,LN HM-LSTM,1.32,98.51,0.1,0.07,1.34,0.81,Bit\\ per\\ Character\\ \\(BPC\\)
3,1,Language Modelling,enwiki8,2016-09,Hypernetworks,1.34,100.0,0.0,0.0,1.34,0.82,Bit\\ per\\ Character\\ \\(BPC\\)
4,1,Language Modelling,Hutter Prize,2016-07,Large RHN,1.27,96.95,1.27,0.97,1.31,0.78,Bit\\ per\\ Character\\ \\(BPC\\)
5,1,Language Modelling,Hutter Prize,2016-07,RHN,1.31,100.0,0.0,0.0,1.31,0.8,Bit\\ per\\ Character\\ \\(BPC\\)
0,1,Question Answering,Children's Book Test,2016-03,AS reader (avg),68.9,73.85,68.9,0.74,93.3,0.74,Accuracy\\-CN
1,1,Question Answering,Children's Book Test,2016-06,NSE,71.9,77.06,3.0,0.03,93.3,0.77,Accuracy\\-CN
2,1,Question Answering,Children's Book Test,2019-02,GPT-2,93.3,100.0,21.4,0.23,93.3,1.0,Accuracy\\-CN
0,1,Question Answering,Children's Book Test,2016-03,AS reader (avg),70.6,79.28,70.6,0.79,89.05,0.79,Accuracy\\-NE
1,1,Question Answering,Children's Book Test,2016-03,AS reader (greedy),71.0,79.73,0.4,0.0,89.05,0.8,Accuracy\\-NE
2,1,Question Answering,Children's Book Test,2016-06,NSE,73.2,82.2,2.2,0.02,89.05,0.82,Accuracy\\-NE
3,1,Question Answering,Children's Book Test,2016-06,GA + feature + fix L(w),74.9,84.11,1.7,0.02,89.05,0.84,Accuracy\\-NE
4,1,Question Answering,Children's Book Test,2019-02,GPT-2,89.05,100.0,14.1,0.16,89.05,1.0,Accuracy\\-NE
0,1,Open-Domain Question Answering,SearchQA,2016-03,ASR,41.3,66.4,41.3,0.66,62.2,0.66,Unigram\\ Acc
1,1,Open-Domain Question Answering,SearchQA,2018-01,AMANDA,46.8,75.24,5.5,0.09,62.2,0.75,Unigram\\ Acc
2,1,Open-Domain Question Answering,SearchQA,2018-10,Bi-Attention + DCU-LSTM,49.4,79.42,2.6,0.04,62.2,0.79,Unigram\\ Acc
3,1,Open-Domain Question Answering,SearchQA,2018-11,DecaProp,62.2,100.0,12.8,0.21,62.2,1.0,Unigram\\ Acc
0,1,Open-Domain Question Answering,SearchQA,2016-03,ASR,22.8,32.2,22.8,0.32,70.8,0.32,N\\-gram\\ F1
1,1,Open-Domain Question Answering,SearchQA,2018-01,AMANDA,56.6,79.94,33.8,0.48,70.8,0.8,N\\-gram\\ F1
2,1,Open-Domain Question Answering,SearchQA,2018-10,Bi-Attention + DCU-LSTM,59.5,84.04,2.9,0.04,70.8,0.84,N\\-gram\\ F1
3,1,Open-Domain Question Answering,SearchQA,2018-11,DecaProp,70.8,100.0,11.3,0.16,70.8,1.0,N\\-gram\\ F1
0,1,Table-to-Text Generation,WikiBio,2016-03,Table NLM,25.8,61.94,25.8,0.62,41.65,0.53,ROUGE
1,1,Table-to-Text Generation,WikiBio,2017-11,Field-gating Seq2seq + dual attention,41.21,98.94,15.4,0.37,41.65,0.84,ROUGE
2,1,Table-to-Text Generation,WikiBio,2017-11,Field-gating Seq2seq + dual attention + beam search,41.65,100.0,0.4,0.01,41.65,0.85,ROUGE
3,1,KB-to-Language Generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model,42.0,100.0,42,1.0,42,0.86,ROUGE
4,1,Table-to-Text Generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model,23.4,100.0,23.4,1.0,23.4,0.48,ROUGE
5,1,Image Captioning,COCO,2019-05,"NIC (ResNet-50, CutMix)",49.0,100.0,49,1.0,49,1.0,ROUGE
0,1,Language Modelling,Text8,2016-03,BN LSTM,16000000.0,5.78,16000000,0.06,277000000,0.0,Number\\ of\\ params
1,1,Language Modelling,Text8,2016-07,Large RHN,46000000.0,16.61,30000000,0.11,277000000,0.01,Number\\ of\\ params
2,1,Language Modelling,Text8,2018-08,64-layer Character Transformer Model,235000000.0,84.84,189000000,0.68,277000000,0.03,Number\\ of\\ params
3,1,Language Modelling,Text8,2019-01,Transformer-XL,277000000.0,100.0,42000000,0.15,277000000,0.03,Number\\ of\\ params
4,1,Language Modelling,Hutter Prize,2016-07,Large RHN,46000000.0,16.61,46000000,0.17,277000000,0.01,Number\\ of\\ params
5,1,Language Modelling,Hutter Prize,2017-05,Large FS-LSTM-4,47000000.0,16.97,1000000,0.0,277000000,0.01,Number\\ of\\ params
6,1,Language Modelling,Hutter Prize,2018-08,64-layer Character Transformer Model,235000000.0,84.84,188000000,0.68,277000000,0.03,Number\\ of\\ params
7,1,Language Modelling,Hutter Prize,2019-01,24-layer Transformer-XL,277000000.0,100.0,42000000,0.15,277000000,0.03,Number\\ of\\ params
8,1,Language Modelling,enwiki8,2016-07,Recurrent highway networks,46000000.0,2.98,46000000,0.03,1542000000,0.01,Number\\ of\\ params
9,1,Language Modelling,enwiki8,2017-05,Large FS-LSTM-4,47000000.0,3.05,1000000,0.0,1542000000,0.01,Number\\ of\\ params
10,1,Language Modelling,enwiki8,2018-04,Sparse Transformer (fixed),95000000.0,6.16,48000000,0.03,1542000000,0.01,Number\\ of\\ params
11,1,Language Modelling,enwiki8,2018-08,64-layer Transformer,235000000.0,15.24,140000000,0.09,1542000000,0.03,Number\\ of\\ params
12,1,Language Modelling,enwiki8,2019-01,Transformer-XL,277000000.0,17.96,42000000,0.03,1542000000,0.03,Number\\ of\\ params
13,1,Language Modelling,enwiki8,2019-02,GPT-2x,1542000000.0,100.0,1265000000,0.82,1542000000,0.19,Number\\ of\\ params
14,1,Language Modelling,WikiText-2,2017-08,AWD-LSTM + continuous cache pointer,33000000.0,2.14,33000000,0.02,1542000000,0.0,Number\\ of\\ params
15,1,Language Modelling,WikiText-2,2017-10,AWD-LSTM 3-layer with Fraternal dropout,34000000.0,2.2,1000000,0.0,1542000000,0.0,Number\\ of\\ params
16,1,Language Modelling,WikiText-2,2017-11,AWD-LSTM-MoS + dynamic eval,35000000.0,2.27,1000000,0.0,1542000000,0.0,Number\\ of\\ params
17,1,Language Modelling,WikiText-2,2018-08,AWD-LSTM-DOC x5,185000000.0,12.0,150000000,0.1,1542000000,0.02,Number\\ of\\ params
18,1,Language Modelling,WikiText-2,2019-02,GPT-2,1542000000.0,100.0,1357000000,0.88,1542000000,0.19,Number\\ of\\ params
19,1,Language Modelling,WikiText-103,2018-03,4 layer QRNN,151000000.0,1.82,151000000,0.02,8300000000,0.02,Number\\ of\\ params
20,1,Language Modelling,WikiText-103,2018-09,Transformer (Adaptive inputs),247000000.0,2.98,96000000,0.01,8300000000,0.03,Number\\ of\\ params
21,1,Language Modelling,WikiText-103,2019-01,Transformer-XL Large,257000000.0,3.1,10000000,0.0,8300000000,0.03,Number\\ of\\ params
22,1,Language Modelling,WikiText-103,2019-02,GPT-2 Large,774000000.0,9.33,517000000,0.06,8300000000,0.09,Number\\ of\\ params
23,1,Language Modelling,WikiText-103,2019-02,GPT-2 Full,1542000000.0,18.58,768000000,0.09,8300000000,0.19,Number\\ of\\ params
24,1,Language Modelling,WikiText-103,2019-09,Megatron-LM,8300000000.0,100.0,6758000000,0.81,8300000000,1.0,Number\\ of\\ params
0,1,Part-Of-Speech Tagging,UD,2016-04,Bi-LSTM,96.4,99.5,96.4,1.0,96.88,1.0,Avg\\ accuracy
1,1,Part-Of-Speech Tagging,UD,2017-11,Adversarial Bi-LSTM,96.73,99.85,0.3,0.0,96.88,1.0,Avg\\ accuracy
2,1,Part-Of-Speech Tagging,UD,2019-08,BiLSTM-LAN,96.88,100.0,0.1,0.0,96.88,1.0,Avg\\ accuracy
0,1,Chinese Word Segmentation,MSR,2019-01,Glyce + BERT,98.2,100.0,98.2,1.0,98.2,1.0,Precision
1,1,Chinese Word Segmentation,AS,2019-01,Glyce + BERT,96.6,100.0,96.6,1.0,96.6,0.98,Precision
2,1,Chinese Word Segmentation,PKU,2019-01,Glyce + BERT,97.1,100.0,97.1,1.0,97.1,0.99,Precision
3,1,Chinese Word Segmentation,CITYU,2019-01,Glyce + BERT,97.9,100.0,97.9,1.0,97.9,1.0,Precision
4,1,Chinese Named Entity Recognition,OntoNotes,2019-01,Glyce + BERT,81.87,100.0,81.87,1.0,81.87,0.83,Precision
5,1,Chinese Named Entity Recognition,MSRA,2019-01,Glyce + BERT,95.57,100.0,95.57,1.0,95.57,0.97,Precision
6,1,Chinese Named Entity Recognition,Weibo NER,2019-01,Glyce + BERT,67.68,100.0,67.68,1.0,67.68,0.69,Precision
7,1,Chinese Named Entity Recognition,Resume NER,2019-01,Glyce + BERT,96.62,100.0,96.62,1.0,96.62,0.98,Precision
8,1,Grammatical Error Correction,CoNLL-2014 Shared Task,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),71.57,100.0,71.57,1.0,71.57,0.73,Precision
9,1,Relation Extraction,FewRel,2019-05,ERNIE,88.49,100.0,88.49,1.0,88.49,0.9,Precision
10,1,Relation Extraction,TACRED,2019-05,ERNIE,69.97,98.83,69.97,0.99,70.8,0.71,Precision
11,1,Relation Extraction,TACRED,2019-07,SpanBERT,70.8,100.0,0.8,0.01,70.8,0.72,Precision
12,1,Entity Typing,Open Entity,2019-05,ERNIE,78.42,100.0,78.42,1.0,78.42,0.8,Precision
13,1,Named Entity Recognition,Long-tail emerging entities,2019-08,Cross-BiLSTM-CNN,58.28,100.0,58.28,1.0,58.28,0.59,Precision
14,1,Named Entity Recognition,French Treebank,2019-11,CamemBERT (subword masking),88.35,100.0,88.35,1.0,88.35,0.9,Precision
15,1,Text Classification,20NEWS,2019-11,SCDV-MS,86.2,100.0,86.2,1.0,86.2,0.88,Precision
16,1,Named Entity Recognition,SoSciSoCi,2020-03,Bi-LSTM-CRF (SSC->GSC),0.83,100.0,0.83,1.0,0.83,0.01,Precision
0,1,Chinese Word Segmentation,MSR,2019-01,Glyce + BERT,98.3,100.0,98.3,1.0,98.3,1.0,Recall
1,1,Chinese Word Segmentation,AS,2019-01,Glyce + BERT,96.8,100.0,96.8,1.0,96.8,0.98,Recall
2,1,Chinese Word Segmentation,PKU,2019-01,Glyce + BERT,96.4,100.0,96.4,1.0,96.4,0.98,Recall
3,1,Chinese Word Segmentation,CITYU,2019-01,Glyce + BERT,98.0,100.0,98,1.0,98,1.0,Recall
4,1,Chinese Named Entity Recognition,MSRA,2019-01,Glyce + BERT,95.51,100.0,95.51,1.0,95.51,0.97,Recall
5,1,Chinese Named Entity Recognition,OntoNotes,2019-01,Glyce + BERT,81.4,100.0,81.4,1.0,81.4,0.83,Recall
6,1,Chinese Named Entity Recognition,Weibo NER,2019-01,Glyce + BERT,67.71,100.0,67.71,1.0,67.71,0.69,Recall
7,1,Chinese Named Entity Recognition,Resume NER,2019-01,Glyce + BERT,96.48,100.0,96.48,1.0,96.48,0.98,Recall
8,1,Grammatical Error Correction,CoNLL-2014 Shared Task,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),38.65,100.0,38.65,1.0,38.65,0.39,Recall
9,1,Entity Typing,Open Entity,2019-05,ERNIE,72.9,100.0,72.9,1.0,72.9,0.74,Recall
10,1,Relation Extraction,FewRel,2019-05,ERNIE,88.44,100.0,88.44,1.0,88.44,0.9,Recall
11,1,Relation Extraction,TACRED,2019-05,ERNIE,66.08,93.2,66.08,0.93,70.9,0.67,Recall
12,1,Relation Extraction,TACRED,2019-07,SpanBERT,70.9,100.0,4.8,0.07,70.9,0.72,Recall
13,1,Named Entity Recognition,Long-tail emerging entities,2019-08,Cross-BiLSTM-CNN,33.92,100.0,33.92,1.0,33.92,0.35,Recall
14,1,Named Entity Recognition,French Treebank,2019-11,CamemBERT (subword masking),87.46,100.0,87.46,1.0,87.46,0.89,Recall
15,1,Text Classification,20NEWS,2019-11,SCDV-MS,86.18,100.0,86.18,1.0,86.18,0.88,Recall
16,1,Named Entity Recognition,SoSciSoCi,2020-03,Bi-LSTM-CRF (SSC->GSC),0.82,100.0,0.82,1.0,0.82,0.01,Recall
0,1,Open-Domain Question Answering,Quasar,2016-06,GA,26.4,62.41,26.4,0.62,42.3,0.62,EM\\ \\(Quasar\\-T\\)
1,1,Open-Domain Question Answering,Quasar,2017-11,Evidence Aggregation via R^3 Re-Ranking,42.3,100.0,15.9,0.38,42.3,1.0,EM\\ \\(Quasar\\-T\\)
0,1,Open-Domain Question Answering,Quasar,2016-06,GA,26.4,53.23,26.4,0.53,49.6,0.53,F1\\ \\(Quasar\\-T\\)
1,1,Open-Domain Question Answering,Quasar,2016-11,BiDAF,28.5,57.46,2.1,0.04,49.6,0.57,F1\\ \\(Quasar\\-T\\)
2,1,Open-Domain Question Answering,Quasar,2017-11,Evidence Aggregation via R^3 Re-Ranking,49.6,100.0,21.1,0.43,49.6,1.0,F1\\ \\(Quasar\\-T\\)
0,1,Dialog State Tracking,Wizard-of-Oz,2016-06,Neural belief tracker,84.4,94.94,84.4,0.95,88.9,0.95,Joint
1,1,Dialog State Tracking,Wizard-of-Oz,2018-05,Zhong et al.,88.1,99.1,3.7,0.04,88.9,0.99,Joint
2,1,Dialog State Tracking,Wizard-of-Oz,2018-10,StateNet,88.9,100.0,0.8,0.01,88.9,1.0,Joint
3,1,Dialog State Tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker,73.4,97.22,73.4,0.97,75.5,0.83,Joint
4,1,Dialog State Tracking,Second dialogue state tracking challenge,2018-05,Zhong et al.,74.5,98.68,1.1,0.01,75.5,0.84,Joint
5,1,Dialog State Tracking,Second dialogue state tracking challenge,2018-10,StateNet,75.5,100.0,1.0,0.01,75.5,0.85,Joint
0,1,Dialog State Tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker,90.0,100.0,90,1.0,90,1.0,Area
0,1,Dialog State Tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker,84.0,100.0,84,1.0,84,1.0,Food
0,1,Dialog State Tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker,94.0,100.0,94,1.0,94,1.0,Price
0,1,Dialog State Tracking,Wizard-of-Oz,2016-06,Neural belief tracker,96.5,99.38,96.5,0.99,97.1,0.99,Request
1,1,Dialog State Tracking,Wizard-of-Oz,2018-05,Zhong et al.,97.1,100.0,0.6,0.01,97.1,1.0,Request
2,1,Dialog State Tracking,Second dialogue state tracking challenge,2016-06,Neural belief tracker,96.5,98.97,96.5,0.99,97.5,0.99,Request
3,1,Dialog State Tracking,Second dialogue state tracking challenge,2018-05,Zhong et al.,97.5,100.0,1.0,0.01,97.5,1.0,Request
0,1,Grammatical Error Detection,CoNLL-2014 A2,2016-07,Bi-LSTM (unrestricted data),44.0,97.56,44.0,0.98,45.1,0.72,F0\\.5
1,1,Grammatical Error Detection,CoNLL-2014 A2,2017-07,Bi-LSTM + POS (unrestricted data),45.1,100.0,1.1,0.02,45.1,0.74,F0\\.5
2,1,Grammatical Error Detection,FCE,2016-07,Bi-LSTM,41.1,78.93,41.1,0.79,52.07,0.67,F0\\.5
3,1,Grammatical Error Detection,FCE,2016-11,Bi-LSTM + charattn,41.88,80.43,0.8,0.02,52.07,0.68,F0\\.5
4,1,Grammatical Error Detection,FCE,2017-04,Bi-LSTM + LMcost,48.48,93.11,6.6,0.13,52.07,0.79,F0\\.5
5,1,Grammatical Error Detection,FCE,2017-07,Ann+PAT+MT,49.11,94.32,0.6,0.01,52.07,0.8,F0\\.5
6,1,Grammatical Error Detection,FCE,2018-11,BiLSTM-JOINT,52.07,100.0,3.0,0.06,52.07,0.85,F0\\.5
7,1,Grammatical Error Detection,CoNLL-2014 A1,2016-07,Bi-LSTM (unrestricted data),34.3,95.01,34.3,0.95,36.1,0.56,F0\\.5
8,1,Grammatical Error Detection,CoNLL-2014 A1,2017-07,Bi-LSTM + POS (unrestricted data),36.1,100.0,1.8,0.05,36.1,0.59,F0\\.5
9,1,Grammatical Error Correction,Restricted,2018-06,SMT + BiGRU,56.25,99.52,56.25,1.0,56.52,0.92,F0\\.5
10,1,Grammatical Error Correction,Restricted,2018-10,CNN Seq2Seq + Quality Estimation,56.52,100.0,0.3,0.01,56.52,0.92,F0\\.5
11,1,Grammatical Error Correction,Unrestricted,2018-07,CNN Seq2Seq + Fluency Boost,61.34,100.0,61.34,1.0,61.34,1.0,F0\\.5
12,1,Grammatical Error Detection,JFLEG,2018-11,BiLSTM-JOINT (trained on FCE),52.52,100.0,52.52,1.0,52.52,0.86,F0\\.5
13,1,Grammatical Error Correction,CoNLL-2014 Shared Task,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),61.15,100.0,61.15,1.0,61.15,1.0,F0\\.5
0,1,Question Answering,SQuAD1.1,2016-08,Match-LSTM with Ans-Ptr (Boundary),60.474,67.27,60.474,0.67,89.898,0.67,EM
1,1,Question Answering,SQuAD1.1,2016-08,Match-LSTM with Ans-Ptr (Boundary) (ensemble),67.901,75.53,7.4,0.08,89.898,0.75,EM
2,1,Question Answering,SQuAD1.1,2016-09,ReasoNet (ensemble),75.034,83.47,7.1,0.08,89.898,0.83,EM
3,1,Question Answering,SQuAD1.1,2017-05,Reinforced Mnemonic Reader (ensemble model),82.283,91.53,7.2,0.08,89.898,0.91,EM
4,1,Question Answering,SQuAD1.1,2018-10,BERT (single model),85.083,94.64,2.8,0.03,89.898,0.94,EM
5,1,Question Answering,SQuAD1.1,2018-10,BERT (ensemble),87.433,97.26,2.4,0.03,89.898,0.97,EM
6,1,Question Answering,SQuAD1.1,2019-06,XLNet (single model),89.898,100.0,2.5,0.03,89.898,0.99,EM
7,1,Question Answering,SQuAD1.1 dev,2016-08,Match-LSTM with Bi-Ans-Ptr (Boundary+Search+b) ,64.1,71.17,64.1,0.71,90.06,0.71,EM
8,1,Question Answering,SQuAD1.1 dev,2016-11,RASOR,66.4,73.73,2.3,0.03,90.06,0.73,EM
9,1,Question Answering,SQuAD1.1 dev,2016-11,BIDAF (single),67.7,75.17,1.3,0.01,90.06,0.75,EM
10,1,Question Answering,SQuAD1.1 dev,2017-03,SEDT-LSTM,67.89,75.38,0.2,0.0,90.06,0.75,EM
11,1,Question Answering,SQuAD1.1 dev,2017-03,FastQAExt (beam-size 5),70.3,78.06,2.4,0.03,90.06,0.78,EM
12,1,Question Answering,SQuAD1.1 dev,2017-04,Ruminating Reader,70.6,78.39,0.3,0.0,90.06,0.78,EM
13,1,Question Answering,SQuAD1.1 dev,2017-05,R.M-Reader (single),78.9,87.61,8.3,0.09,90.06,0.87,EM
14,1,Question Answering,SQuAD1.1 dev,2018-10,BERT large (+TriviaQA),84.2,93.49,5.3,0.06,90.06,0.93,EM
15,1,Question Answering,SQuAD1.1 dev,2019-06,XLNet (single model),89.7,99.6,5.5,0.06,90.06,0.99,EM
16,1,Question Answering,SQuAD1.1 dev,2019-10,T5-11B,90.06,100.0,0.4,0.0,90.06,0.99,EM
17,1,Open-Domain Question Answering,SQuAD1.1,2016-11,DCN,66.2,94.57,66.2,0.95,70.0,0.73,EM
18,1,Open-Domain Question Answering,SQuAD1.1,2017-03,DrQA,70.0,100.0,3.8,0.05,70.0,0.77,EM
19,1,Question Answering,NewsQA,2017-03,FastQAExt,43.7,82.3,43.7,0.82,53.1,0.48,EM
20,1,Question Answering,NewsQA,2018-01,AMANDA,48.4,91.15,4.7,0.09,53.1,0.53,EM
21,1,Question Answering,NewsQA,2018-05,MINIMAL(Dyn),50.1,94.35,1.7,0.03,53.1,0.55,EM
22,1,Question Answering,NewsQA,2018-11,DecaProp,53.1,100.0,3.0,0.06,53.1,0.59,EM
23,1,Question Answering,TriviaQA,2017-05,Mnemonic Reader,46.94,69.84,46.94,0.7,67.21,0.52,EM
24,1,Question Answering,TriviaQA,2017-06,Reading Twice for NLU,50.56,75.23,3.6,0.05,67.21,0.56,EM
25,1,Question Answering,TriviaQA,2017-10,S-Norm,66.37,98.75,15.8,0.24,67.21,0.73,EM
26,1,Question Answering,TriviaQA,2018-10,MemoReader,67.21,100.0,0.8,0.01,67.21,0.74,EM
27,1,Question Answering,SQuAD2.0,2017-11,FusionNet++ (ensemble),70.3,77.61,70.3,0.78,90.578,0.78,EM
28,1,Question Answering,SQuAD2.0,2017-12,SAN (ensemble model),71.316,78.73,1.0,0.01,90.578,0.79,EM
29,1,Question Answering,SQuAD2.0,2018-08,Reinforced Mnemonic Reader + Answer Verifier (single model),71.767,79.23,0.5,0.01,90.578,0.79,EM
30,1,Question Answering,SQuAD2.0,2018-10,BERT (single model),80.005,88.33,8.2,0.09,90.578,0.88,EM
31,1,Question Answering,SQuAD2.0,2019-06,XLNet (single model),87.926,97.07,7.9,0.09,90.578,0.97,EM
32,1,Question Answering,SQuAD2.0,2019-08,XLNet + SG-Net Verifier (ensemble),88.174,97.35,0.2,0.0,90.578,0.97,EM
33,1,Question Answering,SQuAD2.0,2019-09,ALBERT (ensemble model),89.731,99.06,1.6,0.02,90.578,0.99,EM
34,1,Question Answering,SQuAD2.0,2020-01,Retro-Reader (ensemble),90.578,100.0,0.8,0.01,90.578,1.0,EM
35,1,Open-Domain Question Answering,SearchQA,2018-07,Denoising QA,58.8,100.0,58.8,1.0,58.8,0.65,EM
36,1,Question Answering,SQuAD2.0 dev,2018-08,RMR + ELMo (Model-III),72.3,82.25,72.3,0.82,87.9,0.8,EM
37,1,Question Answering,SQuAD2.0 dev,2018-10,BERT large,78.7,89.53,6.4,0.07,87.9,0.87,EM
38,1,Question Answering,SQuAD2.0 dev,2019-06,XLNet (single model),87.9,100.0,9.2,0.1,87.9,0.97,EM
39,1,Open-Domain Question Answering,DuReader,2019-07,ERNIE 2.0 Large,64.2,100.0,64.2,1.0,64.2,0.71,EM
40,1,Question Answering,FQuAD,2020-02,CamemBERTQA,77.9,100.0,77.9,1.0,77.9,0.86,EM
0,1,Text Generation,COCO Captions,2016-09,SeqGAN,0.521,66.97,0.521,0.67,0.778,0.01,BLEU\\-4
1,1,Text Generation,COCO Captions,2017-05,RankGAN,0.557,71.59,0.0,0.0,0.778,0.02,BLEU\\-4
2,1,Text Generation,COCO Captions,2017-09,LeakGAN,0.778,100.0,0.2,0.26,0.778,0.02,BLEU\\-4
3,1,Text Generation,EMNLP2017 WMT,2016-09,SeqGAN,0.4541,72.42,0.4541,0.72,0.627,0.01,BLEU\\-4
4,1,Text Generation,EMNLP2017 WMT,2017-09,LeakGAN,0.627,100.0,0.2,0.32,0.627,0.02,BLEU\\-4
5,1,Question Answering,NarrativeQA,2016-11,BiDAF,15.69,51.56,15.69,0.52,30.43,0.43,BLEU\\-4
6,1,Question Answering,NarrativeQA,2018-09,MHPGM + NOIC,21.07,69.24,5.4,0.18,30.43,0.58,BLEU\\-4
7,1,Question Answering,NarrativeQA,2018-10,ConZNet,22.49,73.91,1.4,0.05,30.43,0.62,BLEU\\-4
8,1,Question Answering,NarrativeQA,2018-11,DecaProp,27.61,90.73,5.1,0.17,30.43,0.76,BLEU\\-4
9,1,Question Answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO),30.43,100.0,2.8,0.09,30.43,0.83,BLEU\\-4
10,1,Question Generation,SQuAD1.1,2017-04,NQG++,13.27,55.5,13.27,0.55,23.91,0.36,BLEU\\-4
11,1,Question Generation,SQuAD1.1,2018-06,MPQG,13.91,58.18,0.6,0.03,23.91,0.38,BLEU\\-4
12,1,Question Generation,SQuAD1.1,2019-05,UniLM,22.78,95.27,8.9,0.37,23.91,0.62,BLEU\\-4
13,1,Question Generation,SQuAD1.1,2020-01,ProphetNet,23.91,100.0,1.1,0.05,23.91,0.66,BLEU\\-4
14,1,Text Generation,DailyDialog,2018-08,AEM+Attention,2.84,100.0,2.84,1.0,2.84,0.08,BLEU\\-4
15,1,Image Captioning,COCO,2019-05,"NIC (ResNet-50, CutMix)",24.9,100.0,24.9,1.0,24.9,0.68,BLEU\\-4
16,1,Image Captioning,Flickr30k Captions test,2019-09,Unified VLP,30.1,100.0,30.1,1.0,30.1,0.82,BLEU\\-4
17,1,Image Captioning,COCO Captions test,2019-09,Unified VLP,36.5,100.0,36.5,1.0,36.5,1.0,BLEU\\-4
0,1,Text Generation,EMNLP2017 WMT,2016-09,SeqGAN,0.859,89.85,0.859,0.9,0.956,0.02,BLEU\\-2
1,1,Text Generation,EMNLP2017 WMT,2017-09,LeakGAN,0.956,100.0,0.1,0.1,0.956,0.02,BLEU\\-2
2,1,Text Generation,Chinese Poems,2016-09,SeqGAN,0.738,90.89,0.738,0.91,0.812,0.02,BLEU\\-2
3,1,Text Generation,Chinese Poems,2017-05,RankGAN,0.812,100.0,0.1,0.12,0.812,0.02,BLEU\\-2
4,1,Text Generation,COCO Captions,2016-09,SeqGAN,0.831,87.47,0.831,0.87,0.95,0.02,BLEU\\-2
5,1,Text Generation,COCO Captions,2017-05,RankGAN,0.85,89.47,0.0,0.0,0.95,0.02,BLEU\\-2
6,1,Text Generation,COCO Captions,2017-09,LeakGAN,0.95,100.0,0.1,0.11,0.95,0.02,BLEU\\-2
7,1,Text Generation,DailyDialog,2018-08,AEM+Attention,5.69,100.0,5.69,1.0,5.69,0.12,BLEU\\-2
8,1,Image Captioning,COCO,2019-05,"NIC (ResNet-50, CutMix)",46.3,100.0,46.3,1.0,46.3,1.0,BLEU\\-2
0,1,Text Generation,EMNLP2017 WMT,2016-09,SeqGAN,0.6015,73.44,0.6015,0.73,0.819,0.02,BLEU\\-3
1,1,Text Generation,EMNLP2017 WMT,2017-09,LeakGAN,0.819,100.0,0.2,0.24,0.819,0.02,BLEU\\-3
2,1,Text Generation,COCO Captions,2016-09,SeqGAN,0.642,72.95,0.642,0.73,0.88,0.02,BLEU\\-3
3,1,Text Generation,COCO Captions,2017-05,RankGAN,0.672,76.36,0.0,0.0,0.88,0.02,BLEU\\-3
4,1,Text Generation,COCO Captions,2017-09,LeakGAN,0.88,100.0,0.2,0.23,0.88,0.03,BLEU\\-3
5,1,Text Generation,CMU-SE,2018-08,STWGAN-GP,0.617,100.0,0.617,1.0,0.617,0.02,BLEU\\-3
6,1,Text Generation,DailyDialog,2018-08,AEM+Attention,3.78,100.0,3.78,1.0,3.78,0.11,BLEU\\-3
7,1,Image Captioning,COCO,2019-05,"NIC (ResNet-50, CutMix)",33.6,100.0,33.6,1.0,33.6,1.0,BLEU\\-3
0,1,Text Generation,EMNLP2017 WMT,2016-09,SeqGAN,0.4498,90.32,0.4498,0.9,0.498,0.66,BLEU\\-5
1,1,Text Generation,EMNLP2017 WMT,2017-05,RankGAN,0.463,92.97,0.0,0.0,0.498,0.67,BLEU\\-5
2,1,Text Generation,EMNLP2017 WMT,2017-09,LeakGAN,0.498,100.0,0.0,0.0,0.498,0.73,BLEU\\-5
3,1,Text Generation,COCO Captions,2016-09,SeqGAN,0.427,62.24,0.427,0.62,0.686,0.62,BLEU\\-5
4,1,Text Generation,COCO Captions,2017-05,RankGAN,0.544,79.3,0.1,0.15,0.686,0.79,BLEU\\-5
5,1,Text Generation,COCO Captions,2017-09,LeakGAN,0.686,100.0,0.1,0.15,0.686,1.0,BLEU\\-5
0,1,Aspect-Based Sentiment Analysis,Sentihood,2016-10,LSTM-LOC,69.3,78.84,69.3,0.79,87.9,0.79,Aspect
1,1,Aspect-Based Sentiment Analysis,Sentihood,2018-04,Sentic LSTM + TA + SA,78.18,88.94,8.9,0.1,87.9,0.89,Aspect
2,1,Aspect-Based Sentiment Analysis,Sentihood,2018-06,Liu et al.,78.5,89.31,0.3,0.0,87.9,0.89,Aspect
3,1,Aspect-Based Sentiment Analysis,Sentihood,2019-03,BERT-pair-QA-B,87.9,100.0,9.4,0.11,87.9,1.0,Aspect
0,1,Aspect-Based Sentiment Analysis,Sentihood,2016-10,LSTM-LOC,81.9,87.5,81.9,0.88,93.6,0.88,Sentiment
1,1,Aspect-Based Sentiment Analysis,Sentihood,2018-04,Sentic LSTM + TA + SA,89.32,95.43,7.4,0.08,93.6,0.95,Sentiment
2,1,Aspect-Based Sentiment Analysis,Sentihood,2018-06,Liu et al.,91.0,97.22,1.7,0.02,93.6,0.97,Sentiment
3,1,Aspect-Based Sentiment Analysis,Sentihood,2019-03,BERT-pair-QA-B,93.3,99.68,2.3,0.02,93.6,1.0,Sentiment
4,1,Aspect-Based Sentiment Analysis,Sentihood,2019-03,BERT-pair-QA-M,93.6,100.0,0.3,0.0,93.6,1.0,Sentiment
0,1,Hypernym Discovery,Medical domain,2016-11,vTE,20.71,56.32,20.71,0.56,36.77,0.33,P\\-at\\-5
1,1,Hypernym Discovery,Medical domain,2018-06,CRIM,36.77,100.0,16.1,0.44,36.77,0.58,P\\-at\\-5
2,1,Hypernym Discovery,Music domain,2016-11,vTE,12.41,30.04,12.41,0.3,41.31,0.2,P\\-at\\-5
3,1,Hypernym Discovery,Music domain,2018-06,CRIM,41.31,100.0,28.9,0.7,41.31,0.65,P\\-at\\-5
4,1,Hypernym Discovery,General,2016-11,vTE,9.91,52.08,9.91,0.52,19.03,0.16,P\\-at\\-5
5,1,Hypernym Discovery,General,2018-06,CRIM,19.03,100.0,9.1,0.48,19.03,0.3,P\\-at\\-5
6,1,Multi-Label Text Classification,EUR-Lex,2019-05,LAHA,50.71,95.99,50.71,0.96,52.83,0.8,P\\-at\\-5
7,1,Multi-Label Text Classification,EUR-Lex,2019-06,NLP-Cap,52.83,100.0,2.1,0.04,52.83,0.84,P\\-at\\-5
8,1,Multi-Label Text Classification,Amazon-12K,2019-05,LAHA,63.16,100.0,63.16,1.0,63.16,1.0,P\\-at\\-5
9,1,Multi-Label Text Classification,Kan-Shan Cup,2019-05,LAHA,25.88,100.0,25.88,1.0,25.88,0.41,P\\-at\\-5
10,1,Multi-Label Text Classification,Wiki-30K,2019-05,LAHA,62.87,100.0,62.87,1.0,62.87,1.0,P\\-at\\-5
11,1,Multi-Label Text Classification,AAPD,2019-05,LAHA,41.19,100.0,41.19,1.0,41.19,0.65,P\\-at\\-5
12,1,Text Classification,RCV1,2019-06,NLP-Cap,56.33,100.0,56.33,1.0,56.33,0.89,P\\-at\\-5
0,1,Question Answering,NarrativeQA,2016-11,BiDAF,15.68,60.01,15.68,0.6,26.13,0.35,METEOR
1,1,Question Answering,NarrativeQA,2018-09,MHPGM + NOIC,19.03,72.83,3.4,0.13,26.13,0.42,METEOR
2,1,Question Answering,NarrativeQA,2018-10,ConZNet,19.24,73.63,0.2,0.01,26.13,0.42,METEOR
3,1,Question Answering,NarrativeQA,2018-11,DecaProp,21.8,83.43,2.6,0.1,26.13,0.48,METEOR
4,1,Question Answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO),26.13,100.0,4.3,0.16,26.13,0.58,METEOR
5,1,Data-to-Text Generation,E2E NLG Challenge,2017-12,Gong,44.69,98.68,44.69,0.99,45.29,0.99,METEOR
6,1,Data-to-Text Generation,E2E NLG Challenge,2018-04,Sys1-Primary,45.17,99.74,0.5,0.01,45.29,1.0,METEOR
7,1,Data-to-Text Generation,E2E NLG Challenge,2018-11,TUDA,45.29,100.0,0.1,0.0,45.29,1.0,METEOR
8,1,Paper generation,ACL Title and Abstract Dataset,2018-05,Writing-editing Network,14.0,100.0,14,1.0,14,0.31,METEOR
9,1,KB-to-Language Generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model,23.4,100.0,23.4,1.0,23.4,0.52,METEOR
10,1,Table-to-Text Generation,Wikipedia Person and Animal Dataset,2018-09,KB-to-Language Generation Model,42.0,100.0,42,1.0,42,0.93,METEOR
11,1,Image Captioning,COCO,2019-05,"NIC (ResNet-50, CutMix)",23.1,100.0,23.1,1.0,23.1,0.51,METEOR
12,1,Image Captioning,Flickr30k Captions test,2019-09,Unified VLP,23.0,100.0,23,1.0,23,0.51,METEOR
13,1,Image Captioning,COCO Captions test,2019-09,Unified VLP,28.4,100.0,28.4,1.0,28.4,0.63,METEOR
0,1,Question Answering,NarrativeQA,2016-11,BiDAF,36.74,61.37,36.74,0.61,59.87,0.61,Rouge\\-L
1,1,Question Answering,NarrativeQA,2018-09,MHPGM + NOIC,44.16,73.76,7.4,0.12,59.87,0.74,Rouge\\-L
2,1,Question Answering,NarrativeQA,2018-10,ConZNet,46.67,77.95,2.5,0.04,59.87,0.78,Rouge\\-L
3,1,Question Answering,NarrativeQA,2019-01,Masque (NarrativeQA + MS MARCO),59.87,100.0,13.2,0.22,59.87,1.0,Rouge\\-L
4,1,Question Answering,MS MARCO,2016-11,BiDaF Baseline,23.96,45.9,23.96,0.46,52.2,0.4,Rouge\\-L
5,1,Question Answering,MS MARCO,2018-05,VNET,51.63,98.91,27.7,0.53,52.2,0.86,Rouge\\-L
6,1,Question Answering,MS MARCO,2018-11,Deep Cascade QA,52.01,99.64,0.4,0.01,52.2,0.87,Rouge\\-L
7,1,Question Answering,MS MARCO,2019-01,Masque Q&A Style,52.2,100.0,0.2,0.0,52.2,0.87,Rouge\\-L
0,1,Dialog Generation,Amazon-5,2017-01,mm,5.0,100.0,5,1.0,5,1.0,1\\ in\\ 10\\ R\\-at\\-2
0,1,Text Generation,Yahoo Questions,2017-02,CNN-VAE,63.9,100.0,63.9,1.0,63.9,1.0,Perplexity
0,1,Text Generation,Yahoo Questions,2017-02,CNN-VAE,10.0,100.0,10.0,1.0,10.0,1.0,KL
0,1,Text Generation,Yahoo Questions,2017-02,CNN-VAE,332.1,100.0,332.1,1.0,332.1,1.0,NLL
0,1,Coreference Resolution,CoNLL 2012,2017-07,Lee et al. (2017) + ELMo,70.4,100.0,70.4,1.0,70.4,1.0,Avg\\ F1
1,1,Knowledge Base Question Answering,WebQSP-WD,2018-08,GGNN,0.2588,100.0,0.2588,1.0,0.2588,0.0,Avg\\ F1
2,1,Dialog Generation,Persona-Chat,2020-04,P^2 Bot,19.77,100.0,19.77,1.0,19.77,0.28,Avg\\ F1
0,1,Sentiment Analysis,SemEval,2017-04,LSTMs+CNNs ensemble with multiple conv. ops ,0.685,100.0,0.685,1.0,0.685,0.7,F1\\-score
1,1,Humor Detection,200k Short Texts for Humor Detection,2020-04,ColBERT,0.981,100.0,0.981,1.0,0.981,1.0,F1\\-score
0,1,Semantic Textual Similarity,SentEval,2017-05,InferSent,86.3,98.29,86.3,0.98,87.8,0.98,SICK\\-E
1,1,Semantic Textual Similarity,SentEval,2018-03,GenSen,87.8,100.0,1.5,0.02,87.8,1.0,SICK\\-E
0,1,Semantic Textual Similarity,SentEval,2017-05,InferSent,0.884,99.55,0.884,1.0,0.888,1.0,SICK\\-R
1,1,Semantic Textual Similarity,SentEval,2018-03,GenSen,0.888,100.0,0.0,0.0,0.888,1.0,SICK\\-R
0,1,Abstract Anaphora Resolution,The ARRAU Corpus,2017-06,MR-LSTM,43.83,100.0,43.83,1.0,43.83,1.0,Average\\ Precision
0,1,Emotion Recognition in Conversation,SEMAINE,2017-07,bc-LSTM+Att,0.213,100.0,0.213,1.0,0.213,1.0,MAE\\ \\(Arousal\\)
0,1,Emotion Recognition in Conversation,SEMAINE,2017-07,bc-LSTM+Att,0.19,97.44,0.19,0.97,0.195,0.97,MAE\\ \\(Expectancy\\)
1,1,Emotion Recognition in Conversation,SEMAINE,2018-06,CMN,0.195,100.0,0.0,0.0,0.195,1.0,MAE\\ \\(Expectancy\\)
0,1,Emotion Recognition in Conversation,SEMAINE,2017-07,bc-LSTM+Att,8.67,99.2,8.67,0.99,8.74,0.99,MAE\\ \\(Power\\)
1,1,Emotion Recognition in Conversation,SEMAINE,2018-06,CMN,8.74,100.0,0.1,0.01,8.74,1.0,MAE\\ \\(Power\\)
0,1,Emotion Recognition in Conversation,SEMAINE,2017-07,bc-LSTM+Att,0.189,98.44,0.189,0.98,0.192,0.98,MAE\\ \\(Valence\\)
1,1,Emotion Recognition in Conversation,SEMAINE,2018-06,CMN,0.192,100.0,0.0,0.0,0.192,1.0,MAE\\ \\(Valence\\)
0,1,Fake News Detection,FNC-1,2017-07,3rd place at FNC-1,81.72,98.36,81.72,0.98,83.08,0.98,Weighted\\ Accuracy
1,1,Fake News Detection,FNC-1,2017-12,Bhatt et al,83.08,100.0,1.4,0.02,83.08,1.0,Weighted\\ Accuracy
0,1,Fake News Detection,FNC-1,2017-07,3rd place at FNC-1,97.9,99.86,97.9,1.0,98.04,1.0,Per\\-class\\ Accuracy\\ \\(Unrelated\\)
1,1,Fake News Detection,FNC-1,2017-12,Bhatt et al,98.04,100.0,0.1,0.0,98.04,1.0,Per\\-class\\ Accuracy\\ \\(Unrelated\\)
0,1,Fake News Detection,FNC-1,2017-07,3rd place at FNC-1,44.04,85.78,44.04,0.86,51.34,0.86,Per\\-class\\ Accuracy\\ \\(Agree\\)
1,1,Fake News Detection,FNC-1,2017-12,"Baseline based on word2vec + hand-crafted features (Bhatt et al., 2017)",50.7,98.75,6.7,0.13,51.34,0.99,Per\\-class\\ Accuracy\\ \\(Agree\\)
2,1,Fake News Detection,FNC-1,2018-11,"Bi-LSTM (max-pooling, attention)",51.34,100.0,0.6,0.01,51.34,1.0,Per\\-class\\ Accuracy\\ \\(Agree\\)
0,1,Fake News Detection,FNC-1,2017-07,3rd place at FNC-1,6.6,63.89,6.6,0.64,10.33,0.64,Per\\-class\\ Accuracy\\ \\(Disagree\\)
1,1,Fake News Detection,FNC-1,2017-12,"Baseline based on word2vec + hand-crafted features (Bhatt et al., 2017)",9.61,93.03,3.0,0.29,10.33,0.93,Per\\-class\\ Accuracy\\ \\(Disagree\\)
2,1,Fake News Detection,FNC-1,2018-11,"Bi-LSTM (max-pooling, attention)",10.33,100.0,0.7,0.07,10.33,1.0,Per\\-class\\ Accuracy\\ \\(Disagree\\)
0,1,Fake News Detection,FNC-1,2017-07,3rd place at FNC-1,81.38,94.98,81.38,0.95,85.68,0.95,Per\\-class\\ Accuracy\\ \\(Discuss\\)
1,1,Fake News Detection,FNC-1,2017-12,Bhatt et al,85.68,100.0,4.3,0.05,85.68,1.0,Per\\-class\\ Accuracy\\ \\(Discuss\\)
0,1,Relation Extraction,Wikipedia-Wikidata relations,2017-09,ContextAtt,0.159,100.0,0.159,1.0,0.159,1.0,Error\\ rate
0,1,Temporal Information Extraction,TempEval-3,2017-09,Ning et al.,67.2,100.0,67.2,1.0,67.2,1.0,Temporal\\ awareness
0,1,Word Sense Disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX</sub>,72.0,90.34,72.0,0.9,79.7,0.9,Senseval\\ 2
1,1,Word Sense Disambiguation,Supervised:,2018-07,GAS<sub>ext</sub>,72.2,90.59,0.2,0.0,79.7,0.91,Senseval\\ 2
2,1,Word Sense Disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",75.15,94.29,3.0,0.04,79.7,0.94,Senseval\\ 2
3,1,Word Sense Disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms",79.7,100.0,4.5,0.06,79.7,1.0,Senseval\\ 2
0,1,Word Sense Disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX</sub>,69.4,89.2,69.4,0.89,77.8,0.89,Senseval\\ 3
1,1,Word Sense Disambiguation,Supervised:,2018-06,ELMo,69.6,89.46,0.2,0.0,77.8,0.89,Senseval\\ 3
2,1,Word Sense Disambiguation,Supervised:,2018-07,GAS<sub>ext</sub>,70.5,90.62,0.9,0.01,77.8,0.91,Senseval\\ 3
3,1,Word Sense Disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms",77.8,100.0,7.3,0.09,77.8,1.0,Senseval\\ 3
0,1,Word Sense Disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX</sub>,66.4,84.37,66.4,0.84,78.7,0.84,SemEval\\ 2013
1,1,Word Sense Disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX+POS</sub>,66.9,85.01,0.5,0.01,78.7,0.85,SemEval\\ 2013
2,1,Word Sense Disambiguation,Supervised:,2018-07,GAS<sub>ext</sub>,67.2,85.39,0.3,0.0,78.7,0.85,SemEval\\ 2013
3,1,Word Sense Disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",72.63,92.29,5.4,0.07,78.7,0.92,SemEval\\ 2013
4,1,Word Sense Disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms",78.7,100.0,6.1,0.08,78.7,1.0,SemEval\\ 2013
5,1,Word Sense Disambiguation,Knowledge-based:,2018-01,WSD-TM,65.3,100.0,65.3,1.0,65.3,0.83,SemEval\\ 2013
0,1,Word Sense Disambiguation,Supervised:,2017-09,Bi-LSTM<sub>att+LEX</sub>,72.4,87.65,72.4,0.88,82.6,0.88,SemEval\\ 2015
1,1,Word Sense Disambiguation,Supervised:,2018-07,GAS<sub>ext</sub>,72.6,87.89,0.2,0.0,82.6,0.88,SemEval\\ 2015
2,1,Word Sense Disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",74.46,90.15,1.9,0.02,82.6,0.9,SemEval\\ 2015
3,1,Word Sense Disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms",82.6,100.0,8.1,0.1,82.6,1.0,SemEval\\ 2015
4,1,Word Sense Disambiguation,Knowledge-based:,2018-01,WSD-TM,69.6,100.0,69.6,1.0,69.6,0.84,SemEval\\ 2015
0,1,Named Entity Recognition,Long-tail emerging entities,2017-09,SpinningBytes,39.33,97.74,39.33,0.98,40.24,0.98,F1\\ \\(surface\\ form\\)
1,1,Named Entity Recognition,Long-tail emerging entities,2018-06,Aguilar et al.,40.24,100.0,0.9,0.02,40.24,1.0,F1\\ \\(surface\\ form\\)
0,1,Machine Translation,20NEWS,2017-09,tensorflow/tensor2tensor,5.0,100.0,5,1.0,5,0.06,1\\-of\\-100\\ Accuracy
1,1,Conversational Response Selection,DSTC7 Ubuntu,2018-12,Sequential Inference Models,60.8,85.39,60.8,0.85,71.2,0.72,1\\-of\\-100\\ Accuracy
2,1,Conversational Response Selection,DSTC7 Ubuntu,2019-01,Sequential Attention-based Network,64.5,90.59,3.7,0.05,71.2,0.77,1\\-of\\-100\\ Accuracy
3,1,Conversational Response Selection,DSTC7 Ubuntu,2019-04,Bi-encoder (v2),70.9,99.58,6.4,0.09,71.2,0.84,1\\-of\\-100\\ Accuracy
4,1,Conversational Response Selection,DSTC7 Ubuntu,2019-11,Multi-context ConveRT,71.2,100.0,0.3,0.0,71.2,0.84,1\\-of\\-100\\ Accuracy
5,1,Conversational Response Selection,PolyAI AmazonQA,2019-04,PolyAI Encoder,71.3,84.58,71.3,0.85,84.3,0.85,1\\-of\\-100\\ Accuracy
6,1,Conversational Response Selection,PolyAI AmazonQA,2019-11,ConveRT,84.3,100.0,13.0,0.15,84.3,1.0,1\\-of\\-100\\ Accuracy
7,1,Conversational Response Selection,PolyAI Reddit,2019-04,PolyAI Encoder,61.3,85.38,61.3,0.85,71.8,0.73,1\\-of\\-100\\ Accuracy
8,1,Conversational Response Selection,PolyAI Reddit,2019-11,Multi-context ConveRT,71.8,100.0,10.5,0.15,71.8,0.85,1\\-of\\-100\\ Accuracy
9,1,Conversational Response Selection,PolyAI OpenSubtitles,2019-04,PolyAI Encoder,30.6,100.0,30.6,1.0,30.6,0.36,1\\-of\\-100\\ Accuracy
0,1,Visual Dialog,VisDial v0.9 val,2017-09,AMEM Seo et al. (2017),48.53,87.98,48.53,0.88,55.16,0.68,R\\-at\\-1
1,1,Visual Dialog,VisDial v0.9 val,2017-11,CoAtt Wu et al. (2018),50.29,91.17,1.8,0.03,55.16,0.71,R\\-at\\-1
2,1,Visual Dialog,VisDial v0.9 val,2019-02,DAN,53.33,96.68,3.0,0.05,55.16,0.75,R\\-at\\-1
3,1,Visual Dialog,VisDial v0.9 val,2019-04,9xFGA (VGG),55.16,100.0,1.8,0.03,55.16,0.77,R\\-at\\-1
4,1,Phrase Grounding,Flickr30k Entities Test,2018-05,BAN (Bottom-Up detector),69.69,97.7,69.69,0.98,71.33,0.98,R\\-at\\-1
5,1,Phrase Grounding,Flickr30k Entities Test,2019-08,VisualBERT,71.33,100.0,1.6,0.02,71.33,1.0,R\\-at\\-1
6,1,Visual Dialog,VisDial v1.0 test-std,2018-09,CorefNMN Kottur et al. (2018),47.55,85.44,47.55,0.85,55.65,0.67,R\\-at\\-1
7,1,Visual Dialog,VisDial v1.0 test-std,2019-02,DAN,49.63,89.18,2.1,0.04,55.65,0.7,R\\-at\\-1
8,1,Visual Dialog,VisDial v1.0 test-std,2019-04,5xFGA (F-RCNNx101),55.65,100.0,6.0,0.11,55.65,0.78,R\\-at\\-1
9,1,Phrase Grounding,Flickr30k Entities Dev,2019-08,VisualBERT,70.4,100.0,70.4,1.0,70.4,0.99,R\\-at\\-1
0,1,Visual Dialog,VisDial v0.9 val,2017-09,AMEM Seo et al. (2017),87.43,94.06,87.43,0.94,92.95,0.93,R\\-at\\-10
1,1,Visual Dialog,VisDial v0.9 val,2017-11,CoAtt Wu et al. (2018),88.81,95.55,1.4,0.02,92.95,0.94,R\\-at\\-10
2,1,Visual Dialog,VisDial v0.9 val,2019-02,DAN,90.38,97.24,1.6,0.02,92.95,0.96,R\\-at\\-10
3,1,Visual Dialog,VisDial v0.9 val,2019-04,9xFGA (VGG),92.95,100.0,2.6,0.03,92.95,0.99,R\\-at\\-10
4,1,Phrase Grounding,Flickr30k Entities Test,2018-05,BAN (Bottom-Up detector),86.35,99.82,86.35,1.0,86.51,0.92,R\\-at\\-10
5,1,Phrase Grounding,Flickr30k Entities Test,2019-08,VisualBERT,86.51,100.0,0.2,0.0,86.51,0.92,R\\-at\\-10
6,1,Visual Dialog,VisDial v1.0 test-std,2018-09,CorefNMN Kottur et al. (2018),88.8,94.42,88.8,0.94,94.05,0.94,R\\-at\\-10
7,1,Visual Dialog,VisDial v1.0 test-std,2019-02,DAN,89.35,95.0,0.5,0.01,94.05,0.95,R\\-at\\-10
8,1,Visual Dialog,VisDial v1.0 test-std,2019-04,5xFGA (F-RCNNx101),94.05,100.0,4.7,0.05,94.05,1.0,R\\-at\\-10
9,1,Phrase Grounding,Flickr30k Entities Dev,2019-08,VisualBERT,86.31,100.0,86.31,1.0,86.31,0.92,R\\-at\\-10
0,1,Visual Dialog,VisDial v0.9 val,2017-09,AMEM Seo et al. (2017),78.66,91.19,78.66,0.91,86.26,0.91,R\\-at\\-5
1,1,Visual Dialog,VisDial v0.9 val,2017-11,CoAtt Wu et al. (2018),80.71,93.57,2.0,0.02,86.26,0.93,R\\-at\\-5
2,1,Visual Dialog,VisDial v0.9 val,2019-02,DAN,82.42,95.55,1.7,0.02,86.26,0.95,R\\-at\\-5
3,1,Visual Dialog,VisDial v0.9 val,2019-04,9xFGA (VGG),86.26,100.0,3.8,0.04,86.26,0.99,R\\-at\\-5
4,1,Phrase Grounding,Flickr30k Entities Test,2018-05,BAN (Bottom-Up detector),84.22,99.11,84.22,0.99,84.98,0.97,R\\-at\\-5
5,1,Phrase Grounding,Flickr30k Entities Test,2019-08,VisualBERT,84.98,100.0,0.8,0.01,84.98,0.98,R\\-at\\-5
6,1,Visual Dialog,VisDial v1.0 test-std,2018-09,CorefNMN Kottur et al. (2018),78.1,90.05,78.1,0.9,86.73,0.9,R\\-at\\-5
7,1,Visual Dialog,VisDial v1.0 test-std,2019-02,DAN,79.75,91.95,1.7,0.02,86.73,0.92,R\\-at\\-5
8,1,Visual Dialog,VisDial v1.0 test-std,2019-04,5xFGA (F-RCNNx101),86.73,100.0,7.0,0.08,86.73,1.0,R\\-at\\-5
9,1,Phrase Grounding,Flickr30k Entities Dev,2019-08,VisualBERT,84.49,100.0,84.49,1.0,84.49,0.97,R\\-at\\-5
0,1,Visual Dialog,VisDial v0.9 val,2017-09,AMEM Seo et al. (2017),4.86,100.0,4.86,1.0,4.86,1.0,Mean\\ Rank
1,1,Visual Dialog,VisDial v1.0 test-std,2018-09,CorefNMN Kottur et al. (2018),4.4,100.0,4.4,1.0,4.4,0.91,Mean\\ Rank
2,1,Visual Dialog,Visual Dialog v1.0,2019-02,HACAN,4.2,97.67,4.2,0.98,4.3,0.86,Mean\\ Rank
3,1,Visual Dialog,Visual Dialog v1.0,2019-02,DAN,4.3,100.0,0.1,0.02,4.3,0.88,Mean\\ Rank
0,1,Question Answering,WikiHop,2017-10,BiDAF,42.9,60.76,42.9,0.61,70.6,0.61,Test
1,1,Question Answering,WikiHop,2018-04,Coref-GRU,59.3,83.99,16.4,0.23,70.6,0.84,Test
2,1,Question Answering,WikiHop,2018-09,MHQA,65.4,92.63,6.1,0.09,70.6,0.93,Test
3,1,Question Answering,WikiHop,2019-01,CFC,70.6,100.0,5.2,0.07,70.6,1.0,Test
4,1,Table-based Fact Verification,TabFact,2019-09,Table-BERT-Horizontal-T+F-Template,65.12,100.0,65.12,1.0,65.12,0.92,Test
0,1,Fine-Grained Opinion Analysis,MPQA,2017-11,FS-MTL,83.8,98.69,83.8,0.99,84.91,0.99,Holder\\ Binary\\ F1
1,1,Fine-Grained Opinion Analysis,MPQA,2019-06,SRL-SAWR,84.91,100.0,1.1,0.01,84.91,1.0,Holder\\ Binary\\ F1
0,1,Fine-Grained Opinion Analysis,MPQA,2017-11,FS-MTL,72.06,98.32,72.06,0.98,73.29,0.98,Target\\ Binary\\ F1
1,1,Fine-Grained Opinion Analysis,MPQA,2019-06,SRL-SAWR,73.29,100.0,1.2,0.02,73.29,1.0,Target\\ Binary\\ F1
0,1,Constituency Grammar Induction,PTB,2017-11,PRPN (tuned),47.3,84.92,47.3,0.85,55.7,0.85,Mean\\ F1\\ \\(WSJ\\)
1,1,Constituency Grammar Induction,PTB,2018-08,DMV + invertible projector,47.9,86.0,0.6,0.01,55.7,0.86,Mean\\ F1\\ \\(WSJ\\)
2,1,Constituency Grammar Induction,PTB,2018-10,ON-LSTM (tuned),48.1,86.36,0.2,0.0,55.7,0.86,Mean\\ F1\\ \\(WSJ\\)
3,1,Constituency Grammar Induction,PTB,2019-06,DIORA (+PP),55.7,100.0,7.6,0.14,55.7,1.0,Mean\\ F1\\ \\(WSJ\\)
0,1,Constituency Grammar Induction,PTB,2017-11,PRPN (tuned),47.9,79.7,47.9,0.8,60.1,0.8,Max\\ F1\\ \\(WSJ\\)
1,1,Constituency Grammar Induction,PTB,2018-10,ON-LSTM (tuned),50.0,83.19,2.1,0.03,60.1,0.83,Max\\ F1\\ \\(WSJ\\)
2,1,Constituency Grammar Induction,PTB,2019-04,URNNG,52.4,87.19,2.4,0.04,60.1,0.87,Max\\ F1\\ \\(WSJ\\)
3,1,Constituency Grammar Induction,PTB,2019-06,DIORA (+PP),56.2,93.51,3.8,0.06,60.1,0.94,Max\\ F1\\ \\(WSJ\\)
4,1,Constituency Grammar Induction,PTB,2019-06,Compound PCFG,60.1,100.0,3.9,0.06,60.1,1.0,Max\\ F1\\ \\(WSJ\\)
0,1,Ad-Hoc Information Retrieval,TREC Robust04,2017-11,DRMM,0.431,80.1,0.431,0.8,0.5381,0.01,nDCG\\-at\\-20
1,1,Ad-Hoc Information Retrieval,TREC Robust04,2018-09,POSIT-DRMM-MV,0.464,86.23,0.0,0.0,0.5381,0.01,nDCG\\-at\\-20
2,1,Ad-Hoc Information Retrieval,TREC Robust04,2019-04,CEDR-KNRM,0.5381,100.0,0.1,0.19,0.5381,0.02,nDCG\\-at\\-20
3,1,Document Ranking,ClueWeb09-B,2019-06,XLNet,31.1,100.0,31.1,1.0,31.1,1.0,nDCG\\-at\\-20
0,1,Ad-Hoc Information Retrieval,TREC Robust04,2017-11,DRMM,0.382,81.85,0.382,0.82,0.4667,0.82,P\\-at\\-20
1,1,Ad-Hoc Information Retrieval,TREC Robust04,2018-09,POSIT-DRMM-MV,0.389,83.35,0.0,0.0,0.4667,0.83,P\\-at\\-20
2,1,Ad-Hoc Information Retrieval,TREC Robust04,2018-10,SNRM-PRF,0.3948,84.59,0.0,0.0,0.4667,0.85,P\\-at\\-20
3,1,Ad-Hoc Information Retrieval,TREC Robust04,2018-10,NPRF-DRMM,0.4064,87.08,0.0,0.0,0.4667,0.87,P\\-at\\-20
4,1,Ad-Hoc Information Retrieval,TREC Robust04,2019-03,BERT FT(Microblog),0.4287,91.86,0.0,0.0,0.4667,0.92,P\\-at\\-20
5,1,Ad-Hoc Information Retrieval,TREC Robust04,2019-04,CEDR-KNRM,0.4667,100.0,0.0,0.0,0.4667,1.0,P\\-at\\-20
0,1,Data-to-Text Generation,E2E NLG Challenge,2017-12,Gong,8.3453,95.59,8.3453,0.96,8.73,0.96,NIST
1,1,Data-to-Text Generation,E2E NLG Challenge,2018-04,Sys1-Primary,8.5105,97.49,0.2,0.02,8.73,0.97,NIST
2,1,Data-to-Text Generation,E2E NLG Challenge,2018-05,Slug,8.613,98.66,0.1,0.01,8.73,0.99,NIST
3,1,Data-to-Text Generation,E2E NLG Challenge,2019-04,S_1^R,8.73,100.0,0.1,0.01,8.73,1.0,NIST
0,1,Data-to-Text Generation,E2E NLG Challenge,2017-12,Gong,2.2721,95.87,2.2721,0.96,2.37,0.02,CIDEr
1,1,Data-to-Text Generation,E2E NLG Challenge,2019-04,S_1^R,2.37,100.0,0.1,0.04,2.37,0.02,CIDEr
2,1,Image Captioning,COCO,2019-05,"NIC (ResNet-50, CutMix)",77.6,100.0,77.6,1.0,77.6,0.66,CIDEr
3,1,Image Captioning,Flickr30k Captions test,2019-09,Unified VLP,67.4,100.0,67.4,1.0,67.4,0.58,CIDEr
4,1,Image Captioning,COCO Captions test,2019-09,Unified VLP,116.9,100.0,116.9,1.0,116.9,1.0,CIDEr
5,1,Text Generation,CommonGen,2019-11,UniLM,14.92,100.0,14.92,1.0,14.92,0.13,CIDEr
0,1,Word Sense Disambiguation,Knowledge-based:,2018-01,WSD-TM,66.9,100.0,66.9,1.0,66.9,1.0,All
0,1,Grammatical Error Correction,_Restricted_,2018-01,CNN Seq2Seq,57.47,93.45,57.47,0.93,61.5,0.92,GLEU
1,1,Grammatical Error Correction,_Restricted_,2018-06,SMT + BiGRU,61.5,100.0,4.0,0.07,61.5,0.99,GLEU
2,1,Grammatical Error Correction,Unrestricted,2018-07,CNN Seq2Seq + Fluency Boost and inference,62.37,100.0,62.37,1.0,62.37,1.0,GLEU
3,1,Grammatical Error Correction,JFLEG,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),61.0,100.0,61,1.0,61,0.98,GLEU
0,1,Speech Emotion Recognition,IEMOCAP,2018-02,CNN+LSTM,0.8,100.0,0.8,1.0,0.8,1.0,UA
1,1,Multimodal Emotion Recognition,IEMOCAP,2018-06,CHFusion (A+T+V),0.765,100.0,0.765,1.0,0.765,0.96,UA
0,1,Question Answering,RACE,2018-03,BiAttention MRU,60.2,100.0,60.2,1.0,60.2,1.0,RACE\\-m
0,1,Question Answering,RACE,2018-03,BiAttention MRU,50.3,100.0,50.3,1.0,50.3,1.0,RACE\\-h
0,1,Question Answering,RACE,2018-03,BiAttention MRU,53.3,100.0,53.3,1.0,53.3,1.0,RACE
0,1,Natural Language Inference,MultiNLI,2018-03,GenSen,71.4,77.61,71.4,0.78,92.0,0.78,Matched
1,1,Natural Language Inference,MultiNLI,2018-04,Multi-task BiLSTM + Attn,72.2,78.48,0.8,0.01,92.0,0.78,Matched
2,1,Natural Language Inference,MultiNLI,2018-12,aESIM,73.9,80.33,1.7,0.02,92.0,0.8,Matched
3,1,Natural Language Inference,MultiNLI,2019-01,MT-DNN,86.7,94.24,12.8,0.14,92.0,0.94,Matched
4,1,Natural Language Inference,MultiNLI,2019-06,XLNet (single model),90.8,98.7,4.1,0.04,92.0,0.99,Matched
5,1,Natural Language Inference,MultiNLI,2019-09,ALBERT,91.3,99.24,0.5,0.01,92.0,0.99,Matched
6,1,Natural Language Inference,MultiNLI,2019-10,T5-11B,92.0,100.0,0.7,0.01,92.0,1.0,Matched
7,1,Natural Language Inference,MultiNLI Dev,2019-09,TinyBERT (M=6;d'=768;d'i=3072),84.5,100.0,84.5,1.0,84.5,0.92,Matched
0,1,Natural Language Inference,MultiNLI,2018-03,GenSen,71.3,77.75,71.3,0.78,91.7,0.78,Mismatched
1,1,Natural Language Inference,MultiNLI,2018-04,Multi-task BiLSTM + Attn,72.1,78.63,0.8,0.01,91.7,0.79,Mismatched
2,1,Natural Language Inference,MultiNLI,2018-11,"Stacked Bi-LSTMs (shortcut connections, max-pooling)",72.2,78.74,0.1,0.0,91.7,0.79,Mismatched
3,1,Natural Language Inference,MultiNLI,2018-12,aESIM,73.9,80.59,1.7,0.02,91.7,0.81,Mismatched
4,1,Natural Language Inference,MultiNLI,2019-01,MT-DNN,86.0,93.78,12.1,0.13,91.7,0.94,Mismatched
5,1,Natural Language Inference,MultiNLI,2019-07,RoBERTa,90.2,98.36,4.2,0.05,91.7,0.98,Mismatched
6,1,Natural Language Inference,MultiNLI,2019-10,T5-11B,91.7,100.0,1.5,0.02,91.7,1.0,Mismatched
7,1,Natural Language Inference,MultiNLI Dev,2019-09,TinyBERT (M=6;d'=768;d'i=3072),84.5,100.0,84.5,1.0,84.5,0.92,Mismatched
0,1,Meeting Summarization,300W,2018-05,abc,10.0,100.0,10,1.0,10,1.0,10%
0,1,Language Acquisition,SLAM 2018,2018-06,Context Based Model,0.821,100.0,0.821,1.0,0.821,1.0,AUC
0,1,Word Sense Disambiguation,Supervised:,2018-06,ELMo,62.2,84.74,62.2,0.85,73.4,0.85,SemEval\\ 2007
1,1,Word Sense Disambiguation,Supervised:,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",66.81,91.02,4.6,0.06,73.4,0.91,SemEval\\ 2007
2,1,Word Sense Disambiguation,Supervised:,2019-05,"SemCor+WNGC, hypernyms",73.4,100.0,6.6,0.09,73.4,1.0,SemEval\\ 2007
0,1,Entity Typing,Freebase FIGER,2018-06,TextEnt-full,94.8,100.0,94.8,1.0,94.8,1.0,BEP
0,1,Text Classification,20NEWS,2018-06,TextEnt-full,83.9,97.33,83.9,0.97,86.2,0.91,F\\-measure
1,1,Text Classification,20NEWS,2019-09,NABoE-full,86.2,100.0,2.3,0.03,86.2,0.94,F\\-measure
2,1,Text Classification,R8,2018-06,TextEnt-full,91.0,99.24,91.0,0.99,91.7,0.99,F\\-measure
3,1,Text Classification,R8,2019-09,NABoE-full,91.7,100.0,0.7,0.01,91.7,1.0,F\\-measure
0,1,Entity Typing,Freebase FIGER,2018-06,TextEnt-full,84.2,100.0,84.2,1.0,84.2,1.0,Macro\\ F1
1,1,Entity Linking,FIGER,2019-05,ERNIE,76.51,100.0,76.51,1.0,76.51,0.91,Macro\\ F1
2,1,Text Classification,RCV1,2019-08,HiLAP (bow-CNN),60.1,100.0,60.1,1.0,60.1,0.71,Macro\\ F1
0,1,Entity Typing,Freebase FIGER,2018-06,TextEnt-full,85.7,100.0,85.7,1.0,85.7,1.0,Micro\\ F1
1,1,Entity Linking,FIGER,2019-05,ERNIE,73.39,100.0,73.39,1.0,73.39,0.86,Micro\\ F1
2,1,Multi-Label Text Classification,EUR-Lex,2019-06,bert-base,73.2,100.0,73.2,1.0,73.2,0.85,Micro\\ F1
3,1,Text Classification,RCV1,2019-08,HiLAP (bow-CNN),83.3,100.0,83.3,1.0,83.3,0.97,Micro\\ F1
0,1,Paraphrase Identification,2017_test set,2018-06,CNN,50.0,100.0,50,1.0,50,1.0,10\\ fold\\ Cross\\ validation
0,1,Multimodal Sentiment Analysis,CMU-MOSEI,2018-07,Graph-MFN,0.71,100.0,0.71,1.0,0.71,1.0,MAE
0,1,Question Answering,CoQA,2018-08,DrQA + seq2seq with copy attention (single model),67.0,81.21,67.0,0.81,82.5,0.81,In\\-domain
1,1,Question Answering,CoQA,2018-09,BiDAF++ (single model),69.4,84.12,2.4,0.03,82.5,0.84,In\\-domain
2,1,Question Answering,CoQA,2018-10,FlowQA (single model),76.3,92.48,6.9,0.08,82.5,0.92,In\\-domain
3,1,Question Answering,CoQA,2018-10,BERT Large Augmented (single model),82.5,100.0,6.2,0.08,82.5,1.0,In\\-domain
0,1,Question Answering,CoQA,2018-08,DrQA + seq2seq with copy attention (single model),65.1,80.27,65.1,0.8,81.1,0.8,Overall
1,1,Question Answering,CoQA,2018-09,BiDAF++ (single model),67.8,83.6,2.7,0.03,81.1,0.84,Overall
2,1,Question Answering,CoQA,2018-10,FlowQA (single model),75.0,92.48,7.2,0.09,81.1,0.92,Overall
3,1,Question Answering,CoQA,2018-10,BERT Large Augmented (single model),81.1,100.0,6.1,0.08,81.1,1.0,Overall
0,1,Question Answering,CoQA,2018-08,DrQA + seq2seq with copy attention (single model),60.4,77.84,60.4,0.78,77.6,0.78,Out\\-of\\-domain
1,1,Question Answering,CoQA,2018-09,BiDAF++ (single model),63.8,82.22,3.4,0.04,77.6,0.82,Out\\-of\\-domain
2,1,Question Answering,CoQA,2018-10,FlowQA (single model),71.8,92.53,8.0,0.1,77.6,0.93,Out\\-of\\-domain
3,1,Question Answering,CoQA,2018-10,BERT Large Augmented (single model),77.6,100.0,5.8,0.07,77.6,1.0,Out\\-of\\-domain
0,1,Machine Translation,WMT2014 English-French,2018-08,Transformer Big + BT,43.8,100.0,43.8,1.0,43.8,1.0,SacreBLEU
1,1,Machine Translation,WMT2014 English-German,2018-08,Transformer Big + BT,33.8,100.0,33.8,1.0,33.8,0.77,SacreBLEU
2,1,Machine Translation,WMT2019 English-German,2019-07,Facebook-FAIR (ensemble),42.7,100.0,42.7,1.0,42.7,0.97,SacreBLEU
0,1,Constituency Grammar Induction,PTB,2018-08,DMV + invertible projector,60.2,88.92,60.2,0.89,67.7,0.89,Mean\\ F1\\ \\(WSJ10\\)
1,1,Constituency Grammar Induction,PTB,2018-10,ON-LSTM,65.1,96.16,4.9,0.07,67.7,0.96,Mean\\ F1\\ \\(WSJ10\\)
2,1,Constituency Grammar Induction,PTB,2019-06,DIORA,67.7,100.0,2.6,0.04,67.7,1.0,Mean\\ F1\\ \\(WSJ10\\)
0,1,Visual Dialog,VisDial v1.0 test-std,2018-09,CorefNMN Kottur et al. (2018),54.7,94.98,54.7,0.95,57.59,0.92,NDCG
1,1,Visual Dialog,VisDial v1.0 test-std,2019-02,DAN,57.59,100.0,2.9,0.05,57.59,0.97,NDCG
2,1,Visual Dialog,Visual Dialog v1.0,2019-02,HACAN,57.17,96.29,57.17,0.96,59.37,0.96,NDCG
3,1,Visual Dialog,Visual Dialog v1.0,2019-02,DAN,57.59,97.0,0.4,0.01,59.37,0.97,NDCG
4,1,Visual Dialog,Visual Dialog v1.0,2020-04,MVAN,59.37,100.0,1.8,0.03,59.37,1.0,NDCG
0,1,Question Answering,QuAC,2018-10,FlowQA (single model),5.8,100.0,5.8,1.0,5.8,1.0,HEQD
0,1,Question Answering,QuAC,2018-10,FlowQA (single model),59.6,100.0,59.6,1.0,59.6,1.0,HEQQ
0,1,Multimodal Emotion Recognition,IEMOCAP,2018-10,Multimodal Dual Recurrent Encoder (MDRE) / (A+T),0.718,100.0,0.718,1.0,0.718,1.0,WAP
1,1,Speech Emotion Recognition,IEMOCAP,2018-10,Multimodal Dual Recurrent Encoder (MDRE) / (A+T),0.718,100.0,0.718,1.0,0.718,1.0,WAP
0,1,Constituency Grammar Induction,PTB,2018-10,ON-LSTM,66.8,97.52,66.8,0.98,68.5,0.98,Max\\ F1\\ \\(WSJ10\\)
1,1,Constituency Grammar Induction,PTB,2019-06,DIORA,68.5,100.0,1.7,0.02,68.5,1.0,Max\\ F1\\ \\(WSJ10\\)
0,1,Information Retrieval,TREC-PM,2018-11,hpipubcommon,0.5605,100.0,0.5605,1.0,0.5605,1.0,infNDCG
0,1,Phrase Grounding,Visual Genome,2018-11,VG_ELMo_PNASNet,55.16,100.0,55.16,1.0,55.16,0.8,Pointing\\ Game\\ Accuracy
1,1,Phrase Grounding,Flickr30k,2018-11,COCO_ELMo_PNASNet,69.19,100.0,69.19,1.0,69.19,1.0,Pointing\\ Game\\ Accuracy
2,1,Phrase Grounding,ReferIt,2018-11,VG_BiLSTM_VGG,62.76,100.0,62.76,1.0,62.76,0.91,Pointing\\ Game\\ Accuracy
0,1,Emotion Classification,SemEval 2018 Task 1E-c,2018-12,Transformer (finetune),56.1,100.0,56.1,1.0,56.1,1.0,Macro\\-F1
0,1,Question Answering,Natural Questions,2019-01,BERT-joint,66.2,100.0,66.2,1.0,66.2,1.0,F1\\ \\(Long\\)
0,1,Question Answering,Natural Questions,2019-01,BERT-joint,52.1,100.0,52.1,1.0,52.1,1.0,F1\\ \\(Short\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Subtask 4,2019-03,BERT-pair-QA-B,89.9,100.0,89.9,1.0,89.9,1.0,Accuracy\\ \\(3\\-way\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Subtask 4,2019-03,BERT-pair-QA-B,85.9,100.0,85.9,1.0,85.9,1.0,Accuracy\\ \\(4\\-way\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Subtask 4,2019-03,BERT-pair-QA-B,95.6,100.0,95.6,1.0,95.6,1.0,Binary\\ Accuracy
0,1,Text-To-Speech Synthesis,CMUDict 0.7b,2019-04,Token-Level Ensemble Distillation,4.6,100.0,4.6,1.0,4.6,1.0,Phoneme\\ Error\\ Rate
0,1,Text-To-Speech Synthesis,CMUDict 0.7b,2019-04,Token-Level Ensemble Distillation,19.88,100.0,19.88,1.0,19.88,1.0,Word\\ Error\\ Rate\\ \\(WER\\)
0,1,Passage Re-Ranking,TREC-PM,2019-04,BERT + Doc2query,36.5,100.0,36.5,1.0,36.5,1.0,mAP
0,1,Arabic Text Diacritization,Tashkeela,2019-04,Shakkala,0.0373,100.0,0.0373,1.0,0.0373,1.0,Diacritic\\ Error\\ Rate
0,1,Question Answering,HotpotQA,2019-05,DFGN,59.82,72.07,59.82,0.72,83.0,0.72,Joint\\ F1
1,1,Question Answering,HotpotQA,2019-07,SpanBERT,83.0,100.0,23.2,0.28,83.0,1.0,Joint\\ F1
0,1,Text-To-Speech Synthesis,LJSpeech,2019-05,FastSpeech,270.0,100.0,270,1.0,270,1.0,Speedup
0,1,Text-To-Speech Synthesis,LJSpeech,2019-05,FastSpeech,3.84,100.0,3.84,1.0,3.84,1.0,MOS
0,1,Multi-Label Text Classification,Wiki-30K,2019-05,LAHA,67.82,100.0,67.82,1.0,67.82,0.73,nDCG\\-at\\-5
1,1,Multi-Label Text Classification,AAPD,2019-05,LAHA,83.7,100.0,83.7,1.0,83.7,0.9,nDCG\\-at\\-5
2,1,Multi-Label Text Classification,EUR-Lex,2019-05,LAHA,59.28,72.03,59.28,0.72,82.3,0.64,nDCG\\-at\\-5
3,1,Multi-Label Text Classification,EUR-Lex,2019-06,bert-base,82.3,100.0,23.0,0.28,82.3,0.88,nDCG\\-at\\-5
4,1,Multi-Label Text Classification,Amazon-12K,2019-05,LAHA,87.57,100.0,87.57,1.0,87.57,0.94,nDCG\\-at\\-5
5,1,Multi-Label Text Classification,Kan-Shan Cup,2019-05,LAHA,54.65,100.0,54.65,1.0,54.65,0.59,nDCG\\-at\\-5
6,1,Text Classification,RCV1,2019-06,NLP-Cap,93.11,100.0,93.11,1.0,93.11,1.0,nDCG\\-at\\-5
0,1,Multi-Label Text Classification,EUR-Lex,2019-05,LAHA,64.89,91.25,64.89,0.91,71.11,0.7,nDCG\\-at\\-3
1,1,Multi-Label Text Classification,EUR-Lex,2019-06,NLP-Cap,71.11,100.0,6.2,0.09,71.11,0.77,nDCG\\-at\\-3
2,1,Multi-Label Text Classification,Amazon-12K,2019-05,LAHA,89.13,100.0,89.13,1.0,89.13,0.96,nDCG\\-at\\-3
3,1,Multi-Label Text Classification,Kan-Shan Cup,2019-05,LAHA,51.7,100.0,51.7,1.0,51.7,0.56,nDCG\\-at\\-3
4,1,Multi-Label Text Classification,Wiki-30K,2019-05,LAHA,75.64,100.0,75.64,1.0,75.64,0.82,nDCG\\-at\\-3
5,1,Multi-Label Text Classification,AAPD,2019-05,LAHA,80.11,100.0,80.11,1.0,80.11,0.87,nDCG\\-at\\-3
6,1,Text Classification,RCV1,2019-06,NLP-Cap,92.47,100.0,92.47,1.0,92.47,1.0,nDCG\\-at\\-3
0,1,Multi-Label Text Classification,Wiki-30K,2019-05,LAHA,73.14,100.0,73.14,1.0,73.14,0.9,P\\-at\\-3
1,1,Multi-Label Text Classification,AAPD,2019-05,LAHA,60.72,100.0,60.72,1.0,60.72,0.75,P\\-at\\-3
2,1,Multi-Label Text Classification,EUR-Lex,2019-05,LAHA,61.48,93.89,61.48,0.94,65.48,0.76,P\\-at\\-3
3,1,Multi-Label Text Classification,EUR-Lex,2019-06,NLP-Cap,65.48,100.0,4.0,0.06,65.48,0.81,P\\-at\\-3
4,1,Multi-Label Text Classification,Amazon-12K,2019-05,LAHA,79.16,100.0,79.16,1.0,79.16,0.97,P\\-at\\-3
5,1,Multi-Label Text Classification,Kan-Shan Cup,2019-05,LAHA,34.6,100.0,34.6,1.0,34.6,0.43,P\\-at\\-3
6,1,Text Classification,RCV1,2019-06,NLP-Cap,81.27,100.0,81.27,1.0,81.27,1.0,P\\-at\\-3
0,1,Fake News Detection,Grover-Mega,2019-05,Grover-Mega,92.0,100.0,92.0,1.0,92.0,1.0,Unpaired\\ Accuracy
0,1,Multi-Label Text Classification,EUR-Lex,2019-06,bert-base,79.6,100.0,79.6,1.0,79.6,1.0,RP\\-at\\-5
0,1,Text Classification,RCV1,2019-06,NLP-Cap,97.05,100.0,97.05,1.0,97.05,1.0,nDCG\\-at\\-1
1,1,Multi-Label Text Classification,EUR-Lex,2019-06,NLP-Cap,80.2,100.0,80.2,1.0,80.2,0.83,nDCG\\-at\\-1
0,1,Document Ranking,ClueWeb09-B,2019-06,XLNet,20.28,100.0,20.28,1.0,20.28,1.0,ERR\\-at\\-20
0,1,Reading Comprehension,RACE,2019-06,XLNet,84.0,100.0,84,1.0,84,1.0,Accuracy\\ \\(High\\)
0,1,Reading Comprehension,RACE,2019-06,XLNet,88.6,100.0,88.6,1.0,88.6,1.0,Accuracy\\ \\(Middle\\)
0,1,Sentiment Analysis,ASTD,2019-08,CNN-LSTM,0.62,100.0,0.62,1.0,0.62,0.69,Average\\ Recall
1,1,Sentiment Analysis,ArSAS,2019-08,CNN-LSTM,0.9,100.0,0.9,1.0,0.9,1.0,Average\\ Recall
2,1,Sentiment Analysis,SemEval 2017 Task 4-A,2019-08,CNN-LSTM,0.61,100.0,0.61,1.0,0.61,0.68,Average\\ Recall
0,1,Sentiment Analysis,FiQA,2019-08,FinBERT,0.55,100.0,0.55,1.0,0.55,1.0,R\\^2
0,1,Table-based Fact Verification,TabFact,2019-09,Table-BERT-Horizontal-T+F-Template,66.1,100.0,66.1,1.0,66.1,1.0,Val
0,1,Image Captioning,Flickr30k Captions test,2019-09,Unified VLP,17.0,100.0,17,1.0,17,0.8,SPICE
1,1,Image Captioning,COCO Captions test,2019-09,Unified VLP,21.2,100.0,21.2,1.0,21.2,1.0,SPICE
0,1,Question Answering,MultiRC,2019-10,T5-11B,88.2,100.0,88.2,1.0,88.2,1.0,F1a
0,1,Part-Of-Speech Tagging,French GSD,2019-11,CamemBERT,98.19,100.0,98.19,1.0,98.19,0.99,UPOS
1,1,Part-Of-Speech Tagging,Sequoia Treebank,2019-11,CamemBERT,99.21,100.0,99.21,1.0,99.21,1.0,UPOS
2,1,Part-Of-Speech Tagging,Spoken Corpus,2019-11,CamemBERT,96.68,100.0,96.68,1.0,96.68,0.97,UPOS
3,1,Part-Of-Speech Tagging,ParTUT,2019-11,CamemBERT,97.63,100.0,97.63,1.0,97.63,0.98,UPOS
0,1,Scientific Concept Extraction,STM-corpus,2020-01,SciBERT (full data),65.5,98.64,65.5,0.99,66.4,0.99,Exact\\ Span\\ F1
1,1,Scientific Concept Extraction,STM-corpus,2020-01,SciBERT (active learning),66.4,100.0,0.9,0.01,66.4,1.0,Exact\\ Span\\ F1
0,1,Aspect Extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,BAT,85.57,100.0,85.57,1.0,85.57,1.0,Laptop\\ \\(F1\\)
0,1,Aspect Extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,BAT,83.54,100.0,83.54,1.0,83.54,1.0,Mean\\ F1\\ \\(Laptop\\ \\+\\ Restaurant\\)
0,1,Aspect Extraction,SemEval 2014 Task 4 Sub Task 2,2020-01,BAT,81.5,100.0,81.5,1.0,81.5,1.0,Restaurant\\ \\(F1\\)
0,1,Bias Detection,StereoSet,2020-04,GPT-2 (small),72.97,100.0,72.97,1.0,72.97,1.0,ICAT\\ Score
0,1,Question Answering,SCDE,2020-04,bert-large-uncased + APN,0.299,100.0,0.299,1.0,0.299,1.0,PA
0,1,Question Answering,SCDE,2020-04,bert-large-uncased + APN,0.661,100.0,0.661,1.0,0.661,1.0,DE
0,1,Question Answering,SCDE,2020-04,bert-large-uncased + APN,0.717,100.0,0.717,1.0,0.717,1.0,BA
