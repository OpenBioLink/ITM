,ds_count,task,ds,date,model_label,value,percent_of_max_sota,gain,ratio,max_sota,percent_of_max_metric,merge
0,1,BIRL: Benchmark on Image Registration methods with Landmark validations,CIMA-10k - BIRL: Benchmark on Image Registration methods with Landmark validations benchmarking,2006-05,bUnwarpJ,-2.82,122.61,-2.82,100,-2.3,1.23,AMrTRE
1,1,BIRL: Benchmark on Image Registration methods with Landmark validations,CIMA-10k - BIRL: Benchmark on Image Registration methods with Landmark validations benchmarking,2008-02,ANTs,-2.3,100.0,0.52,1.0,-2.3,1.0,AMrTRE
0,1,BIRL: Benchmark on Image Registration methods with Landmark validations,CIMA-10k - BIRL: Benchmark on Image Registration methods with Landmark validations benchmarking,2006-05,bUnwarpJ,3.0,100.0,3.0,100,3.0,1.0,MMrTRE
0,1,Diffeomorphic Medical Image Registration,CUMC12 - Diffeomorphic Medical Image Registration benchmarking,2008-02,SyN,0.514,98.85,0.514,100,0.52,0.99,Mean\\ target\\ overlap\\ ratio
1,1,Diffeomorphic Medical Image Registration,CUMC12 - Diffeomorphic Medical Image Registration benchmarking,2019-04,Metric Net (Local Reg),0.52,100.0,0.006,1.0,0.52,1.0,Mean\\ target\\ overlap\\ ratio
0,1,Monocular Depth Estimation,NYU-Depth V2 - Monocular Depth Estimation benchmarking,2014-11,Eigen et al.,-0.641,163.52,-0.641,100,-0.392,1.64,RMSE
1,1,Monocular Depth Estimation,NYU-Depth V2 - Monocular Depth Estimation benchmarking,2016-07,Li et al.,-0.635,161.99,0.006,0.0241,-0.392,1.62,RMSE
2,1,Monocular Depth Estimation,NYU-Depth V2 - Monocular Depth Estimation benchmarking,2017-04,Xu et al.,-0.586,149.49,0.049,0.1968,-0.392,1.49,RMSE
3,1,Monocular Depth Estimation,NYU-Depth V2 - Monocular Depth Estimation benchmarking,2018-03,SENet-154,-0.53,135.2,0.056,0.2249,-0.392,1.35,RMSE
4,1,Monocular Depth Estimation,NYU-Depth V2 - Monocular Depth Estimation benchmarking,2018-06,DORN,-0.509,129.85,0.021,0.0843,-0.392,1.3,RMSE
5,1,Monocular Depth Estimation,NYU-Depth V2 - Monocular Depth Estimation benchmarking,2018-12,DenseDepth,-0.465,118.62,0.044,0.1767,-0.392,1.19,RMSE
6,1,Monocular Depth Estimation,NYU-Depth V2 - Monocular Depth Estimation benchmarking,2019-07,BTS,-0.392,100.0,0.073,0.2932,-0.392,1.0,RMSE
7,1,Object Counting,CARPK - Object Counting benchmarking,2015-06,Faster R-CNN (2015),-47.67,559.51,-47.67,100,-8.52,121.61,RMSE
8,1,Object Counting,CARPK - Object Counting benchmarking,2016-09,One-Look Regression (2016),-36.73,431.1,10.94,0.2794,-8.52,93.7,RMSE
9,1,Object Counting,CARPK - Object Counting benchmarking,2017-07,LPN Counting (2017),-34.46,404.46,2.27,0.058,-8.52,87.91,RMSE
10,1,Object Counting,CARPK - Object Counting benchmarking,2017-07,RetinaNet (2018),-22.3,261.74,12.16,0.3106,-8.52,56.89,RMSE
11,1,Object Counting,CARPK - Object Counting benchmarking,2019-04,Soft-IoU + EM-Merger unit,-8.52,100.0,13.78,0.352,-8.52,21.73,RMSE
12,1,Monocular Depth Estimation,Mid-Air Dataset - Monocular Depth Estimation benchmarking,2016-09,Monodepth,-13.595,100.0,-13.595,100,-13.595,34.68,RMSE
13,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2017-08,SparseConvs,-1601.0,207.15,-1601.0,100,-772.87,4084.18,RMSE
14,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-08,Spade-sD,-1035.0,133.92,566.0,0.6835,-772.87,2640.31,RMSE
15,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-08,Spade-RGBsD,-918.0,118.78,117.0,0.1413,-772.87,2341.84,RMSE
16,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-11,NConv-CNN-L2,-830.0,107.39,88.0,0.1063,-772.87,2117.35,RMSE
17,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2019-02,FusionNet (RGB_guide&certainty),-772.87,100.0,57.13,0.069,-772.87,1971.61,RMSE
18,1,Monocular Depth Estimation,Make3D - Monocular Depth Estimation benchmarking,2018-06,Monodepth2,-7.417,100.0,-7.417,100,-7.417,18.92,RMSE
19,1,Depth Completion,VOID - Depth Completion benchmarking,2019-05,VOICED,-169.79,100.0,-169.79,100,-169.79,433.14,RMSE
20,1,Stereo-LiDAR Fusion,KITTI Depth Completion Validation - Stereo-LiDAR Fusion benchmarking,2019-08,GuideNet,-777.78,100.0,-777.78,100,-777.78,1984.13,RMSE
0,1,Multi-tissue Nucleus Segmentation,Kumar - Multi-tissue Nucleus Segmentation benchmarking,2014-11,FCN8 (e),31.2,61.3,31.2,100,50.9,0.16,Hausdorff\\ Distance\\ \\(mm\\)
1,1,Multi-tissue Nucleus Segmentation,Kumar - Multi-tissue Nucleus Segmentation benchmarking,2015-05,U-Net (e),47.8,93.91,16.6,0.8426,50.9,0.24,Hausdorff\\ Distance\\ \\(mm\\)
2,1,Multi-tissue Nucleus Segmentation,Kumar - Multi-tissue Nucleus Segmentation benchmarking,2017-03,Mask R-CNN (e),50.9,100.0,3.1,0.1574,50.9,0.26,Hausdorff\\ Distance\\ \\(mm\\)
3,1,Colorectal Gland Segmentation:,CRAG - Colorectal Gland Segmentation: benchmarking,2015-05,FCN8 (e),199.5,100.0,199.5,100,199.5,1.0,Hausdorff\\ Distance\\ \\(mm\\)
0,1,Multi-tissue Nucleus Segmentation,Kumar - Multi-tissue Nucleus Segmentation benchmarking,2014-11,FCN8 (e),0.797,100.0,0.797,100,0.797,0.01,Dice
1,1,Medical Image Segmentation,RITE - Medical Image Segmentation benchmarking,2015-05,U-Net,55.24,100.0,55.24,100,55.24,0.61,Dice
2,1,Colorectal Gland Segmentation:,CRAG - Colorectal Gland Segmentation: benchmarking,2015-05,FCN8 (e),0.835,98.93,0.835,100,0.844,0.01,Dice
3,1,Colorectal Gland Segmentation:,CRAG - Colorectal Gland Segmentation: benchmarking,2015-05,U-Net (e),0.844,100.0,0.009,1.0,0.844,0.01,Dice
4,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2016-03,FnsNet,0.6165,86.98,0.6165,100,0.7088,0.01,Dice
5,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2016-11,Pix2Pix,0.6351,89.6,0.0186,0.2015,0.7088,0.01,Dice
6,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2017-03,Mask R-CNN,0.707,99.75,0.0719,0.779,0.7088,0.01,Dice
7,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2018-09,Cell R-CNN,0.7088,100.0,0.0018,0.0195,0.7088,0.01,Dice
8,1,Medical Image Segmentation,2018 Data Science Bowl - Medical Image Segmentation benchmarking,2018-07,Unet++,0.8974,100.0,0.8974,100,0.8974,0.01,Dice
9,1,Image Registration,Osteoarthritis Initiative - Image Registration benchmarking,2019-03,vSVF-net [shen2019networks],67.59,99.13,67.59,100,68.18,0.74,Dice
10,1,Image Registration,Osteoarthritis Initiative - Image Registration benchmarking,2019-06,Region-specific Diffeomorphic Metric Mapping,68.18,100.0,0.59,1.0,68.18,0.75,Dice
11,1,Lung Nodule Segmentation,LIDC-IDRI - Lung Nodule Segmentation benchmarking,2019-08,ModelGenesis,75.86,100.0,75.86,100,75.86,0.83,Dice
12,1,Liver Segmentation,LiTS2017 - Liver Segmentation benchmarking,2019-08,ModelGenesis,91.13,100.0,91.13,100,91.13,1.0,Dice
0,1,3D Human Pose Estimation,HumanEva-I - 3D Human Pose Estimation benchmarking,2010-03,TGP,48.7,100.0,48.7,100,48.7,1.0,Mean\\ Reconstruction\\ Error\\ \\(mm\\)
1,1,3D Face Reconstruction,NoW Benchmark - 3D Face Reconstruction benchmarking,2016-12,3DMM-CNN,2.33,100.0,2.33,100,2.33,0.05,Mean\\ Reconstruction\\ Error\\ \\(mm\\)
0,1,Multivariate Time Series Imputation,KDD CUP Challenge 2018 - Multivariate Time Series Imputation benchmarking,2010-11,MICE,0.468,100.0,0.468,100,0.468,1.0,MSE\\ \\(10%\\ missing\\)
0,1,Multivariate Time Series Imputation,UCI localization data - Multivariate Time Series Imputation benchmarking,2010-11,MICE,0.477,100.0,0.477,100,0.477,1.0,MAE\\ \\(10%\\ missing\\)
0,1,Multivariate Time Series Imputation,PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking,2010-11,MICE,0.634,100.0,0.634,100,0.634,1.0,MAE\\ \\(10%\\ of\\ data\\ as\\ GT\\)
0,1,Multivariate Time Series Imputation,Beijing Air Quality - Multivariate Time Series Imputation benchmarking,2010-11,MICE,27.42,100.0,27.42,100,27.42,1.0,MAE\\ \\(PM2\\.5\\)
0,1,Image Classification,MNIST - Image Classification benchmarking,2012-02,MCDNN,0.23,4.6,0.23,100,5.0,0.0,Percentage\\ error
1,1,Image Classification,MNIST - Image Classification benchmarking,2013-02,Maxout Networks,0.5,10.0,0.27,0.0566,5.0,0.01,Percentage\\ error
2,1,Image Classification,MNIST - Image Classification benchmarking,2014-04,PCANet,0.6,12.0,0.1,0.021,5.0,0.01,Percentage\\ error
3,1,Image Classification,MNIST - Image Classification benchmarking,2014-12,Explaining and Harnessing Adversarial Examples,0.8,16.0,0.2,0.0419,5.0,0.01,Percentage\\ error
4,1,Image Classification,MNIST - Image Classification benchmarking,2015-06,Zhao et al. (2015) (auto-encoder),4.76,95.2,3.96,0.8302,5.0,0.06,Percentage\\ error
5,1,Image Classification,MNIST - Image Classification benchmarking,2017-08,ProjectionNet,5.0,100.0,0.24,0.0503,5.0,0.06,Percentage\\ error
6,1,Image Classification,SVHN - Image Classification benchmarking,2013-01,Stochastic Pooling,2.8,3.59,2.8,100,77.93,0.04,Percentage\\ error
7,1,Image Classification,SVHN - Image Classification benchmarking,2014-06,M1+M2,36.02,46.22,33.22,0.4422,77.93,0.46,Percentage\\ error
8,1,Image Classification,SVHN - Image Classification benchmarking,2014-06,M1+KNN,65.63,84.22,29.61,0.3941,77.93,0.84,Percentage\\ error
9,1,Image Classification,SVHN - Image Classification benchmarking,2015-11,KNN,77.93,100.0,12.3,0.1637,77.93,1.0,Percentage\\ error
10,1,Image Classification,Fashion-MNIST - Image Classification benchmarking,2017-08,Random Erasing,3.65,48.03,3.65,100,7.6,0.05,Percentage\\ error
11,1,Image Classification,Fashion-MNIST - Image Classification benchmarking,2019-01,VGG8B(2x) + LocalLearning + CO,4.14,54.47,0.49,0.1241,7.6,0.05,Percentage\\ error
12,1,Image Classification,Fashion-MNIST - Image Classification benchmarking,2019-04,TextCaps,6.29,82.76,2.15,0.5443,7.6,0.08,Percentage\\ error
13,1,Image Classification,Fashion-MNIST - Image Classification benchmarking,2019-08,NeuPDE,7.6,100.0,1.31,0.3316,7.6,0.1,Percentage\\ error
14,1,Image Classification,MultiMNIST - Image Classification benchmarking,2017-10,CapsNet,5.2,100.0,5.2,100,5.2,0.07,Percentage\\ error
15,1,Semi-Supervised Image Classification,"CIFAR-10, 40 Labels - Semi-Supervised Image Classification benchmarking",2018-05,UL-Hopfield (ULH),16.9,88.48,16.9,100,19.1,0.22,Percentage\\ error
16,1,Semi-Supervised Image Classification,"CIFAR-10, 40 Labels - Semi-Supervised Image Classification benchmarking",2019-11,ReMixMatch,19.1,100.0,2.2,1.0,19.1,0.25,Percentage\\ error
0,1,Traffic Sign Recognition,GTSRB - Traffic Sign Recognition benchmarking,2012-02,MCDNN,99.5,100.0,99.5,100,99.5,1.0,Accuracy
1,1,Image Clustering,Extended Yale-B - Image Clustering benchmarking,2012-03,SSC,0.706,71.17,0.706,100,0.992,0.01,Accuracy
2,1,Image Clustering,Extended Yale-B - Image Clustering benchmarking,2017-09,DSC-2,0.973,98.08,0.267,0.9336,0.992,0.01,Accuracy
3,1,Image Clustering,Extended Yale-B - Image Clustering benchmarking,2018-04,DMSC,0.992,100.0,0.019,0.0664,0.992,0.01,Accuracy
4,1,Skeleton Based Action Recognition,UWA3D - Skeleton Based Action Recognition benchmarking,2012-07,HOJ3D,17.7,21.74,17.7,100,81.4,0.18,Accuracy
5,1,Skeleton Based Action Recognition,UWA3D - Skeleton Based Action Recognition benchmarking,2017-08,ESV (Synthesized + Pre-trained),73.8,90.66,56.1,0.8807,81.4,0.74,Accuracy
6,1,Skeleton Based Action Recognition,UWA3D - Skeleton Based Action Recognition benchmarking,2018-04,VA-fusion (aug.),81.4,100.0,7.6,0.1193,81.4,0.81,Accuracy
7,1,Image Clustering,coil-100 - Image Clustering benchmarking,2012-08,GDL,0.731,94.32,0.731,100,0.775,0.01,Accuracy
8,1,Image Clustering,coil-100 - Image Clustering benchmarking,2017-03,DBC,0.775,100.0,0.044,1.0,0.775,0.01,Accuracy
9,1,Image Clustering,Fashion-MNIST - Image Clustering benchmarking,2012-08,GDL,0.627,100.0,0.627,100,0.627,0.01,Accuracy
10,1,Image Clustering,Coil-20 - Image Clustering benchmarking,2012-08,AGDL,0.858,100.0,0.858,100,0.858,0.01,Accuracy
11,1,Image Clustering,MNIST-full - Image Clustering benchmarking,2012-08,GDL,0.964,99.48,0.964,100,0.969,0.01,Accuracy
12,1,Image Clustering,MNIST-full - Image Clustering benchmarking,2018-12,DDC-DA,0.969,100.0,0.005,1.0,0.969,0.01,Accuracy
13,1,Skeleton Based Action Recognition,CAD-120 - Skeleton Based Action Recognition benchmarking,2012-10,KGS,86.0,96.3,86.0,100,89.3,0.86,Accuracy
14,1,Skeleton Based Action Recognition,CAD-120 - Skeleton Based Action Recognition benchmarking,2013-02,All Features (w ground truth),89.3,100.0,3.3,1.0,89.3,0.89,Accuracy
15,1,Trajectory Prediction,GPS - Trajectory Prediction benchmarking,2012-11,Support Vector Machines,88.0,100.0,88,100,88,0.88,Accuracy
16,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2012-12,AlexNet,62.88,69.45,62.88,100,90.54,0.63,Accuracy
17,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2014-09,VGG-16,79.01,87.27,16.13,0.5832,90.54,0.79,Accuracy
18,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2014-09,VGG-16 BN,80.6,89.02,1.59,0.0575,90.54,0.81,Accuracy
19,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2015-12,ResNet-50,82.94,91.61,2.34,0.0846,90.54,0.83,Accuracy
20,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2015-12,ResNet-152,84.79,93.65,1.85,0.0669,90.54,0.85,Accuracy
21,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2017-07,NASNet-A Large,87.56,96.71,2.77,0.1001,90.54,0.88,Accuracy
22,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2019-06,FixResNeXt-101 32x48d,89.73,99.11,2.17,0.0785,90.54,0.9,Accuracy
23,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2019-12,BiT-L,90.54,100.0,0.81,0.0293,90.54,0.91,Accuracy
24,1,Unsupervised Domain Adaptation,Office-Home - Unsupervised Domain Adaptation benchmarking,2012-12,AlexNet [cite:NIPS12CNN],54.9,71.48,54.9,100,76.8,0.55,Accuracy
25,1,Unsupervised Domain Adaptation,Office-Home - Unsupervised Domain Adaptation benchmarking,2015-02,DAN [cite:ICML15DAN],74.3,96.74,19.4,0.8858,76.8,0.74,Accuracy
26,1,Unsupervised Domain Adaptation,Office-Home - Unsupervised Domain Adaptation benchmarking,2015-05,DANN [cite:JMLR16RevGrad],76.8,100.0,2.5,0.1142,76.8,0.77,Accuracy
27,1,Hand Gesture Recognition,Cambridge - Hand Gesture Recognition benchmarking,2013-03,Sanin et al. [sanin2013spatio],93.0,94.68,93.0,100,98.23,0.93,Accuracy
28,1,Hand Gesture Recognition,Cambridge - Hand Gesture Recognition benchmarking,2019-01,Key Frames + Feature Fusion,98.23,100.0,5.23,1.0,98.23,0.98,Accuracy
29,1,Human Interaction Recognition,UT - Human Interaction Recognition benchmarking,2013-06,Raptis et al.,93.3,94.88,93.3,100,98.33,0.93,Accuracy
30,1,Human Interaction Recognition,UT - Human Interaction Recognition benchmarking,2017-06,Co-LSTSM,95.0,96.61,1.7,0.338,98.33,0.95,Accuracy
31,1,Human Interaction Recognition,UT - Human Interaction Recognition benchmarking,2018-11,H-LSTCM,98.33,100.0,3.33,0.662,98.33,0.98,Accuracy
32,1,Image Classification,MNIST - Image Classification benchmarking,2013-06,DropConnect,99.79,99.96,99.79,100,99.83,1.0,Accuracy
33,1,Image Classification,MNIST - Image Classification benchmarking,2018-05,RMDL (30 RDLs),99.82,99.99,0.03,0.75,99.83,1.0,Accuracy
34,1,Image Classification,MNIST - Image Classification benchmarking,2020-01,SOPCNN (Only a single Model),99.83,100.0,0.01,0.25,99.83,1.0,Accuracy
35,1,Facial Expression Recognition,FER2013 - Facial Expression Recognition benchmarking,2013-07,Residual Masking Network,74.14,96.51,74.14,100,76.82,0.74,Accuracy
36,1,Facial Expression Recognition,FER2013 - Facial Expression Recognition benchmarking,2013-07,Ensemble ResMaskingNet with 6 other CNNs,76.82,100.0,2.68,1.0,76.82,0.77,Accuracy
37,1,Few-Shot Image Classification,ImageNet - 0-Shot - Few-Shot Image Classification benchmarking,2013-12,ConSE,1.4,93.33,1.4,100,1.5,0.01,Accuracy
38,1,Few-Shot Image Classification,ImageNet - 0-Shot - Few-Shot Image Classification benchmarking,2016-03,Synthesised Classifier,1.5,100.0,0.1,1.0,1.5,0.02,Accuracy
39,1,Image Clustering,STL-10 - Image Clustering benchmarking,2013-12,VAE,0.282,58.51,0.282,100,0.482,0.0,Accuracy
40,1,Image Clustering,STL-10 - Image Clustering benchmarking,2015-11,GAN,0.298,61.83,0.016,0.08,0.482,0.0,Accuracy
41,1,Image Clustering,STL-10 - Image Clustering benchmarking,2015-11,DEC,0.359,74.48,0.061,0.305,0.482,0.0,Accuracy
42,1,Image Clustering,STL-10 - Image Clustering benchmarking,2017-10,DAC,0.47,97.51,0.111,0.555,0.482,0.0,Accuracy
43,1,Image Clustering,STL-10 - Image Clustering benchmarking,2019-04,DCCM,0.482,100.0,0.012,0.06,0.482,0.0,Accuracy
44,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2013-12,VAE,0.179,46.74,0.179,100,0.383,0.0,Accuracy
45,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2015-11,DEC,0.195,50.91,0.016,0.0784,0.383,0.0,Accuracy
46,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2017-10,DAC,0.275,71.8,0.08,0.3922,0.383,0.0,Accuracy
47,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2019-04,DCCM,0.383,100.0,0.108,0.5294,0.383,0.0,Accuracy
48,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2013-12,VAE,0.334,47.04,0.334,100,0.71,0.0,Accuracy
49,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2015-11,DEC,0.381,53.66,0.047,0.125,0.71,0.0,Accuracy
50,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2017-10,DAC,0.527,74.23,0.146,0.3883,0.71,0.01,Accuracy
51,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2019-04,DCCM,0.71,100.0,0.183,0.4867,0.71,0.01,Accuracy
52,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2013-12,VAE,0.036,33.33,0.036,100,0.108,0.0,Accuracy
53,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2015-11,GAN,0.041,37.96,0.005,0.0694,0.108,0.0,Accuracy
54,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2017-10,DAC,0.066,61.11,0.025,0.3472,0.108,0.0,Accuracy
55,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2019-04,DCCM,0.108,100.0,0.042,0.5833,0.108,0.0,Accuracy
56,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2013-12,VAE,0.291,46.71,0.291,100,0.623,0.0,Accuracy
57,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2015-11,GAN,0.315,50.56,0.024,0.0723,0.623,0.0,Accuracy
58,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2017-10,DAC,0.522,83.79,0.207,0.6235,0.623,0.01,Accuracy
59,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2018-07,IIC,0.617,99.04,0.095,0.2861,0.623,0.01,Accuracy
60,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2019-04,DCCM,0.623,100.0,0.006,0.0181,0.623,0.01,Accuracy
61,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2013-12,VAE,0.152,46.48,0.152,100,0.327,0.0,Accuracy
62,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2015-11,DEC,0.185,56.57,0.033,0.1886,0.327,0.0,Accuracy
63,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2017-10,DAC,0.238,72.78,0.053,0.3029,0.327,0.0,Accuracy
64,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2019-04,DCCM,0.327,100.0,0.089,0.5086,0.327,0.0,Accuracy
65,1,Multimodal Activity Recognition,MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking,2014-03,DL-GSGC (RGB+D),95.0,97.44,95.0,100,97.5,0.95,Accuracy
66,1,Multimodal Activity Recognition,MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking,2016-03,DSSCA-SSLM (RGB+D),97.5,100.0,2.5,1.0,97.5,0.98,Accuracy
67,1,Face Verification,Labeled Faces in the Wild - Face Verification benchmarking,2014-04,GaussianFace,98.52,98.67,98.52,100,99.85,0.99,Accuracy
68,1,Face Verification,Labeled Faces in the Wild - Face Verification benchmarking,2014-06,DeepId2,99.15,99.3,0.63,0.4737,99.85,0.99,Accuracy
69,1,Face Verification,Labeled Faces in the Wild - Face Verification benchmarking,2014-12,DeepId2+,99.47,99.62,0.32,0.2406,99.85,0.99,Accuracy
70,1,Face Verification,Labeled Faces in the Wild - Face Verification benchmarking,2015-02,DeepID3,99.53,99.68,0.06,0.0451,99.85,1.0,Accuracy
71,1,Face Verification,Labeled Faces in the Wild - Face Verification benchmarking,2015-03,FaceNet,99.63,99.78,0.1,0.0752,99.85,1.0,Accuracy
72,1,Face Verification,Labeled Faces in the Wild - Face Verification benchmarking,2018-01,"ArcFace + MS1MV2 + R100, ",99.83,99.98,0.2,0.1504,99.85,1.0,Accuracy
73,1,Face Verification,Labeled Faces in the Wild - Face Verification benchmarking,2019-10,VarGFaceNet,99.85,100.0,0.02,0.015,99.85,1.0,Accuracy
74,1,Domain Adaptation,UCF-to-Olympic - Domain Adaptation benchmarking,2014-06,W. Sultani et al.,33.33,33.96,33.33,100,98.15,0.33,Accuracy
75,1,Domain Adaptation,UCF-to-Olympic - Domain Adaptation benchmarking,2014-09,TemPooling + RevGrad,98.15,100.0,64.82,1.0,98.15,0.98,Accuracy
76,1,Domain Adaptation,HMDBsmall-to-UCF - Domain Adaptation benchmarking,2014-06,W. Sultani et al.,68.67,69.04,68.67,100,99.47,0.69,Accuracy
77,1,Domain Adaptation,HMDBsmall-to-UCF - Domain Adaptation benchmarking,2014-09,TemPooling + RevGrad,98.41,98.93,29.74,0.9656,99.47,0.98,Accuracy
78,1,Domain Adaptation,HMDBsmall-to-UCF - Domain Adaptation benchmarking,2019-05,TA3N,99.47,100.0,1.06,0.0344,99.47,0.99,Accuracy
79,1,Domain Adaptation,UCF-to-HMDBsmall - Domain Adaptation benchmarking,2014-06,W. Sultani et al.,68.7,69.16,68.7,100,99.33,0.69,Accuracy
80,1,Domain Adaptation,UCF-to-HMDBsmall - Domain Adaptation benchmarking,2014-09,TemPooling + RevGrad,99.33,100.0,30.63,1.0,99.33,0.99,Accuracy
81,1,Domain Adaptation,Olympic-to-HMDBsmall - Domain Adaptation benchmarking,2014-06,W. Sultani et al.,47.91,51.56,47.91,100,92.92,0.48,Accuracy
82,1,Domain Adaptation,Olympic-to-HMDBsmall - Domain Adaptation benchmarking,2014-09,TemPooling + RevGrad,90.0,96.86,42.09,0.9351,92.92,0.9,Accuracy
83,1,Domain Adaptation,Olympic-to-HMDBsmall - Domain Adaptation benchmarking,2019-05,TA3N,92.92,100.0,2.92,0.0649,92.92,0.93,Accuracy
84,1,Hand Gesture Recognition,VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking,2014-06,Two Stream CNNs,68.0,79.0,68.0,100,86.08,0.68,Accuracy
85,1,Hand Gesture Recognition,VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking,2017-05,I3D,83.1,96.54,15.1,0.8352,86.08,0.83,Accuracy
86,1,Hand Gesture Recognition,VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking,2018-12,MTUT,86.08,100.0,2.98,0.1648,86.08,0.86,Accuracy
87,1,Skeleton Based Action Recognition,Florence 3D - Skeleton Based Action Recognition benchmarking,2014-06,Lie Group,90.9,91.73,90.9,100,99.1,0.91,Accuracy
88,1,Skeleton Based Action Recognition,Florence 3D - Skeleton Based Action Recognition benchmarking,2016-06,Rolling Rotations (FTP),91.4,92.23,0.5,0.061,99.1,0.91,Accuracy
89,1,Skeleton Based Action Recognition,Florence 3D - Skeleton Based Action Recognition benchmarking,2018-02,Deep STGC_K,99.1,100.0,7.7,0.939,99.1,0.99,Accuracy
90,1,Skeleton Based Action Recognition,UT-Kinect - Skeleton Based Action Recognition benchmarking,2014-06,Lie Group,97.1,98.58,97.1,100,98.5,0.97,Accuracy
91,1,Skeleton Based Action Recognition,UT-Kinect - Skeleton Based Action Recognition benchmarking,2018-06,DPRL,98.5,100.0,1.4,1.0,98.5,0.99,Accuracy
92,1,Fine-Grained Image Classification,CUB-200-2011 - Fine-Grained Image Classification benchmarking,2014-07,Part RCNN,76.4,84.51,76.4,100,90.4,0.76,Accuracy
93,1,Fine-Grained Image Classification,CUB-200-2011 - Fine-Grained Image Classification benchmarking,2015-12,Bilinear-CNN,85.1,94.14,8.7,0.6214,90.4,0.85,Accuracy
94,1,Fine-Grained Image Classification,CUB-200-2011 - Fine-Grained Image Classification benchmarking,2016-11,DFL-CNN,87.4,96.68,2.3,0.1643,90.4,0.87,Accuracy
95,1,Fine-Grained Image Classification,CUB-200-2011 - Fine-Grained Image Classification benchmarking,2017-12,MPN-COV,88.7,98.12,1.3,0.0929,90.4,0.89,Accuracy
96,1,Fine-Grained Image Classification,CUB-200-2011 - Fine-Grained Image Classification benchmarking,2018-01,PAIRS,89.2,98.67,0.5,0.0357,90.4,0.89,Accuracy
97,1,Fine-Grained Image Classification,CUB-200-2011 - Fine-Grained Image Classification benchmarking,2018-06,Inception-V3,89.6,99.12,0.4,0.0286,90.4,0.9,Accuracy
98,1,Fine-Grained Image Classification,CUB-200-2011 - Fine-Grained Image Classification benchmarking,2019-03,Stacked LSTM,90.4,100.0,0.8,0.0571,90.4,0.9,Accuracy
99,1,Activity Recognition In Videos,DogCentric - Activity Recognition In Videos benchmarking,2014-09,VGG [[Simonyan and Zisserman2015]],59.9,73.59,59.9,100,81.4,0.6,Accuracy
100,1,Activity Recognition In Videos,DogCentric - Activity Recognition In Videos benchmarking,2014-12,"PoT [[Ryoo, Rothrock, and Matthies2015]]",73.0,89.68,13.1,0.6093,81.4,0.73,Accuracy
101,1,Activity Recognition In Videos,DogCentric - Activity Recognition In Videos benchmarking,2015-05,"TDD [[Wang, Qiao, and Tang2015]]",76.6,94.1,3.6,0.1674,81.4,0.77,Accuracy
102,1,Activity Recognition In Videos,DogCentric - Activity Recognition In Videos benchmarking,2016-05,Sub-events (temporal filters + LSTM),81.4,100.0,4.8,0.2233,81.4,0.81,Accuracy
103,1,Domain Adaptation,UCF-to-HMDBfull - Domain Adaptation benchmarking,2014-09,RevGrad,74.44,95.03,74.44,100,78.33,0.74,Accuracy
104,1,Domain Adaptation,UCF-to-HMDBfull - Domain Adaptation benchmarking,2016-05,JAN,74.72,95.39,0.28,0.072,78.33,0.75,Accuracy
105,1,Domain Adaptation,UCF-to-HMDBfull - Domain Adaptation benchmarking,2019-05,TA3N,78.33,100.0,3.61,0.928,78.33,0.78,Accuracy
106,1,Few-Shot Image Classification,CUB-200 - 0-Shot Learning - Few-Shot Image Classification benchmarking,2014-09,SJE,50.1,88.05,50.1,100,56.9,0.5,Accuracy
107,1,Few-Shot Image Classification,CUB-200 - 0-Shot Learning - Few-Shot Image Classification benchmarking,2019-04,TAFE-Net,56.9,100.0,6.8,1.0,56.9,0.57,Accuracy
108,1,Human Interaction Recognition,BIT - Human Interaction Recognition benchmarking,2014-11,Donahue et al.,80.13,85.22,80.13,100,94.03,0.8,Accuracy
109,1,Human Interaction Recognition,BIT - Human Interaction Recognition benchmarking,2017-06,Co-LSTSM,92.88,98.78,12.75,0.9173,94.03,0.93,Accuracy
110,1,Human Interaction Recognition,BIT - Human Interaction Recognition benchmarking,2018-11,H-LSTCM,94.03,100.0,1.15,0.0827,94.03,0.94,Accuracy
111,1,Face Verification,YouTube Faces DB - Face Verification benchmarking,2014-12,DeepId2+,93.2,94.99,93.2,100,98.12,0.93,Accuracy
112,1,Face Verification,YouTube Faces DB - Face Verification benchmarking,2015-03,FaceNet,95.12,96.94,1.92,0.3902,98.12,0.95,Accuracy
113,1,Face Verification,YouTube Faces DB - Face Verification benchmarking,2015-11,Light CNN-29,95.54,97.37,0.42,0.0854,98.12,0.96,Accuracy
114,1,Face Verification,YouTube Faces DB - Face Verification benchmarking,2017-04,QAN,96.17,98.01,0.63,0.128,98.12,0.96,Accuracy
115,1,Face Verification,YouTube Faces DB - Face Verification benchmarking,2018-01,"ArcFace + MS1MV2 + R100, ",98.02,99.9,1.85,0.376,98.12,0.98,Accuracy
116,1,Face Verification,YouTube Faces DB - Face Verification benchmarking,2018-03,"SeqFace, 1 ResNet-64",98.12,100.0,0.1,0.0203,98.12,0.98,Accuracy
117,1,Face Verification,Oulu-CASIA - Face Verification benchmarking,2014-12,DeepId2+,96.5,100.0,96.5,100,96.5,0.97,Accuracy
118,1,Domain Adaptation,SVNH-to-MNIST - Domain Adaptation benchmarking,2015-02,MMD [tzeng2015ddc]; [long2015learning],71.1,71.88,71.1,100,98.91,0.71,Accuracy
119,1,Domain Adaptation,SVNH-to-MNIST - Domain Adaptation benchmarking,2016-08,DSN (DANN),82.7,83.61,11.6,0.4171,98.91,0.83,Accuracy
120,1,Domain Adaptation,SVNH-to-MNIST - Domain Adaptation benchmarking,2019-03,rRevGrad+CAT,98.8,99.89,16.1,0.5789,98.91,0.99,Accuracy
121,1,Domain Adaptation,SVNH-to-MNIST - Domain Adaptation benchmarking,2019-05,SRDA (RAN),98.91,100.0,0.11,0.004,98.91,0.99,Accuracy
122,1,Domain Adaptation,Synth Digits-to-SVHN - Domain Adaptation benchmarking,2015-02,MMD [tzeng2015ddc]; [long2015learning],88.0,96.49,88.0,100,91.2,0.88,Accuracy
123,1,Domain Adaptation,Synth Digits-to-SVHN - Domain Adaptation benchmarking,2015-05,DANN [ganin2016domain],90.3,99.01,2.3,0.7187,91.2,0.9,Accuracy
124,1,Domain Adaptation,Synth Digits-to-SVHN - Domain Adaptation benchmarking,2016-08,DSN (DANN),91.2,100.0,0.9,0.2812,91.2,0.91,Accuracy
125,1,Domain Adaptation,SYNSIG-to-GTSRB - Domain Adaptation benchmarking,2015-02,DAN,91.1,96.5,91.1,100,94.4,0.91,Accuracy
126,1,Domain Adaptation,SYNSIG-to-GTSRB - Domain Adaptation benchmarking,2017-12,MCD,94.4,100.0,3.3,1.0,94.4,0.94,Accuracy
127,1,Domain Adaptation,ImageCLEF-DA - Domain Adaptation benchmarking,2015-02,DAN,76.9,85.16,76.9,100,90.3,0.77,Accuracy
128,1,Domain Adaptation,ImageCLEF-DA - Domain Adaptation benchmarking,2018-11,IAFN+ENT,88.9,98.45,12.0,0.8955,90.3,0.89,Accuracy
129,1,Domain Adaptation,ImageCLEF-DA - Domain Adaptation benchmarking,2019-11,SPL,90.3,100.0,1.4,0.1045,90.3,0.9,Accuracy
130,1,Domain Adaptation,Synth Signs-to-GTSRB - Domain Adaptation benchmarking,2015-02,MMD [tzeng2015ddc]; [long2015learning],91.1,97.85,91.1,100,93.1,0.91,Accuracy
131,1,Domain Adaptation,Synth Signs-to-GTSRB - Domain Adaptation benchmarking,2016-08,DSN (DANN),93.1,100.0,2.0,1.0,93.1,0.93,Accuracy
132,1,Domain Adaptation,MNIST-to-MNIST-M - Domain Adaptation benchmarking,2015-02,MMD [tzeng2015ddc]; [long2015learning],76.9,92.43,76.9,100,83.2,0.77,Accuracy
133,1,Domain Adaptation,MNIST-to-MNIST-M - Domain Adaptation benchmarking,2015-05,DANN [ganin2016domain],77.4,93.03,0.5,0.0794,83.2,0.77,Accuracy
134,1,Domain Adaptation,MNIST-to-MNIST-M - Domain Adaptation benchmarking,2016-08,DSN (DANN),83.2,100.0,5.8,0.9206,83.2,0.83,Accuracy
135,1,Document Image Classification,RVL-CDIP - Document Image Classification benchmarking,2015-02,Document section-based models + AlexNet transfer learning,89.8,97.39,89.8,100,92.21,0.9,Accuracy
136,1,Document Image Classification,RVL-CDIP - Document Image Classification benchmarking,2017-04,"Transfer Learning from AlexNet, VGG-16, GoogLeNet and ResNet50",90.97,98.66,1.17,0.4855,92.21,0.91,Accuracy
137,1,Document Image Classification,RVL-CDIP - Document Image Classification benchmarking,2018-01,Transfer Learning from VGG16 trained on Imagenet,92.21,100.0,1.24,0.5145,92.21,0.92,Accuracy
138,1,Face Identification,MegaFace - Face Identification benchmarking,2015-03,FaceNet,70.49,71.67,70.49,100,98.35,0.71,Accuracy
139,1,Face Identification,MegaFace - Face Identification benchmarking,2015-11,Light CNN-29,73.749,74.99,3.259,0.117,98.35,0.74,Accuracy
140,1,Face Identification,MegaFace - Face Identification benchmarking,2017-04,SphereFace (3-patch ensemble),75.766,77.04,2.017,0.0724,98.35,0.76,Accuracy
141,1,Face Identification,MegaFace - Face Identification benchmarking,2018-01,ArcFace + MS1MV2 + R100 + R,98.35,100.0,22.584,0.8106,98.35,0.98,Accuracy
142,1,Disguised Face Verification,MegaFace - Disguised Face Verification benchmarking,2015-03,FaceNet,86.47,100.0,86.47,100,86.47,0.86,Accuracy
143,1,Facial Expression Recognition,JAFFE - Facial Expression Recognition benchmarking,2015-05,Salient Facial Patch,91.8,98.92,91.8,100,92.8,0.92,Accuracy
144,1,Facial Expression Recognition,JAFFE - Facial Expression Recognition benchmarking,2019-02,DeepEmotion,92.8,100.0,1.0,1.0,92.8,0.93,Accuracy
145,1,Synthetic-to-Real Translation,Syn2Real-C - Synthetic-to-Real Translation benchmarking,2015-05,DANN,57.4,71.93,57.4,100,79.8,0.57,Accuracy
146,1,Synthetic-to-Real Translation,Syn2Real-C - Synthetic-to-Real Translation benchmarking,2017-11,ADR,74.8,93.73,17.4,0.7768,79.8,0.75,Accuracy
147,1,Synthetic-to-Real Translation,Syn2Real-C - Synthetic-to-Real Translation benchmarking,2019-11,DADA,79.8,100.0,5.0,0.2232,79.8,0.8,Accuracy
148,1,Semi-Supervised Image Classification,"STL-10, 1000 Labels - Semi-Supervised Image Classification benchmarking",2015-06,SWWAE,74.3,79.19,74.3,100,93.82,0.74,Accuracy
149,1,Semi-Supervised Image Classification,"STL-10, 1000 Labels - Semi-Supervised Image Classification benchmarking",2016-11,CC-GAN²,77.8,82.92,3.5,0.1793,93.82,0.78,Accuracy
150,1,Semi-Supervised Image Classification,"STL-10, 1000 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,89.82,95.74,12.02,0.6158,93.82,0.9,Accuracy
151,1,Semi-Supervised Image Classification,"STL-10, 1000 Labels - Semi-Supervised Image Classification benchmarking",2019-11,ReMixMatch,93.82,100.0,4.0,0.2049,93.82,0.94,Accuracy
152,1,Fine-Grained Image Classification,CompCars - Fine-Grained Image Classification benchmarking,2015-06,GoogLeNet,91.2,95.6,91.2,100,95.4,0.91,Accuracy
153,1,Fine-Grained Image Classification,CompCars - Fine-Grained Image Classification benchmarking,2019-01,A3M,95.4,100.0,4.2,1.0,95.4,0.95,Accuracy
154,1,Semi-Supervised Image Classification,"CIFAR-10, 4000 Labels - Semi-Supervised Image Classification benchmarking",2015-07,Γ-model,79.6,83.79,79.6,100,95.0,0.8,Accuracy
155,1,Semi-Supervised Image Classification,"CIFAR-10, 4000 Labels - Semi-Supervised Image Classification benchmarking",2016-06,GAN,84.41,88.85,4.81,0.3123,95.0,0.84,Accuracy
156,1,Semi-Supervised Image Classification,"CIFAR-10, 4000 Labels - Semi-Supervised Image Classification benchmarking",2016-10,Pi Model,87.84,92.46,3.43,0.2227,95.0,0.88,Accuracy
157,1,Semi-Supervised Image Classification,"CIFAR-10, 4000 Labels - Semi-Supervised Image Classification benchmarking",2017-03,Mean Teacher,93.72,98.65,5.88,0.3818,95.0,0.94,Accuracy
158,1,Semi-Supervised Image Classification,"CIFAR-10, 4000 Labels - Semi-Supervised Image Classification benchmarking",2018-06,SWSA,95.0,100.0,1.28,0.0831,95.0,0.95,Accuracy
159,1,Scene Text Recognition,ICDAR 2003 - Scene Text Recognition benchmarking,2015-07,CRNN,89.4,100.0,89.4,100,89.4,0.89,Accuracy
160,1,Scene Text Recognition,ICDAR2013 - Scene Text Recognition benchmarking,2015-07,CRNN,86.7,100.0,86.7,100,86.7,0.87,Accuracy
161,1,Scene Text Recognition,SVT - Scene Text Recognition benchmarking,2015-07,CRNN,80.8,100.0,80.8,100,80.8,0.81,Accuracy
162,1,Satellite Image Classification,SAT-6 - Satellite Image Classification benchmarking,2015-09,DeepSat,93.92,94.07,93.92,100,99.84,0.94,Accuracy
163,1,Satellite Image Classification,SAT-6 - Satellite Image Classification benchmarking,2019-11,DeepSat V2,99.84,100.0,5.92,1.0,99.84,1.0,Accuracy
164,1,Satellite Image Classification,SAT-4 - Satellite Image Classification benchmarking,2015-09,DeepSat,97.95,98.05,97.95,100,99.9,0.98,Accuracy
165,1,Satellite Image Classification,SAT-4 - Satellite Image Classification benchmarking,2015-12,Contrastive loss,98.74,98.84,0.79,0.4051,99.9,0.99,Accuracy
166,1,Satellite Image Classification,SAT-4 - Satellite Image Classification benchmarking,2019-11,DeepSat V2,99.9,100.0,1.16,0.5949,99.9,1.0,Accuracy
167,1,Facial Expression Recognition,MMI - Facial Expression Recognition benchmarking,2015-09,DeXpression,98.63,100.0,98.63,100,98.63,0.99,Accuracy
168,1,Multimodal Activity Recognition,EV-Action - Multimodal Activity Recognition benchmarking,2015-11,WHDMM (Depth),40.2,50.19,40.2,100,80.1,0.4,Accuracy
169,1,Multimodal Activity Recognition,EV-Action - Multimodal Activity Recognition benchmarking,2016-08,TSN (RGB),73.6,91.89,33.4,0.8371,80.1,0.74,Accuracy
170,1,Multimodal Activity Recognition,EV-Action - Multimodal Activity Recognition benchmarking,2017-04,TCN (Skeleton Kinect),80.1,100.0,6.5,0.1629,80.1,0.8,Accuracy
171,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2015-11,SAN (VGG),58.9,91.18,58.9,100,64.6,0.59,Accuracy
172,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2016-03,DMN+,60.4,93.5,1.5,0.2632,64.6,0.6,Accuracy
173,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2016-05,HieCoAtt (ResNet),62.1,96.13,1.7,0.2982,64.6,0.62,Accuracy
174,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2016-06,RAU (ResNet),63.2,97.83,1.1,0.193,64.6,0.63,Accuracy
175,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2017-04,SAAA (ResNet),64.6,100.0,1.4,0.2456,64.6,0.65,Accuracy
176,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2015-11,NMN+LSTM+FT,58.6,90.85,58.6,100,64.5,0.59,Accuracy
177,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-03,DMN+,60.3,93.49,1.7,0.2881,64.5,0.6,Accuracy
178,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-05,HieCoAtt (ResNet),61.8,95.81,1.5,0.2542,64.5,0.62,Accuracy
179,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-06,MCB (ResNet),64.2,99.53,2.4,0.4068,64.5,0.64,Accuracy
180,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-11,DAN (ResNet),64.3,99.69,0.1,0.0169,64.5,0.64,Accuracy
181,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2017-04,SAAA (ResNet),64.5,100.0,0.2,0.0339,64.5,0.65,Accuracy
182,1,Age-Invariant Face Recognition,CACDVS - Age-Invariant Face Recognition benchmarking,2015-11,MFM-CNN,97.95,98.19,97.95,100,99.76,0.98,Accuracy
183,1,Age-Invariant Face Recognition,CACDVS - Age-Invariant Face Recognition benchmarking,2017-03,DeepVisage,99.13,99.37,1.18,0.6519,99.76,0.99,Accuracy
184,1,Age-Invariant Face Recognition,CACDVS - Age-Invariant Face Recognition benchmarking,2018-09,AIM + CAFR,99.76,100.0,0.63,0.3481,99.76,1.0,Accuracy
185,1,Face Verification,MegaFace - Face Verification benchmarking,2015-11,Light CNN-29,85.133,86.45,85.133,100,98.48,0.85,Accuracy
186,1,Face Verification,MegaFace - Face Verification benchmarking,2017-04,SphereFace (single model),85.561,86.88,0.428,0.0321,98.48,0.86,Accuracy
187,1,Face Verification,MegaFace - Face Verification benchmarking,2017-04,SphereFace (3-patch ensemble),89.142,90.52,3.581,0.2683,98.48,0.89,Accuracy
188,1,Face Verification,MegaFace - Face Verification benchmarking,2018-01,ArcFace + MS1MV2 + R100 + R,98.48,100.0,9.338,0.6996,98.48,0.99,Accuracy
189,1,Age-Invariant Face Recognition,CAFR - Age-Invariant Face Recognition benchmarking,2015-11,Light CNN,73.56,86.74,73.56,100,84.81,0.74,Accuracy
190,1,Age-Invariant Face Recognition,CAFR - Age-Invariant Face Recognition benchmarking,2018-09,AIM,84.81,100.0,11.25,1.0,84.81,0.85,Accuracy
191,1,Group Activity Recognition,Collective Activity - Group Activity Recognition benchmarking,2015-11,Deng et al.,81.2,89.23,81.2,100,91.0,0.81,Accuracy
192,1,Group Activity Recognition,Collective Activity - Group Activity Recognition benchmarking,2018-11,H-LSTCM,83.75,92.03,2.55,0.2602,91.0,0.84,Accuracy
193,1,Group Activity Recognition,Collective Activity - Group Activity Recognition benchmarking,2019-04,GT (Inception-v3),91.0,100.0,7.25,0.7398,91.0,0.91,Accuracy
194,1,Image Clustering,YouTube Faces DB - Image Clustering benchmarking,2015-11,DEC (KL based),0.371,60.72,0.371,100,0.611,0.0,Accuracy
195,1,Image Clustering,YouTube Faces DB - Image Clustering benchmarking,2017-04,DEPICT,0.611,100.0,0.24,1.0,0.611,0.01,Accuracy
196,1,Image Clustering,CMU-PIE - Image Clustering benchmarking,2015-11,DEC (KL based),0.801,88.8,0.801,100,0.902,0.01,Accuracy
197,1,Image Clustering,CMU-PIE - Image Clustering benchmarking,2017-04,DEPICT,0.85,94.24,0.049,0.4851,0.902,0.01,Accuracy
198,1,Image Clustering,CMU-PIE - Image Clustering benchmarking,2018-10,SR-K-means,0.902,100.0,0.052,0.5149,0.902,0.01,Accuracy
199,1,Aesthetics Quality Assessment,AVA - Aesthetics Quality Assessment benchmarking,2015-12,DMA-Net,75.4,90.84,75.4,100,83.0,0.75,Accuracy
200,1,Aesthetics Quality Assessment,AVA - Aesthetics Quality Assessment benchmarking,2016-04,MTRLCNN,79.1,95.3,3.7,0.4868,83.0,0.79,Accuracy
201,1,Aesthetics Quality Assessment,AVA - Aesthetics Quality Assessment benchmarking,2017-04,A-Lamp,82.5,99.4,3.4,0.4474,83.0,0.83,Accuracy
202,1,Aesthetics Quality Assessment,AVA - Aesthetics Quality Assessment benchmarking,2018-10,MP_adam,83.0,100.0,0.5,0.0658,83.0,0.83,Accuracy
203,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2015-12,STAPLE_CA,53.59,73.11,53.59,100,73.3,0.54,Accuracy
204,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2016-11,ECO,56.13,76.58,2.54,0.1289,73.3,0.56,Accuracy
205,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2018-11,ATOM,70.34,95.96,14.21,0.721,73.3,0.7,Accuracy
206,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2018-12,SiamRPN++,73.3,100.0,2.96,0.1502,73.3,0.73,Accuracy
207,1,Fine-Grained Image Classification,Stanford Cars - Fine-Grained Image Classification benchmarking,2015-12,ResNet-101,91.2,94.8,91.2,100,96.2,0.91,Accuracy
208,1,Fine-Grained Image Classification,Stanford Cars - Fine-Grained Image Classification benchmarking,2016-11,DFL-CNN,93.8,97.51,2.6,0.52,96.2,0.94,Accuracy
209,1,Fine-Grained Image Classification,Stanford Cars - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,94.8,98.54,1.0,0.2,96.2,0.95,Accuracy
210,1,Fine-Grained Image Classification,Stanford Cars - Fine-Grained Image Classification benchmarking,2018-11,DAT,96.2,100.0,1.4,0.28,96.2,0.96,Accuracy
211,1,Smile Recognition,DISFA - Smile Recognition benchmarking,2016-01,Deep CNN,99.45,100.0,99.45,100,99.45,0.99,Accuracy
212,1,Few-Shot Image Classification,SUN - 0-Shot - Few-Shot Image Classification benchmarking,2016-03,Synthesised Classifier,62.7,100.0,62.7,100,62.7,0.63,Accuracy
213,1,Few-Shot Image Classification,AWA - 0-Shot - Few-Shot Image Classification benchmarking,2016-03,Synthesised Classifier,72.9,100.0,72.9,100,72.9,0.73,Accuracy
214,1,Image Classification,Kuzushiji-MNIST - Image Classification benchmarking,2016-03,PreActResNet-18,97.82,98.8,97.82,100,99.01,0.98,Accuracy
215,1,Image Classification,Kuzushiji-MNIST - Image Classification benchmarking,2017-10,PreActResNet-18 + Input Mixup,98.41,99.39,0.59,0.4958,99.01,0.98,Accuracy
216,1,Image Classification,Kuzushiji-MNIST - Image Classification benchmarking,2019-01,VGG8B(2x) + LocalLearning + CO,99.01,100.0,0.6,0.5042,99.01,0.99,Accuracy
217,1,3D Object Classification,ModelNet10 - 3D Object Classification benchmarking,2016-04,ORION,93.8,100.0,93.8,100,93.8,0.94,Accuracy
218,1,Face Verification,Trillion Pairs Dataset - Face Verification benchmarking,2016-04,HM-Softmax,34.46,47.39,34.46,100,72.71,0.34,Accuracy
219,1,Face Verification,Trillion Pairs Dataset - Face Verification benchmarking,2017-04,A-Softmax,43.76,60.18,9.3,0.2431,72.71,0.44,Accuracy
220,1,Face Verification,Trillion Pairs Dataset - Face Verification benchmarking,2018-01,AM-Softmax,61.61,84.73,17.85,0.4667,72.71,0.62,Accuracy
221,1,Face Verification,Trillion Pairs Dataset - Face Verification benchmarking,2018-12,SV-AM-Softmax,72.71,100.0,11.1,0.2902,72.71,0.73,Accuracy
222,1,Face Identification,Trillion Pairs Dataset - Face Identification benchmarking,2016-04,HM-Softmax,36.75,49.96,36.75,100,73.56,0.37,Accuracy
223,1,Face Identification,Trillion Pairs Dataset - Face Identification benchmarking,2017-04,A-Softmax,43.89,59.67,7.14,0.194,73.56,0.44,Accuracy
224,1,Face Identification,Trillion Pairs Dataset - Face Identification benchmarking,2018-01,AM-Softmax,61.8,84.01,17.91,0.4866,73.56,0.62,Accuracy
225,1,Face Identification,Trillion Pairs Dataset - Face Identification benchmarking,2018-12,SV-AM-Softmax,73.56,100.0,11.76,0.3195,73.56,0.74,Accuracy
226,1,3D Object Recognition,ModelNet40 - 3D Object Recognition benchmarking,2016-04,MVCNN-MultiRes,93.8,100.0,93.8,100,93.8,0.94,Accuracy
227,1,Image Clustering,CUB Birds - Image Clustering benchmarking,2016-04,JULE,0.044,34.92,0.044,100,0.126,0.0,Accuracy
228,1,Image Clustering,CUB Birds - Image Clustering benchmarking,2017-04,DEPICT-Large,0.061,48.41,0.017,0.2073,0.126,0.0,Accuracy
229,1,Image Clustering,CUB Birds - Image Clustering benchmarking,2018-11,FineGAN,0.126,100.0,0.065,0.7927,0.126,0.0,Accuracy
230,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2016-04,JULE,0.043,54.43,0.043,100,0.079,0.0,Accuracy
231,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2017-04,DEPICT,0.052,65.82,0.009,0.25,0.079,0.0,Accuracy
232,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2017-04,DEPICT-Large,0.054,68.35,0.002,0.0556,0.079,0.0,Accuracy
233,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2018-11,FineGAN,0.079,100.0,0.025,0.6944,0.079,0.0,Accuracy
234,1,Image Clustering,Stanford Cars - Image Clustering benchmarking,2016-04,JULE,0.046,58.97,0.046,100,0.078,0.0,Accuracy
235,1,Image Clustering,Stanford Cars - Image Clustering benchmarking,2017-04,DEPICT-Large,0.062,79.49,0.016,0.5,0.078,0.0,Accuracy
236,1,Image Clustering,Stanford Cars - Image Clustering benchmarking,2017-04,DEPICT,0.063,80.77,0.001,0.0312,0.078,0.0,Accuracy
237,1,Image Clustering,Stanford Cars - Image Clustering benchmarking,2018-11,FineGAN,0.078,100.0,0.015,0.4688,0.078,0.0,Accuracy
238,1,Few-Shot Image Classification,Flowers-102 - 0-Shot - Few-Shot Image Classification benchmarking,2016-05,Word CNN-RNN (DS-SJE Embedding),65.6,100.0,65.6,100,65.6,0.66,Accuracy
239,1,Domain Adaptation,VisDA2017 - Domain Adaptation benchmarking,2016-05,JAN,58.3,66.86,58.3,100,87.2,0.58,Accuracy
240,1,Domain Adaptation,VisDA2017 - Domain Adaptation benchmarking,2017-05,CDAN,73.7,84.52,15.4,0.5329,87.2,0.74,Accuracy
241,1,Domain Adaptation,VisDA2017 - Domain Adaptation benchmarking,2018-11,IAFN,76.1,87.27,2.4,0.083,87.2,0.76,Accuracy
242,1,Domain Adaptation,VisDA2017 - Domain Adaptation benchmarking,2019-01,Contrastive Adaptation Network,87.2,100.0,11.1,0.3841,87.2,0.87,Accuracy
243,1,Domain Adaptation,HMDBfull-to-UCF - Domain Adaptation benchmarking,2016-05,JAN,79.69,97.43,79.69,100,81.79,0.8,Accuracy
244,1,Domain Adaptation,HMDBfull-to-UCF - Domain Adaptation benchmarking,2019-05,TA3N,81.79,100.0,2.1,1.0,81.79,0.82,Accuracy
245,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2016-06,MCB,48.69,100.0,48.69,100,48.69,0.49,Accuracy
246,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2016-06,MCB,64.7,90.12,64.7,100,71.79,0.65,Accuracy
247,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2017-04,"N2NMN (ResNet-152, policy search)",64.9,90.4,0.2,0.0282,71.79,0.65,Accuracy
248,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2017-05,MUTAN,67.42,93.91,2.52,0.3554,71.79,0.67,Accuracy
249,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2017-08,"Image features from bottom-up attention (adaptive K, ensemble)",69.87,97.33,2.45,0.3456,71.79,0.7,Accuracy
250,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2018-05,BAN+Glove+Counter,70.04,97.56,0.17,0.024,71.79,0.7,Accuracy
251,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-06,MCANed-6,70.63,98.38,0.59,0.0832,71.79,0.71,Accuracy
252,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-08,VisualBERT,70.8,98.62,0.17,0.024,71.79,0.71,Accuracy
253,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-08,VL-BERTBASE,71.16,99.12,0.36,0.0508,71.79,0.71,Accuracy
254,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-08,VL-BERTLARGE,71.79,100.0,0.63,0.0889,71.79,0.72,Accuracy
255,1,Phrase Grounding,ReferIt - Phrase Grounding benchmarking,2016-06,MCB,28.91,100.0,28.91,100,28.91,0.29,Accuracy
256,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 5-way - Few-Shot Image Classification benchmarking",2016-06,Neural Statistician,98.1,98.13,98.1,100,99.97,0.98,Accuracy
257,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 5-way - Few-Shot Image Classification benchmarking",2017-03,ConvNet with Memory Module,98.4,98.43,0.3,0.1604,99.97,0.98,Accuracy
258,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 5-way - Few-Shot Image Classification benchmarking",2017-03,MAML,98.7,98.73,0.3,0.1604,99.97,0.99,Accuracy
259,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 5-way - Few-Shot Image Classification benchmarking",2017-03,Prototypical Networks,98.8,98.83,0.1,0.0535,99.97,0.99,Accuracy
260,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 5-way - Few-Shot Image Classification benchmarking",2017-11,Relation Net,99.6,99.63,0.8,0.4278,99.97,1.0,Accuracy
261,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 5-way - Few-Shot Image Classification benchmarking",2019-02,MC2+,99.97,100.0,0.37,0.1979,99.97,1.0,Accuracy
262,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 20-way - Few-Shot Image Classification benchmarking",2016-06,Neural Statistician,98.1,98.44,98.1,100,99.65,0.98,Accuracy
263,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 20-way - Few-Shot Image Classification benchmarking",2016-06,Matching Nets,98.5,98.85,0.4,0.2581,99.65,0.99,Accuracy
264,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 20-way - Few-Shot Image Classification benchmarking",2017-03,ConvNet with Memory Module,98.6,98.95,0.1,0.0645,99.65,0.99,Accuracy
265,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 20-way - Few-Shot Image Classification benchmarking",2017-03,Prototypical Networks,98.9,99.25,0.3,0.1935,99.65,0.99,Accuracy
266,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 20-way - Few-Shot Image Classification benchmarking",2017-11,Relation Net,99.1,99.45,0.2,0.129,99.65,0.99,Accuracy
267,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 20-way - Few-Shot Image Classification benchmarking",2018-10,MAML++,99.33,99.68,0.23,0.1484,99.65,0.99,Accuracy
268,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 20-way - Few-Shot Image Classification benchmarking",2019-02,MC2+,99.65,100.0,0.32,0.2065,99.65,1.0,Accuracy
269,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 5-way - Few-Shot Image Classification benchmarking",2016-06,Neural Statistician,99.5,99.6,99.5,100,99.9,1.0,Accuracy
270,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 5-way - Few-Shot Image Classification benchmarking",2017-03,MAML,99.9,100.0,0.4,1.0,99.9,1.0,Accuracy
271,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2016-06,Neural Statistician,93.2,93.55,93.2,100,99.63,0.93,Accuracy
272,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2016-06,Matching Nets,93.8,94.15,0.6,0.0933,99.63,0.94,Accuracy
273,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2017-03,ConvNet with Memory Module,95.0,95.35,1.2,0.1866,99.63,0.95,Accuracy
274,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2017-03,Prototypical Networks,96.0,96.36,1.0,0.1555,99.63,0.96,Accuracy
275,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2017-11,Relation Net,97.6,97.96,1.6,0.2488,99.63,0.98,Accuracy
276,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2018-10,MAML++,97.65,98.01,0.05,0.0078,99.63,0.98,Accuracy
277,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2019-05,TapNet,98.07,98.43,0.42,0.0653,99.63,0.98,Accuracy
278,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 20-way - Few-Shot Image Classification benchmarking",2019-08,GCR,99.63,100.0,1.56,0.2426,99.63,1.0,Accuracy
279,1,Semi-Supervised Image Classification,"SVHN, 1000 labels - Semi-Supervised Image Classification benchmarking",2016-06,GAN,91.89,94.57,91.89,100,97.17,0.92,Accuracy
280,1,Semi-Supervised Image Classification,"SVHN, 1000 labels - Semi-Supervised Image Classification benchmarking",2016-10,Pi Model,95.58,98.36,3.69,0.6989,97.17,0.96,Accuracy
281,1,Semi-Supervised Image Classification,"SVHN, 1000 labels - Semi-Supervised Image Classification benchmarking",2017-03,Mean Teacher,96.05,98.85,0.47,0.089,97.17,0.96,Accuracy
282,1,Semi-Supervised Image Classification,"SVHN, 1000 labels - Semi-Supervised Image Classification benchmarking",2017-04,VAT+EntMin,96.14,98.94,0.09,0.017,97.17,0.96,Accuracy
283,1,Semi-Supervised Image Classification,"SVHN, 1000 labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,96.73,99.55,0.59,0.1117,97.17,0.97,Accuracy
284,1,Semi-Supervised Image Classification,"SVHN, 1000 labels - Semi-Supervised Image Classification benchmarking",2019-11,ReMixMatch,97.17,100.0,0.44,0.0833,97.17,0.97,Accuracy
285,1,Few-Shot Image Classification,Meta-Dataset - Few-Shot Image Classification benchmarking,2016-06,Matching Networks,56.247,92.86,56.247,100,60.573,0.56,Accuracy
286,1,Few-Shot Image Classification,Meta-Dataset - Few-Shot Image Classification benchmarking,2017-03,fo-MAML,57.024,94.14,0.777,0.1796,60.573,0.57,Accuracy
287,1,Few-Shot Image Classification,Meta-Dataset - Few-Shot Image Classification benchmarking,2017-03,Prototypical Networks,60.573,100.0,3.549,0.8204,60.573,0.61,Accuracy
288,1,Semi-Supervised Image Classification,"cifar-100, 10000 Labels - Semi-Supervised Image Classification benchmarking",2016-06,Ⅱ-Model,60.81,90.45,60.81,100,67.23,0.61,Accuracy
289,1,Semi-Supervised Image Classification,"cifar-100, 10000 Labels - Semi-Supervised Image Classification benchmarking",2016-10,Temporal ensembling,61.35,91.25,0.54,0.0841,67.23,0.61,Accuracy
290,1,Semi-Supervised Image Classification,"cifar-100, 10000 Labels - Semi-Supervised Image Classification benchmarking",2019-09,Dual Student (480),67.23,100.0,5.88,0.9159,67.23,0.67,Accuracy
291,1,Semi-Supervised Image Classification,"SVHN, 250 Labels - Semi-Supervised Image Classification benchmarking",2016-06,Ⅱ-model,82.35,85.36,82.35,100,96.47,0.82,Accuracy
292,1,Semi-Supervised Image Classification,"SVHN, 250 Labels - Semi-Supervised Image Classification benchmarking",2017-03,MeanTeacher,93.55,96.97,11.2,0.7932,96.47,0.94,Accuracy
293,1,Semi-Supervised Image Classification,"SVHN, 250 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,96.22,99.74,2.67,0.1891,96.47,0.96,Accuracy
294,1,Semi-Supervised Image Classification,"SVHN, 250 Labels - Semi-Supervised Image Classification benchmarking",2019-12,RealMix,96.47,100.0,0.25,0.0177,96.47,0.96,Accuracy
295,1,Skeleton Based Action Recognition,SBU - Skeleton Based Action Recognition benchmarking,2016-06,ChebyNet,96.0,96.95,96.0,100,99.02,0.96,Accuracy
296,1,Skeleton Based Action Recognition,SBU - Skeleton Based Action Recognition benchmarking,2017-03,Joint Line Distance,99.02,100.0,3.02,1.0,99.02,0.99,Accuracy
297,1,Facial Expression Recognition,Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking,2016-10,VGG-VD-16,54.82,94.29,54.82,100,58.14,0.55,Accuracy
298,1,Facial Expression Recognition,Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking,2018-05,Covariance Pooling,58.14,100.0,3.32,1.0,58.14,0.58,Accuracy
299,1,Semi-Supervised Image Classification,"CIFAR-10, 250 Labels - Semi-Supervised Image Classification benchmarking",2016-10,Ⅱ-Model,46.88,50.02,46.88,100,93.73,0.47,Accuracy
300,1,Semi-Supervised Image Classification,"CIFAR-10, 250 Labels - Semi-Supervised Image Classification benchmarking",2017-03,MeanTeacher,52.68,56.2,5.8,0.1238,93.73,0.53,Accuracy
301,1,Semi-Supervised Image Classification,"CIFAR-10, 250 Labels - Semi-Supervised Image Classification benchmarking",2017-04,VAT,63.97,68.25,11.29,0.241,93.73,0.64,Accuracy
302,1,Semi-Supervised Image Classification,"CIFAR-10, 250 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,88.92,94.87,24.95,0.5326,93.73,0.89,Accuracy
303,1,Semi-Supervised Image Classification,"CIFAR-10, 250 Labels - Semi-Supervised Image Classification benchmarking",2019-11,ReMixMatch,93.73,100.0,4.81,0.1027,93.73,0.94,Accuracy
304,1,Action Recognition,Volleyball - Action Recognition benchmarking,2016-11,GTT (VGG19),82.6,100.0,82.6,100,82.6,0.83,Accuracy
305,1,Fine-Grained Image Classification,FGVC Aircraft - Fine-Grained Image Classification benchmarking,2016-11,DFB-CNN,92.0,97.98,92.0,100,93.9,0.92,Accuracy
306,1,Fine-Grained Image Classification,FGVC Aircraft - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,92.67,98.69,0.67,0.3526,93.9,0.93,Accuracy
307,1,Fine-Grained Image Classification,FGVC Aircraft - Fine-Grained Image Classification benchmarking,2019-01,WS-DAN,93.0,99.04,0.33,0.1737,93.9,0.93,Accuracy
308,1,Fine-Grained Image Classification,FGVC Aircraft - Fine-Grained Image Classification benchmarking,2019-10,BCN,93.5,99.57,0.5,0.2632,93.9,0.94,Accuracy
309,1,Fine-Grained Image Classification,FGVC Aircraft - Fine-Grained Image Classification benchmarking,2020-02,API-Net,93.9,100.0,0.4,0.2105,93.9,0.94,Accuracy
310,1,Skeleton Based Action Recognition,SYSU 3D - Skeleton Based Action Recognition benchmarking,2016-12,Dynamic Skeletons,75.5,86.88,75.5,100,86.9,0.76,Accuracy
311,1,Skeleton Based Action Recognition,SYSU 3D - Skeleton Based Action Recognition benchmarking,2017-03,VA-LSTM,77.5,89.18,2.0,0.1754,86.9,0.78,Accuracy
312,1,Skeleton Based Action Recognition,SYSU 3D - Skeleton Based Action Recognition benchmarking,2018-04,VA-fusion (aug.),86.7,99.77,9.2,0.807,86.9,0.87,Accuracy
313,1,Skeleton Based Action Recognition,SYSU 3D - Skeleton Based Action Recognition benchmarking,2019-04,SGN,86.9,100.0,0.2,0.0175,86.9,0.87,Accuracy
314,1,Hand Gesture Recognition,ChaLearn val - Hand Gesture Recognition benchmarking,2017-01,Wang et al.,39.23,68.34,39.23,100,57.4,0.39,Accuracy
315,1,Hand Gesture Recognition,ChaLearn val - Hand Gesture Recognition benchmarking,2018-04,8-MFFs-3f1c (5 crop),57.4,100.0,18.17,1.0,57.4,0.57,Accuracy
316,1,Facial Expression Recognition,Cohn-Kanade - Facial Expression Recognition benchmarking,2017-01,Sequential forward selection,88.7,100.0,88.7,100,88.7,0.89,Accuracy
317,1,Domain Adaptation,SVHN-to-MNIST - Domain Adaptation benchmarking,2017-02,ADDN,80.1,82.15,80.1,100,97.5,0.8,Accuracy
318,1,Domain Adaptation,SVHN-to-MNIST - Domain Adaptation benchmarking,2017-05,CDAN,89.2,91.49,9.1,0.523,97.5,0.89,Accuracy
319,1,Domain Adaptation,SVHN-to-MNIST - Domain Adaptation benchmarking,2017-11,CYCADA,90.4,92.72,1.2,0.069,97.5,0.9,Accuracy
320,1,Domain Adaptation,SVHN-to-MNIST - Domain Adaptation benchmarking,2017-12,MCD,95.8,98.26,5.4,0.3103,97.5,0.96,Accuracy
321,1,Domain Adaptation,SVHN-to-MNIST - Domain Adaptation benchmarking,2019-11,CyCleGAN (Light-weight Calibrator),97.5,100.0,1.7,0.0977,97.5,0.98,Accuracy
322,1,Domain Adaptation,MNIST-to-USPS - Domain Adaptation benchmarking,2017-02,ADDN,90.1,92.79,90.1,100,97.1,0.9,Accuracy
323,1,Domain Adaptation,MNIST-to-USPS - Domain Adaptation benchmarking,2017-12,MCD,93.8,96.6,3.7,0.5286,97.1,0.94,Accuracy
324,1,Domain Adaptation,MNIST-to-USPS - Domain Adaptation benchmarking,2019-03,rRevGrad+CAT,96.0,98.87,2.2,0.3143,97.1,0.96,Accuracy
325,1,Domain Adaptation,MNIST-to-USPS - Domain Adaptation benchmarking,2019-09,3CATN,96.1,98.97,0.1,0.0143,97.1,0.96,Accuracy
326,1,Domain Adaptation,MNIST-to-USPS - Domain Adaptation benchmarking,2019-11,CyCleGAN (Light-weight Calibrator),97.1,100.0,1.0,0.1429,97.1,0.97,Accuracy
327,1,Image Clustering,USPS - Image Clustering benchmarking,2017-03,DBC,0.743,75.74,0.743,100,0.981,0.01,Accuracy
328,1,Image Clustering,USPS - Image Clustering benchmarking,2018-04,DMSC,0.951,96.94,0.208,0.8739,0.981,0.01,Accuracy
329,1,Image Clustering,USPS - Image Clustering benchmarking,2018-10,SR-K-means,0.974,99.29,0.023,0.0966,0.981,0.01,Accuracy
330,1,Image Clustering,USPS - Image Clustering benchmarking,2018-12,DDC-DA,0.977,99.59,0.003,0.0126,0.981,0.01,Accuracy
331,1,Image Clustering,USPS - Image Clustering benchmarking,2019-01,DynAE,0.981,100.0,0.004,0.0168,0.981,0.01,Accuracy
332,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2017-03,BB8,83.9,84.57,83.9,100,99.21,0.84,Accuracy
333,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2017-11,Single-shot Deep CNN,90.37,91.09,6.47,0.4226,99.21,0.9,Accuracy
334,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2018-03,PoseCNN + DeepIM,97.5,98.28,7.13,0.4657,99.21,0.98,Accuracy
335,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2018-12,PVNet,99.0,99.79,1.5,0.098,99.21,0.99,Accuracy
336,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2019-09,HRNet+DSNT+BPnP,99.21,100.0,0.21,0.0137,99.21,0.99,Accuracy
337,1,3D Object Classification,ModelNet40 - 3D Object Classification benchmarking,2017-04,ECC (12 votes),83.2,100.0,83.2,100,83.2,0.83,Accuracy
338,1,Group Activity Recognition,Volleyball - Group Activity Recognition benchmarking,2017-04,Shu et al.,83.6,90.28,83.6,100,92.6,0.84,Accuracy
339,1,Group Activity Recognition,Volleyball - Group Activity Recognition benchmarking,2018-11,H-LSTCM,88.4,95.46,4.8,0.5333,92.6,0.88,Accuracy
340,1,Group Activity Recognition,Volleyball - Group Activity Recognition benchmarking,2019-04,GTT (VGG19),92.6,100.0,4.2,0.4667,92.6,0.93,Accuracy
341,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2017-04,ST-VQA,0.309,86.8,0.309,100,0.356,0.0,Accuracy
342,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2018-03,Co-Mem,0.32,89.89,0.011,0.234,0.356,0.0,Accuracy
343,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2019-04,HMEMA,0.33,92.7,0.01,0.2128,0.356,0.0,Accuracy
344,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2020-02,HCRN,0.356,100.0,0.026,0.5532,0.356,0.0,Accuracy
345,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2017-04,ST-VQA,0.313,86.7,0.313,100,0.361,0.0,Accuracy
346,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2018-03,Co-Mem,0.317,87.81,0.004,0.0833,0.361,0.0,Accuracy
347,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2019-04,HMEMA,0.337,93.35,0.02,0.4167,0.361,0.0,Accuracy
348,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2020-02,HCRN,0.361,100.0,0.024,0.5,0.361,0.0,Accuracy
349,1,Skeleton Based Action Recognition,Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking,2017-04,Res-TCN,20.3,53.7,20.3,100,37.8,0.2,Accuracy
350,1,Skeleton Based Action Recognition,Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking,2018-01,ST-GCN,30.7,81.22,10.4,0.5943,37.8,0.31,Accuracy
351,1,Skeleton Based Action Recognition,Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking,2018-05,2s-AGCN,36.1,95.5,5.4,0.3086,37.8,0.36,Accuracy
352,1,Skeleton Based Action Recognition,Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking,2018-11,SLnL-rFA,36.6,96.83,0.5,0.0286,37.8,0.37,Accuracy
353,1,Skeleton Based Action Recognition,Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking,2019-06,DGNN,36.9,97.62,0.3,0.0171,37.8,0.37,Accuracy
354,1,Skeleton Based Action Recognition,Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking,2019-11,GCN-NAS,37.1,98.15,0.2,0.0114,37.8,0.37,Accuracy
355,1,Skeleton Based Action Recognition,Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking,2019-12,MS-AAGCN,37.8,100.0,0.7,0.04,37.8,0.38,Accuracy
356,1,Image Clustering,FRGC - Image Clustering benchmarking,2017-04,DEPICT,0.432,100.0,0.432,100,0.432,0.0,Accuracy
357,1,Hand Gesture Recognition,EgoGesture - Hand Gesture Recognition benchmarking,2017-05,I3D,92.78,98.67,92.78,100,94.03,0.93,Accuracy
358,1,Hand Gesture Recognition,EgoGesture - Hand Gesture Recognition benchmarking,2018-12,MTUT,93.87,99.83,1.09,0.872,94.03,0.94,Accuracy
359,1,Hand Gesture Recognition,EgoGesture - Hand Gesture Recognition benchmarking,2019-01,ResNeXt-101,94.03,100.0,0.16,0.128,94.03,0.94,Accuracy
360,1,Fine-Grained Image Classification,NABirds - Fine-Grained Image Classification benchmarking,2017-05,PC-DenseNet-161,82.79,92.81,82.79,100,89.2,0.83,Accuracy
361,1,Fine-Grained Image Classification,NABirds - Fine-Grained Image Classification benchmarking,2018-01,PAIRS,87.9,98.54,5.11,0.7972,89.2,0.88,Accuracy
362,1,Fine-Grained Image Classification,NABirds - Fine-Grained Image Classification benchmarking,2019-06,FixSENet-154,89.2,100.0,1.3,0.2028,89.2,0.89,Accuracy
363,1,Fine-Grained Image Classification,Stanford Dogs - Fine-Grained Image Classification benchmarking,2017-05,PC-DenseNet-161,83.75,92.75,83.75,100,90.3,0.84,Accuracy
364,1,Fine-Grained Image Classification,Stanford Dogs - Fine-Grained Image Classification benchmarking,2019-12,DB,87.7,97.12,3.95,0.6031,90.3,0.88,Accuracy
365,1,Fine-Grained Image Classification,Stanford Dogs - Fine-Grained Image Classification benchmarking,2020-02,API-Net,90.3,100.0,2.6,0.3969,90.3,0.9,Accuracy
366,1,Fine-Grained Image Classification,Oxford 102 Flowers - Fine-Grained Image Classification benchmarking,2017-05,PC Bilinear CNN,93.65,94.0,93.65,100,99.63,0.94,Accuracy
367,1,Fine-Grained Image Classification,Oxford 102 Flowers - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,95.36,95.71,1.71,0.286,99.63,0.95,Accuracy
368,1,Fine-Grained Image Classification,Oxford 102 Flowers - Fine-Grained Image Classification benchmarking,2019-06,FixInceptionResNet-V2,95.7,96.06,0.34,0.0569,99.63,0.96,Accuracy
369,1,Fine-Grained Image Classification,Oxford 102 Flowers - Fine-Grained Image Classification benchmarking,2019-12,BiT-M (ResNet),99.3,99.67,3.6,0.602,99.63,0.99,Accuracy
370,1,Fine-Grained Image Classification,Oxford 102 Flowers - Fine-Grained Image Classification benchmarking,2019-12,BiT-L (ResNet),99.63,100.0,0.33,0.0552,99.63,1.0,Accuracy
371,1,Domain Adaptation,USPS-to-MNIST - Domain Adaptation benchmarking,2017-05,CDAN,98.0,99.69,98.0,100,98.3,0.98,Accuracy
372,1,Domain Adaptation,USPS-to-MNIST - Domain Adaptation benchmarking,2019-09,3CATN,98.3,100.0,0.3,1.0,98.3,0.98,Accuracy
373,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,56.32,86.31,56.32,100,65.25,0.56,Accuracy
374,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-06,CMN,56.56,86.68,0.24,0.0269,65.25,0.57,Accuracy
375,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-10,ICON,59.09,90.56,2.53,0.2833,65.25,0.59,Accuracy
376,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,63.4,97.16,4.31,0.4826,65.25,0.63,Accuracy
377,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,65.25,100.0,1.85,0.2072,65.25,0.65,Accuracy
378,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,57.5,96.57,57.5,100,59.54,0.58,Accuracy
379,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,59.54,100.0,2.04,1.0,59.54,0.6,Accuracy
380,1,Hand Gesture Recognition,SmartWatch - Hand Gesture Recognition benchmarking,2017-07,F-BGRU,97.4,100.0,97.4,100,97.4,0.97,Accuracy
381,1,Hand Gesture Recognition,MGB - Hand Gesture Recognition benchmarking,2017-07,F-BLSTM,98.04,100.0,98.04,100,98.04,0.98,Accuracy
382,1,Hand Gesture Recognition,BUAA - Hand Gesture Recognition benchmarking,2017-07,F-BGRU,99.25,100.0,99.25,100,99.25,0.99,Accuracy
383,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,49.74,79.32,49.74,100,62.71,0.5,Accuracy
384,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",60.33,96.2,10.59,0.8165,62.71,0.6,Accuracy
385,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",62.71,100.0,2.38,0.1835,62.71,0.63,Accuracy
386,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,40.4,57.14,40.4,100,70.7,0.4,Accuracy
387,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,45.8,64.78,5.4,0.1782,70.7,0.46,Accuracy
388,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,67.9,96.04,22.1,0.7294,70.7,0.68,Accuracy
389,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,70.7,100.0,2.8,0.0924,70.7,0.71,Accuracy
390,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,68.6,82.75,68.6,100,82.9,0.69,Accuracy
391,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,71.6,86.37,3.0,0.2098,82.9,0.72,Accuracy
392,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2018-04,SIMS,74.7,90.11,3.1,0.2168,82.9,0.75,Accuracy
393,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,82.9,100.0,8.2,0.5734,82.9,0.83,Accuracy
394,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,68.8,82.99,68.8,100,82.9,0.69,Accuracy
395,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,69.2,83.47,0.4,0.0284,82.9,0.69,Accuracy
396,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,79.9,96.38,10.7,0.7589,82.9,0.8,Accuracy
397,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,82.9,100.0,3.0,0.2128,82.9,0.83,Accuracy
398,1,Lane Detection,TuSimple - Lane Detection benchmarking,2017-08,Discriminative loss function,96.4,99.75,96.4,100,96.64,0.96,Accuracy
399,1,Lane Detection,TuSimple - Lane Detection benchmarking,2017-12,Spatial CNN,96.53,99.89,0.13,0.5417,96.64,0.97,Accuracy
400,1,Lane Detection,TuSimple - Lane Detection benchmarking,2019-08,ENet-SAD,96.64,100.0,0.11,0.4583,96.64,0.97,Accuracy
401,1,Semi-Supervised Image Classification,STL-10 - Semi-Supervised Image Classification benchmarking,2017-08,CutOut,87.26,98.27,87.26,100,88.8,0.87,Accuracy
402,1,Semi-Supervised Image Classification,STL-10 - Semi-Supervised Image Classification benchmarking,2018-07,IIC,88.8,100.0,1.54,1.0,88.8,0.89,Accuracy
403,1,Face Identification,IJB-B - Face Identification benchmarking,2017-08,FPN,91.1,100.0,91.1,100,91.1,0.91,Accuracy
404,1,Face Identification,IJB-A - Face Identification benchmarking,2017-08,FPN,91.4,96.62,91.4,100,94.6,0.91,Accuracy
405,1,Face Identification,IJB-A - Face Identification benchmarking,2018-03,Deep Residual Equivariant Mapping,94.6,100.0,3.2,1.0,94.6,0.95,Accuracy
406,1,Video Story QA,MovieQA - Video Story QA benchmarking,2017-09,RWMN,36.25,85.23,36.25,100,42.53,0.36,Accuracy
407,1,Video Story QA,MovieQA - Video Story QA benchmarking,2019-04,PAMN,42.53,100.0,6.28,1.0,42.53,0.43,Accuracy
408,1,Pedestrian Attribute Recognition,RAP - Pedestrian Attribute Recognition benchmarking,2017-09,HP-net,65.39,95.92,65.39,100,68.17,0.65,Accuracy
409,1,Pedestrian Attribute Recognition,RAP - Pedestrian Attribute Recognition benchmarking,2019-10,Attribute-Specific Localization,68.17,100.0,2.78,1.0,68.17,0.68,Accuracy
410,1,Pedestrian Attribute Recognition,PETA - Pedestrian Attribute Recognition benchmarking,2017-09,HP-net,76.13,95.74,76.13,100,79.52,0.76,Accuracy
411,1,Pedestrian Attribute Recognition,PETA - Pedestrian Attribute Recognition benchmarking,2019-10,Attribute-Specific Localization,79.52,100.0,3.39,1.0,79.52,0.8,Accuracy
412,1,Pedestrian Attribute Recognition,PA-100K - Pedestrian Attribute Recognition benchmarking,2017-09,HP-net,72.19,93.66,72.19,100,77.08,0.72,Accuracy
413,1,Pedestrian Attribute Recognition,PA-100K - Pedestrian Attribute Recognition benchmarking,2019-10,Attribute-Specific Localization,77.08,100.0,4.89,1.0,77.08,0.77,Accuracy
414,1,Facial Expression Recognition,SFEW - Facial Expression Recognition benchmarking,2017-10,Island Loss,52.52,93.12,52.52,100,56.4,0.53,Accuracy
415,1,Facial Expression Recognition,SFEW - Facial Expression Recognition benchmarking,2019-05,RAN (VGG16+ResNet18),56.4,100.0,3.88,1.0,56.4,0.56,Accuracy
416,1,Image Classification,EMNIST-Balanced - Image Classification benchmarking,2017-10,TextCaps,90.46,100.0,90.46,100,90.46,0.9,Accuracy
417,1,Few-Shot Image Classification,CUB 200 5-way 5-shot - Few-Shot Image Classification benchmarking,2017-11,Relation Net,65.32,71.9,65.32,100,90.85,0.65,Accuracy
418,1,Few-Shot Image Classification,CUB 200 5-way 5-shot - Few-Shot Image Classification benchmarking,2019-03,DN4-DA (k=1),81.9,90.15,16.58,0.6494,90.85,0.82,Accuracy
419,1,Few-Shot Image Classification,CUB 200 5-way 5-shot - Few-Shot Image Classification benchmarking,2019-05,High-End MAML++,83.8,92.24,1.9,0.0744,90.85,0.84,Accuracy
420,1,Few-Shot Image Classification,CUB 200 5-way 5-shot - Few-Shot Image Classification benchmarking,2019-05,Self-Critique and Adapt + High-End MAML++,85.63,94.25,1.83,0.0717,90.85,0.86,Accuracy
421,1,Few-Shot Image Classification,CUB 200 5-way 5-shot - Few-Shot Image Classification benchmarking,2019-07,S2M2R,90.85,100.0,5.22,0.2045,90.85,0.91,Accuracy
422,1,Few-Shot Image Classification,CUB 200 5-way 1-shot - Few-Shot Image Classification benchmarking,2017-11,Relation Net,50.44,62.52,50.44,100,80.68,0.5,Accuracy
423,1,Few-Shot Image Classification,CUB 200 5-way 1-shot - Few-Shot Image Classification benchmarking,2018-06,Delta-encoder,69.8,86.51,19.36,0.6402,80.68,0.7,Accuracy
424,1,Few-Shot Image Classification,CUB 200 5-way 1-shot - Few-Shot Image Classification benchmarking,2019-05,Self-Critique and Adapt + High-End MAML++,70.46,87.33,0.66,0.0218,80.68,0.7,Accuracy
425,1,Few-Shot Image Classification,CUB 200 5-way 1-shot - Few-Shot Image Classification benchmarking,2019-07,S2M2R,80.68,100.0,10.22,0.338,80.68,0.81,Accuracy
426,1,Image Classification,Clothing1M - Image Classification benchmarking,2017-11,"CLeanNet, w_{soft}",74.69,100.0,74.69,100,74.69,0.75,Accuracy
427,1,Image Classification,Food-101N - Image Classification benchmarking,2017-11,CleanNet,90.39,100.0,90.39,100,90.39,0.9,Accuracy
428,1,Gesture Recognition,Chalearn 2014 - Gesture Recognition benchmarking,2017-12,3D-CNN + LSTM,93.2,100.0,93.2,100,93.2,0.93,Accuracy
429,1,Action Recognition,IRD - Action Recognition benchmarking,2018-01,ST-GCN,74.03,92.41,74.03,100,80.11,0.74,Accuracy
430,1,Action Recognition,IRD - Action Recognition benchmarking,2019-01,OHA-GCN (Two stream; HP + OHP-hands + informative samples),80.11,100.0,6.08,1.0,80.11,0.8,Accuracy
431,1,Action Recognition,ICVL-4 - Action Recognition benchmarking,2018-01,ST-GCN,80.23,87.34,80.23,100,91.86,0.8,Accuracy
432,1,Action Recognition,ICVL-4 - Action Recognition benchmarking,2019-01,OHA-GCN (Two stream; HP + OHP-hands + informative samples),91.86,100.0,11.63,1.0,91.86,0.92,Accuracy
433,1,Unsupervised Image Classification,MNIST - Unsupervised Image Classification benchmarking,2018-02,ACOL + GAR + k-means,98.32,100.0,98.32,100,98.32,0.98,Accuracy
434,1,Skeleton Based Action Recognition,N-UCLA - Skeleton Based Action Recognition benchmarking,2018-02,Glimpse Clouds,87.6,94.7,87.6,100,92.5,0.88,Accuracy
435,1,Skeleton Based Action Recognition,N-UCLA - Skeleton Based Action Recognition benchmarking,2018-04,VA-fusion (aug.),88.1,95.24,0.5,0.102,92.5,0.88,Accuracy
436,1,Skeleton Based Action Recognition,N-UCLA - Skeleton Based Action Recognition benchmarking,2018-12,Action Machine,92.3,99.78,4.2,0.8571,92.5,0.92,Accuracy
437,1,Skeleton Based Action Recognition,N-UCLA - Skeleton Based Action Recognition benchmarking,2019-04,SGN,92.5,100.0,0.2,0.0408,92.5,0.93,Accuracy
438,1,Image Clustering,ARL Polarimetric Thermal Face Dataset - Image Clustering benchmarking,2018-04,DMSC,0.983,100.0,0.983,100,0.983,0.01,Accuracy
439,1,Multi-view Subspace Clustering,ORL - Multi-view Subspace Clustering benchmarking,2018-04,DMSC,0.833,95.75,0.833,100,0.87,0.01,Accuracy
440,1,Multi-view Subspace Clustering,ORL - Multi-view Subspace Clustering benchmarking,2019-08,MvDSCN,0.87,100.0,0.037,1.0,0.87,0.01,Accuracy
441,1,Multi-view Subspace Clustering,ARL Polarimetric Thermal Face Dataset - Multi-view Subspace Clustering benchmarking,2018-04,DMSC,0.988,100.0,0.988,100,0.988,0.01,Accuracy
442,1,Hand Gesture Recognition,ChaLean test - Hand Gesture Recognition benchmarking,2018-04,8-MFFs-3f1c,56.7,100.0,56.7,100,56.7,0.57,Accuracy
443,1,Hand Gesture Recognition,NVGesture - Hand Gesture Recognition benchmarking,2018-04,8-MFFs-3f1c,84.7,97.43,84.7,100,86.93,0.85,Accuracy
444,1,Hand Gesture Recognition,NVGesture - Hand Gesture Recognition benchmarking,2018-12,MTUT,86.93,100.0,2.23,1.0,86.93,0.87,Accuracy
445,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2018-05,UL-Hopfield (ULH),83.1,100.0,83.1,100,83.1,0.83,Accuracy
446,1,Semi-Supervised Image Classification,"Caltech-256, 1024 Labels - Semi-Supervised Image Classification benchmarking",2018-05,UL-Hopfield (ULH),77.4,100.0,77.4,100,77.4,0.77,Accuracy
447,1,Fine-Grained Image Classification,Caltech-101 - Fine-Grained Image Classification benchmarking,2018-05,UL-Hopfield (ULH),91.0,100.0,91,100,91,0.91,Accuracy
448,1,Facial Expression Recognition,Real-World Affective Faces - Facial Expression Recognition benchmarking,2018-05,Covariance Pooling,87.0,100.0,87,100,87,0.87,Accuracy
449,1,Fine-Grained Image Classification,Oxford-IIIT Pets - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,88.98,92.09,88.98,100,96.62,0.89,Accuracy
450,1,Fine-Grained Image Classification,Oxford-IIIT Pets - Fine-Grained Image Classification benchmarking,2019-06,FixSENet-154,94.8,98.12,5.82,0.7618,96.62,0.95,Accuracy
451,1,Fine-Grained Image Classification,Oxford-IIIT Pets - Fine-Grained Image Classification benchmarking,2019-12,BiT-L (ResNet),96.62,100.0,1.82,0.2382,96.62,0.97,Accuracy
452,1,Few-Shot Image Classification,Mini-ImageNet - 1-Shot Learning - Few-Shot Image Classification benchmarking,2018-06,PLATIPUS,50.13,74.6,50.13,100,67.2,0.5,Accuracy
453,1,Few-Shot Image Classification,Mini-ImageNet - 1-Shot Learning - Few-Shot Image Classification benchmarking,2019-03,DivCoop,63.73,94.84,13.6,0.7967,67.2,0.64,Accuracy
454,1,Few-Shot Image Classification,Mini-ImageNet - 1-Shot Learning - Few-Shot Image Classification benchmarking,2019-06,Multiple-semantics,67.2,100.0,3.47,0.2033,67.2,0.67,Accuracy
455,1,Document Image Classification,n-MNIST - Document Image Classification benchmarking,2018-06,Pixel-level RC,97.62,99.18,97.62,100,98.43,0.98,Accuracy
456,1,Document Image Classification,n-MNIST - Document Image Classification benchmarking,2019-08,PCGAN-CHAR,98.43,100.0,0.81,1.0,98.43,0.98,Accuracy
457,1,Document Image Classification,Noisy Bangla Numeral - Document Image Classification benchmarking,2018-06,Pixel-level RC,95.46,98.74,95.46,100,96.68,0.95,Accuracy
458,1,Document Image Classification,Noisy Bangla Numeral - Document Image Classification benchmarking,2019-08,PCGAN-CHAR,96.68,100.0,1.22,1.0,96.68,0.97,Accuracy
459,1,Document Image Classification,Noisy Bangla Characters - Document Image Classification benchmarking,2018-06,Pixel-level RC,77.22,86.24,77.22,100,89.54,0.77,Accuracy
460,1,Document Image Classification,Noisy Bangla Characters - Document Image Classification benchmarking,2019-08,PCGAN-CHAR,89.54,100.0,12.32,1.0,89.54,0.9,Accuracy
461,1,License Plate Recognition,Chinese License Plates - License Plate Recognition benchmarking,2018-06,LPRNet baseline,94.1,100.0,94.1,100,94.1,0.94,Accuracy
462,1,Unsupervised Image Classification,CIFAR-20 - Unsupervised Image Classification benchmarking,2018-07,IIC,25.7,100.0,25.7,100,25.7,0.26,Accuracy
463,1,Unsupervised Image Classification,STL-10 - Unsupervised Image Classification benchmarking,2018-07,IIC,61.0,100.0,61,100,61,0.61,Accuracy
464,1,Unsupervised Image Classification,CIFAR-10 - Unsupervised Image Classification benchmarking,2018-07,IIC,61.7,100.0,61.7,100,61.7,0.62,Accuracy
465,1,Unsupervised Semantic Segmentation,COCO-Stuff-3 - Unsupervised Semantic Segmentation benchmarking,2018-07,IIC,72.3,100.0,72.3,100,72.3,0.72,Accuracy
466,1,Unsupervised Semantic Segmentation,COCO-Stuff-15 - Unsupervised Semantic Segmentation benchmarking,2018-07,IIC,27.7,100.0,27.7,100,27.7,0.28,Accuracy
467,1,Unsupervised Semantic Segmentation,Potsdam - Unsupervised Semantic Segmentation benchmarking,2018-07,IIC,65.1,100.0,65.1,100,65.1,0.65,Accuracy
468,1,Unsupervised Semantic Segmentation,Potsdam-3 - Unsupervised Semantic Segmentation benchmarking,2018-07,IIC,45.4,100.0,45.4,100,45.4,0.45,Accuracy
469,1,Classification Of Hyperspectral Images,Pavia University - Classification Of Hyperspectral Images benchmarking,2018-07,WCRN,99.43,100.0,99.43,100,99.43,0.99,Accuracy
470,1,Visual Question Answering,CLEVR - Visual Question Answering benchmarking,2018-08,CNN + LSTM + RN + HAN,98.8,100.0,98.8,100,98.8,0.99,Accuracy
471,1,Facial Expression Recognition,FERPlus - Facial Expression Recognition benchmarking,2018-08,SENet Teacher,88.8,99.6,88.8,100,89.16,0.89,Accuracy
472,1,Facial Expression Recognition,FERPlus - Facial Expression Recognition benchmarking,2019-05,RAN (VGG-16),89.16,100.0,0.36,1.0,89.16,0.89,Accuracy
473,1,Age-Invariant Face Recognition,FG-NET - Age-Invariant Face Recognition benchmarking,2018-09,AIM,93.2,100.0,93.2,100,93.2,0.93,Accuracy
474,1,Image Classification,CINIC-10 - Image Classification benchmarking,2018-10,ResNeXt29_2x64d,91.45,100.0,91.45,100,91.45,0.91,Accuracy
475,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2018-10,SR-K-means,0.863,87.44,0.863,100,0.987,0.01,Accuracy
476,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2018-12,DDC-DA,0.97,98.28,0.107,0.8629,0.987,0.01,Accuracy
477,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2019-01,DynAE,0.987,100.0,0.017,0.1371,0.987,0.01,Accuracy
478,1,Visual Question Answering,HowmanyQA - Visual Question Answering benchmarking,2018-10,RCN (Ours),60.3,100.0,60.3,100,60.3,0.6,Accuracy
479,1,Visual Question Answering,TallyQA - Visual Question Answering benchmarking,2018-10,RCN (Ours),71.8,100.0,71.8,100,71.8,0.72,Accuracy
480,1,Steering Control,BDD100K - Steering Control benchmarking,2018-11,FM-Net,85.03,100.0,85.03,100,85.03,0.85,Accuracy
481,1,Fine-Grained Image Classification,Birdsnap - Fine-Grained Image Classification benchmarking,2018-11,GPIPE,83.6,99.17,83.6,100,84.3,0.84,Accuracy
482,1,Fine-Grained Image Classification,Birdsnap - Fine-Grained Image Classification benchmarking,2019-06,FixSENet-154,84.3,100.0,0.7,1.0,84.3,0.84,Accuracy
483,1,Gesture Recognition,ChaLearn 2013 - Gesture Recognition benchmarking,2018-11,3S Net TTM,92.08,100.0,92.08,100,92.08,0.92,Accuracy
484,1,Gesture Recognition,MSRC-12 - Gesture Recognition benchmarking,2018-11,3S Net TTM,99.01,100.0,99.01,100,99.01,0.99,Accuracy
485,1,Gesture Recognition,ChaLearn 2016 - Gesture Recognition benchmarking,2018-11,3S Net TTM,39.95,100.0,39.95,100,39.95,0.4,Accuracy
486,1,Action Recognition,Diving-48 - Action Recognition benchmarking,2018-12,SlowFast,77.6,100.0,77.6,100,77.6,0.78,Accuracy
487,1,Image Clustering,LetterA-J - Image Clustering benchmarking,2018-12,DDC-DA,0.691,100.0,0.691,100,0.691,0.01,Accuracy
488,1,Action Recognition,UTD-MHAD - Action Recognition benchmarking,2018-12,Action Machine (RGB only),92.5,100.0,92.5,100,92.5,0.93,Accuracy
489,1,6D Pose Estimation using RGB,Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking,2018-12,PVNet,61.06,100.0,61.06,100,61.06,0.61,Accuracy
490,1,Multimodal Activity Recognition,LboroHAR - Multimodal Activity Recognition benchmarking,2019-01,Deep Neural Net,95.0,97.04,95.0,100,97.9,0.95,Accuracy
491,1,Multimodal Activity Recognition,LboroHAR - Multimodal Activity Recognition benchmarking,2019-01,Cubic SVM,97.9,100.0,2.9,1.0,97.9,0.98,Accuracy
492,1,Image Retrieval,street2shop - topwear - Image Retrieval benchmarking,2019-01,Ranknet,94.98,100.0,94.98,100,94.98,0.95,Accuracy
493,1,Hand Gesture Recognition,Northwestern University - Hand Gesture Recognition benchmarking,2019-01,Key Frames + Feature Fusion,96.89,100.0,96.89,100,96.89,0.97,Accuracy
494,1,Gesture Recognition,Ninapro DB-1 8 gestures - Gesture Recognition benchmarking,2019-01,2SRNN,90.7,100.0,90.7,100,90.7,0.91,Accuracy
495,1,Gesture Recognition,CapgMyo DB-c - Gesture Recognition benchmarking,2019-01,2SRNN,96.8,100.0,96.8,100,96.8,0.97,Accuracy
496,1,Gesture Recognition,CapgMyo DB-b - Gesture Recognition benchmarking,2019-01,2SRNN,97.1,100.0,97.1,100,97.1,0.97,Accuracy
497,1,Gesture Recognition,Ninapro DB-1 12 gestures - Gesture Recognition benchmarking,2019-01,2SRNN,84.7,100.0,84.7,100,84.7,0.85,Accuracy
498,1,Facial Expression Recognition,FERG - Facial Expression Recognition benchmarking,2019-02,DeepEmotion,99.3,100.0,99.3,100,99.3,0.99,Accuracy
499,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 1000 way - Few-Shot Image Classification benchmarking",2019-02,APL,78.9,100.0,78.9,100,78.9,0.79,Accuracy
500,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 423 way - Few-Shot Image Classification benchmarking",2019-02,APL,73.5,100.0,73.5,100,73.5,0.74,Accuracy
501,1,Few-Shot Image Classification,"OMNIGLOT - 5-Shot, 423 way - Few-Shot Image Classification benchmarking",2019-02,APL,88.0,100.0,88,100,88,0.88,Accuracy
502,1,Few-Shot Image Classification,"OMNIGLOT - 1-Shot, 1000 way - Few-Shot Image Classification benchmarking",2019-02,APL,68.9,100.0,68.9,100,68.9,0.69,Accuracy
503,1,Visual Question Answering,TDIUC - Visual Question Answering benchmarking,2019-02,Accuracy,88.2,100.0,88.2,100,88.2,0.88,Accuracy
504,1,Visual Question Answering,GQA test-std - Visual Question Answering benchmarking,2019-02,CNN+LSTM,46.55,73.69,46.55,100,63.17,0.47,Accuracy
505,1,Visual Question Answering,GQA test-std - Visual Question Answering benchmarking,2019-02,MAC,54.06,85.58,7.51,0.4519,63.17,0.54,Accuracy
506,1,Visual Question Answering,GQA test-std - Visual Question Answering benchmarking,2019-07,NSM,63.17,100.0,9.11,0.5481,63.17,0.63,Accuracy
507,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2019-03,CE-Net,0.99,100.0,0.99,100,0.99,0.01,Accuracy
508,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2019-03,CE-Net,0.9545,99.84,0.9545,100,0.956,0.01,Accuracy
509,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2019-07,ET-Net,0.956,100.0,0.0015,1.0,0.956,0.01,Accuracy
510,1,Domain Adaptation,Office-Home - Domain Adaptation benchmarking,2019-04,IDDA,49.46,69.66,49.46,100,71.0,0.49,Accuracy
511,1,Domain Adaptation,Office-Home - Domain Adaptation benchmarking,2019-06,CADA,70.2,98.87,20.74,0.9629,71.0,0.7,Accuracy
512,1,Domain Adaptation,Office-Home - Domain Adaptation benchmarking,2019-11,SPL,71.0,100.0,0.8,0.0371,71.0,0.71,Accuracy
513,1,Action Recognition,miniSports - Action Recognition benchmarking,2019-04,IF+MD+RGB-R (ResNet-18),74.9,100.0,74.9,100,74.9,0.75,Accuracy
514,1,Few-Shot Image Classification,AWA2 - 0-Shot - Few-Shot Image Classification benchmarking,2019-04,TAFE-Net,69.3,100.0,69.3,100,69.3,0.69,Accuracy
515,1,Few-Shot Image Classification,AWA1 - 0-Shot - Few-Shot Image Classification benchmarking,2019-04,TAFE-Net,70.8,100.0,70.8,100,70.8,0.71,Accuracy
516,1,Few-Shot Image Classification,aPY - 0-Shot - Few-Shot Image Classification benchmarking,2019-04,TAFE-Net,42.2,100.0,42.2,100,42.2,0.42,Accuracy
517,1,Image Classification,EMNIST-Letters - Image Classification benchmarking,2019-04,TextCaps,95.39,100.0,95.39,100,95.39,0.95,Accuracy
518,1,Sparse Representation-based Classification,SVHN - Sparse Representation-based Classification benchmarking,2019-04,DSRC,67.75,100.0,67.75,100,67.75,0.68,Accuracy
519,1,Semi-Supervised Image Classification,"CIFAR-10, 500 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,91.35,100.0,91.35,100,91.35,0.91,Accuracy
520,1,Semi-Supervised Image Classification,"SVHN, 2000 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,96.96,100.0,96.96,100,96.96,0.97,Accuracy
521,1,Semi-Supervised Image Classification,"SVHN, 4000 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,97.11,100.0,97.11,100,97.11,0.97,Accuracy
522,1,Semi-Supervised Image Classification,"CIFAR-10, 1000 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,92.25,100.0,92.25,100,92.25,0.92,Accuracy
523,1,Semi-Supervised Image Classification,"SVHN, 500 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,96.36,100.0,96.36,100,96.36,0.96,Accuracy
524,1,Semi-Supervised Image Classification,"STL-10, 5000 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,94.41,100.0,94.41,100,94.41,0.94,Accuracy
525,1,Semi-Supervised Image Classification,"CIFAR-10, 2000 Labels - Semi-Supervised Image Classification benchmarking",2019-05,MixMatch,92.97,100.0,92.97,100,92.97,0.93,Accuracy
526,1,Skeleton Based Action Recognition,UPenn Action - Skeleton Based Action Recognition benchmarking,2019-06,HDM-BG,93.4,100.0,93.4,100,93.4,0.93,Accuracy
527,1,Skeleton Based Action Recognition,MSR Action3D - Skeleton Based Action Recognition benchmarking,2019-06,HDM-BG,86.1,100.0,86.1,100,86.1,0.86,Accuracy
528,1,Visual Question Answering,GQA test-dev - Visual Question Answering benchmarking,2019-07,NSM,62.95,100.0,62.95,100,62.95,0.63,Accuracy
529,1,Face Verification,AgeDB-30 - Face Verification benchmarking,2019-07,VarGNet,0.97333,99.17,0.97333,100,0.9815,0.01,Accuracy
530,1,Face Verification,AgeDB-30 - Face Verification benchmarking,2019-10,VarGFaceNet,0.9815,100.0,0.0082,1.0037,0.9815,0.01,Accuracy
531,1,Face Verification,CFP-FP - Face Verification benchmarking,2019-07,VarGNet,0.89829,91.2,0.89829,100,0.985,0.01,Accuracy
532,1,Face Verification,CFP-FP - Face Verification benchmarking,2019-08,Seesaw-shuffleFaceNet (mobi),0.9307,94.49,0.0324,0.3737,0.985,0.01,Accuracy
533,1,Face Verification,CFP-FP - Face Verification benchmarking,2019-10,VarGFaceNet,0.985,100.0,0.0543,0.6262,0.985,0.01,Accuracy
534,1,Hand Gesture Recognition,DHG-28 - Hand Gesture Recognition benchmarking,2019-07,DG-STA,88.0,100.0,88,100,88,0.88,Accuracy
535,1,Hand Gesture Recognition,DHG-14 - Hand Gesture Recognition benchmarking,2019-07,DG-STA,91.9,100.0,91.9,100,91.9,0.92,Accuracy
536,1,Lung Nodule Segmentation,Montgomery County - Lung Nodule Segmentation benchmarking,2019-07,ET-Net,0.9865,100.0,0.9865,100,0.9865,0.01,Accuracy
537,1,Lane Detection,BDD100K - Lane Detection benchmarking,2019-08,ENet-SAD,36.56,100.0,36.56,100,36.56,0.37,Accuracy
538,1,Visual Reasoning,NLVR2 Dev - Visual Reasoning benchmarking,2019-08,VisualBERT,66.7,89.05,66.7,100,74.9,0.67,Accuracy
539,1,Visual Reasoning,NLVR2 Dev - Visual Reasoning benchmarking,2019-08,LXMERT (Pre-train + scratch),74.9,100.0,8.2,1.0,74.9,0.75,Accuracy
540,1,Few-Shot Image Classification,mini-ImageNet - 100-Way - Few-Shot Image Classification benchmarking,2019-08,GCR,39.14,100.0,39.14,100,39.14,0.39,Accuracy
541,1,Visual Reasoning,NLVR2 Test - Visual Reasoning benchmarking,2019-08,LXMERT,76.2,100.0,76.2,100,76.2,0.76,Accuracy
542,1,Multimodal Activity Recognition,Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking,2019-09,KNN,80.2,100.0,80.2,100,80.2,0.8,Accuracy
543,1,Image Classification,Flowers-102 - Image Classification benchmarking,2019-12,BiT-L (ResNet),99.63,100.0,99.63,100,99.63,1.0,Accuracy
544,1,Fine-Grained Image Classification,Food-101 - Fine-Grained Image Classification benchmarking,2020-01,Assemble-ResNet-FGVC-50,92.5,100.0,92.5,100,92.5,0.93,Accuracy
545,1,Few-Shot Image Classification,Mini-ImageNet to CUB - 5 shot learning - Few-Shot Image Classification benchmarking,2020-03,Neg-Margin,69.3,100.0,69.3,100,69.3,0.69,Accuracy
546,1,Handwritten Digit Recognition,MNIST - Handwritten Digit Recognition benchmarking,2020-04,CNN,96.95,100.0,96.95,100,96.95,0.97,Accuracy
0,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2012-02,MCDNN,88.8,89.36,88.8,100,99.37,0.89,Percentage\\ correct
1,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2012-12,DCNN,89.0,89.56,0.2,0.0189,99.37,0.9,Percentage\\ correct
2,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2013-02,Maxout Network (k=2),90.65,91.22,1.65,0.1561,99.37,0.91,Percentage\\ correct
3,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2013-12,Network in Network,91.2,91.78,0.55,0.052,99.37,0.92,Percentage\\ correct
4,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2014-09,DSN,91.8,92.38,0.6,0.0568,99.37,0.92,Percentage\\ correct
5,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2014-09,SSCNN,93.7,94.29,1.9,0.1798,99.37,0.94,Percentage\\ correct
6,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2014-12,Fractional MP,96.5,97.11,2.8,0.2649,99.37,0.97,Percentage\\ correct
7,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2016-08,DenseNet (DenseNet-BC-190),96.54,97.15,0.04,0.0038,99.37,0.97,Percentage\\ correct
8,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2016-10,Deep pyramidal residual network,96.69,97.3,0.15,0.0142,99.37,0.97,Percentage\\ correct
9,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2017-09,SENet + ShakeShake + Cutout,97.88,98.5,1.19,0.1126,99.37,0.99,Percentage\\ correct
10,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2018-11,GPIPE + transfer learning,99.0,99.63,1.12,0.106,99.37,1.0,Percentage\\ correct
11,1,Image Classification,CIFAR-10 - Image Classification benchmarking,2019-12,BiT-L (ResNet),99.37,100.0,0.37,0.035,99.37,1.0,Percentage\\ correct
12,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2013-01,Stochastic Pooling,57.5,61.49,57.5,100,93.51,0.58,Percentage\\ correct
13,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2013-02,Maxout Network (k=2),61.43,65.69,3.93,0.1091,93.51,0.62,Percentage\\ correct
14,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2013-12,Tree Priors,63.2,67.59,1.77,0.0492,93.51,0.64,Percentage\\ correct
15,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2013-12,NiN,64.3,68.76,1.1,0.0305,93.51,0.65,Percentage\\ correct
16,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2014-09,DSN,65.4,69.94,1.1,0.0305,93.51,0.66,Percentage\\ correct
17,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2014-09,SSCNN,75.7,80.95,10.3,0.286,93.51,0.76,Percentage\\ correct
18,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2016-03,ResNet-1001,77.3,82.66,1.6,0.0444,93.51,0.78,Percentage\\ correct
19,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2016-05,Wide ResNet,81.15,86.78,3.85,0.1069,93.51,0.82,Percentage\\ correct
20,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2016-08,DenseNet-BC,82.82,88.57,1.67,0.0464,93.51,0.83,Percentage\\ correct
21,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2017-09,SENet + ShakeEven + Cutout,84.59,90.46,1.77,0.0492,93.51,0.85,Percentage\\ correct
22,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2018-05,PyramidNet+ShakeDrop,89.3,95.5,4.71,0.1308,93.51,0.9,Percentage\\ correct
23,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2018-11,GPIPE,91.3,97.64,2.0,0.0555,93.51,0.92,Percentage\\ correct
24,1,Image Classification,CIFAR-100 - Image Classification benchmarking,2019-12,BiT-L (ResNet),93.51,100.0,2.21,0.0614,93.51,0.94,Percentage\\ correct
25,1,Image Classification,STL-10 - Image Classification benchmarking,2013-12,Multi-Task Bayesian Optimization,70.1,74.18,70.1,100,94.5,0.71,Percentage\\ correct
26,1,Image Classification,STL-10 - Image Classification benchmarking,2014-12,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,72.8,77.04,2.7,0.1107,94.5,0.73,Percentage\\ correct
27,1,Image Classification,STL-10 - Image Classification benchmarking,2015-06,SWWAE,74.3,78.62,1.5,0.0615,94.5,0.75,Percentage\\ correct
28,1,Image Classification,STL-10 - Image Classification benchmarking,2016-11,CC-GAN²,77.8,82.33,3.5,0.1434,94.5,0.78,Percentage\\ correct
29,1,Image Classification,STL-10 - Image Classification benchmarking,2017-08,Cutout,87.26,92.34,9.46,0.3877,94.5,0.88,Percentage\\ correct
30,1,Image Classification,STL-10 - Image Classification benchmarking,2018-07,IIC,88.8,93.97,1.54,0.0631,94.5,0.89,Percentage\\ correct
31,1,Image Classification,STL-10 - Image Classification benchmarking,2019-04,Harmonic WRN-16-8,90.45,95.71,1.65,0.0676,94.5,0.91,Percentage\\ correct
32,1,Image Classification,STL-10 - Image Classification benchmarking,2019-05,MixMatch,94.41,99.9,3.96,0.1623,94.5,0.95,Percentage\\ correct
33,1,Image Classification,STL-10 - Image Classification benchmarking,2019-06,AMDIM,94.5,100.0,0.09,0.0037,94.5,0.95,Percentage\\ correct
34,1,Visual Question Answering,Visual7W - Visual Question Answering benchmarking,2016-06,MCB+Att.,62.2,85.76,62.2,100,72.53,0.63,Percentage\\ correct
35,1,Visual Question Answering,Visual7W - Visual Question Answering benchmarking,2016-11,CMN,72.53,100.0,10.33,1.0,72.53,0.73,Percentage\\ correct
36,1,Semi-Supervised Image Classification,"cifar10, 250 Labels - Semi-Supervised Image Classification benchmarking",2017-04,VAT,63.97,68.25,63.97,100,93.73,0.64,Percentage\\ correct
37,1,Semi-Supervised Image Classification,"cifar10, 250 Labels - Semi-Supervised Image Classification benchmarking",2019-11,ReMixMatch,93.73,100.0,29.76,1.0,93.73,0.94,Percentage\\ correct
0,1,Motion Segmentation,Hopkins155 - Motion Segmentation benchmarking,2012-03,SSC,2.18,100.0,2.18,100,2.18,0.25,Classification\\ Error
1,1,Image-to-Image Translation,RaFD - Image-to-Image Translation benchmarking,2016-10,DIA,4.1,50.81,4.1,100,8.07,0.46,Classification\\ Error
2,1,Image-to-Image Translation,RaFD - Image-to-Image Translation benchmarking,2016-11,IcGAN,8.07,100.0,3.97,1.0,8.07,0.91,Classification\\ Error
3,1,Superpixel Image Classification,75 Superpixel MNIST - Superpixel Image Classification benchmarking,2016-11,Monet,8.89,100.0,8.89,100,8.89,1.0,Classification\\ Error
4,1,Image Classification,smallNORB - Image Classification benchmarking,2017-10,CapsNet,3.77,100.0,3.77,100,3.77,0.42,Classification\\ Error
5,1,Motion Segmentation,MTPV62 - Motion Segmentation benchmarking,2018-04,MVC,0.65,100.0,0.65,100,0.65,0.07,Classification\\ Error
0,1,Image Clustering,coil-100 - Image Clustering benchmarking,2012-08,GDL-U,0.929,94.31,0.929,100,0.985,0.93,NMI
1,1,Image Clustering,coil-100 - Image Clustering benchmarking,2016-04,JULE-RC,0.985,100.0,0.056,1.0,0.985,0.98,NMI
2,1,Image Clustering,USPS - Image Clustering benchmarking,2012-08,AGDL,0.824,86.92,0.824,100,0.948,0.82,NMI
3,1,Image Clustering,USPS - Image Clustering benchmarking,2018-04,DMSC,0.929,98.0,0.105,0.8468,0.948,0.93,NMI
4,1,Image Clustering,USPS - Image Clustering benchmarking,2018-10,SR-K-means,0.936,98.73,0.007,0.0565,0.948,0.94,NMI
5,1,Image Clustering,USPS - Image Clustering benchmarking,2018-12,DDC-DA,0.939,99.05,0.003,0.0242,0.948,0.94,NMI
6,1,Image Clustering,USPS - Image Clustering benchmarking,2019-01,DynAE,0.948,100.0,0.009,0.0726,0.948,0.95,NMI
7,1,Image Clustering,Fashion-MNIST - Image Clustering benchmarking,2012-08,GDL,0.66,96.77,0.66,100,0.682,0.66,NMI
8,1,Image Clustering,Fashion-MNIST - Image Clustering benchmarking,2018-12,DDC-DA,0.661,96.92,0.001,0.0455,0.682,0.66,NMI
9,1,Image Clustering,Fashion-MNIST - Image Clustering benchmarking,2018-12,DDC,0.682,100.0,0.021,0.9545,0.682,0.68,NMI
10,1,Image Clustering,Extended Yale-B - Image Clustering benchmarking,2012-08,GDL-U,0.91,92.11,0.91,100,0.988,0.91,NMI
11,1,Image Clustering,Extended Yale-B - Image Clustering benchmarking,2017-09,DSC-2,0.97,98.18,0.06,0.7692,0.988,0.97,NMI
12,1,Image Clustering,Extended Yale-B - Image Clustering benchmarking,2018-04,DMSC,0.988,100.0,0.018,0.2308,0.988,0.99,NMI
13,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2012-08,AGDL,0.844,87.64,0.844,100,0.963,0.84,NMI
14,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2012-08,GDL,0.91,94.5,0.066,0.5546,0.963,0.91,NMI
15,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2016-04,OURS-RC,0.915,95.02,0.005,0.042,0.963,0.92,NMI
16,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2018-12,DDC-DA,0.927,96.26,0.012,0.1008,0.963,0.93,NMI
17,1,Image Clustering,MNIST-test - Image Clustering benchmarking,2019-01,DynAE,0.963,100.0,0.036,0.3025,0.963,0.96,NMI
18,1,Image Clustering,Coil-20 - Image Clustering benchmarking,2012-08,AGDL,0.937,93.7,0.937,100,1.0,0.94,NMI
19,1,Image Clustering,Coil-20 - Image Clustering benchmarking,2016-04,JULE-RC,1.0,100.0,0.063,1.0,1.0,1.0,NMI
20,1,Image Clustering,MNIST-full - Image Clustering benchmarking,2012-08,GDL,0.91,96.71,0.91,100,0.941,0.91,NMI
21,1,Image Clustering,MNIST-full - Image Clustering benchmarking,2016-04,JULE-RC,0.913,97.02,0.003,0.0968,0.941,0.91,NMI
22,1,Image Clustering,MNIST-full - Image Clustering benchmarking,2017-03,DBC,0.917,97.45,0.004,0.129,0.941,0.92,NMI
23,1,Image Clustering,MNIST-full - Image Clustering benchmarking,2018-12,DDC-DA,0.941,100.0,0.024,0.7742,0.941,0.94,NMI
24,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2013-12,VAE,0.108,37.89,0.108,100,0.285,0.11,NMI
25,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2015-11,DEC,0.136,47.72,0.028,0.1582,0.285,0.14,NMI
26,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2017-10,DAC,0.185,64.91,0.049,0.2768,0.285,0.18,NMI
27,1,Image Clustering,CIFAR-100 - Image Clustering benchmarking,2019-04,DCCM,0.285,100.0,0.1,0.565,0.285,0.28,NMI
28,1,Image Clustering,STL-10 - Image Clustering benchmarking,2013-12,VAE,0.2,53.19,0.2,100,0.376,0.2,NMI
29,1,Image Clustering,STL-10 - Image Clustering benchmarking,2015-11,DEC,0.276,73.4,0.076,0.4318,0.376,0.28,NMI
30,1,Image Clustering,STL-10 - Image Clustering benchmarking,2017-10,DAC,0.366,97.34,0.09,0.5114,0.376,0.37,NMI
31,1,Image Clustering,STL-10 - Image Clustering benchmarking,2019-04,DCCM,0.376,100.0,0.01,0.0568,0.376,0.38,NMI
32,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2013-12,VAE,0.113,50.45,0.113,100,0.224,0.11,NMI
33,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2015-11,GAN,0.135,60.27,0.022,0.1982,0.224,0.14,NMI
34,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2017-10,DAC,0.19,84.82,0.055,0.4955,0.224,0.19,NMI
35,1,Image Clustering,Tiny-ImageNet - Image Clustering benchmarking,2019-04,DCCM,0.224,100.0,0.034,0.3063,0.224,0.22,NMI
36,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2013-12,VAE,0.107,33.33,0.107,100,0.321,0.11,NMI
37,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2015-11,DEC,0.122,38.01,0.015,0.0701,0.321,0.12,NMI
38,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2017-10,DAC,0.219,68.22,0.097,0.4533,0.321,0.22,NMI
39,1,Image Clustering,Imagenet-dog-15 - Image Clustering benchmarking,2019-04,DCCM,0.321,100.0,0.102,0.4766,0.321,0.32,NMI
40,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2013-12,VAE,0.193,31.74,0.193,100,0.608,0.19,NMI
41,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2015-11,DEC,0.282,46.38,0.089,0.2145,0.608,0.28,NMI
42,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2017-10,DAC,0.394,64.8,0.112,0.2699,0.608,0.39,NMI
43,1,Image Clustering,ImageNet-10 - Image Clustering benchmarking,2019-04,DCCM,0.608,100.0,0.214,0.5157,0.608,0.61,NMI
44,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2013-12,VAE,0.245,47.95,0.245,100,0.511,0.24,NMI
45,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2015-11,GAN,0.265,51.86,0.02,0.0752,0.511,0.26,NMI
46,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2017-10,DAC,0.4,78.28,0.135,0.5075,0.511,0.4,NMI
47,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2018-07,IIC,0.511,100.0,0.111,0.4173,0.511,0.51,NMI
48,1,Image Clustering,CMU-PIE - Image Clustering benchmarking,2015-11,DEC (KL based),0.924,92.4,0.924,100,1.0,0.92,NMI
49,1,Image Clustering,CMU-PIE - Image Clustering benchmarking,2016-04,JULE-RC,1.0,100.0,0.076,1.0,1.0,1.0,NMI
50,1,Image Clustering,YouTube Faces DB - Image Clustering benchmarking,2015-11,DEC (KL based),0.446,52.59,0.446,100,0.848,0.45,NMI
51,1,Image Clustering,YouTube Faces DB - Image Clustering benchmarking,2016-04,JULE-RC,0.848,100.0,0.402,1.0,0.848,0.85,NMI
52,1,Image Clustering,Stanford Cars - Image Clustering benchmarking,2016-04,JULE,0.232,65.54,0.232,100,0.354,0.23,NMI
53,1,Image Clustering,Stanford Cars - Image Clustering benchmarking,2017-04,DEPICT-Large,0.33,93.22,0.098,0.8033,0.354,0.33,NMI
54,1,Image Clustering,Stanford Cars - Image Clustering benchmarking,2018-11,FineGAN,0.354,100.0,0.024,0.1967,0.354,0.35,NMI
55,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2016-04,JULE,0.142,60.94,0.142,100,0.233,0.14,NMI
56,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2017-04,DEPICT,0.182,78.11,0.04,0.4396,0.233,0.18,NMI
57,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2017-04,DEPICT-Large,0.183,78.54,0.001,0.011,0.233,0.18,NMI
58,1,Image Clustering,Stanford Dogs - Image Clustering benchmarking,2018-11,FineGAN,0.233,100.0,0.05,0.5495,0.233,0.23,NMI
59,1,Image Clustering,UMist - Image Clustering benchmarking,2016-04,JULE-RC,0.877,100.0,0.877,100,0.877,0.88,NMI
60,1,Image Clustering,FRGC - Image Clustering benchmarking,2016-04,JULE-RC,0.574,98.46,0.574,100,0.583,0.57,NMI
61,1,Image Clustering,FRGC - Image Clustering benchmarking,2017-04,DEPICT,0.583,100.0,0.009,1.0,0.583,0.58,NMI
62,1,Image Clustering,CUB Birds - Image Clustering benchmarking,2016-04,JULE,0.203,50.37,0.203,100,0.403,0.2,NMI
63,1,Image Clustering,CUB Birds - Image Clustering benchmarking,2017-04,DEPICT-Large,0.297,73.7,0.094,0.47,0.403,0.3,NMI
64,1,Image Clustering,CUB Birds - Image Clustering benchmarking,2018-11,FineGAN,0.403,100.0,0.106,0.53,0.403,0.4,NMI
65,1,Image Clustering,LetterA-J - Image Clustering benchmarking,2018-12,DDC-DA,0.629,100.0,0.629,100,0.629,0.63,NMI
0,1,Image Classification,ImageNet - Image Classification benchmarking,2012-12,AlexNet,84.6,85.92,84.6,100,98.46,0.85,Top\\ 5\\ Accuracy
1,1,Image Classification,ImageNet - Image Classification benchmarking,2013-11,"ZFNet (ensemble, 6 convnets)",85.3,86.63,0.7,0.0505,98.46,0.85,Top\\ 5\\ Accuracy
2,1,Image Classification,ImageNet - Image Classification benchmarking,2013-12,Five Base + Five HiRes,86.3,87.65,1.0,0.0722,98.46,0.86,Top\\ 5\\ Accuracy
3,1,Image Classification,ImageNet - Image Classification benchmarking,2013-12,OverFeat - 7 accurate models,86.76,88.12,0.46,0.0332,98.46,0.87,Top\\ 5\\ Accuracy
4,1,Image Classification,ImageNet - Image Classification benchmarking,2014-06,SPPNet,91.86,93.3,5.1,0.368,98.46,0.92,Top\\ 5\\ Accuracy
5,1,Image Classification,ImageNet - Image Classification benchmarking,2014-09,VGG-16,91.9,93.34,0.04,0.0029,98.46,0.92,Top\\ 5\\ Accuracy
6,1,Image Classification,ImageNet - Image Classification benchmarking,2014-09,VGG-19,92.0,93.44,0.1,0.0072,98.46,0.92,Top\\ 5\\ Accuracy
7,1,Image Classification,ImageNet - Image Classification benchmarking,2015-02,Inception V2,92.2,93.64,0.2,0.0144,98.46,0.92,Top\\ 5\\ Accuracy
8,1,Image Classification,ImageNet - Image Classification benchmarking,2015-12,ResNet-50,93.29,94.75,1.09,0.0786,98.46,0.93,Top\\ 5\\ Accuracy
9,1,Image Classification,ImageNet - Image Classification benchmarking,2015-12,ResNet-101,93.95,95.42,0.66,0.0476,98.46,0.94,Top\\ 5\\ Accuracy
10,1,Image Classification,ImageNet - Image Classification benchmarking,2015-12,ResNet-152 (60M),94.29,95.76,0.34,0.0245,98.46,0.94,Top\\ 5\\ Accuracy
11,1,Image Classification,ImageNet - Image Classification benchmarking,2016-02,Inception ResNet V2,95.1,96.59,0.81,0.0584,98.46,0.95,Top\\ 5\\ Accuracy
12,1,Image Classification,ImageNet - Image Classification benchmarking,2016-03,ResNet-200,95.2,96.69,0.1,0.0072,98.46,0.95,Top\\ 5\\ Accuracy
13,1,Image Classification,ImageNet - Image Classification benchmarking,2016-11,ResNeXt-101  64x4,95.6,97.1,0.4,0.0289,98.46,0.96,Top\\ 5\\ Accuracy
14,1,Image Classification,ImageNet - Image Classification benchmarking,2017-07,DPN-131 (320x320),95.77,97.27,0.17,0.0123,98.46,0.96,Top\\ 5\\ Accuracy
15,1,Image Classification,ImageNet - Image Classification benchmarking,2017-07,NASNET-A(6),96.2,97.7,0.43,0.031,98.46,0.96,Top\\ 5\\ Accuracy
16,1,Image Classification,ImageNet - Image Classification benchmarking,2018-02,AmoebaNet-A,96.6,98.11,0.4,0.0289,98.46,0.97,Top\\ 5\\ Accuracy
17,1,Image Classification,ImageNet - Image Classification benchmarking,2018-05,ResNeXt-101 32×16d,97.2,98.72,0.6,0.0433,98.46,0.97,Top\\ 5\\ Accuracy
18,1,Image Classification,ImageNet - Image Classification benchmarking,2018-05,ResNeXt-101 32x32d,97.5,99.02,0.3,0.0216,98.46,0.98,Top\\ 5\\ Accuracy
19,1,Image Classification,ImageNet - Image Classification benchmarking,2018-05,ResNeXt-101 32x48d,97.6,99.13,0.1,0.0072,98.46,0.98,Top\\ 5\\ Accuracy
20,1,Image Classification,ImageNet - Image Classification benchmarking,2019-06,FixResNeXt-101 32x48d,98.0,99.53,0.4,0.0289,98.46,0.98,Top\\ 5\\ Accuracy
21,1,Image Classification,ImageNet - Image Classification benchmarking,2019-12,BiT-L (ResNet),98.46,100.0,0.46,0.0332,98.46,0.99,Top\\ 5\\ Accuracy
22,1,Semi-Supervised Image Classification,ImageNet - 10% labeled data - Semi-Supervised Image Classification benchmarking,2017-03,Mean Teacher (ResNeXt-152),90.89,99.63,90.89,100,91.23,0.91,Top\\ 5\\ Accuracy
23,1,Semi-Supervised Image Classification,ImageNet - 10% labeled data - Semi-Supervised Image Classification benchmarking,2019-05,S4L-MOAM (ResNet-50 4×),91.23,100.0,0.34,1.0,91.23,0.91,Top\\ 5\\ Accuracy
24,1,Action Classification,Moments in Time - Action Classification benchmarking,2017-05,TSN-2Stream,50.1,83.5,50.1,100,60.0,0.5,Top\\ 5\\ Accuracy
25,1,Action Classification,Moments in Time - Action Classification benchmarking,2017-05,I3D,56.06,93.43,5.96,0.602,60.0,0.56,Top\\ 5\\ Accuracy
26,1,Action Classification,Moments in Time - Action Classification benchmarking,2019-06,"CoST (ResNet-101, 32 frames)",60.0,100.0,3.94,0.398,60.0,0.6,Top\\ 5\\ Accuracy
27,1,Image Classification,iNaturalist - Image Classification benchmarking,2017-07,IncResNetV2 SE,87.5,100.0,87.5,100,87.5,0.88,Top\\ 5\\ Accuracy
28,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2017-12,S3D-G (ImageNet pretrained),78.7,91.41,78.7,100,86.1,0.79,Top\\ 5\\ Accuracy
29,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2019-08,TRG (ResNet-50),86.1,100.0,7.4,1.0,86.1,0.86,Top\\ 5\\ Accuracy
30,1,Hand Gesture Recognition,Jester val - Hand Gesture Recognition benchmarking,2018-04,8-MFFs-3f1c (5 crop),99.86,100.0,99.86,100,99.86,1.0,Top\\ 5\\ Accuracy
31,1,Semi-Supervised Image Classification,ImageNet - 1% labeled data - Semi-Supervised Image Classification benchmarking,2018-06,Instance Discrimination (ResNet-50),39.2,61.22,39.2,100,64.03,0.39,Top\\ 5\\ Accuracy
32,1,Semi-Supervised Image Classification,ImageNet - 1% labeled data - Semi-Supervised Image Classification benchmarking,2018-07,CPC,64.03,100.0,24.83,1.0,64.03,0.64,Top\\ 5\\ Accuracy
33,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2018-07,CPC (ResNet-101 V2),73.6,81.69,73.6,100,90.1,0.74,Top\\ 5\\ Accuracy
34,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-01,Revisited Rotation (RevNet-50 ×4),77.9,86.46,4.3,0.2606,90.1,0.78,Top\\ 5\\ Accuracy
35,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-07,BigBiGAN (RevNet-50 ×4),81.4,90.34,3.5,0.2121,90.1,0.82,Top\\ 5\\ Accuracy
36,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-07,"BigBiGAN (RevNet-50 ×4, BN+CReLU)",81.9,90.9,0.5,0.0303,90.1,0.82,Top\\ 5\\ Accuracy
37,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2020-03,MoCo v2 (ResNet-50),90.1,100.0,8.2,0.497,90.1,0.9,Top\\ 5\\ Accuracy
38,1,Image Classification,ObjectNet - Image Classification benchmarking,2019-12,ResNet-152,49.3,61.62,49.3,100,80.0,0.49,Top\\ 5\\ Accuracy
39,1,Image Classification,ObjectNet - Image Classification benchmarking,2019-12,NASNet-A,56.4,70.5,7.1,0.2313,80.0,0.56,Top\\ 5\\ Accuracy
40,1,Image Classification,ObjectNet - Image Classification benchmarking,2019-12,BiT-L (ResNet-152x4),80.0,100.0,23.6,0.7687,80.0,0.8,Top\\ 5\\ Accuracy
0,1,Image Classification,ImageNet - Image Classification benchmarking,2012-12,AlexNet,63.3,72.31,63.3,100,87.54,0.66,Top\\ 1\\ Accuracy
1,1,Image Classification,ImageNet - Image Classification benchmarking,2013-11,"ZFNet (ensemble, 6 convnets)",64.0,73.11,0.7,0.0289,87.54,0.66,Top\\ 1\\ Accuracy
2,1,Image Classification,ImageNet - Image Classification benchmarking,2013-12,Five Base + Five HiRes,66.3,75.74,2.3,0.0949,87.54,0.69,Top\\ 1\\ Accuracy
3,1,Image Classification,ImageNet - Image Classification benchmarking,2014-06,MSRA,71.32,81.47,5.02,0.2071,87.54,0.74,Top\\ 1\\ Accuracy
4,1,Image Classification,ImageNet - Image Classification benchmarking,2014-06,SPPNet,72.14,82.41,0.82,0.0338,87.54,0.75,Top\\ 1\\ Accuracy
5,1,Image Classification,ImageNet - Image Classification benchmarking,2014-09,VGG-19,74.5,85.1,2.36,0.0974,87.54,0.77,Top\\ 1\\ Accuracy
6,1,Image Classification,ImageNet - Image Classification benchmarking,2015-02,Inception V2,74.8,85.45,0.3,0.0124,87.54,0.77,Top\\ 1\\ Accuracy
7,1,Image Classification,ImageNet - Image Classification benchmarking,2015-12,Inception V3,78.8,90.02,4.0,0.165,87.54,0.82,Top\\ 1\\ Accuracy
8,1,Image Classification,ImageNet - Image Classification benchmarking,2016-02,Inception ResNet V2,80.1,91.5,1.3,0.0536,87.54,0.83,Top\\ 1\\ Accuracy
9,1,Image Classification,ImageNet - Image Classification benchmarking,2016-11,ResNeXt-101  64x4,80.9,92.41,0.8,0.033,87.54,0.84,Top\\ 1\\ Accuracy
10,1,Image Classification,ImageNet - Image Classification benchmarking,2017-07,"DPN-98 (320x320, Mean-Max Pooling)",81.28,92.85,0.38,0.0157,87.54,0.84,Top\\ 1\\ Accuracy
11,1,Image Classification,ImageNet - Image Classification benchmarking,2017-07,DPN-131 (320x320),81.38,92.96,0.1,0.0041,87.54,0.84,Top\\ 1\\ Accuracy
12,1,Image Classification,ImageNet - Image Classification benchmarking,2017-07,NASNET-A(6),82.7,94.47,1.32,0.0545,87.54,0.86,Top\\ 1\\ Accuracy
13,1,Image Classification,ImageNet - Image Classification benchmarking,2017-12,PNASNet-5,82.9,94.7,0.2,0.0083,87.54,0.86,Top\\ 1\\ Accuracy
14,1,Image Classification,ImageNet - Image Classification benchmarking,2018-02,AmoebaNet-A,83.9,95.84,1.0,0.0413,87.54,0.87,Top\\ 1\\ Accuracy
15,1,Image Classification,ImageNet - Image Classification benchmarking,2018-05,ResNeXt-101 32×16d,84.2,96.18,0.3,0.0124,87.54,0.87,Top\\ 1\\ Accuracy
16,1,Image Classification,ImageNet - Image Classification benchmarking,2018-05,ResNeXt-101 32x48d,85.4,97.56,1.2,0.0495,87.54,0.88,Top\\ 1\\ Accuracy
17,1,Image Classification,ImageNet - Image Classification benchmarking,2019-06,FixResNeXt-101 32x48d,86.4,98.7,1.0,0.0413,87.54,0.89,Top\\ 1\\ Accuracy
18,1,Image Classification,ImageNet - Image Classification benchmarking,2019-12,BiT-L (ResNet),87.54,100.0,1.14,0.047,87.54,0.91,Top\\ 1\\ Accuracy
19,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2016-03,Colorization (AlexNet),35.2,49.51,35.2,100,71.1,0.36,Top\\ 1\\ Accuracy
20,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2018-06,InstDisc (ResNet-50),54.0,75.95,18.8,0.5237,71.1,0.56,Top\\ 1\\ Accuracy
21,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-01,Revisited Rotation (RevNet-50 ×4),55.4,77.92,1.4,0.039,71.1,0.57,Top\\ 1\\ Accuracy
22,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-06,AMDIM (small),63.5,89.31,8.1,0.2256,71.1,0.66,Top\\ 1\\ Accuracy
23,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-06,AMDIM (large),68.1,95.78,4.6,0.1281,71.1,0.7,Top\\ 1\\ Accuracy
24,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-11,MoCo (ResNet-50 4x),68.6,96.48,0.5,0.0139,71.1,0.71,Top\\ 1\\ Accuracy
25,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2020-03,MoCo v2 (ResNet-50),71.1,100.0,2.5,0.0696,71.1,0.74,Top\\ 1\\ Accuracy
26,1,Action Classification,Moments in Time - Action Classification benchmarking,2017-05,I3D,29.51,91.08,29.51,100,32.4,0.31,Top\\ 1\\ Accuracy
27,1,Action Classification,Moments in Time - Action Classification benchmarking,2018-11,EvaNet,31.8,98.15,2.29,0.7924,32.4,0.33,Top\\ 1\\ Accuracy
28,1,Action Classification,Moments in Time - Action Classification benchmarking,2019-06,"CoST (ResNet-101, 32 frames)",32.4,100.0,0.6,0.2076,32.4,0.34,Top\\ 1\\ Accuracy
29,1,Image Classification,iNaturalist - Image Classification benchmarking,2017-07,IncResNetV2 SE,67.3,89.26,67.3,100,75.4,0.7,Top\\ 1\\ Accuracy
30,1,Image Classification,iNaturalist - Image Classification benchmarking,2019-06,FixSENet-154,75.4,100.0,8.1,1.0,75.4,0.78,Top\\ 1\\ Accuracy
31,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2017-11,NL I3D,44.4,80.49,44.4,100,55.16,0.46,Top\\ 1\\ Accuracy
32,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2017-12,S3D-G (ImageNet pretrained),48.2,87.38,3.8,0.3532,55.16,0.5,Top\\ 1\\ Accuracy
33,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2018-01,ResNet50 I3D (Kinetics pretrained),48.6,88.11,0.4,0.0372,55.16,0.5,Top\\ 1\\ Accuracy
34,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2018-01,ResNet50 I3D (Moments pretrained),50.0,90.65,1.4,0.1301,55.16,0.52,Top\\ 1\\ Accuracy
35,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2018-11,TSM (RGB + Flow),50.7,91.91,0.7,0.0651,55.16,0.52,Top\\ 1\\ Accuracy
36,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2019-04,R(2+1)D-152 (IG-65M pretraining),51.6,93.55,0.9,0.0836,55.16,0.53,Top\\ 1\\ Accuracy
37,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2019-04,ir-CSN-152 (IG-65M pretraining),52.1,94.45,0.5,0.0465,55.16,0.54,Top\\ 1\\ Accuracy
38,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2019-04,ip-CSN-152 (IG-65M pretraining),53.3,96.63,1.2,0.1115,55.16,0.55,Top\\ 1\\ Accuracy
39,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2019-08,"GB + DF + LB (ResNet152, ImageNet pretrained)",53.4,96.81,0.1,0.0093,55.16,0.55,Top\\ 1\\ Accuracy
40,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2019-12,GSM Ensemble InceptionV3 (ImageNet pretrained),55.16,100.0,1.76,0.1636,55.16,0.57,Top\\ 1\\ Accuracy
41,1,Hand Gesture Recognition,Jester test - Hand Gesture Recognition benchmarking,2017-11,Multiscale TRN,94.78,98.12,94.78,100,96.6,0.98,Top\\ 1\\ Accuracy
42,1,Hand Gesture Recognition,Jester test - Hand Gesture Recognition benchmarking,2018-04,DRX3D,96.6,100.0,1.82,1.0,96.6,1.0,Top\\ 1\\ Accuracy
43,1,Hand Gesture Recognition,Jester val - Hand Gesture Recognition benchmarking,2018-04,8-MFFs-3f1c (5 crop),96.33,100.0,96.33,100,96.33,1.0,Top\\ 1\\ Accuracy
44,1,Fine-Grained Image Classification,iNaturalist - Fine-Grained Image Classification benchmarking,2019-03,TASN,68.2,100.0,68.2,100,68.2,0.71,Top\\ 1\\ Accuracy
45,1,Semi-Supervised Image Classification,ImageNet - 10% labeled data - Semi-Supervised Image Classification benchmarking,2019-05,S4L-MOAM (ResNet-50 4×),73.21,100.0,73.21,100,73.21,0.76,Top\\ 1\\ Accuracy
0,1,Action Recognition,UCF101 - Action Recognition benchmarking,2012-12,Baseline UCF101,43.9,44.7,43.9,100,98.2,0.45,3\\-fold\\ Accuracy
1,1,Action Recognition,UCF101 - Action Recognition benchmarking,2014-06,Two-Stream (ImageNet pretrained),88.0,89.61,44.1,0.8122,98.2,0.9,3\\-fold\\ Accuracy
2,1,Action Recognition,UCF101 - Action Recognition benchmarking,2015-03,Two-stream+LSTM,88.6,90.22,0.6,0.011,98.2,0.9,3\\-fold\\ Accuracy
3,1,Action Recognition,UCF101 - Action Recognition benchmarking,2015-05,TDD + IDT,91.5,93.18,2.9,0.0534,98.2,0.93,3\\-fold\\ Accuracy
4,1,Action Recognition,UCF101 - Action Recognition benchmarking,2016-04,LTC,91.7,93.38,0.2,0.0037,98.2,0.93,3\\-fold\\ Accuracy
5,1,Action Recognition,UCF101 - Action Recognition benchmarking,2016-04,"S:VGG-16, T:VGG-16 (ImageNet pretrain)",92.5,94.2,0.8,0.0147,98.2,0.94,3\\-fold\\ Accuracy
6,1,Action Recognition,UCF101 - Action Recognition benchmarking,2016-08,Temporal Segment Networks,94.2,95.93,1.7,0.0313,98.2,0.96,3\\-fold\\ Accuracy
7,1,Action Recognition,UCF101 - Action Recognition benchmarking,2017-04,Hidden Two-Stream,97.1,98.88,2.9,0.0534,98.2,0.99,3\\-fold\\ Accuracy
8,1,Action Recognition,UCF101 - Action Recognition benchmarking,2017-05,Two-Stream I3D (Imagenet+Kinetics pre-training),98.0,99.8,0.9,0.0166,98.2,1.0,3\\-fold\\ Accuracy
9,1,Action Recognition,UCF101 - Action Recognition benchmarking,2019-06,LGD-3D Two-stream,98.2,100.0,0.2,0.0037,98.2,1.0,3\\-fold\\ Accuracy
10,1,Self-Supervised Action Recognition,UCF101 - Self-Supervised Action Recognition benchmarking,2016-09,VideoGan (C3D),52.1,88.61,52.1,100,58.8,0.53,3\\-fold\\ Accuracy
11,1,Self-Supervised Action Recognition,UCF101 - Self-Supervised Action Recognition benchmarking,2019-04,Motion & Appearance (C3D),58.8,100.0,6.7,1.0,58.8,0.6,3\\-fold\\ Accuracy
0,1,Few-Shot Image Classification,CUB-200-2011 - 0-Shot - Few-Shot Image Classification benchmarking,2013-06,ALE,18.0,31.69,18.0,100,56.8,0.19,Top\\-1\\ Accuracy
1,1,Few-Shot Image Classification,CUB-200-2011 - 0-Shot - Few-Shot Image Classification benchmarking,2014-09,SJE,50.1,88.2,32.1,0.8273,56.8,0.53,Top\\-1\\ Accuracy
2,1,Few-Shot Image Classification,CUB-200-2011 - 0-Shot - Few-Shot Image Classification benchmarking,2016-03,Synthesised Classifier,54.7,96.3,4.6,0.1186,56.8,0.58,Top\\-1\\ Accuracy
3,1,Few-Shot Image Classification,CUB-200-2011 - 0-Shot - Few-Shot Image Classification benchmarking,2016-05,Word CNN-RNN (DS-SJE Embedding),56.8,100.0,2.1,0.0541,56.8,0.6,Top\\-1\\ Accuracy
4,1,Action Recognition,Something-Something V2 - Action Recognition benchmarking,2017-06,model3D_1 with left-right augmentation and fps jitter,51.33,77.07,51.33,100,66.6,0.54,Top\\-1\\ Accuracy
5,1,Action Recognition,Something-Something V2 - Action Recognition benchmarking,2017-11,2-Stream TRN,55.52,83.36,4.19,0.2744,66.6,0.59,Top\\-1\\ Accuracy
6,1,Action Recognition,Something-Something V2 - Action Recognition benchmarking,2018-11,TSM (RGB + Flow),66.6,100.0,11.08,0.7256,66.6,0.71,Top\\-1\\ Accuracy
7,1,Image Classification,iNaturalist 2018 - Image Classification benchmarking,2017-07,Inception-V3,60.2,100.0,60.2,100,60.2,0.64,Top\\-1\\ Accuracy
8,1,Action Classification,Kinetics-600 - Action Classification benchmarking,2017-12,S3D-G (RGB+Flow),78.6,94.58,78.6,100,83.1,0.83,Top\\-1\\ Accuracy
9,1,Action Classification,Kinetics-600 - Action Classification benchmarking,2018-12,SlowFast 4x16 (ResNet-50),78.8,94.83,0.2,0.0444,83.1,0.84,Top\\-1\\ Accuracy
10,1,Action Classification,Kinetics-600 - Action Classification benchmarking,2018-12,SlowFast 8x8 (ResNet-50),79.9,96.15,1.1,0.2444,83.1,0.85,Top\\-1\\ Accuracy
11,1,Action Classification,Kinetics-600 - Action Classification benchmarking,2018-12,SlowFast 16x8 (ResNet-101 + NL),81.8,98.44,1.9,0.4222,83.1,0.87,Top\\-1\\ Accuracy
12,1,Action Classification,Kinetics-600 - Action Classification benchmarking,2019-06,LGD-3D Two-stream,83.1,100.0,1.3,0.2889,83.1,0.88,Top\\-1\\ Accuracy
13,1,Action Recognition,Something-Something V1 - Action Recognition benchmarking,2017-12,S3D,47.3,100.0,47.3,100,47.3,0.5,Top\\-1\\ Accuracy
14,1,Self-Supervised Action Recognition,HMDB51 - Self-Supervised Action Recognition benchmarking,2019-04,Motion & Appearance (C3D),20.3,100.0,20.3,100,20.3,0.22,Top\\-1\\ Accuracy
15,1,Action Classification,MiniKinetics - Action Classification benchmarking,2019-06,MARS+RGB+Flow (16 frames),73.5,100.0,73.5,100,73.5,0.78,Top\\-1\\ Accuracy
16,1,Image Classification,VTAB-1k - Image Classification benchmarking,2019-10,SelfSup-Jigsaw-ResNet50,51.1,64.91,51.1,100,78.72,0.54,Top\\-1\\ Accuracy
17,1,Image Classification,VTAB-1k - Image Classification benchmarking,2019-10,S4L-10%-Rotation-ResNet50,64.8,82.32,13.7,0.496,78.72,0.69,Top\\-1\\ Accuracy
18,1,Image Classification,VTAB-1k - Image Classification benchmarking,2019-10,S4L-Rotation-ResNet50,67.5,85.75,2.7,0.0978,78.72,0.72,Top\\-1\\ Accuracy
19,1,Image Classification,VTAB-1k - Image Classification benchmarking,2019-10,ImageNet-ResNet50-LargeHyperSweep,71.2,90.45,3.7,0.134,78.72,0.76,Top\\-1\\ Accuracy
20,1,Image Classification,VTAB-1k - Image Classification benchmarking,2019-10,S4L-Exemplar-ResNet50-LargeHyperSweep,72.7,92.35,1.5,0.0543,78.72,0.77,Top\\-1\\ Accuracy
21,1,Image Classification,VTAB-1k - Image Classification benchmarking,2019-12,BiT-L,76.3,96.93,3.6,0.1303,78.72,0.81,Top\\-1\\ Accuracy
22,1,Image Classification,VTAB-1k - Image Classification benchmarking,2019-12,BiT-L (50 hypers/task),78.72,100.0,2.42,0.0876,78.72,0.83,Top\\-1\\ Accuracy
23,1,Image Classification,ObjectNet - Image Classification benchmarking,2019-12,BiT-L (ResNet-152x4),58.7,100.0,58.7,100,58.7,0.62,Top\\-1\\ Accuracy
24,1,Action Recognition,EPIC-KITCHENS-55 - Action Recognition benchmarking,2020-04,TSM+W3 - full res,34.2,100.0,34.2,100,34.2,0.36,Top\\-1\\ Accuracy
25,1,Action Recognition,EgoGesture - Action Recognition benchmarking,2020-04,TSM+W3,94.3,100.0,94.3,100,94.3,1.0,Top\\-1\\ Accuracy
0,1,3D Human Pose Estimation,Human3.6M - 3D Human Pose Estimation benchmarking,2013-12,LinKDE,162.14,100.0,162.14,100,162.14,1.0,Average\\ MPJPE\\ \\(mm\\)
1,1,Monocular 3D Human Pose Estimation,Human3.6M - Monocular 3D Human Pose Estimation benchmarking,2015-11,Sparseness Meets Deepness,113.0,98.18,113.0,100,115.1,0.7,Average\\ MPJPE\\ \\(mm\\)
2,1,Monocular 3D Human Pose Estimation,Human3.6M - Monocular 3D Human Pose Estimation benchmarking,2018-05,Ordinal Depth Supervision,115.1,100.0,2.1,1.0,115.1,0.71,Average\\ MPJPE\\ \\(mm\\)
3,1,3D Human Pose Estimation,Total Capture - 3D Human Pose Estimation benchmarking,2016-01,Tri-CPM,99.0,92.52,99.0,100,107.0,0.61,Average\\ MPJPE\\ \\(mm\\)
4,1,3D Human Pose Estimation,Total Capture - 3D Human Pose Estimation benchmarking,2017-09,PVH,107.0,100.0,8.0,1.0,107.0,0.66,Average\\ MPJPE\\ \\(mm\\)
5,1,Weakly-supervised 3D Human Pose Estimation,Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking,2017-01,Tome et al.,88.4,74.66,88.4,100,118.4,0.55,Average\\ MPJPE\\ \\(mm\\)
6,1,Weakly-supervised 3D Human Pose Estimation,Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking,2017-04,Pavlakos et al.,118.4,100.0,30.0,1.0,118.4,0.73,Average\\ MPJPE\\ \\(mm\\)
0,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2013-12,VAE,0.168,40.88,0.168,100,0.411,0.0,ARI
1,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2015-11,GAN,0.176,42.82,0.008,0.0329,0.411,0.0,ARI
2,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2017-10,DAC,0.301,73.24,0.125,0.5144,0.411,0.01,ARI
3,1,Image Clustering,CIFAR-10 - Image Clustering benchmarking,2018-07,IIC,0.411,100.0,0.11,0.4527,0.411,0.01,ARI
4,1,3D Object Detection,nuScenes-FB - 3D Object Detection benchmarking,2019-05,RRPN + R101 - FB,51.4,100.0,51.4,100,51.4,0.88,ARI
5,1,3D Object Detection,nuScenes-F - 3D Object Detection benchmarking,2019-05,RRPN + R101 - F,58.2,100.0,58.2,100,58.2,1.0,ARI
0,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2014-03,Cover + SLSVM,22.7,41.65,22.7,100,54.5,0.24,MAP
1,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2015-11,WSDDN-Ens,39.3,72.11,16.6,0.522,54.5,0.41,MAP
2,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2016-11,WCCN,42.8,78.53,3.5,0.1101,54.5,0.44,MAP
3,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2017-04,OICR-Ens + FRCNN,47.0,86.24,4.2,0.1321,54.5,0.49,MAP
4,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2018-02,pipeline method,51.2,93.94,4.2,0.1321,54.5,0.53,MAP
5,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2018-06,WSD+PGE+PGA+FSD2,52.4,96.15,1.2,0.0377,54.5,0.54,MAP
6,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2018-11,Pred Net (Ens),53.6,98.35,1.2,0.0377,54.5,0.56,MAP
7,1,Weakly Supervised Object Detection,PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking,2019-11,Our-Ens,54.5,100.0,0.9,0.0283,54.5,0.57,MAP
8,1,Action Classification,Charades - Action Classification benchmarking,2014-06,2-Strm,18.6,41.15,18.6,100,45.2,0.19,MAP
9,1,Action Classification,Charades - Action Classification benchmarking,2016-12,Asyn-TF,22.4,49.56,3.8,0.1429,45.2,0.23,MAP
10,1,Action Classification,Charades - Action Classification benchmarking,2017-05,I3D,32.9,72.79,10.5,0.3947,45.2,0.34,MAP
11,1,Action Classification,Charades - Action Classification benchmarking,2018-06,PoTion + (GCN + I3D + NL I3D),40.8,90.27,7.9,0.297,45.2,0.42,MAP
12,1,Action Classification,Charades - Action Classification benchmarking,2018-12,"SlowFast (Kinetics-400 pretraining, NL)",42.5,94.03,1.7,0.0639,45.2,0.44,MAP
13,1,Action Classification,Charades - Action Classification benchmarking,2018-12,"SlowFast (Kinetics-600 pretraining, NL)",45.2,100.0,2.7,0.1015,45.2,0.47,MAP
14,1,Object Detection,PASCAL VOC 2007 - Object Detection benchmarking,2014-06,SPP (Overfeat-7),82.44,94.87,82.44,100,86.9,0.86,MAP
15,1,Object Detection,PASCAL VOC 2007 - Object Detection benchmarking,2017-08,CoupleNet,82.7,95.17,0.26,0.0583,86.9,0.86,MAP
16,1,Object Detection,PASCAL VOC 2007 - Object Detection benchmarking,2017-11,RefineDet512+,83.8,96.43,1.1,0.2466,86.9,0.87,MAP
17,1,Object Detection,PASCAL VOC 2007 - Object Detection benchmarking,2018-05,SNIPER,86.9,100.0,3.1,0.6951,86.9,0.9,MAP
18,1,Object Detection,PASCAL VOC 2012 - Object Detection benchmarking,2014-07,SDS,50.7,63.38,50.7,100,80.0,0.53,MAP
19,1,Object Detection,PASCAL VOC 2012 - Object Detection benchmarking,2015-06,YOLO,57.9,72.38,7.2,0.2457,80.0,0.6,MAP
20,1,Object Detection,PASCAL VOC 2012 - Object Detection benchmarking,2015-12,SSD512 (07+12+COCO),80.0,100.0,22.1,0.7543,80.0,0.83,MAP
21,1,Weakly Supervised Object Detection,Charades - Weakly Supervised Object Detection benchmarking,2015-05,R*CNN,0.99,9.87,0.99,100,10.03,0.01,MAP
22,1,Weakly Supervised Object Detection,Charades - Weakly Supervised Object Detection benchmarking,2016-09,ContextLocNet,1.12,11.17,0.13,0.0144,10.03,0.01,MAP
23,1,Weakly Supervised Object Detection,Charades - Weakly Supervised Object Detection benchmarking,2017-08,TD-LSTM,1.98,19.74,0.86,0.0951,10.03,0.02,MAP
24,1,Weakly Supervised Object Detection,Charades - Weakly Supervised Object Detection benchmarking,2018-07,PCL,2.83,28.22,0.85,0.094,10.03,0.03,MAP
25,1,Weakly Supervised Object Detection,Charades - Weakly Supervised Object Detection benchmarking,2019-04,Spatial Prior,10.03,100.0,7.2,0.7965,10.03,0.1,MAP
26,1,Weakly Supervised Object Detection,HICO-DET - Weakly Supervised Object Detection benchmarking,2015-05,R*CNN,2.15,39.89,2.15,100,5.39,0.02,MAP
27,1,Weakly Supervised Object Detection,HICO-DET - Weakly Supervised Object Detection benchmarking,2015-11,WSDDN,3.27,60.67,1.12,0.3457,5.39,0.03,MAP
28,1,Weakly Supervised Object Detection,HICO-DET - Weakly Supervised Object Detection benchmarking,2018-07,PCL,3.62,67.16,0.35,0.108,5.39,0.04,MAP
29,1,Weakly Supervised Object Detection,HICO-DET - Weakly Supervised Object Detection benchmarking,2019-04,Spatial Prior,5.39,100.0,1.77,0.5463,5.39,0.06,MAP
30,1,Real-Time Object Detection,PASCAL VOC 2007 - Real-Time Object Detection benchmarking,2015-06,Faster R-CNN,73.2,89.82,73.2,100,81.5,0.76,MAP
31,1,Real-Time Object Detection,PASCAL VOC 2007 - Real-Time Object Detection benchmarking,2016-05,R-FCN,80.5,98.77,7.3,0.8795,81.5,0.84,MAP
32,1,Real-Time Object Detection,PASCAL VOC 2007 - Real-Time Object Detection benchmarking,2017-08,BlitzNet512 (s8),81.5,100.0,1.0,0.1205,81.5,0.85,MAP
33,1,Weakly Supervised Object Detection,Watercolor2k - Weakly Supervised Object Detection benchmarking,2015-11,WSDDN,12.7,23.39,12.7,100,54.3,0.13,MAP
34,1,Weakly Supervised Object Detection,Watercolor2k - Weakly Supervised Object Detection benchmarking,2018-03,DT+PL,54.3,100.0,41.6,1.0,54.3,0.56,MAP
35,1,Weakly Supervised Object Detection,COCO - Weakly Supervised Object Detection benchmarking,2015-11,ProNet,43.5,76.86,43.5,100,56.6,0.45,MAP
36,1,Weakly Supervised Object Detection,COCO - Weakly Supervised Object Detection benchmarking,2016-03,Deep Feature Maps,47.9,84.63,4.4,0.3359,56.6,0.5,MAP
37,1,Weakly Supervised Object Detection,COCO - Weakly Supervised Object Detection benchmarking,2017-06,MSLPD,56.6,100.0,8.7,0.6641,56.6,0.59,MAP
38,1,Image Retrieval,Oxf105k - Image Retrieval benchmarking,2015-11,R-MAC,61.6,64.71,61.6,100,95.2,0.64,MAP
39,1,Image Retrieval,Oxf105k - Image Retrieval benchmarking,2015-11,R-MAC+R+QE,73.2,76.89,11.6,0.3452,95.2,0.76,MAP
40,1,Image Retrieval,Oxf105k - Image Retrieval benchmarking,2016-04,DIR+QE*,87.8,92.23,14.6,0.4345,95.2,0.91,MAP
41,1,Image Retrieval,Oxf105k - Image Retrieval benchmarking,2016-12,DELF+FT+ATT+DIR+QE,88.5,92.96,0.7,0.0208,95.2,0.92,MAP
42,1,Image Retrieval,Oxf105k - Image Retrieval benchmarking,2018-11,Offline Diffusion,95.2,100.0,6.7,0.1994,95.2,0.99,MAP
43,1,Object Detection,Visual Genome - Object Detection benchmarking,2015-11,AP (%),5.39,72.54,5.39,100,7.43,0.06,MAP
44,1,Object Detection,Visual Genome - Object Detection benchmarking,2017-07,MSDN,7.43,100.0,2.04,1.0,7.43,0.08,MAP
45,1,Image Retrieval,Oxf5k - Image Retrieval benchmarking,2016-04,DIR+QE*,89.0,92.52,89.0,100,96.2,0.93,MAP
46,1,Image Retrieval,Oxf5k - Image Retrieval benchmarking,2016-12,DELF+FT+ATT+DIR+QE,90.0,93.56,1.0,0.1389,96.2,0.94,MAP
47,1,Image Retrieval,Oxf5k - Image Retrieval benchmarking,2018-11,Offline Diffusion,96.2,100.0,6.2,0.8611,96.2,1.0,MAP
48,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2016-04,OIM Loss 45,72.5,79.41,72.5,100,91.3,0.75,MAP
49,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2017-01,VI+LSRO 3,87.4,95.73,14.9,0.7926,91.3,0.91,MAP
50,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2018-10,FD-GAN,91.3,100.0,3.9,0.2074,91.3,0.95,MAP
51,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2016-09,WSDDN + context,35.3,70.18,35.3,100,50.3,0.37,MAP
52,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2016-11,WCCN,37.9,75.35,2.6,0.1733,50.3,0.39,MAP
53,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2017-04,OICR-Ens + FRCNN,42.5,84.49,4.6,0.3067,50.3,0.44,MAP
54,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2017-07,WebRelETH,42.8,85.09,0.3,0.02,50.3,0.44,MAP
55,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2018-04,ZLDN-L,42.9,85.29,0.1,0.0067,50.3,0.45,MAP
56,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2018-06,WSD+PGE+PGA+FSD2,47.8,95.03,4.9,0.3267,50.3,0.5,MAP
57,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2018-11,Pred Net (Ens),49.5,98.41,1.7,0.1133,50.3,0.51,MAP
58,1,Weakly Supervised Object Detection,PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking,2019-10,C-MIDN+FRCNN,50.3,100.0,0.8,0.0533,50.3,0.52,MAP
59,1,Person Re-Identification,CUHK-SYSU - Person Re-Identification benchmarking,2016-10,Deep *,74.0,78.39,74.0,100,94.4,0.77,MAP
60,1,Person Re-Identification,CUHK-SYSU - Person Re-Identification benchmarking,2017-07,Neural,77.9,82.52,3.9,0.1912,94.4,0.81,MAP
61,1,Person Re-Identification,CUHK-SYSU - Person Re-Identification benchmarking,2017-11,AlignedReID,94.4,100.0,16.5,0.8088,94.4,0.98,MAP
62,1,Object Detection,PeopleArt - Object Detection benchmarking,2016-10,Fast R-CNN (VGG16),59.0,100.0,59,100,59,0.61,MAP
63,1,Weakly Supervised Object Detection,ImageNet - Weakly Supervised Object Detection benchmarking,2016-11,WCCN,16.3,83.16,16.3,100,19.6,0.17,MAP
64,1,Weakly Supervised Object Detection,ImageNet - Weakly Supervised Object Detection benchmarking,2018-07,PCL-OB-G-Ens + FRCNN,19.6,100.0,3.3,1.0,19.6,0.2,MAP
65,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R,19.7,25.52,19.7,100,77.2,0.2,MAP
66,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R+XQDA,28.2,36.53,8.5,0.1478,77.2,0.29,MAP
67,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-03,SVDNet-ResNet50,37.3,48.32,9.1,0.1583,77.2,0.39,MAP
68,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-02,HA-CNN (CVPR\'18),38.6,50.0,1.3,0.0226,77.2,0.4,MAP
69,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-04,MGN (ACM MM\'18),66.0,85.49,27.4,0.4765,77.2,0.69,MAP
70,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-10,Pyramid (CVPR\'19),74.8,96.89,8.8,0.153,77.2,0.78,MAP
71,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2020-01,PLR-OSNet,77.2,100.0,2.4,0.0417,77.2,0.8,MAP
72,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2017-01,IDE-R,21.0,26.09,21.0,100,80.5,0.22,MAP
73,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2017-01,IDE-R+XQDA,29.6,36.77,8.6,0.1445,80.5,0.31,MAP
74,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2017-07,PAN+re-rank,45.8,56.89,16.2,0.2723,80.5,0.48,MAP
75,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2018-04,MGN (ACM MM\'18),67.4,83.73,21.6,0.363,80.5,0.7,MAP
76,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2018-10,Pyramid (CVPR\' 19),76.9,95.53,9.5,0.1597,80.5,0.8,MAP
77,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2020-01,PLR-OSNet,80.5,100.0,3.6,0.0605,80.5,0.84,MAP
78,1,Real-Time Object Detection,COCO minival - Real-Time Object Detection benchmarking,2017-03,Mask R-CNN X-101-FPN,37.6,100.0,37.6,100,37.6,0.39,MAP
79,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2017-03,Mask R-CNN X-152-32x8d,40.3,83.44,40.3,100,48.3,0.42,MAP
80,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2019-04,CenterNet Hourglass-104,42.1,87.16,1.8,0.225,48.3,0.44,MAP
81,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2019-04,NAS-FPN AmoebaNet (7 @ 384) + DropBlock,48.3,100.0,6.2,0.775,48.3,0.5,MAP
82,1,Video Object Detection,ImageNet VID - Video Object Detection benchmarking,2017-03,FGFA + Seq-NMS,80.1,93.79,80.1,100,85.4,0.83,MAP
83,1,Video Object Detection,ImageNet VID - Video Object Detection benchmarking,2018-11,Tracklet-Conditioned Detection+DCNv2+FGFA,83.5,97.78,3.4,0.6415,85.4,0.87,MAP
84,1,Video Object Detection,ImageNet VID - Video Object Detection benchmarking,2019-07,SELSA + Faster R-CNN,84.3,98.71,0.8,0.1509,85.4,0.88,MAP
85,1,Video Object Detection,ImageNet VID - Video Object Detection benchmarking,2020-03,MEGA + ResNeXt101,85.4,100.0,1.1,0.2075,85.4,0.89,MAP
86,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,16.4,29.34,16.4,100,55.9,0.17,MAP
87,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,26.2,46.87,9.8,0.2481,55.9,0.27,MAP
88,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),55.9,100.0,29.7,0.7519,55.9,0.58,MAP
89,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,20.5,28.67,20.5,100,71.5,0.21,MAP
90,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,26.7,37.34,6.2,0.1216,71.5,0.28,MAP
91,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),71.5,100.0,44.8,0.8784,71.5,0.74,MAP
92,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R,19.7,25.52,19.7,100,77.2,0.2,MAP
93,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R+XQDA,28.2,36.53,8.5,0.1478,77.2,0.29,MAP
94,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-03,SVDNet-ResNet50,37.3,48.32,9.1,0.1583,77.2,0.39,MAP
95,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-02,HA-CNN (CVPR\'18),38.6,50.0,1.3,0.0226,77.2,0.4,MAP
96,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-04,MGN (ACM MM\'18),66.0,85.49,27.4,0.4765,77.2,0.69,MAP
97,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-10,Pyramid (CVPR\'19),74.8,96.89,8.8,0.153,77.2,0.78,MAP
98,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2020-01,PLR-OSNet,77.2,100.0,2.4,0.0417,77.2,0.8,MAP
99,1,Occluded Face Detection,MAFA - Occluded Face Detection benchmarking,2017-09,AOFD,77.3,87.54,77.3,100,88.3,0.8,MAP
100,1,Occluded Face Detection,MAFA - Occluded Face Detection benchmarking,2017-11,FAN,88.3,100.0,11.0,1.0,88.3,0.92,MAP
101,1,3D Object Detection,NYU Depth v2 - 3D Object Detection benchmarking,2017-11,SGPN-CNN,41.3,100.0,41.3,100,41.3,0.43,MAP
102,1,6D Pose Estimation using RGB,OCCLUSION - 6D Pose Estimation using RGB benchmarking,2017-11,Single-shot deep CNN,0.48,100.0,0.48,100,0.48,0.0,MAP
103,1,Weakly Supervised Object Detection,Comic2k - Weakly Supervised Object Detection benchmarking,2018-03,DT+PL,37.2,100.0,37.2,100,37.2,0.39,MAP
104,1,Weakly Supervised Object Detection,Clipart1k - Weakly Supervised Object Detection benchmarking,2018-03,DT+PL,46.0,100.0,46,100,46,0.48,MAP
105,1,Traffic Sign Recognition,Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking,2018-06,Background Threshold Model,0.41,89.13,0.41,100,0.46,0.0,MAP
106,1,Traffic Sign Recognition,Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking,2018-06,Hierarchical + Background Threshold Model,0.46,100.0,0.05,1.0,0.46,0.0,MAP
107,1,Traffic Sign Recognition,Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking,2018-06,Hierarchical Model,0.3,93.75,0.3,100,0.32,0.0,MAP
108,1,Traffic Sign Recognition,Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking,2018-06,Background Threshold Model,0.32,100.0,0.02,1.0,0.32,0.0,MAP
109,1,Human-Object Interaction Detection,V-COCO - Human-Object Interaction Detection benchmarking,2018-08,GPNN,44.0,85.01,44.0,100,51.76,0.46,MAP
110,1,Human-Object Interaction Detection,V-COCO - Human-Object Interaction Detection benchmarking,2018-08,iCAN,44.7,86.36,0.7,0.0902,51.76,0.46,MAP
111,1,Human-Object Interaction Detection,V-COCO - Human-Object Interaction Detection benchmarking,2018-11,Interactiveness,49.0,94.67,4.3,0.5541,51.76,0.51,MAP
112,1,Human-Object Interaction Detection,V-COCO - Human-Object Interaction Detection benchmarking,2020-03,VSGNet,51.76,100.0,2.76,0.3557,51.76,0.54,MAP
113,1,Weakly Supervised Object Detection,IconArt - Weakly Supervised Object Detection benchmarking,2018-10,MI-max-C,13.2,100.0,13.2,100,13.2,0.14,MAP
114,1,Weakly Supervised Object Detection,PeopleArt - Weakly Supervised Object Detection benchmarking,2018-10,MI-max,55.4,100.0,55.4,100,55.4,0.58,MAP
115,1,Quantization,CIFAR-10 - Quantization benchmarking,2019-02,DTQ,0.792,100.0,0.792,100,0.792,0.01,MAP
116,1,Image Retrieval,NUS-WIDE - Image Retrieval benchmarking,2019-02,DTQ,0.801,100.0,0.801,100,0.801,0.01,MAP
0,1,Face Alignment,AFLW2000 - Face Alignment benchmarking,2014-06,Dlib  (68 points),10.545,100.0,10.545,100,10.545,1.0,Error\\ rate
1,1,Gesture Recognition,Montalbano - Gesture Recognition benchmarking,2015-06,Temp Conv + LSTM,2.77,100.0,2.77,100,2.77,0.26,Error\\ rate
0,1,Action Recognition,HMDB-51 - Action Recognition benchmarking,2014-06,Two-Stream (ImageNet pretrained),59.4,72.02,59.4,100,82.48,0.72,Average\\ accuracy\\ of\\ 3\\ splits
1,1,Action Recognition,HMDB-51 - Action Recognition benchmarking,2015-05,TDD + IDT,65.9,79.9,6.5,0.2816,82.48,0.8,Average\\ accuracy\\ of\\ 3\\ splits
2,1,Action Recognition,HMDB-51 - Action Recognition benchmarking,2016-08,Temporal Segment Networks,69.4,84.14,3.5,0.1516,82.48,0.84,Average\\ accuracy\\ of\\ 3\\ splits
3,1,Action Recognition,HMDB-51 - Action Recognition benchmarking,2017-04,Hidden Two-Stream,78.7,95.42,9.3,0.4029,82.48,0.95,Average\\ accuracy\\ of\\ 3\\ splits
4,1,Action Recognition,HMDB-51 - Action Recognition benchmarking,2017-05,Two-stream I3D,80.9,98.08,2.2,0.0953,82.48,0.98,Average\\ accuracy\\ of\\ 3\\ splits
5,1,Action Recognition,HMDB-51 - Action Recognition benchmarking,2018-10,"RepFlow-50 ([2+1]D CNN, FcF, Non-local block)",81.1,98.33,0.2,0.0087,82.48,0.98,Average\\ accuracy\\ of\\ 3\\ splits
6,1,Action Recognition,HMDB-51 - Action Recognition benchmarking,2019-06,HAF+BoW/FV halluc,82.48,100.0,1.38,0.0598,82.48,1.0,Average\\ accuracy\\ of\\ 3\\ splits
0,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2014-06,LOMO + XQDA,30.75,32.54,30.75,100,94.5,0.31,Rank\\-1
1,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2016-04,OIM,68.1,72.06,37.35,0.5859,94.5,0.69,Rank\\-1
2,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2016-11,DLCE,68.9,72.91,0.8,0.0125,94.5,0.7,Rank\\-1
3,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-03,SVDNet,76.7,81.16,7.8,0.1224,94.5,0.78,Rank\\-1
4,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-08,SVDNet + Random Erasing,79.3,83.92,2.6,0.0408,94.5,0.81,Rank\\-1
5,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-11,PCB (RPP),83.3,88.15,4.0,0.0627,94.5,0.85,Rank\\-1
6,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-11,PSE+ ECN (rank-dist),85.2,90.16,1.9,0.0298,94.5,0.87,Rank\\-1
7,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2018-04,MGN,88.7,93.86,3.5,0.0549,94.5,0.91,Rank\\-1
8,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2018-10,Pyramid (CVPR\'19),89.0,94.18,0.3,0.0047,94.5,0.91,Rank\\-1
9,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2018-12,"st-ReID(RE, RK,Cam)",94.5,100.0,5.5,0.0863,94.5,0.96,Rank\\-1
10,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2014-06,LOMO + XQDA,43.79,44.68,43.79,100,98.0,0.45,Rank\\-1
11,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-03,WARCA,45.16,46.08,1.37,0.0253,98.0,0.46,Rank\\-1
12,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-03,DNS,61.02,62.27,15.86,0.2926,98.0,0.62,Rank\\-1
13,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-07,S-CNN,65.88,67.22,4.86,0.0897,98.0,0.67,Rank\\-1
14,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-10,IDE,72.54,74.02,6.66,0.1229,98.0,0.74,Rank\\-1
15,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-11,DLCE,79.5,81.12,6.96,0.1284,98.0,0.81,Rank\\-1
16,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-01,GAN,83.97,85.68,4.47,0.0825,98.0,0.86,Rank\\-1
17,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,LuNet (RK),84.59,86.32,0.62,0.0114,98.0,0.86,Rank\\-1
18,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,TriNet,84.92,86.65,0.33,0.0061,98.0,0.87,Rank\\-1
19,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,TriNet (RK),86.67,88.44,1.75,0.0323,98.0,0.88,Rank\\-1
20,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-07,PAN (GAN)+re-rank,88.57,90.38,1.9,0.035,98.0,0.9,Rank\\-1
21,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-09,GLAD*,89.9,91.73,1.33,0.0245,98.0,0.92,Rank\\-1
22,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-11,AlignedReID (RK),94.4,96.33,4.5,0.083,98.0,0.96,Rank\\-1
23,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2018-04,MGN,95.7,97.65,1.3,0.024,98.0,0.98,Rank\\-1
24,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2018-12,"st-ReID(RE, RK)",98.0,100.0,2.3,0.0424,98.0,1.0,Rank\\-1
25,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2016-04,OIM Loss 45,77.5,79.24,77.5,100,97.8,0.79,Rank\\-1
26,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2017-01,VI+LSRO 3,84.6,86.5,7.1,0.3498,97.8,0.86,Rank\\-1
27,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2017-03,TriNet,89.63,91.65,5.03,0.2478,97.8,0.91,Rank\\-1
28,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2017-11,AlignedReID (RK),97.8,100.0,8.17,0.4025,97.8,1.0,Rank\\-1
29,1,Person Re-Identification,CUHK-SYSU - Person Re-Identification benchmarking,2016-10,Deep *,76.7,80.15,76.7,100,95.7,0.78,Rank\\-1
30,1,Person Re-Identification,CUHK-SYSU - Person Re-Identification benchmarking,2017-07,Neural,81.2,84.85,4.5,0.2368,95.7,0.83,Rank\\-1
31,1,Person Re-Identification,CUHK-SYSU - Person Re-Identification benchmarking,2017-11,AlignedReID,95.7,100.0,14.5,0.7632,95.7,0.98,Rank\\-1
32,1,Person Re-Identification,MSMT17 - Person Re-Identification benchmarking,2016-11,DLCE,60.48,73.49,60.48,100,82.3,0.62,Rank\\-1
33,1,Person Re-Identification,MSMT17 - Person Re-Identification benchmarking,2019-05,OSNet,78.7,95.63,18.22,0.835,82.3,0.8,Rank\\-1
34,1,Person Re-Identification,MSMT17 - Person Re-Identification benchmarking,2019-08,ABD-Net (ResNet-50),82.3,100.0,3.6,0.165,82.3,0.84,Rank\\-1
35,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2017-01,IDE-R,22.2,26.24,22.2,100,84.6,0.23,Rank\\-1
36,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2017-01,IDE-R+XQDA,32.0,37.83,9.8,0.1571,84.6,0.33,Rank\\-1
37,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2017-07,PAN+re-rank,43.9,51.89,11.9,0.1907,84.6,0.45,Rank\\-1
38,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2018-02,HA-CNN (CVPR\'18),44.4,52.48,0.5,0.008,84.6,0.45,Rank\\-1
39,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2018-04,MGN (ACM MM\'18),68.0,80.38,23.6,0.3782,84.6,0.69,Rank\\-1
40,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2018-10,Pyramid (CVPR\' 19),78.9,93.26,10.9,0.1747,84.6,0.81,Rank\\-1
41,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2018-11,BDB  (ICCV\'19),79.4,93.85,0.5,0.008,84.6,0.81,Rank\\-1
42,1,Person Re-Identification,CUHK03 labeled - Person Re-Identification benchmarking,2020-01,PLR-OSNet,84.6,100.0,5.2,0.0833,84.6,0.86,Rank\\-1
43,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R,21.3,26.49,21.3,100,80.4,0.22,Rank\\-1
44,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R+XQDA,31.1,38.68,9.8,0.1658,80.4,0.32,Rank\\-1
45,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-03,SVDNet-ResNet50,41.5,51.62,10.4,0.176,80.4,0.42,Rank\\-1
46,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-02,HA-CNN (CVPR\'18),41.7,51.87,0.2,0.0034,80.4,0.43,Rank\\-1
47,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-04,MGN (ACM MM\'18),68.0,84.58,26.3,0.445,80.4,0.69,Rank\\-1
48,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-10,Pyramid (CVPR\'19),78.9,98.13,10.9,0.1844,80.4,0.81,Rank\\-1
49,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2020-01,PLR-OSNet,80.4,100.0,1.5,0.0254,80.4,0.82,Rank\\-1
50,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2017-03,LuNet (RK),78.48,87.2,78.48,100,90.0,0.8,Rank\\-1
51,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2017-03,TriNet,79.8,88.67,1.32,0.1146,90.0,0.81,Rank\\-1
52,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2017-03,TriNet (RK),81.21,90.23,1.41,0.1224,90.0,0.83,Rank\\-1
53,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2019-08,NVAN,90.0,100.0,8.79,0.763,90.0,0.92,Rank\\-1
54,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,30.0,41.44,30.0,100,72.4,0.31,Rank\\-1
55,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,46.4,64.09,16.4,0.3868,72.4,0.47,Rank\\-1
56,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),72.4,100.0,26.0,0.6132,72.4,0.74,Rank\\-1
57,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,45.5,52.0,45.5,100,87.5,0.46,Rank\\-1
58,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,57.7,65.94,12.2,0.2905,87.5,0.59,Rank\\-1
59,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),87.5,100.0,29.8,0.7095,87.5,0.89,Rank\\-1
60,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R,21.3,26.49,21.3,100,80.4,0.22,Rank\\-1
61,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-01,IDE-R+XQDA,31.1,38.68,9.8,0.1658,80.4,0.32,Rank\\-1
62,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2017-03,SVDNet-ResNet50,41.5,51.62,10.4,0.176,80.4,0.42,Rank\\-1
63,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-02,HA-CNN (CVPR\'18),41.7,51.87,0.2,0.0034,80.4,0.43,Rank\\-1
64,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-04,MGN (ACM MM\'18),68.0,84.58,26.3,0.445,80.4,0.69,Rank\\-1
65,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2018-10,Pyramid (CVPR\'19),78.9,98.13,10.9,0.1844,80.4,0.81,Rank\\-1
66,1,Person Re-Identification,CUHK03 detected - Person Re-Identification benchmarking,2020-01,PLR-OSNet,80.4,100.0,1.5,0.0254,80.4,0.82,Rank\\-1
67,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-09,DGM+IDE+,56.4,58.39,56.4,100,96.6,0.58,Rank\\-1
68,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-09,DGM+MLAPG+,73.1,75.67,16.7,0.4154,96.6,0.75,Rank\\-1
69,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-10,SMP*,80.9,83.75,7.8,0.194,96.6,0.83,Rank\\-1
70,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2018-06,Snippet (Supervised),93.0,96.27,12.1,0.301,96.6,0.95,Rank\\-1
71,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2019-11,B-BOT + Attention and CL loss*,96.6,100.0,3.6,0.0896,96.6,0.99,Rank\\-1
72,1,Unsupervised Person Re-Identification,MSMT17->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN,46.4,100.0,46.4,100,46.4,0.47,Rank\\-1
73,1,Person Re-Identification,UAV-Human - Person Re-Identification benchmarking,2017-11,PCB,62.19,99.54,62.19,100,62.48,0.63,Rank\\-1
74,1,Person Re-Identification,UAV-Human - Person Re-Identification benchmarking,2019-03,Tricks,62.48,100.0,0.29,1.0,62.48,0.64,Rank\\-1
75,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2018-09,TAUDL,26.1,59.59,26.1,100,43.8,0.27,Rank\\-1
76,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2019-03,UTAL,43.8,100.0,17.7,1.0,43.8,0.45,Rank\\-1
77,1,Unsupervised Person Re-Identification,Market-1501->MSMT17 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),27.6,100.0,27.6,100,27.6,0.28,Rank\\-1
78,1,Unsupervised Person Re-Identification,DukeMTMC-reID->MSMT17 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),43.6,100.0,43.6,100,43.6,0.44,Rank\\-1
79,1,Person Re-Identification,iLIDS-VID - Person Re-Identification benchmarking,2019-03,UTAL,35.1,64.29,35.1,100,54.6,0.36,Rank\\-1
80,1,Person Re-Identification,iLIDS-VID - Person Re-Identification benchmarking,2019-08,TKP,54.6,100.0,19.5,1.0,54.6,0.56,Rank\\-1
81,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,63.3,81.15,63.3,100,78.0,0.65,Rank\\-1
82,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,78.0,100.0,14.7,1.0,78.0,0.8,Rank\\-1
83,1,Unsupervised Person Re-Identification,DukeMTMC-reID->Market-1501 - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,75.1,100.0,75.1,100,75.1,0.77,Rank\\-1
0,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2014-06,LOMO + XQDA,22.22,23.27,22.22,100,95.5,0.23,mAP
1,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-03,DNS,35.68,37.36,13.46,0.1837,95.5,0.36,mAP
2,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-07,S-CNN,39.55,41.41,3.87,0.0528,95.5,0.4,mAP
3,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-10,IDE,46.0,48.17,6.45,0.088,95.5,0.47,mAP
4,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2016-11,DLCE,59.9,62.72,13.9,0.1897,95.5,0.61,mAP
5,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-01,GAN,66.07,69.18,6.17,0.0842,95.5,0.68,mAP
6,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,LuNet (RK),75.62,79.18,9.55,0.1303,95.5,0.77,mAP
7,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,TriNet (RK),81.07,84.89,5.45,0.0744,95.5,0.83,mAP
8,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-07,PAN (GAN)+re-rank,81.53,85.37,0.46,0.0063,95.5,0.83,mAP
9,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-11,AlignedReID (RK),90.7,94.97,9.17,0.1251,95.5,0.93,mAP
10,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2018-07,SSP-ReID (RR),90.8,95.08,0.1,0.0014,95.5,0.93,mAP
11,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2018-11,Parameter-Free Spatial Attention (RK),91.7,96.02,0.9,0.0123,95.5,0.94,mAP
12,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2018-12,"st-ReID(RE, RK)",95.5,100.0,3.8,0.0519,95.5,0.98,mAP
13,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2014-06,LOMO + XQDA,17.04,18.38,17.04,100,92.7,0.17,mAP
14,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2016-04,OIM,47.4,51.13,30.36,0.4013,92.7,0.48,mAP
15,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2016-11,DLCE,49.3,53.18,1.9,0.0251,92.7,0.5,mAP
16,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-03,SVDNet,56.8,61.27,7.5,0.0991,92.7,0.58,mAP
17,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-07,PAN + re-rank,66.74,72.0,9.94,0.1314,92.7,0.68,mAP
18,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-11,PCB (RPP),69.2,74.65,2.46,0.0325,92.7,0.71,mAP
19,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2017-11,PSE+ ECN (rank-dist),79.8,86.08,10.6,0.1401,92.7,0.82,mAP
20,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2018-05,DaRe(De)+RE+RR [wang2018resource],80.0,86.3,0.2,0.0026,92.7,0.82,mAP
21,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2018-07,SSP-ReID (RR),83.7,90.29,3.7,0.0489,92.7,0.86,mAP
22,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2018-11,Parameter-Free Spatial Attention,85.9,92.66,2.2,0.0291,92.7,0.88,mAP
23,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2018-12,"st-ReID(RE, RK,Cam)",92.7,100.0,6.8,0.0899,92.7,0.95,mAP
24,1,Human-Object Interaction Detection,HICO - Human-Object Interaction Detection benchmarking,2015-05,R*CNN,28.5,60.51,28.5,100,47.1,0.29,mAP
25,1,Human-Object Interaction Detection,HICO - Human-Object Interaction Detection benchmarking,2016-04,Mallya & Lazebnik,36.1,76.65,7.6,0.4086,47.1,0.37,mAP
26,1,Human-Object Interaction Detection,HICO - Human-Object Interaction Detection benchmarking,2018-07,Pairwise-Part,39.9,84.71,3.8,0.2043,47.1,0.41,mAP
27,1,Human-Object Interaction Detection,HICO - Human-Object Interaction Detection benchmarking,2019-04,HAKE,47.1,100.0,7.2,0.3871,47.1,0.48,mAP
28,1,Action Detection,Multi-THUMOS - Action Detection benchmarking,2015-07,Two-stream + LSTM,28.1,60.56,28.1,100,46.4,0.29,mAP
29,1,Action Detection,Multi-THUMOS - Action Detection benchmarking,2017-12,I3D + our super-event,36.4,78.45,8.3,0.4536,46.4,0.37,mAP
30,1,Action Detection,Multi-THUMOS - Action Detection benchmarking,2018-03,TGM,46.4,100.0,10.0,0.5464,46.4,0.47,mAP
31,1,Image Retrieval,Par6k - Image Retrieval benchmarking,2015-11,R-MAC,83.0,84.87,83.0,100,97.8,0.85,mAP
32,1,Image Retrieval,Par6k - Image Retrieval benchmarking,2015-11,R-MAC+R+QE,86.5,88.45,3.5,0.2365,97.8,0.88,mAP
33,1,Image Retrieval,Par6k - Image Retrieval benchmarking,2016-04,DIR+QE*,93.8,95.91,7.3,0.4932,97.8,0.96,mAP
34,1,Image Retrieval,Par6k - Image Retrieval benchmarking,2016-12,DELF+FT+ATT+DIR+QE,95.7,97.85,1.9,0.1284,97.8,0.98,mAP
35,1,Image Retrieval,Par6k - Image Retrieval benchmarking,2018-11,Offline Diffusion,97.8,100.0,2.1,0.1419,97.8,1.0,mAP
36,1,Image Retrieval,Par106k - Image Retrieval benchmarking,2015-11,R-MAC,75.7,78.69,75.7,100,96.2,0.77,mAP
37,1,Image Retrieval,Par106k - Image Retrieval benchmarking,2015-11,R-MAC+R+QE,79.8,82.95,4.1,0.2,96.2,0.82,mAP
38,1,Image Retrieval,Par106k - Image Retrieval benchmarking,2016-04,DIR+QE*,90.5,94.07,10.7,0.522,96.2,0.93,mAP
39,1,Image Retrieval,Par106k - Image Retrieval benchmarking,2016-12,DELF+FT+ATT+DIR+QE,92.8,96.47,2.3,0.1122,96.2,0.95,mAP
40,1,Image Retrieval,Par106k - Image Retrieval benchmarking,2018-11,Offline Diffusion,96.2,100.0,3.4,0.1659,96.2,0.98,mAP
41,1,Action Recognition,ActivityNet - Action Recognition benchmarking,2015-12,VGG19,52.3,97.21,52.3,100,53.8,0.53,mAP
42,1,Action Recognition,ActivityNet - Action Recognition benchmarking,2015-12,VGG19 + 393K webcam images,53.8,100.0,1.5,1.0,53.8,0.55,mAP
43,1,Temporal Action Localization,MEXaction2 - Temporal Action Localization benchmarking,2016-01,S-CNN,7.4,100.0,7.4,100,7.4,0.08,mAP
44,1,Person Re-Identification,MSMT17 - Person Re-Identification benchmarking,2016-11,DLCE,31.58,51.94,31.58,100,60.8,0.32,mAP
45,1,Person Re-Identification,MSMT17 - Person Re-Identification benchmarking,2019-05,OSNet,52.9,87.01,21.32,0.7296,60.8,0.54,mAP
46,1,Person Re-Identification,MSMT17 - Person Re-Identification benchmarking,2019-08,ABD-Net (ResNet-50),60.8,100.0,7.9,0.2704,60.8,0.62,mAP
47,1,Pose Estimation,UAV-Human - Pose Estimation benchmarking,2016-12,AlphaPose,56.9,100.0,56.9,100,56.9,0.58,mAP
48,1,Action Detection,Charades - Action Detection benchmarking,2016-12,Sigurdsson et al.,9.6,43.05,9.6,100,22.3,0.1,mAP
49,1,Action Detection,Charades - Action Detection benchmarking,2017-03,R-C3D,12.4,55.61,2.8,0.2205,22.3,0.13,mAP
50,1,Action Detection,Charades - Action Detection benchmarking,2017-12,Super-events (RGB+Flow),19.41,87.04,7.01,0.552,22.3,0.2,mAP
51,1,Action Detection,Charades - Action Detection benchmarking,2018-03,TGM (RGB+Flow),22.3,100.0,2.89,0.2276,22.3,0.23,mAP
52,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2017-03,SSN,32.26,94.63,32.26,100,34.09,0.33,mAP
53,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2019-07,BMN,33.85,99.3,1.59,0.8689,34.09,0.35,mAP
54,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2019-11,G-TAD,34.09,100.0,0.24,0.1311,34.09,0.35,mAP
55,1,Action Classification,THUMOS’14 - Action Classification benchmarking,2017-03,UntrimmedNets,82.2,96.03,82.2,100,85.6,0.84,mAP
56,1,Action Classification,THUMOS’14 - Action Classification benchmarking,2018-07,W-TALC,85.6,100.0,3.4,1.0,85.6,0.88,mAP
57,1,Action Classification,ActivityNet-1.2 - Action Classification benchmarking,2017-03,UntrimmedNets,87.7,94.1,87.7,100,93.2,0.9,mAP
58,1,Action Classification,ActivityNet-1.2 - Action Classification benchmarking,2018-07,W-TALC,93.2,100.0,5.5,1.0,93.2,0.95,mAP
59,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2017-03,LuNet (RK),73.68,83.25,73.68,100,88.5,0.75,mAP
60,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2017-03,TriNet (RK),77.43,87.49,3.75,0.253,88.5,0.79,mAP
61,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2019-08,NVAN,82.8,93.56,5.37,0.3623,88.5,0.85,mAP
62,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2019-11,B-BOT + OSM + CL Centers* (Re-rank),88.5,100.0,5.7,0.3846,88.5,0.9,mAP
63,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2017-04,InteractNet,9.94,43.89,9.94,100,22.65,0.1,mAP
64,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2018-08,GPNN,13.11,57.88,3.17,0.2494,22.65,0.13,mAP
65,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2018-08,iCAN,14.84,65.52,1.73,0.1361,22.65,0.15,mAP
66,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2018-11,Interactiveness (CVPR\'19),17.54,77.44,2.7,0.2124,22.65,0.18,mAP
67,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2018-11,TIN-PAMI,17.84,78.76,0.3,0.0236,22.65,0.18,mAP
68,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2019-12,PPDM,21.92,96.78,4.08,0.321,22.65,0.22,mAP
69,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2020-04,HAKE,22.65,100.0,0.73,0.0574,22.65,0.23,mAP
70,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2017-10,PoseTrack,59.22,79.01,59.22,100,74.95,0.61,mAP
71,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2017-12,ProTracker,59.56,79.47,0.34,0.0216,74.95,0.61,mAP
72,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2018-02,PoseFlow,62.95,83.99,3.39,0.2155,74.95,0.64,mAP
73,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2018-04,MSRA (FlowTrack),74.57,99.49,11.62,0.7387,74.95,0.76,mAP
74,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2019-02,HRNet-W48 COCO,74.95,100.0,0.38,0.0242,74.95,0.77,mAP
75,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,22.8,32.02,22.8,100,71.2,0.23,mAP
76,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,58.3,81.88,35.5,0.7335,71.2,0.6,mAP
77,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,71.2,100.0,12.9,0.2665,71.2,0.73,mAP
78,1,Unsupervised Person Re-Identification,MSMT17->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN,26.2,100.0,26.2,100,26.2,0.27,mAP
79,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,22.3,34.25,22.3,100,65.1,0.23,mAP
80,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,53.4,82.03,31.1,0.7266,65.1,0.55,mAP
81,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,65.1,100.0,11.7,0.2734,65.1,0.67,mAP
82,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2017-11,PTGAN,3.3,14.16,3.3,100,23.3,0.03,mAP
83,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,13.3,57.08,10.0,0.5,23.3,0.14,mAP
84,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,23.3,100.0,10.0,0.5,23.3,0.24,mAP
85,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2017-11,PTGAN,2.9,12.66,2.9,100,22.9,0.03,mAP
86,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,13.2,57.64,10.3,0.515,22.9,0.13,mAP
87,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,22.9,100.0,9.7,0.485,22.9,0.23,mAP
88,1,Person Re-Identification,UAV-Human - Person Re-Identification benchmarking,2017-11,PCB,61.05,96.28,61.05,100,63.41,0.62,mAP
89,1,Person Re-Identification,UAV-Human - Person Re-Identification benchmarking,2019-03,Tricks,63.41,100.0,2.36,1.0,63.41,0.65,mAP
90,1,Image-to-Image Translation,Cityscapes-to-Foggy Cityscapes - Image-to-Image Translation benchmarking,2018-03,FRCNN in the wild,27.6,74.8,27.6,100,36.9,0.28,mAP
91,1,Image-to-Image Translation,Cityscapes-to-Foggy Cityscapes - Image-to-Image Translation benchmarking,2019-05,Diversify & Match,34.6,93.77,7.0,0.7527,36.9,0.35,mAP
92,1,Image-to-Image Translation,Cityscapes-to-Foggy Cityscapes - Image-to-Image Translation benchmarking,2019-10,Progressive Domain Adaptation,36.9,100.0,2.3,0.2473,36.9,0.38,mAP
93,1,Zero-Shot Object Detection,ImageNet Detection - Zero-Shot Object Detection benchmarking,2018-03,ZSD-Rahman etal,16.4,100.0,16.4,100,16.4,0.17,mAP
94,1,Pose Tracking,PoseTrack2018 - Pose Tracking benchmarking,2018-04,MSRA,74.03,100.0,74.03,100,74.03,0.76,mAP
95,1,Object Detection In Aerial Images,DOTA - Object Detection In Aerial Images benchmarking,2018-07,Multi-class object detection in unconstrained RS imagery,68.16,100.0,68.16,100,68.16,0.7,mAP
96,1,Human-Object Interaction Detection,Ambiguious-HOI - Human-Object Interaction Detection benchmarking,2018-08,iCAN,8.14,99.03,8.14,100,8.22,0.08,mAP
97,1,Human-Object Interaction Detection,Ambiguious-HOI - Human-Object Interaction Detection benchmarking,2018-11,TIN,8.22,100.0,0.08,1.0,8.22,0.08,mAP
98,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2018-09,TAUDL,20.8,56.83,20.8,100,36.6,0.21,mAP
99,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2019-03,UTAL,36.6,100.0,15.8,1.0,36.6,0.37,mAP
100,1,Zero-Shot Object Detection,MS-COCO - Zero-Shot Object Detection benchmarking,2018-11,ZSD-Polarity Loss,12.62,100.0,12.62,100,12.62,0.13,mAP
101,1,Unsupervised Person Re-Identification,Market-1501->MSMT17 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),11.8,51.53,11.8,100,22.9,0.12,mAP
102,1,Unsupervised Person Re-Identification,Market-1501->MSMT17 - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,22.9,100.0,11.1,1.0,22.9,0.23,mAP
103,1,Unsupervised Person Re-Identification,DukeMTMC-reID->MSMT17 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),23.6,100.0,23.6,100,23.6,0.24,mAP
104,1,Action Recognition,AVA v2.2 - Action Recognition benchmarking,2018-12,"SlowFast, 8x8, R101 (Kinetics-400 pretraining)",23.8,86.55,23.8,100,27.5,0.24,mAP
105,1,Action Recognition,AVA v2.2 - Action Recognition benchmarking,2018-12,"SlowFast, 8x8 R101+NL (Kinetics-600 pretraining)",27.1,98.55,3.3,0.8919,27.5,0.28,mAP
106,1,Action Recognition,AVA v2.2 - Action Recognition benchmarking,2018-12,"SlowFast, 16x8 R101+NL (Kinetics-600 pretraining)",27.5,100.0,0.4,0.1081,27.5,0.28,mAP
107,1,Activity Prediction,ActEV - Activity Prediction benchmarking,2019-02,Next,0.192,100.0,0.192,100,0.192,0.0,mAP
108,1,3D Instance Segmentation,ScanNet - 3D Instance Segmentation benchmarking,2019-02,MASC,0.447,100.0,0.447,100,0.447,0.0,mAP
109,1,Action Detection,UCF101-24 - Action Detection benchmarking,2019-04,two-in-one two stream,22.02,100.0,22.02,100,22.02,0.23,mAP
110,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,40.4,62.06,40.4,100,65.1,0.41,mAP
111,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,65.1,100.0,24.7,1.0,65.1,0.67,mAP
112,1,Unsupervised Person Re-Identification,DukeMTMC-reID->Market-1501 - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,43.0,60.39,43.0,100,71.2,0.44,mAP
113,1,Unsupervised Person Re-Identification,DukeMTMC-reID->Market-1501 - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,71.2,100.0,28.2,1.0,71.2,0.73,mAP
114,1,Action Classification,THUMOS'14 - Action Classification benchmarking,2019-08,3C-Net,86.9,100.0,86.9,100,86.9,0.89,mAP
115,1,3D Object Detection,nuScenes - 3D Object Detection benchmarking,2019-08,MEGVII,0.528,100.0,0.528,100,0.528,0.01,mAP
116,1,Depth Estimation,NYU-Depth V2 - Depth Estimation benchmarking,2019-08,A2J,8.61,100.0,8.61,100,8.61,0.09,mAP
117,1,Fine-Grained Image Classification,Con-Text - Fine-Grained Image Classification benchmarking,2020-01,PHOC descriptor + Fisher Vector Encoding,80.2,100.0,80.2,100,80.2,0.82,mAP
118,1,Fine-Grained Image Classification,Bottles - Fine-Grained Image Classification benchmarking,2020-01,PHOC descriptor + Fisher Vector Encoding,77.4,100.0,77.4,100,77.4,0.79,mAP
0,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2014-06,DeepVideo’s Slow Fusion,60.9,80.66,60.9,100,75.5,0.81,Video\\ hit\\-at\\-1
1,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2014-12,C3D,61.1,80.93,0.2,0.0137,75.5,0.81,Video\\ hit\\-at\\-1
2,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2015-03,Conv pooling,71.7,94.97,10.6,0.726,75.5,0.95,Video\\ hit\\-at\\-1
3,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2017-11,R[2+1]D-Two-Stream-32frame,73.3,97.09,1.6,0.1096,75.5,0.97,Video\\ hit\\-at\\-1
4,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2019-04,ip-CSN-101 (RGB),74.9,99.21,1.6,0.1096,75.5,0.99,Video\\ hit\\-at\\-1
5,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2019-04,ip-CSN-152 (RGB),75.5,100.0,0.6,0.0411,75.5,1.0,Video\\ hit\\-at\\-1
6,1,Action Recognition,miniSports - Action Recognition benchmarking,2019-05,G-Blend,62.8,100.0,62.8,100,62.8,0.83,Video\\ hit\\-at\\-1
0,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2014-06,DeepVideo’s Slow Fusion,80.2,86.42,80.2,100,92.8,0.86,Video\\ hit\\-at\\-5
1,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2014-12,C3D,85.5,92.13,5.3,0.4206,92.8,0.92,Video\\ hit\\-at\\-5
2,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2015-03,Conv pooling,90.4,97.41,4.9,0.3889,92.8,0.97,Video\\ hit\\-at\\-5
3,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2017-11,R[2+1]D-Two-Stream-32frame,91.9,99.03,1.5,0.119,92.8,0.99,Video\\ hit\\-at\\-5
4,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2019-04,ip-CSN-101 (RGB),92.6,99.78,0.7,0.0556,92.8,1.0,Video\\ hit\\-at\\-5
5,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2019-04,ip-CSN-152 (RGB),92.8,100.0,0.2,0.0159,92.8,1.0,Video\\ hit\\-at\\-5
6,1,Action Recognition,miniSports - Action Recognition benchmarking,2019-05,G-Blend,85.5,100.0,85.5,100,85.5,0.92,Video\\ hit\\-at\\-5
0,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2014-06,DeepVideo’s Slow Fusion,41.9,73.51,41.9,100,57.0,0.74,Clip\\ Hit\\-at\\-1
1,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2014-12,C3D,46.1,80.88,4.2,0.2781,57.0,0.81,Clip\\ Hit\\-at\\-1
2,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2017-11,P3D,47.9,84.04,1.8,0.1192,57.0,0.84,Clip\\ Hit\\-at\\-1
3,1,Action Recognition,Sports-1M - Action Recognition benchmarking,2017-11,R[2+1]D-RGB-32frame,57.0,100.0,9.1,0.6026,57.0,1.0,Clip\\ Hit\\-at\\-1
4,1,Action Recognition,miniSports - Action Recognition benchmarking,2019-05,G-Blend,49.7,100.0,49.7,100,49.7,0.87,Clip\\ Hit\\-at\\-1
0,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-04,P-LSTM,60.0,78.95,60,100,76,0.63,Accuracy\\ \\(CS\\)
1,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2017-04,Res-TCN,63.0,82.89,3,0.1875,76,0.67,Accuracy\\ \\(CS\\)
2,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2018-01,ST-GCN,71.0,93.42,8,0.5,76,0.75,Accuracy\\ \\(CS\\)
3,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2019-04,VS-CNN,76.0,100.0,5,0.3125,76,0.8,Accuracy\\ \\(CS\\)
4,1,Multimodal Activity Recognition,UTD-MHAD - Multimodal Activity Recognition benchmarking,2018-06,PoseMap,94.5,100.0,94.5,100,94.5,1.0,Accuracy\\ \\(CS\\)
0,1,Action Recognition,VIRAT Ground 2.0 - Action Recognition benchmarking,2014-06,DHCM,66.45,100.0,66.45,100,66.45,0.71,Average\\ Accuracy
1,1,Domain Adaptation,Office-Caltech - Domain Adaptation benchmarking,2014-12,DDC[[Tzeng et al.2014]],88.2,94.84,88.2,100,93.0,0.95,Average\\ Accuracy
2,1,Domain Adaptation,Office-Caltech - Domain Adaptation benchmarking,2015-02,DAN[[Long et al.2015]],90.1,96.88,1.9,0.3958,93.0,0.97,Average\\ Accuracy
3,1,Domain Adaptation,Office-Caltech - Domain Adaptation benchmarking,2018-07,MEDA[[Wang et al.2018]],92.8,99.78,2.7,0.5625,93.0,1.0,Average\\ Accuracy
4,1,Domain Adaptation,Office-Caltech - Domain Adaptation benchmarking,2019-11,SPL,93.0,100.0,0.2,0.0417,93.0,1.0,Average\\ Accuracy
5,1,Domain Adaptation,Office-31 - Domain Adaptation benchmarking,2015-12,ResNet-50,76.1,84.0,76.1,100,90.6,0.82,Average\\ Accuracy
6,1,Domain Adaptation,Office-31 - Domain Adaptation benchmarking,2017-04,GTA,86.5,95.47,10.4,0.7172,90.6,0.93,Average\\ Accuracy
7,1,Domain Adaptation,Office-31 - Domain Adaptation benchmarking,2018-11,IAFN+ENT,87.1,96.14,0.6,0.0414,90.6,0.94,Average\\ Accuracy
8,1,Domain Adaptation,Office-31 - Domain Adaptation benchmarking,2019-01,Contrastive Adaptation Network,90.6,100.0,3.5,0.2414,90.6,0.97,Average\\ Accuracy
9,1,3D Reconstruction,Scan2CAD - 3D Reconstruction benchmarking,2016-03,3DMatch,10.29,32.48,10.29,100,31.68,0.11,Average\\ Accuracy
10,1,3D Reconstruction,Scan2CAD - 3D Reconstruction benchmarking,2018-11,Scan2CAD,31.68,100.0,21.39,1.0,31.68,0.34,Average\\ Accuracy
11,1,Scene Segmentation,ScanNet - Scene Segmentation benchmarking,2016-12,PointNet++,60.2,80.27,60.2,100,75.0,0.65,Average\\ Accuracy
12,1,Scene Segmentation,ScanNet - Scene Segmentation benchmarking,2018-03,3DMV,75.0,100.0,14.8,1.0,75.0,0.81,Average\\ Accuracy
13,1,Facial Action Unit Detection,BP4D - Facial Action Unit Detection benchmarking,2017-02,Baseline,56.1,68.58,56.1,100,81.8,0.6,Average\\ Accuracy
14,1,Facial Action Unit Detection,BP4D - Facial Action Unit Detection benchmarking,2017-04,Multi-View,81.8,100.0,25.7,1.0,81.8,0.88,Average\\ Accuracy
15,1,Skeleton Based Action Recognition,UAV-Human - Skeleton Based Action Recognition benchmarking,2018-01,ST-GCN,30.25,86.83,30.25,100,34.84,0.33,Average\\ Accuracy
16,1,Skeleton Based Action Recognition,UAV-Human - Skeleton Based Action Recognition benchmarking,2018-05,2S-AGCN,34.84,100.0,4.59,1.0,34.84,0.37,Average\\ Accuracy
17,1,Egocentric Activity Recognition,EGTEA - Egocentric Activity Recognition benchmarking,2018-07,Ego-RNN,60.8,96.97,60.8,100,62.7,0.65,Average\\ Accuracy
18,1,Egocentric Activity Recognition,EGTEA - Egocentric Activity Recognition benchmarking,2018-11,LSTA,61.9,98.72,1.1,0.5789,62.7,0.67,Average\\ Accuracy
19,1,Egocentric Activity Recognition,EGTEA - Egocentric Activity Recognition benchmarking,2020-02,SAP,62.7,100.0,0.8,0.4211,62.7,0.67,Average\\ Accuracy
0,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2014-07,SDS,51.6,57.98,51.6,100,89.0,0.52,Mean\\ IoU
1,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2014-11,FCN (VGG-16),62.2,69.89,10.6,0.2834,89.0,0.62,Mean\\ IoU
2,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2014-12,DeepLab-MSc-CRF-LargeFOV (VGG-16),71.6,80.45,9.4,0.2513,89.0,0.72,Mean\\ IoU
3,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2015-02,CRF-RNN,74.7,83.93,3.1,0.0829,89.0,0.75,Mean\\ IoU
4,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2016-03,CentraleSupelec Deep G-CRF,80.2,90.11,5.5,0.1471,89.0,0.8,Mean\\ IoU
5,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2016-11,Multipath-RefineNet,84.2,94.61,4.0,0.107,89.0,0.84,Mean\\ IoU
6,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2016-11,ResNet-38 MS COCO,84.9,95.39,0.7,0.0187,89.0,0.85,Mean\\ IoU
7,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2016-12,PSPNet,85.4,95.96,0.5,0.0134,89.0,0.85,Mean\\ IoU
8,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2017-06,DeepLabv3-JFT,86.9,97.64,1.5,0.0401,89.0,0.87,Mean\\ IoU
9,1,Semantic Segmentation,PASCAL VOC 2012 test - Semantic Segmentation benchmarking,2018-02,DeepLabv3+ (Xception-JFT),89.0,100.0,2.1,0.0561,89.0,0.89,Mean\\ IoU
10,1,Semantic Segmentation,SkyScapes-Lane - Semantic Segmentation benchmarking,2014-11,FCN8s (ResNet-50),13.74,26.98,13.74,100,50.93,0.14,Mean\\ IoU
11,1,Semantic Segmentation,SkyScapes-Lane - Semantic Segmentation benchmarking,2019-10,SkyScapesNet-Lane,50.93,100.0,37.19,1.0,50.93,0.51,Mean\\ IoU
12,1,Semantic Segmentation,SkyScapes-Dense - Semantic Segmentation benchmarking,2014-11,FCN8s (ResNet-50),33.06,82.38,33.06,100,40.13,0.33,Mean\\ IoU
13,1,Semantic Segmentation,SkyScapes-Dense - Semantic Segmentation benchmarking,2018-02,DeepLabv3+,38.2,95.19,5.14,0.727,40.13,0.38,Mean\\ IoU
14,1,Semantic Segmentation,SkyScapes-Dense - Semantic Segmentation benchmarking,2019-10,SkyScapesNet-Dense,40.13,100.0,1.93,0.273,40.13,0.4,Mean\\ IoU
15,1,Semantic Segmentation,CamVid - Semantic Segmentation benchmarking,2014-12,DeepLab-MSc-CRF-LargeFOV,61.6,75.4,61.6,100,81.7,0.62,Mean\\ IoU
16,1,Semantic Segmentation,CamVid - Semantic Segmentation benchmarking,2015-11,Dilated Convolutions,65.3,79.93,3.7,0.1841,81.7,0.65,Mean\\ IoU
17,1,Semantic Segmentation,CamVid - Semantic Segmentation benchmarking,2016-11,FC-DenseNet103,66.9,81.88,1.6,0.0796,81.7,0.67,Mean\\ IoU
18,1,Semantic Segmentation,CamVid - Semantic Segmentation benchmarking,2016-12,PSPNet,69.1,84.58,2.2,0.1095,81.7,0.69,Mean\\ IoU
19,1,Semantic Segmentation,CamVid - Semantic Segmentation benchmarking,2018-12,DeepLabV3Plus + SDCNetAug,81.7,100.0,12.6,0.6269,81.7,0.82,Mean\\ IoU
20,1,Scene Segmentation,SUN-RGBD - Scene Segmentation benchmarking,2014-12,DeepLab-LargeFOV,32.08,95.82,32.08,100,33.48,0.32,Mean\\ IoU
21,1,Scene Segmentation,SUN-RGBD - Scene Segmentation benchmarking,2019-08,Index Network,33.48,100.0,1.4,1.0,33.48,0.34,Mean\\ IoU
22,1,Cell Segmentation,PhC-U373 - Cell Segmentation benchmarking,2015-05,U-Net,0.9203,100.0,0.9203,100,0.9203,0.01,Mean\\ IoU
23,1,Cell Segmentation,DIC-HeLa - Cell Segmentation benchmarking,2015-05,U-Net,0.7756,100.0,0.7756,100,0.7756,0.01,Mean\\ IoU
24,1,Semantic Segmentation,PASCAL VOC 2011 - Semantic Segmentation benchmarking,2016-11,DLDL-8s+CRF,67.6,100.0,67.6,100,67.6,0.68,Mean\\ IoU
25,1,Semantic Segmentation,PASCAL VOC 2012 - Semantic Segmentation benchmarking,2016-11,DLDL-8s+CRF,67.1,100.0,67.1,100,67.1,0.67,Mean\\ IoU
26,1,Semantic Segmentation,NYU Depth v2 - Semantic Segmentation benchmarking,2016-11,RefineNet (ResNet-101),40.6,93.33,40.6,100,43.5,0.41,Mean\\ IoU
27,1,Semantic Segmentation,NYU Depth v2 - Semantic Segmentation benchmarking,2020-04,TD2-PSP50,43.5,100.0,2.9,1.0,43.5,0.44,Mean\\ IoU
28,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2016-12,PointNet,47.6,67.42,47.6,100,70.6,0.48,Mean\\ IoU
29,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2017-11,SPG,62.1,87.96,14.5,0.6304,70.6,0.62,Mean\\ IoU
30,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2018-12,PointCNN,65.4,92.63,3.3,0.1435,70.6,0.65,Mean\\ IoU
31,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2019-04,ConvPoint,68.2,96.6,2.8,0.1217,70.6,0.68,Mean\\ IoU
32,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2019-04,KPConv,70.6,100.0,2.4,0.1043,70.6,0.71,Mean\\ IoU
33,1,Lesion Segmentation,ISIC 2017 - Lesion Segmentation benchmarking,2017-03,Automatic skin lesion segmentation with fully convolutional-deconvolutional networks,0.765,100.0,0.765,100,0.765,0.01,Mean\\ IoU
34,1,Semantic Segmentation,ShapeNet - Semantic Segmentation benchmarking,2017-06,PointNet++,84.6,98.6,84.6,100,85.8,0.85,Mean\\ IoU
35,1,Semantic Segmentation,ShapeNet - Semantic Segmentation benchmarking,2017-11,SGPN,85.8,100.0,1.2,1.0,85.8,0.86,Mean\\ IoU
36,1,Semantic Segmentation,PASCAL VOC 2007 - Semantic Segmentation benchmarking,2017-07,DeepLabv3 (ImageNet+300M),81.3,97.95,81.3,100,83.0,0.81,Mean\\ IoU
37,1,Semantic Segmentation,PASCAL VOC 2007 - Semantic Segmentation benchmarking,2019-09,GALDNet,83.0,100.0,1.7,1.0,83.0,0.83,Mean\\ IoU
38,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2017-11,Single-shot Deep CNN,99.92,100.0,99.92,100,99.92,1.0,Mean\\ IoU
39,1,6D Pose Estimation using RGBD,LineMOD - 6D Pose Estimation using RGBD benchmarking,2017-11,SSD-6D,96.5,100.0,96.5,100,96.5,0.97,Mean\\ IoU
40,1,Human Part Segmentation,CIHP - Human Part Segmentation benchmarking,2018-08,PGN + ResNet101,55.8,82.7,55.8,100,67.47,0.56,Mean\\ IoU
41,1,Human Part Segmentation,CIHP - Human Part Segmentation benchmarking,2018-11,Parsing R-CNN + ResNext101,61.1,90.56,5.3,0.4542,67.47,0.61,Mean\\ IoU
42,1,Human Part Segmentation,CIHP - Human Part Segmentation benchmarking,2019-10,ResNet101,67.47,100.0,6.37,0.5458,67.47,0.68,Mean\\ IoU
43,1,Semantic Segmentation,Freiburg Forest - Semantic Segmentation benchmarking,2018-08,AdapNet++,83.09,98.71,83.09,100,84.18,0.83,Mean\\ IoU
44,1,Semantic Segmentation,Freiburg Forest - Semantic Segmentation benchmarking,2018-08,SSMA,84.18,100.0,1.09,1.0,84.18,0.84,Mean\\ IoU
45,1,Semantic Segmentation,ScanNetV2 - Semantic Segmentation benchmarking,2018-08,AdapNet++,50.3,87.18,50.3,100,57.7,0.5,Mean\\ IoU
46,1,Semantic Segmentation,ScanNetV2 - Semantic Segmentation benchmarking,2018-08,SSMA,57.7,100.0,7.4,1.0,57.7,0.58,Mean\\ IoU
47,1,Semantic Segmentation,SYNTHIA-CVPR’16 - Semantic Segmentation benchmarking,2018-08,SSMA,92.1,100.0,92.1,100,92.1,0.92,Mean\\ IoU
48,1,Semantic Segmentation,SUN-RGBD - Semantic Segmentation benchmarking,2018-08,SSMA,45.73,100.0,45.73,100,45.73,0.46,Mean\\ IoU
49,1,Human Part Segmentation,MHP v2.0 - Human Part Segmentation benchmarking,2018-11,Parsing R-CNN + ResNext101,41.8,100.0,41.8,100,41.8,0.42,Mean\\ IoU
50,1,Weakly-Supervised Semantic Segmentation,PASCAL VOC 2012 val - Weakly-Supervised Semantic Segmentation benchmarking,2019-04,IRNet (ResNet-50),63.5,100.0,63.5,100,63.5,0.64,Mean\\ IoU
51,1,Weakly-Supervised Semantic Segmentation,PASCAL VOC 2012 test - Weakly-Supervised Semantic Segmentation benchmarking,2019-04,IRNet (ResNet-50),64.8,100.0,64.8,100,64.8,0.65,Mean\\ IoU
52,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2019-04,KPConv,67.1,100.0,67.1,100,67.1,0.67,Mean\\ IoU
53,1,Few-Shot Semantic Segmentation,FSS-1000 - Few-Shot Semantic Segmentation benchmarking,2019-07,Adapted relation network,80.12,100.0,80.12,100,80.12,0.8,Mean\\ IoU
54,1,Scene Understanding,ADE20K val - Scene Understanding benchmarking,2020-04,CPN(ResNet-101),46.3,100.0,46.3,100,46.3,0.46,Mean\\ IoU
0,1,Pose Estimation,Leeds Sports Poses - Pose Estimation benchmarking,2014-07,"Chen&Yuille, NIPS\'14",73.4,77.43,73.4,100,94.8,0.77,PCK
1,1,Pose Estimation,Leeds Sports Poses - Pose Estimation benchmarking,2016-01,Convolutional Pose Machines,90.5,95.46,17.1,0.7991,94.8,0.95,PCK
2,1,Pose Estimation,Leeds Sports Poses - Pose Estimation benchmarking,2016-09,Part heatmap regression (ResNet-152),90.7,95.68,0.2,0.0093,94.8,0.96,PCK
3,1,Pose Estimation,Leeds Sports Poses - Pose Estimation benchmarking,2017-02,Multi-Context Attention,92.6,97.68,1.9,0.0888,94.8,0.98,PCK
4,1,Pose Estimation,Leeds Sports Poses - Pose Estimation benchmarking,2017-05,Stacked hourglass + Inception-resnet,93.9,99.05,1.3,0.0607,94.8,0.99,PCK
5,1,Pose Estimation,Leeds Sports Poses - Pose Estimation benchmarking,2018-05,Residual Hourglass + ASR + AHO,94.5,99.68,0.6,0.028,94.8,1.0,PCK
6,1,Pose Estimation,Leeds Sports Poses - Pose Estimation benchmarking,2020-02,Soft-gated Skip Connections,94.8,100.0,0.3,0.014,94.8,1.0,PCK
0,1,Face Detection,Annotated Faces in the Wild - Face Detection benchmarking,2014-08,DPM,0.9721,97.34,0.9721,100,0.9987,0.01,AP
1,1,Face Detection,Annotated Faces in the Wild - Face Detection benchmarking,2016-03,HyperFace-ResNet,0.994,99.53,0.0219,0.8233,0.9987,0.01,AP
2,1,Face Detection,Annotated Faces in the Wild - Face Detection benchmarking,2017-08,S3FD,0.9985,99.98,0.0045,0.1692,0.9987,0.01,AP
3,1,Face Detection,Annotated Faces in the Wild - Face Detection benchmarking,2018-09,SRN,0.9987,100.0,0.0002,0.0075,0.9987,0.01,AP
4,1,Face Detection,FDDB - Face Detection benchmarking,2014-08,DPM,0.864,87.18,0.864,100,0.991,0.01,AP
5,1,Face Detection,FDDB - Face Detection benchmarking,2016-03,HyperFace,0.901,90.92,0.037,0.2913,0.991,0.01,AP
6,1,Face Detection,FDDB - Face Detection benchmarking,2017-08,S3FD,0.983,99.19,0.082,0.6457,0.991,0.01,AP
7,1,Face Detection,FDDB - Face Detection benchmarking,2017-09,Face R-FCN,0.99,99.9,0.007,0.0551,0.991,0.01,AP
8,1,Face Detection,FDDB - Face Detection benchmarking,2018-10,DSFD,0.991,100.0,0.001,0.0079,0.991,0.01,AP
9,1,Face Detection,PASCAL Face - Face Detection benchmarking,2014-08,DPM,0.9029,91.12,0.9029,100,0.9909,0.01,AP
10,1,Face Detection,PASCAL Face - Face Detection benchmarking,2016-03,HyperFace-ResNet,0.962,97.08,0.0591,0.6716,0.9909,0.01,AP
11,1,Face Detection,PASCAL Face - Face Detection benchmarking,2017-08,FaceBoxes,0.963,97.18,0.001,0.0114,0.9909,0.01,AP
12,1,Face Detection,PASCAL Face - Face Detection benchmarking,2017-08,S3FD,0.9849,99.39,0.0219,0.2489,0.9909,0.01,AP
13,1,Face Detection,PASCAL Face - Face Detection benchmarking,2018-09,SRN,0.9909,100.0,0.006,0.0682,0.9909,0.01,AP
14,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2015-06,Faster-RCNN,0.045,9.15,0.045,100,0.492,0.0,AP
15,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2016-12,YOLO9000opt,0.094,19.11,0.049,0.1096,0.492,0.0,AP
16,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2017-08,RetinaNet,0.455,92.48,0.361,0.8076,0.492,0.01,AP
17,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2019-04,Soft-IoU + EM-Merger unit,0.492,100.0,0.037,0.0828,0.492,0.01,AP
18,1,Birds Eye View Object Detection,KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking,2015-12,3DOP,9.49,11.19,9.49,100,84.81,0.11,AP
19,1,Birds Eye View Object Detection,KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking,2016-08,VeloFCN,32.08,37.83,22.59,0.2999,84.81,0.36,AP
20,1,Birds Eye View Object Detection,KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking,2016-11,MV (BV+FV),77.32,91.17,45.24,0.6006,84.81,0.86,AP
21,1,Birds Eye View Object Detection,KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,84.81,100.0,7.49,0.0994,84.81,0.94,AP
22,1,Multi-Person Pose Estimation,MPII Multi-Person - Multi-Person Pose Estimation benchmarking,2016-05,DeeperCut,59.4,72.35,59.4,100,82.1,0.66,AP
23,1,Multi-Person Pose Estimation,MPII Multi-Person - Multi-Person Pose Estimation benchmarking,2016-08,Local Joint-to-Person Association,62.2,75.76,2.8,0.1233,82.1,0.69,AP
24,1,Multi-Person Pose Estimation,MPII Multi-Person - Multi-Person Pose Estimation benchmarking,2016-11,Associative Embedding,77.5,94.4,15.3,0.674,82.1,0.86,AP
25,1,Multi-Person Pose Estimation,MPII Multi-Person - Multi-Person Pose Estimation benchmarking,2016-12,AlphaPose,82.1,100.0,4.6,0.2026,82.1,0.91,AP
26,1,Object Detection,KITTI Cars Hard - Object Detection benchmarking,2016-08,VeloFCN,42.74,62.02,42.74,100,68.91,0.48,AP
27,1,Object Detection,KITTI Cars Hard - Object Detection benchmarking,2017-11,F-PointNet,62.19,90.25,19.45,0.7432,68.91,0.69,AP
28,1,Object Detection,KITTI Cars Hard - Object Detection benchmarking,2018-12,PointRCNN Shi et al. (2019),68.32,99.14,6.13,0.2342,68.91,0.76,AP
29,1,Object Detection,KITTI Cars Hard - Object Detection benchmarking,2019-10,Patches,68.91,100.0,0.59,0.0225,68.91,0.77,AP
30,1,Object Detection,KITTI Cars Moderate - Object Detection benchmarking,2016-08,VeloFCN,47.51,61.57,47.51,100,77.16,0.53,AP
31,1,Object Detection,KITTI Cars Moderate - Object Detection benchmarking,2018-12,PointRCNN Shi et al. (2019),75.76,98.19,28.25,0.9528,77.16,0.84,AP
32,1,Object Detection,KITTI Cars Moderate - Object Detection benchmarking,2019-10,Patches,77.16,100.0,1.4,0.0472,77.16,0.86,AP
33,1,Object Detection,KITTI Cars Easy - Object Detection benchmarking,2016-08,VeloFCN,60.34,68.67,60.34,100,87.87,0.67,AP
34,1,Object Detection,KITTI Cars Easy - Object Detection benchmarking,2018-11,Roarnet,83.71,95.27,23.37,0.8489,87.87,0.93,AP
35,1,Object Detection,KITTI Cars Easy - Object Detection benchmarking,2018-12,PointRCNN Shi et al. (2019),85.94,97.8,2.23,0.081,87.87,0.96,AP
36,1,Object Detection,KITTI Cars Easy - Object Detection benchmarking,2019-10,Patches,87.87,100.0,1.93,0.0701,87.87,0.98,AP
37,1,Multi-Person Pose Estimation,COCO - Multi-Person Pose Estimation benchmarking,2016-11,Associative Embedding,0.655,84.63,0.655,100,0.774,0.01,AP
38,1,Multi-Person Pose Estimation,COCO - Multi-Person Pose Estimation benchmarking,2017-01,G-RMI*,0.685,88.5,0.03,0.2521,0.774,0.01,AP
39,1,Multi-Person Pose Estimation,COCO - Multi-Person Pose Estimation benchmarking,2017-11,CPN+,0.73,94.32,0.045,0.3782,0.774,0.01,AP
40,1,Multi-Person Pose Estimation,COCO - Multi-Person Pose Estimation benchmarking,2019-10,DarkPose,0.774,100.0,0.044,0.3697,0.774,0.01,AP
41,1,Birds Eye View Object Detection,KITTI Cars Easy val - Birds Eye View Object Detection benchmarking,2016-11,MV (BV+FV),86.18,96.18,86.18,100,89.6,0.96,AP
42,1,Birds Eye View Object Detection,KITTI Cars Easy val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,89.6,100.0,3.42,1.0,89.6,1.0,AP
43,1,Birds Eye View Object Detection,KITTI Cars Hard val - Birds Eye View Object Detection benchmarking,2016-11,MV (BV+FV),76.33,97.15,76.33,100,78.57,0.85,AP
44,1,Birds Eye View Object Detection,KITTI Cars Hard val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,78.57,100.0,2.24,1.0,78.57,0.88,AP
45,1,3D Object Detection,KITTI Cars Hard val - 3D Object Detection benchmarking,2016-11,MV3D,56.56,88.64,56.56,100,63.81,0.63,AP
46,1,3D Object Detection,KITTI Cars Hard val - 3D Object Detection benchmarking,2017-11,F-PointNet [Qi:2018fd],62.56,98.04,6.0,0.8276,63.81,0.7,AP
47,1,3D Object Detection,KITTI Cars Hard val - 3D Object Detection benchmarking,2019-07,PVCNN,63.81,100.0,1.25,0.1724,63.81,0.71,AP
48,1,3D Object Detection,KITTI Cars Easy val - 3D Object Detection benchmarking,2016-11,MV3D,71.29,84.85,71.29,100,84.02,0.79,AP
49,1,3D Object Detection,KITTI Cars Easy val - 3D Object Detection benchmarking,2017-11,F-PointNet [Qi:2018fd],83.26,99.1,11.97,0.9403,84.02,0.93,AP
50,1,3D Object Detection,KITTI Cars Easy val - 3D Object Detection benchmarking,2019-07,PVCNN,84.02,100.0,0.76,0.0597,84.02,0.94,AP
51,1,3D Object Detection,KITTI Cars Moderate val - 3D Object Detection benchmarking,2016-11,MV3D,62.68,87.62,62.68,100,71.54,0.7,AP
52,1,3D Object Detection,KITTI Cars Moderate val - 3D Object Detection benchmarking,2017-11,F-PointNet [Qi:2018fd],69.28,96.84,6.6,0.7449,71.54,0.77,AP
53,1,3D Object Detection,KITTI Cars Moderate val - 3D Object Detection benchmarking,2019-07,PVCNN,71.54,100.0,2.26,0.2551,71.54,0.8,AP
54,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-11,CMU-Pose,61.8,79.84,61.8,100,77.4,0.69,AP
55,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-12,RMPE++,72.3,93.41,10.5,0.6731,77.4,0.81,AP
56,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2017-11,"CPN+ [6, 9]",73.0,94.32,0.7,0.0449,77.4,0.81,AP
57,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-04,Flow-based (ResNet-152),73.7,95.22,0.7,0.0449,77.4,0.82,AP
58,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-12,PoseFix,74.7,96.51,1.0,0.0641,77.4,0.83,AP
59,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-01,MSPN,76.1,98.32,1.4,0.0897,77.4,0.85,AP
60,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-02,HRNet-W48,77.0,99.48,0.9,0.0577,77.4,0.86,AP
61,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-10,HRNet-W48+DARK,77.4,100.0,0.4,0.0256,77.4,0.86,AP
62,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2016-11,CMU-Pose,61.8,87.66,61.8,100,70.5,0.69,AP
63,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2017-01,G-RMI,64.9,92.06,3.1,0.3563,70.5,0.72,AP
64,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2018-03,PersonLab,68.7,97.45,3.8,0.4368,70.5,0.77,AP
65,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet (HR-Net-48),70.5,100.0,1.8,0.2069,70.5,0.79,AP
66,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,69.1,90.45,69.1,100,76.4,0.77,AP
67,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,72.1,94.37,3.0,0.411,76.4,0.8,AP
68,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,74.5,97.51,2.4,0.3288,76.4,0.83,AP
69,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,76.4,100.0,1.9,0.2603,76.4,0.85,AP
70,1,Multi-Person Pose Estimation,WAF - Multi-Person Pose Estimation benchmarking,2017-05,Generative Partition Networks,84.8,100.0,84.8,100,84.8,0.94,AP
71,1,Edge Detection,Cityscapes test - Edge Detection benchmarking,2017-05,CASENet,70.8,100.0,70.8,100,70.8,0.79,AP
72,1,Birds Eye View Object Detection,KITTI Cars Hard - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,77.39,89.07,77.39,100,86.89,0.86,AP
73,1,Birds Eye View Object Detection,KITTI Cars Hard - Birds Eye View Object Detection benchmarking,2018-12,PointPillars,79.83,91.87,2.44,0.2568,86.89,0.89,AP
74,1,Birds Eye View Object Detection,KITTI Cars Hard - Birds Eye View Object Detection benchmarking,2019-07,STD,86.89,100.0,7.06,0.7432,86.89,0.97,AP
75,1,Birds Eye View Object Detection,KITTI Pedestrian Easy val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,65.95,100.0,65.95,100,65.95,0.73,AP
76,1,Birds Eye View Object Detection,KITTI Cars Easy - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,89.35,99.52,89.35,100,89.78,1.0,AP
77,1,Birds Eye View Object Detection,KITTI Cars Easy - Birds Eye View Object Detection benchmarking,2019-07,STD,89.66,99.87,0.31,0.7209,89.78,1.0,AP
78,1,Birds Eye View Object Detection,KITTI Cars Easy - Birds Eye View Object Detection benchmarking,2019-10,Patches,89.78,100.0,0.12,0.2791,89.78,1.0,AP
79,1,Birds Eye View Object Detection,KITTI Pedestrian Moderate val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,61.05,100.0,61.05,100,61.05,0.68,AP
80,1,Birds Eye View Object Detection,KITTI Cyclist Moderate val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,52.18,100.0,52.18,100,52.18,0.58,AP
81,1,Birds Eye View Object Detection,KITTI Pedestrian Hard val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,56.98,100.0,56.98,100,56.98,0.63,AP
82,1,Object Localization,KITTI Pedestrians Hard - Object Localization benchmarking,2017-11,VoxelNet,38.11,80.74,38.11,100,47.2,0.42,AP
83,1,Object Localization,KITTI Pedestrians Hard - Object Localization benchmarking,2017-11,Frustum PointNets,47.2,100.0,9.09,1.0,47.2,0.53,AP
84,1,Object Localization,KITTI Pedestrians Easy - Object Localization benchmarking,2017-11,VoxelNet,46.13,79.41,46.13,100,58.09,0.51,AP
85,1,Object Localization,KITTI Pedestrians Easy - Object Localization benchmarking,2017-11,Frustum PointNets,58.09,100.0,11.96,1.0,58.09,0.65,AP
86,1,Object Localization,KITTI Cars Hard - Object Localization benchmarking,2017-11,VoxelNet,77.39,100.0,77.39,100,77.39,0.86,AP
87,1,Birds Eye View Object Detection,KITTI Cyclist Easy val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,74.41,100.0,74.41,100,74.41,0.83,AP
88,1,Object Localization,KITTI Pedestrians Moderate - Object Localization benchmarking,2017-11,VoxelNet,40.74,81.12,40.74,100,50.22,0.45,AP
89,1,Object Localization,KITTI Pedestrians Moderate - Object Localization benchmarking,2017-11,Frustum PointNets,50.22,100.0,9.48,1.0,50.22,0.56,AP
90,1,Object Localization,KITTI Cyclists Hard - Object Localization benchmarking,2017-11,VoxelNe,50.55,92.45,50.55,100,54.68,0.56,AP
91,1,Object Localization,KITTI Cyclists Hard - Object Localization benchmarking,2017-11,Frustum PointNets,54.68,100.0,4.13,1.0,54.68,0.61,AP
92,1,Object Localization,KITTI Cars Easy - Object Localization benchmarking,2017-11,VoxelNet,89.35,100.0,89.35,100,89.35,1.0,AP
93,1,Object Localization,KITTI Cars Moderate - Object Localization benchmarking,2017-11,VoxelNet,79.26,94.36,79.26,100,84.0,0.88,AP
94,1,Object Localization,KITTI Cars Moderate - Object Localization benchmarking,2017-11,Frustum PointNets,84.0,100.0,4.74,1.0,84.0,0.94,AP
95,1,Birds Eye View Object Detection,KITTI Cyclist Hard val - Birds Eye View Object Detection benchmarking,2017-11,VoxelNet,50.49,100.0,50.49,100,50.49,0.56,AP
96,1,Object Localization,KITTI Cyclists Easy - Object Localization benchmarking,2017-11,VoxelNet,66.7,88.49,66.7,100,75.38,0.74,AP
97,1,Object Localization,KITTI Cyclists Easy - Object Localization benchmarking,2017-11,Frustum PointNets,75.38,100.0,8.68,1.0,75.38,0.84,AP
98,1,Object Localization,KITTI Cyclists Moderate - Object Localization benchmarking,2017-11,VoxelNet,54.76,88.38,54.76,100,61.96,0.61,AP
99,1,Object Localization,KITTI Cyclists Moderate - Object Localization benchmarking,2017-11,Frustum PointNets,61.96,100.0,7.2,1.0,61.96,0.69,AP
100,1,3D Object Detection,KITTI Cars Easy - 3D Object Detection benchmarking,2017-11,VoxelNet,77.47,89.45,77.47,100,86.61,0.86,AP
101,1,3D Object Detection,KITTI Cars Easy - 3D Object Detection benchmarking,2017-11,Frustum PointNets,81.2,93.75,3.73,0.4081,86.61,0.9,AP
102,1,3D Object Detection,KITTI Cars Easy - 3D Object Detection benchmarking,2017-12,AVOD + Feature Pyramid,81.94,94.61,0.74,0.081,86.61,0.91,AP
103,1,3D Object Detection,KITTI Cars Easy - 3D Object Detection benchmarking,2018-02,PC-CNN-V2,84.33,97.37,2.39,0.2615,86.61,0.94,AP
104,1,3D Object Detection,KITTI Cars Easy - 3D Object Detection benchmarking,2019-03,F-ConvNet,85.88,99.16,1.55,0.1696,86.61,0.96,AP
105,1,3D Object Detection,KITTI Cars Easy - 3D Object Detection benchmarking,2019-07,STD,86.61,100.0,0.73,0.0799,86.61,0.96,AP
106,1,3D Object Detection,KITTI Cars Moderate - 3D Object Detection benchmarking,2017-11,VoxelNet,65.11,83.87,65.11,100,77.63,0.73,AP
107,1,3D Object Detection,KITTI Cars Moderate - 3D Object Detection benchmarking,2017-11,Frustum PointNets,70.39,90.67,5.28,0.4217,77.63,0.78,AP
108,1,3D Object Detection,KITTI Cars Moderate - 3D Object Detection benchmarking,2017-12,AVOD + Feature Pyramid,71.88,92.59,1.49,0.119,77.63,0.8,AP
109,1,3D Object Detection,KITTI Cars Moderate - 3D Object Detection benchmarking,2018-02,PC-CNN-V2,73.8,95.07,1.92,0.1534,77.63,0.82,AP
110,1,3D Object Detection,KITTI Cars Moderate - 3D Object Detection benchmarking,2018-12,PointRCNN,75.42,97.15,1.62,0.1294,77.63,0.84,AP
111,1,3D Object Detection,KITTI Cars Moderate - 3D Object Detection benchmarking,2019-03,F-ConvNet,76.51,98.56,1.09,0.0871,77.63,0.85,AP
112,1,3D Object Detection,KITTI Cars Moderate - 3D Object Detection benchmarking,2019-07,STD,77.63,100.0,1.12,0.0895,77.63,0.86,AP
113,1,3D Object Detection,KITTI Pedestrians Hard - 3D Object Detection benchmarking,2017-11,VoxelNet,31.51,74.33,31.51,100,42.39,0.35,AP
114,1,3D Object Detection,KITTI Pedestrians Hard - 3D Object Detection benchmarking,2017-11,Frustum PointNets,40.23,94.9,8.72,0.8015,42.39,0.45,AP
115,1,3D Object Detection,KITTI Pedestrians Hard - 3D Object Detection benchmarking,2017-12,AVOD + Feature Pyramid,40.88,96.44,0.65,0.0597,42.39,0.46,AP
116,1,3D Object Detection,KITTI Pedestrians Hard - 3D Object Detection benchmarking,2018-12,IPOD,42.39,100.0,1.51,0.1388,42.39,0.47,AP
117,1,3D Object Detection,KITTI Cyclists Easy - 3D Object Detection benchmarking,2017-11,VoxelNet,61.22,76.93,61.22,100,79.58,0.68,AP
118,1,3D Object Detection,KITTI Cyclists Easy - 3D Object Detection benchmarking,2017-11,Frustum PointNets,71.96,90.42,10.74,0.585,79.58,0.8,AP
119,1,3D Object Detection,KITTI Cyclists Easy - 3D Object Detection benchmarking,2018-12,PointRCNN,73.93,92.9,1.97,0.1073,79.58,0.82,AP
120,1,3D Object Detection,KITTI Cyclists Easy - 3D Object Detection benchmarking,2018-12,PointPillars,75.78,95.22,1.85,0.1008,79.58,0.84,AP
121,1,3D Object Detection,KITTI Cyclists Easy - 3D Object Detection benchmarking,2019-03,F-ConvNet,79.58,100.0,3.8,0.207,79.58,0.89,AP
122,1,3D Object Detection,KITTI Cyclists Hard - 3D Object Detection benchmarking,2017-11,VoxelNet,44.37,77.8,44.37,100,57.03,0.49,AP
123,1,3D Object Detection,KITTI Cyclists Hard - 3D Object Detection benchmarking,2017-11,Frustum PointNets,50.39,88.36,6.02,0.4755,57.03,0.56,AP
124,1,3D Object Detection,KITTI Cyclists Hard - 3D Object Detection benchmarking,2018-12,PointRCNN,53.59,93.97,3.2,0.2528,57.03,0.6,AP
125,1,3D Object Detection,KITTI Cyclists Hard - 3D Object Detection benchmarking,2019-03,F-ConvNets,57.03,100.0,3.44,0.2717,57.03,0.64,AP
126,1,3D Object Detection,KITTI Pedestrians Moderate - 3D Object Detection benchmarking,2017-11,VoxelNet,33.69,75.4,33.69,100,44.68,0.38,AP
127,1,3D Object Detection,KITTI Pedestrians Moderate - 3D Object Detection benchmarking,2017-11,Frustum PointNets,42.15,94.34,8.46,0.7698,44.68,0.47,AP
128,1,3D Object Detection,KITTI Pedestrians Moderate - 3D Object Detection benchmarking,2017-12,AVOD + Feature Pyramid,42.81,95.81,0.66,0.0601,44.68,0.48,AP
129,1,3D Object Detection,KITTI Pedestrians Moderate - 3D Object Detection benchmarking,2018-12,IPOD,44.68,100.0,1.87,0.1702,44.68,0.5,AP
130,1,3D Object Detection,KITTI Pedestrians Easy - 3D Object Detection benchmarking,2017-11,VoxelNet,39.48,69.36,39.48,100,56.92,0.44,AP
131,1,3D Object Detection,KITTI Pedestrians Easy - 3D Object Detection benchmarking,2017-11,Frustum PointNets,51.21,89.97,11.73,0.6726,56.92,0.57,AP
132,1,3D Object Detection,KITTI Pedestrians Easy - 3D Object Detection benchmarking,2018-12,IPOD,56.92,100.0,5.71,0.3274,56.92,0.63,AP
133,1,3D Object Detection,KITTI Cyclists Moderate - 3D Object Detection benchmarking,2017-11,VoxelNet,48.36,74.77,48.36,100,64.68,0.54,AP
134,1,3D Object Detection,KITTI Cyclists Moderate - 3D Object Detection benchmarking,2017-11,Frustum PointNets,56.77,87.77,8.41,0.5153,64.68,0.63,AP
135,1,3D Object Detection,KITTI Cyclists Moderate - 3D Object Detection benchmarking,2018-12,PointRCNN,59.6,92.15,2.83,0.1734,64.68,0.66,AP
136,1,3D Object Detection,KITTI Cyclists Moderate - 3D Object Detection benchmarking,2019-03,F-ConvNet,64.68,100.0,5.08,0.3113,64.68,0.72,AP
137,1,3D Object Detection,KITTI Cars Hard - 3D Object Detection benchmarking,2017-11,VoxelNet,57.73,75.9,57.73,100,76.06,0.64,AP
138,1,3D Object Detection,KITTI Cars Hard - 3D Object Detection benchmarking,2017-11,Frustum PointNets,62.19,81.76,4.46,0.2433,76.06,0.69,AP
139,1,3D Object Detection,KITTI Cars Hard - 3D Object Detection benchmarking,2017-12,AVOD + Feature Pyramid,66.38,87.27,4.19,0.2286,76.06,0.74,AP
140,1,3D Object Detection,KITTI Cars Hard - 3D Object Detection benchmarking,2018-12,PointRCNN,67.86,89.22,1.48,0.0807,76.06,0.76,AP
141,1,3D Object Detection,KITTI Cars Hard - 3D Object Detection benchmarking,2019-03,F-ConvNet,68.08,89.51,0.22,0.012,76.06,0.76,AP
142,1,3D Object Detection,KITTI Cars Hard - 3D Object Detection benchmarking,2019-07,STD,76.06,100.0,7.98,0.4354,76.06,0.85,AP
143,1,Birds Eye View Object Detection,KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking,2017-11,F-PointNet,50.22,97.72,50.22,100,51.39,0.56,AP
144,1,Birds Eye View Object Detection,KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking,2017-12,AVOD-FPN,51.05,99.34,0.83,0.7094,51.39,0.57,AP
145,1,Birds Eye View Object Detection,KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking,2019-07,STD,51.39,100.0,0.34,0.2906,51.39,0.57,AP
146,1,Birds Eye View Object Detection,KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking,2017-11,F-PointNet,61.96,94.86,61.96,100,65.32,0.69,AP
147,1,Birds Eye View Object Detection,KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking,2018-12,PointPillars,62.25,95.3,0.29,0.0863,65.32,0.69,AP
148,1,Birds Eye View Object Detection,KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking,2019-07,STD,65.32,100.0,3.07,0.9137,65.32,0.73,AP
149,1,3D Object Detection,KITTI Pedestrian Easy val - 3D Object Detection benchmarking,2017-11,F-PointNet [Qi:2018fd],65.08,88.91,65.08,100,73.2,0.72,AP
150,1,3D Object Detection,KITTI Pedestrian Easy val - 3D Object Detection benchmarking,2017-11,F-PointNet++ [Qi:2018fd],70.0,95.63,4.92,0.6059,73.2,0.78,AP
151,1,3D Object Detection,KITTI Pedestrian Easy val - 3D Object Detection benchmarking,2019-07,PVCNN,73.2,100.0,3.2,0.3941,73.2,0.82,AP
152,1,3D Object Detection,KITTI Pedestrian Moderate val - 3D Object Detection benchmarking,2017-11,F-PointNet++ [Qi:2018fd],61.32,94.76,61.32,100,64.71,0.68,AP
153,1,3D Object Detection,KITTI Pedestrian Moderate val - 3D Object Detection benchmarking,2019-07,PVCNN,64.71,100.0,3.39,1.0,64.71,0.72,AP
154,1,3D Object Detection,KITTI Cyclist Easy val - 3D Object Detection benchmarking,2017-11,F-PointNet++ [Qi:2018fd],77.15,94.78,77.15,100,81.4,0.86,AP
155,1,3D Object Detection,KITTI Cyclist Easy val - 3D Object Detection benchmarking,2019-07,PVCNN,81.4,100.0,4.25,1.0,81.4,0.91,AP
156,1,3D Object Detection,KITTI Cyclist Hard val - 3D Object Detection benchmarking,2017-11,F-PointNet [Qi:2018fd],52.65,93.62,52.65,100,56.24,0.59,AP
157,1,3D Object Detection,KITTI Cyclist Hard val - 3D Object Detection benchmarking,2017-11,F-PointNet++ [Qi:2018fd],53.37,94.9,0.72,0.2006,56.24,0.59,AP
158,1,3D Object Detection,KITTI Cyclist Hard val - 3D Object Detection benchmarking,2019-07,PVCNN,56.24,100.0,2.87,0.7994,56.24,0.63,AP
159,1,3D Object Detection,KITTI Cyclist Moderate val - 3D Object Detection benchmarking,2017-11,F-PointNet [Qi:2018fd],55.95,93.3,55.95,100,59.97,0.62,AP
160,1,3D Object Detection,KITTI Cyclist Moderate val - 3D Object Detection benchmarking,2017-11,F-PointNet++ [Qi:2018fd],56.49,94.2,0.54,0.1343,59.97,0.63,AP
161,1,3D Object Detection,KITTI Cyclist Moderate val - 3D Object Detection benchmarking,2019-07,PVCNN,59.97,100.0,3.48,0.8657,59.97,0.67,AP
162,1,3D Object Detection,KITTI Pedestrian Hard val - 3D Object Detection benchmarking,2017-11,F-PointNet++ [Qi:2018fd],53.59,94.38,53.59,100,56.78,0.6,AP
163,1,3D Object Detection,KITTI Pedestrian Hard val - 3D Object Detection benchmarking,2019-07,PVCNN,56.78,100.0,3.19,1.0,56.78,0.63,AP
164,1,Birds Eye View Object Detection,KITTI Cars Moderate - Birds Eye View Object Detection benchmarking,2017-12,AVOD-FPN,83.79,95.48,83.79,100,87.76,0.93,AP
165,1,Birds Eye View Object Detection,KITTI Cars Moderate - Birds Eye View Object Detection benchmarking,2018-12,PointPillars,86.1,98.11,2.31,0.5819,87.76,0.96,AP
166,1,Birds Eye View Object Detection,KITTI Cars Moderate - Birds Eye View Object Detection benchmarking,2019-07,STD,87.76,100.0,1.66,0.4181,87.76,0.98,AP
167,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-01,MRCNN + PSPNet (ResNet-101),36.4,93.33,36.4,100,39.0,0.41,AP
168,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-12,TASCNet (ResNet-50),37.6,96.41,1.2,0.4615,39.0,0.42,AP
169,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-12,"TASCNet (ResNet-50, multi-scale)",39.0,100.0,1.4,0.5385,39.0,0.43,AP
170,1,Pose Estimation,DensePose-COCO - Pose Estimation benchmarking,2018-02,DensePose + keypoints,55.8,90.58,55.8,100,61.6,0.62,AP
171,1,Pose Estimation,DensePose-COCO - Pose Estimation benchmarking,2018-11,Parsing R-CNN + ResNext101,61.6,100.0,5.8,1.0,61.6,0.69,AP
172,1,Human Instance Segmentation,OCHuman - Human Instance Segmentation benchmarking,2018-03,Pose2Seg (plus ground-truth keypoints),0.552,100.0,0.552,100,0.552,0.01,AP
173,1,Pose Estimation,COCO minival - Pose Estimation benchmarking,2019-01,MSPN,75.9,100.0,75.9,100,75.9,0.85,AP
174,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-01,MSPN,76.1,100.0,76.1,100,76.1,0.85,AP
175,1,3D Object Detection,nuScenes-FB - 3D Object Detection benchmarking,2019-05,RRPN + R101 - FB,35.5,100.0,35.5,100,35.5,0.4,AP
176,1,3D Object Detection,nuScenes-F - 3D Object Detection benchmarking,2019-05,RRPN + R101 - F,43.0,100.0,43,100,43,0.48,AP
177,1,Birds Eye View Object Detection,KITTI Cyclists Easy - Birds Eye View Object Detection benchmarking,2019-07,STD,81.04,100.0,81.04,100,81.04,0.9,AP
178,1,Birds Eye View Object Detection,KITTI Pedestrians Hard - Birds Eye View Object Detection benchmarking,2019-07,STD,45.89,100.0,45.89,100,45.89,0.51,AP
179,1,Birds Eye View Object Detection,KITTI Pedestrians Easy - Birds Eye View Object Detection benchmarking,2019-07,STD,60.99,100.0,60.99,100,60.99,0.68,AP
180,1,Birds Eye View Object Detection,KITTI Cyclists Hard - Birds Eye View Object Detection benchmarking,2019-07,STD,57.85,100.0,57.85,100,57.85,0.64,AP
0,1,Unsupervised Facial Landmark Detection,MAFL - Unsupervised Facial Landmark Detection benchmarking,2014-08,TCDCN,-7.95,312.99,-7.95,100,-2.54,3.13,NME
1,1,Unsupervised Facial Landmark Detection,MAFL - Unsupervised Facial Landmark Detection benchmarking,2017-05,FSE,-6.67,262.6,1.28,0.2366,-2.54,2.63,NME
2,1,Unsupervised Facial Landmark Detection,MAFL - Unsupervised Facial Landmark Detection benchmarking,2017-06,DEIL,-4.02,158.27,2.65,0.4898,-2.54,1.58,NME
3,1,Unsupervised Facial Landmark Detection,MAFL - Unsupervised Facial Landmark Detection benchmarking,2018-04,LMDIS-REP,-3.15,124.02,0.87,0.1608,-2.54,1.24,NME
4,1,Unsupervised Facial Landmark Detection,MAFL - Unsupervised Facial Landmark Detection benchmarking,2018-06,Conditional Image Generation,-2.54,100.0,0.61,0.1128,-2.54,1.0,NME
5,1,Facial Landmark Detection,300W - Facial Landmark Detection benchmarking,2015-11,3DDFA,-7.01,223.96,-7.01,100,-3.13,2.76,NME
6,1,Facial Landmark Detection,300W - Facial Landmark Detection benchmarking,2015-11,CFSS,-5.76,184.03,1.25,0.3222,-3.13,2.27,NME
7,1,Facial Landmark Detection,300W - Facial Landmark Detection benchmarking,2018-03,SAN GT,-3.98,127.16,1.78,0.4588,-3.13,1.57,NME
8,1,Facial Landmark Detection,300W - Facial Landmark Detection benchmarking,2019-02,3DDE (Inter-ocular Norm),-3.13,100.0,0.85,0.2191,-3.13,1.23,NME
9,1,Unsupervised Facial Landmark Detection,300W - Unsupervised Facial Landmark Detection benchmarking,2017-05,FSE,-7.97,171.4,-7.97,100,-4.65,3.14,NME
10,1,Unsupervised Facial Landmark Detection,300W - Unsupervised Facial Landmark Detection benchmarking,2018-08,FAb-Net,-5.71,122.8,2.26,0.6807,-4.65,2.25,NME
11,1,Unsupervised Facial Landmark Detection,300W - Unsupervised Facial Landmark Detection benchmarking,2019-08,DVE,-4.65,100.0,1.06,0.3193,-4.65,1.83,NME
12,1,Unsupervised Facial Landmark Detection,AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking,2017-05,FSE,-10.53,139.84,-10.53,100,-7.53,4.15,NME
13,1,Unsupervised Facial Landmark Detection,AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking,2019-08,DVE,-7.53,100.0,3.0,1.0,-7.53,2.96,NME
0,1,Face Anti-Spoofing,CASIA-MFSD - Face Anti-Spoofing benchmarking,2014-08,Multi-Scale,-4.92,100.0,-4.92,100,-4.92,12.3,EER
1,1,Face Anti-Spoofing,Replay-Attack - Face Anti-Spoofing benchmarking,2014-08,Multi-Scale,-2.14,535.0,-2.14,100,-0.4,5.35,EER
2,1,Face Anti-Spoofing,Replay-Attack - Face Anti-Spoofing benchmarking,2015-11,YCbCr+HSV-LBP,-0.4,100.0,1.74,1.0,-0.4,1.0,EER
3,1,Abnormal Event Detection In Video,UBI-Fights - Abnormal Event Detection In Video benchmarking,2017-08,Adversarial Generator,-0.504,100.0,-0.504,100,-0.504,1.26,EER
4,1,Semi-supervised Anomaly Detection,UBI-Fights - Semi-supervised Anomaly Detection benchmarking,2017-08,Adversarial Generator,-0.484,100.0,-0.484,100,-0.484,1.21,EER
0,1,Image-to-Image Translation,GTAV-to-Cityscapes Labels - Image-to-Image Translation benchmarking,2014-09,VGG16 60.3,41.3,92.19,41.3,100,44.8,0.46,mIoU
1,1,Image-to-Image Translation,GTAV-to-Cityscapes Labels - Image-to-Image Translation benchmarking,2015-12,ResNet101 65.1,41.7,93.08,0.4,0.1143,44.8,0.47,mIoU
2,1,Image-to-Image Translation,GTAV-to-Cityscapes Labels - Image-to-Image Translation benchmarking,2018-11,ADVENT,44.8,100.0,3.1,0.8857,44.8,0.5,mIoU
3,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2014-11,FCN-8s,37.8,70.0,37.8,100,54.0,0.42,mIoU
4,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2015-02,CRF-RNN,39.3,72.78,1.5,0.0926,54.0,0.44,mIoU
5,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2015-03,BoxSup,40.5,75.0,1.2,0.0741,54.0,0.45,mIoU
6,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2015-04,Piecewise,43.3,80.19,2.8,0.1728,54.0,0.48,mIoU
7,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2016-05,VeryDeep,44.5,82.41,1.2,0.0741,54.0,0.5,mIoU
8,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2016-06,DeepLabV2,45.7,84.63,1.2,0.0741,54.0,0.51,mIoU
9,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2016-11,RefineNet,47.3,87.59,1.6,0.0988,54.0,0.53,mIoU
10,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2016-11,ResNet-38,48.1,89.07,0.8,0.0494,54.0,0.54,mIoU
11,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2018-03,EncNet (ResNet-101),51.7,95.74,3.6,0.2222,54.0,0.58,mIoU
12,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2018-09,DANet (ResNet-101),52.6,97.41,0.9,0.0556,54.0,0.59,mIoU
13,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2019-03,Joint Pyramid Upsampling + EncNet,53.1,98.33,0.5,0.0309,54.0,0.59,mIoU
14,1,Semantic Segmentation,PASCAL Context - Semantic Segmentation benchmarking,2019-06,CFNet (ResNet-101),54.0,100.0,0.9,0.0556,54.0,0.6,mIoU
15,1,Semantic Segmentation,COCO-Stuff test - Semantic Segmentation benchmarking,2014-11,FCN (VGG-16),22.7,57.18,22.7,100,39.7,0.25,mIoU
16,1,Semantic Segmentation,COCO-Stuff test - Semantic Segmentation benchmarking,2015-09,DAG-RNN (VGG-16),31.2,78.59,8.5,0.5,39.7,0.35,mIoU
17,1,Semantic Segmentation,COCO-Stuff test - Semantic Segmentation benchmarking,2016-11,RefineNet (ResNet-101),33.6,84.63,2.4,0.1412,39.7,0.38,mIoU
18,1,Semantic Segmentation,COCO-Stuff test - Semantic Segmentation benchmarking,2018-06,CCL (ResNet-101),35.7,89.92,2.1,0.1235,39.7,0.4,mIoU
19,1,Semantic Segmentation,COCO-Stuff test - Semantic Segmentation benchmarking,2018-09,DANet (ResNet-101),39.7,100.0,4.0,0.2353,39.7,0.44,mIoU
20,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2014-12,DeepLab,63.1,83.14,63.1,100,75.9,0.71,mIoU
21,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2015-11,Dilation10,67.1,88.41,4.0,0.3125,75.9,0.75,mIoU
22,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2016-11,FRRN,71.8,94.6,4.7,0.3672,75.9,0.8,mIoU
23,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2018-08,BiSeNet(ResNet-18),74.7,98.42,2.9,0.2266,75.9,0.84,mIoU
24,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2018-11,ShelfNet18,74.8,98.55,0.1,0.0078,75.9,0.84,mIoU
25,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2019-03,SwiftNetRN-18,75.5,99.47,0.7,0.0547,75.9,0.84,mIoU
26,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2019-09,U-HarDNet-70,75.9,100.0,0.4,0.0312,75.9,0.85,mIoU
27,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2014-12,DeepLab,61.6,78.47,61.6,100,78.5,0.69,mIoU
28,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2015-11,Dilation10,65.3,83.18,3.7,0.2189,78.5,0.73,mIoU
29,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet,69.1,88.03,3.8,0.2249,78.5,0.77,mIoU
30,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2020-04,TD4-PSP18,72.6,92.48,3.5,0.2071,78.5,0.81,mIoU
31,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2020-04,TD2-PSP50,76.0,96.82,3.4,0.2012,78.5,0.85,mIoU
32,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2020-04,BiSeNet V2-Large(Cityscapes-Pretrained),78.5,100.0,2.5,0.1479,78.5,0.88,mIoU
33,1,Semantic Segmentation,Kvasir-Instrument - Semantic Segmentation benchmarking,2015-05,UNet,0.8578,100.0,0.8578,100,0.8578,0.01,mIoU
34,1,Human Part Segmentation,PASCAL-Part - Human Part Segmentation benchmarking,2015-11,HAZN,57.54,80.52,57.54,100,71.46,0.64,mIoU
35,1,Human Part Segmentation,PASCAL-Part - Human Part Segmentation benchmarking,2017-08,"Joint (ResNet-101, +ms)",64.39,90.11,6.85,0.4921,71.46,0.72,mIoU
36,1,Human Part Segmentation,PASCAL-Part - Human Part Segmentation benchmarking,2018-05,WSHP,67.6,94.6,3.21,0.2306,71.46,0.76,mIoU
37,1,Human Part Segmentation,PASCAL-Part - Human Part Segmentation benchmarking,2018-09,DPC,71.34,99.83,3.74,0.2687,71.46,0.8,mIoU
38,1,Human Part Segmentation,PASCAL-Part - Human Part Segmentation benchmarking,2019-10,SCHP,71.46,100.0,0.12,0.0086,71.46,0.8,mIoU
39,1,Semantic Segmentation,Cityscapes val - Semantic Segmentation benchmarking,2015-12,Dilated-ResNet (Dilated-ResNet-101),75.7,92.88,75.7,100,81.5,0.85,mIoU
40,1,Semantic Segmentation,Cityscapes val - Semantic Segmentation benchmarking,2016-12,PSPNet (Dilated-ResNet-101),79.7,97.79,4.0,0.6897,81.5,0.89,mIoU
41,1,Semantic Segmentation,Cityscapes val - Semantic Segmentation benchmarking,2019-01,Auto-DeepLab-L,80.33,98.56,0.63,0.1086,81.5,0.9,mIoU
42,1,Semantic Segmentation,Cityscapes val - Semantic Segmentation benchmarking,2019-08,HRNetV2 (HRNetV2-W48),81.1,99.51,0.77,0.1328,81.5,0.91,mIoU
43,1,Semantic Segmentation,Cityscapes val - Semantic Segmentation benchmarking,2019-11,Panoptic-DeepLab,81.5,100.0,0.4,0.069,81.5,0.91,mIoU
44,1,Semantic Segmentation,Semantic3D - Semantic Segmentation benchmarking,2016-03,TMLC-MSR,54.2,70.03,54.2,100,77.4,0.61,mIoU
45,1,Semantic Segmentation,Semantic3D - Semantic Segmentation benchmarking,2017-04,SnapNet_,59.1,76.36,4.9,0.2112,77.4,0.66,mIoU
46,1,Semantic Segmentation,Semantic3D - Semantic Segmentation benchmarking,2017-10,SEGCloud,61.3,79.2,2.2,0.0948,77.4,0.69,mIoU
47,1,Semantic Segmentation,Semantic3D - Semantic Segmentation benchmarking,2017-11,SPG,76.2,98.45,14.9,0.6422,77.4,0.85,mIoU
48,1,Semantic Segmentation,Semantic3D - Semantic Segmentation benchmarking,2019-11,RandLA-Net,77.4,100.0,1.2,0.0517,77.4,0.87,mIoU
49,1,Video Semantic Segmentation,Cityscapes val - Video Semantic Segmentation benchmarking,2016-05,FCN-50 [14],70.1,87.73,70.1,100,79.9,0.78,mIoU
50,1,Video Semantic Segmentation,Cityscapes val - Video Semantic Segmentation benchmarking,2016-12,PSPNet-50 [20],78.1,97.75,8.0,0.8163,79.9,0.87,mIoU
51,1,Video Semantic Segmentation,Cityscapes val - Video Semantic Segmentation benchmarking,2016-12,PSPNet-101 [20],79.7,99.75,1.6,0.1633,79.9,0.89,mIoU
52,1,Video Semantic Segmentation,Cityscapes val - Video Semantic Segmentation benchmarking,2020-04,TDNet-50 [9],79.9,100.0,0.2,0.0204,79.9,0.89,mIoU
53,1,Semi-Supervised Video Object Segmentation,YouTube - Semi-Supervised Video Object Segmentation benchmarking,2016-06,ObjFlow,0.776,94.52,0.776,100,0.821,0.01,mIoU
54,1,Semi-Supervised Video Object Segmentation,YouTube - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,0.783,95.37,0.007,0.1556,0.821,0.01,mIoU
55,1,Semi-Supervised Video Object Segmentation,YouTube - Semi-Supervised Video Object Segmentation benchmarking,2018-03,MRFCNN,0.784,95.49,0.001,0.0222,0.821,0.01,mIoU
56,1,Semi-Supervised Video Object Segmentation,YouTube - Semi-Supervised Video Object Segmentation benchmarking,2019-02,FEELVOS,0.821,100.0,0.037,0.8222,0.821,0.01,mIoU
57,1,Semantic Segmentation,PASCAL VOC 2012 val - Semantic Segmentation benchmarking,2016-06,DeepLab-CRF (ResNet-101),77.69,90.55,77.69,100,85.8,0.87,mIoU
58,1,Semantic Segmentation,PASCAL VOC 2012 val - Semantic Segmentation benchmarking,2017-03,ResNet-GCN,81.0,94.41,3.31,0.4081,85.8,0.91,mIoU
59,1,Semantic Segmentation,PASCAL VOC 2012 val - Semantic Segmentation benchmarking,2017-06,DeepLabv3-JFT,82.7,96.39,1.7,0.2096,85.8,0.93,mIoU
60,1,Semantic Segmentation,PASCAL VOC 2012 val - Semantic Segmentation benchmarking,2018-02,DeepLabv3+ (Xception-JFT),84.56,98.55,1.86,0.2293,85.8,0.95,mIoU
61,1,Semantic Segmentation,PASCAL VOC 2012 val - Semantic Segmentation benchmarking,2018-04,ExFuse (ResNeXt-131),85.8,100.0,1.24,0.1529,85.8,0.96,mIoU
62,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2016-11,RefineNet (ResNet-152),40.7,87.96,40.7,100,46.27,0.46,mIoU
63,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2016-12,PSPNet (ResNet-152),43.51,94.04,2.81,0.5045,46.27,0.49,mIoU
64,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2018-03,DSSPN (ResNet-101),43.68,94.4,0.17,0.0305,46.27,0.49,mIoU
65,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2018-03,EncNet (ResNet-101),44.65,96.5,0.97,0.1741,46.27,0.5,mIoU
66,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2019-06,CFNet (ResNet-101),44.89,97.02,0.24,0.0431,46.27,0.5,mIoU
67,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2019-08,Asymmetric ALNN,45.24,97.77,0.35,0.0628,46.27,0.51,mIoU
68,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2020-04,CPN(ResNet-101),46.27,100.0,1.03,0.1849,46.27,0.52,mIoU
69,1,3D Semantic Segmentation,SemanticKITTI - 3D Semantic Segmentation benchmarking,2016-12,PointNet,14.6,24.83,14.6,100,58.8,0.16,mIoU
70,1,3D Semantic Segmentation,SemanticKITTI - 3D Semantic Segmentation benchmarking,2017-06,PointNet++,20.1,34.18,5.5,0.1244,58.8,0.22,mIoU
71,1,3D Semantic Segmentation,SemanticKITTI - 3D Semantic Segmentation benchmarking,2017-10,SqueezeSeg,29.5,50.17,9.4,0.2127,58.8,0.33,mIoU
72,1,3D Semantic Segmentation,SemanticKITTI - 3D Semantic Segmentation benchmarking,2018-07,TangentConv,35.9,61.05,6.4,0.1448,58.8,0.4,mIoU
73,1,3D Semantic Segmentation,SemanticKITTI - 3D Semantic Segmentation benchmarking,2018-09,SqueezeSegV2,39.7,67.52,3.8,0.086,58.8,0.44,mIoU
74,1,3D Semantic Segmentation,SemanticKITTI - 3D Semantic Segmentation benchmarking,2019-04,KPConv,58.8,100.0,19.1,0.4321,58.8,0.66,mIoU
75,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2016-12,PointNet,41.1,61.25,41.1,100,67.1,0.46,mIoU
76,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2017-10,SegCloud,48.9,72.88,7.8,0.3,67.1,0.55,mIoU
77,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2017-11,SPG,58.04,86.5,9.14,0.3515,67.1,0.65,mIoU
78,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2019-04,MinkowskiNet,65.4,97.47,7.36,0.2831,67.1,0.73,mIoU
79,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2019-04,KPConv,67.1,100.0,1.7,0.0654,67.1,0.75,mIoU
80,1,Real-Time Semantic Segmentation,NYU Depth v2 - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet18,35.9,82.53,35.9,100,43.5,0.4,mIoU
81,1,Real-Time Semantic Segmentation,NYU Depth v2 - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet50,41.8,96.09,5.9,0.7763,43.5,0.47,mIoU
82,1,Real-Time Semantic Segmentation,NYU Depth v2 - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet101,43.2,99.31,1.4,0.1842,43.5,0.48,mIoU
83,1,Real-Time Semantic Segmentation,NYU Depth v2 - Real-Time Semantic Segmentation benchmarking,2020-04,TD2-PSP50,43.5,100.0,0.3,0.0395,43.5,0.49,mIoU
84,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2016-12,FCNs in the wild,27.1,53.98,27.1,100,50.2,0.3,mIoU
85,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2017-07,CDA,28.9,57.57,1.8,0.0779,50.2,0.32,mIoU
86,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2017-11,CyCADA pixel+feat,39.5,78.69,10.6,0.4589,50.2,0.44,mIoU
87,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2018-08,Domain adaptation (ResNet-101),43.2,86.06,3.7,0.1602,50.2,0.48,mIoU
88,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2018-10,CBST-SP (ResNet-38),46.2,92.03,3.0,0.1299,50.2,0.52,mIoU
89,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2018-10,"CBST-SP (ResNet-38, MST)",47.0,93.63,0.8,0.0346,50.2,0.53,mIoU
90,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2019-04,BDL (ResNet-101),48.5,96.61,1.5,0.0649,50.2,0.54,mIoU
91,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2019-09,MLSL (SISC-PWL),49.0,97.61,0.5,0.0216,50.2,0.55,mIoU
92,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2019-10,CAG-UDA,50.2,100.0,1.2,0.0519,50.2,0.56,mIoU
93,1,Image-to-Image Translation,SYNTHIA Fall-to-Winter - Image-to-Image Translation benchmarking,2016-12,FCNs in the wild,59.6,94.15,59.6,100,63.3,0.67,mIoU
94,1,Image-to-Image Translation,SYNTHIA Fall-to-Winter - Image-to-Image Translation benchmarking,2017-11,CyCADA,63.3,100.0,3.7,1.0,63.3,0.71,mIoU
95,1,Semantic Segmentation,LIP val - Semantic Segmentation benchmarking,2017-03,Attention+SSL (ResNet-101),44.73,75.35,44.73,100,59.36,0.5,mIoU
96,1,Semantic Segmentation,LIP val - Semantic Segmentation benchmarking,2018-04,JPPNet (ResNet-101),51.37,86.54,6.64,0.4539,59.36,0.57,mIoU
97,1,Semantic Segmentation,LIP val - Semantic Segmentation benchmarking,2018-09,CE2P (ResNet-101),53.1,89.45,1.73,0.1183,59.36,0.59,mIoU
98,1,Semantic Segmentation,LIP val - Semantic Segmentation benchmarking,2019-04,HRNetV2 (HRNetV2-W48),55.9,94.17,2.8,0.1914,59.36,0.63,mIoU
99,1,Semantic Segmentation,LIP val - Semantic Segmentation benchmarking,2019-10,SCHP (ResNet-101),59.36,100.0,3.46,0.2365,59.36,0.66,mIoU
100,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,16.5,53.57,16.5,100,30.8,0.18,mIoU
101,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,17.4,56.49,0.9,0.0629,30.8,0.19,mIoU
102,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,30.8,100.0,13.4,0.9371,30.8,0.34,mIoU
103,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2017-07,CRN,52.4,80.0,52.4,100,65.5,0.59,mIoU
104,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,58.3,89.01,5.9,0.4504,65.5,0.65,mIoU
105,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2019-03,SPADE,62.3,95.11,4.0,0.3053,65.5,0.7,mIoU
106,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,65.5,100.0,3.2,0.2443,65.5,0.73,mIoU
107,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,23.7,56.97,23.7,100,41.6,0.27,mIoU
108,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,37.4,89.9,13.7,0.7654,41.6,0.42,mIoU
109,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,41.6,100.0,4.2,0.2346,41.6,0.47,mIoU
110,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,22.4,51.26,22.4,100,43.7,0.25,mIoU
111,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,38.5,88.1,16.1,0.7559,43.7,0.43,mIoU
112,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,43.7,100.0,5.2,0.2441,43.7,0.49,mIoU
113,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2018-01,PointCNN,65.39,100.0,65.39,100,65.39,0.73,mIoU
114,1,Medical Image Segmentation,2018 Data Science Bowl - Medical Image Segmentation benchmarking,2018-07,Unet++,0.9255,100.0,0.9255,100,0.9255,0.01,mIoU
115,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-08,Dynamically Instantiated Network (ResNet-101),79.8,97.91,79.8,100,81.5,0.89,mIoU
116,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2019-11,Panoptic-DeepLab (X71),81.5,100.0,1.7,1.0,81.5,0.91,mIoU
117,1,Domain Adaptation,SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking,2018-11,ADVENT (ResNet-101),41.2,88.22,41.2,100,46.7,0.46,mIoU
118,1,Domain Adaptation,SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking,2019-08,PyCDA (ResNet-101),46.7,100.0,5.5,1.0,46.7,0.52,mIoU
119,1,Iris Segmentation,CASIA - Iris Segmentation benchmarking,2019-01,IrisParseNet (ASPP) CASIA,89.4,100.0,89.4,100,89.4,1.0,mIoU
120,1,Iris Segmentation,UBIRIS - Iris Segmentation benchmarking,2019-01,IrisParseNet (ASPP),85.39,100.0,85.39,100,85.39,0.96,mIoU
121,1,Iris Segmentation,MICHE - Iris Segmentation benchmarking,2019-01,IrisParseNet (PSP),85.07,100.0,85.07,100,85.07,0.95,mIoU
122,1,Semantic Segmentation,ParisLille3D - Semantic Segmentation benchmarking,2019-04,KPConv deform,75.9,100.0,75.9,100,75.9,0.85,mIoU
123,1,3D Semantic Segmentation,S3DIS - 3D Semantic Segmentation benchmarking,2019-07,PVCNN++,58.98,100.0,58.98,100,58.98,0.66,mIoU
124,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2019-07,ET-Net,0.9623,100.0,0.9623,100,0.9623,0.01,mIoU
125,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2019-07,ET-Net,0.7744,100.0,0.7744,100,0.7744,0.01,mIoU
126,1,Lung Nodule Segmentation,Montgomery County - Lung Nodule Segmentation benchmarking,2019-07,ET-Net,0.942,100.0,0.942,100,0.942,0.01,mIoU
127,1,Panoptic Segmentation,Mapillary val - Panoptic Segmentation benchmarking,2019-09,AdaptIS (ResNeXt-101),56.8,100.0,56.8,100,56.8,0.64,mIoU
128,1,Real-Time Semantic Segmentation,Cityscapes val - Real-Time Semantic Segmentation benchmarking,2019-12,LiteSeg-MobileNet,67.8,100.0,67.8,100,67.8,0.76,mIoU
129,1,Semantic Segmentation,BDD - Semantic Segmentation benchmarking,2019-12,FasterSeg,55.1,100.0,55.1,100,55.1,0.62,mIoU
130,1,Semantic Segmentation,GTAV-to-Cityscapes Labels - Semantic Segmentation benchmarking,2019-12,MRNet,48.3,100.0,48.3,100,48.3,0.54,mIoU
131,1,Unsupervised Video Object Segmentation,YouTube - Unsupervised Video Object Segmentation benchmarking,2020-01,COSNet,0.705,100.0,0.705,100,0.705,0.01,mIoU
132,1,Real-Time Semantic Segmentation,COCO-Stuff - Real-Time Semantic Segmentation benchmarking,2020-04,BiSeNet V2-Large,28.7,100.0,28.7,100,28.7,0.32,mIoU
0,1,Unsupervised Image-To-Image Translation,SVNH-to-MNIST - Unsupervised Image-To-Image Translation benchmarking,2014-09,DANN,73.6,81.42,73.6,100,90.4,0.74,Classification\\ Accuracy
1,1,Unsupervised Image-To-Image Translation,SVNH-to-MNIST - Unsupervised Image-To-Image Translation benchmarking,2016-11,DTN,84.4,93.36,10.8,0.6429,90.4,0.84,Classification\\ Accuracy
2,1,Unsupervised Image-To-Image Translation,SVNH-to-MNIST - Unsupervised Image-To-Image Translation benchmarking,2017-11,CyCADA pixel+feat,90.4,100.0,6.0,0.3571,90.4,0.9,Classification\\ Accuracy
3,1,Domain Adaptation,Synth Objects-to-LINEMOD - Domain Adaptation benchmarking,2016-08,DSN (DANN),100.0,100.0,100,100,100,1.0,Classification\\ Accuracy
4,1,3D Object Classification,ModelNet40 - 3D Object Classification benchmarking,2018-12,3D-PointCapsNet,89.3,100.0,89.3,100,89.3,0.89,Classification\\ Accuracy
0,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2014-10,NICE,4.5,100.0,4.5,100,4.5,1.0,bits/dimension
1,1,Image Generation,MNIST - Image Generation benchmarking,2018-11,i-ResNet,1.06,100.0,1.06,100,1.06,0.24,bits/dimension
0,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2014-11,FCN,29.39,63.52,29.39,100,46.27,0.41,Validation\\ mIoU
1,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2015-11,DilatedNet,32.31,69.83,2.92,0.173,46.27,0.45,Validation\\ mIoU
2,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2016-11,RefineNet,40.7,87.96,8.39,0.497,46.27,0.57,Validation\\ mIoU
3,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2016-12,PSPNet,44.94,97.13,4.24,0.2512,46.27,0.63,Validation\\ mIoU
4,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2019-11,LaU-regression-loss,45.02,97.3,0.08,0.0047,46.27,0.63,Validation\\ mIoU
5,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2020-04,CPN(ResNet-101),46.27,100.0,1.25,0.0741,46.27,0.65,Validation\\ mIoU
6,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 2% labeled - Semi-Supervised Semantic Segmentation benchmarking,2018-02,Adversarial (DeepLab v2 ImageNet pre-trained),49.2,77.73,49.2,100,63.3,0.69,Validation\\ mIoU
7,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 2% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN + MLMT (DeepLab v2 MSCOCO/ImageNet pre-trained),63.3,100.0,14.1,1.0,63.3,0.89,Validation\\ mIoU
8,1,Semi-Supervised Semantic Segmentation,Cityscapes 50% labeled - Semi-Supervised Semantic Segmentation benchmarking,2018-02,Adversarial (DeepLab v2 ImageNet pre-trained),65.7,100.0,65.7,100,65.7,0.92,Validation\\ mIoU
9,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 12.5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2018-02,Adversarial (DeepLab v2 ImageNet pre-trained),64.3,90.06,64.3,100,71.4,0.9,Validation\\ mIoU
10,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 12.5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN+MLMT (DeepLab v2 ImageNet pre-trained),67.3,94.26,3.0,0.4225,71.4,0.94,Validation\\ mIoU
11,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 12.5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN+MLMT (DeepLab v3 ImageNet pre-trained),70.4,98.6,3.1,0.4366,71.4,0.99,Validation\\ mIoU
12,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 12.5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN + MLMT (DeepLab v2 MSCOCO/ImageNet pre-trained),71.4,100.0,1.0,0.1408,71.4,1.0,Validation\\ mIoU
13,1,Semi-Supervised Semantic Segmentation,Cityscapes 12.5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2018-02,Adversarial (DeepLab v2 ImageNet pre-trained),57.1,96.29,57.1,100,59.3,0.8,Validation\\ mIoU
14,1,Semi-Supervised Semantic Segmentation,Cityscapes 12.5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN (DeepLab v2 ImageNet pre-trained),59.3,100.0,2.2,1.0,59.3,0.83,Validation\\ mIoU
15,1,Semi-Supervised Semantic Segmentation,Cityscapes 25% labeled - Semi-Supervised Semantic Segmentation benchmarking,2018-02,Adversarial (DeepLab v2 ImageNet pre-trained),60.5,97.74,60.5,100,61.9,0.85,Validation\\ mIoU
16,1,Semi-Supervised Semantic Segmentation,Cityscapes 25% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN (DeepLab v2 ImageNet pre-trained),61.9,100.0,1.4,1.0,61.9,0.87,Validation\\ mIoU
17,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2018-02,Adversarial (DeepLab v2 ImageNet pre-trained),59.1,87.95,59.1,100,67.2,0.83,Validation\\ mIoU
18,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN+MLMT (DeepLab v3+ ImageNet pre-trained),66.6,99.11,7.5,0.9259,67.2,0.93,Validation\\ mIoU
19,1,Semi-Supervised Semantic Segmentation,Pascal VOC 2012 5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN + MLMT (DeepLab v2 MSCOCO/ImageNet pre-trained),67.2,100.0,0.6,0.0741,67.2,0.94,Validation\\ mIoU
20,1,Semi-Supervised Semantic Segmentation,Cityscapes 5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,"S4GAN (DeepLabv2 with ResNet101, MSCOCO pre-trained)",55.61,100.0,55.61,100,55.61,0.78,Validation\\ mIoU
21,1,Semi-Supervised Semantic Segmentation,PASCAL Context 25% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN+MLMT (DeepLab v2 ImageNet pre-trained),37.8,100.0,37.8,100,37.8,0.53,Validation\\ mIoU
22,1,Semi-Supervised Semantic Segmentation,PASCAL Context 12.5% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,s4GAN+MLMT (DeepLab v2 ImageNet pre-trained),35.3,100.0,35.3,100,35.3,0.49,Validation\\ mIoU
23,1,Semi-Supervised Semantic Segmentation,Cityscapes 2% labeled - Semi-Supervised Semantic Segmentation benchmarking,2019-08,"S4GAN (DeepLabv2 with ResNet101, MSCOCO pre-trained)",50.48,100.0,50.48,100,50.48,0.71,Validation\\ mIoU
0,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2014-11,Tompson et al.,82.0,87.14,82.0,100,94.1,0.87,PCKh\\-0\\.5
1,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2015-11,DeepCut,82.4,87.57,0.4,0.0331,94.1,0.88,PCKh\\-0\\.5
2,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2016-01,Convolutional Pose Machines,88.52,94.07,6.12,0.5058,94.1,0.94,PCKh\\-0\\.5
3,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2016-03,Stacked Hourglass Networks,90.9,96.6,2.38,0.1967,94.1,0.97,PCKh\\-0\\.5
4,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2017-02,Multi-Context Attention,91.5,97.24,0.6,0.0496,94.1,0.97,PCKh\\-0\\.5
5,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2017-04,Chen et al. ICCV\'17,91.9,97.66,0.4,0.0331,94.1,0.98,PCKh\\-0\\.5
6,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2017-08,Pyramid Residual Modules (PRMs),92.0,97.77,0.1,0.0083,94.1,0.98,PCKh\\-0\\.5
7,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2018-03,Multi-Scale Structure-Aware Network,92.1,97.87,0.1,0.0083,94.1,0.98,PCKh\\-0\\.5
8,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2019-01,MSPN,92.6,98.41,0.5,0.0413,94.1,0.98,PCKh\\-0\\.5
9,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2019-02,Cascade Feature Aggregation,93.9,99.79,1.3,0.1074,94.1,1.0,PCKh\\-0\\.5
10,1,Pose Estimation,MPII Human Pose - Pose Estimation benchmarking,2020-02,Soft-gated Skip Connections,94.1,100.0,0.2,0.0165,94.1,1.0,PCKh\\-0\\.5
0,1,Skeleton Based Action Recognition,J-HMDB - Skeleton Based Action Recognition benchmarking,2014-11,Action Tubes,62.5,69.14,62.5,100,90.4,0.69,Accuracy\\ \\(RGB\\+pose\\)
1,1,Skeleton Based Action Recognition,J-HMDB - Skeleton Based Action Recognition benchmarking,2016-09,MR Two-Sream R-CNN,71.1,78.65,8.6,0.3082,90.4,0.79,Accuracy\\ \\(RGB\\+pose\\)
2,1,Skeleton Based Action Recognition,J-HMDB - Skeleton Based Action Recognition benchmarking,2017-04,Chained (RGB+Flow +Pose),76.1,84.18,5.0,0.1792,90.4,0.84,Accuracy\\ \\(RGB\\+pose\\)
3,1,Skeleton Based Action Recognition,J-HMDB - Skeleton Based Action Recognition benchmarking,2017-05,I3D,84.1,93.03,8.0,0.2867,90.4,0.93,Accuracy\\ \\(RGB\\+pose\\)
4,1,Skeleton Based Action Recognition,J-HMDB - Skeleton Based Action Recognition benchmarking,2018-06,I3D + Potion,85.5,94.58,1.4,0.0502,90.4,0.95,Accuracy\\ \\(RGB\\+pose\\)
5,1,Skeleton Based Action Recognition,J-HMDB - Skeleton Based Action Recognition benchmarking,2018-06,Potion,90.4,100.0,4.9,0.1756,90.4,1.0,Accuracy\\ \\(RGB\\+pose\\)
0,1,Pedestrian Detection,Caltech - Pedestrian Detection benchmarking,2014-11,TA-CNN,20.9,84.27,20.9,100,24.8,0.84,Reasonable\\ Miss\\ Rate
1,1,Pedestrian Detection,Caltech - Pedestrian Detection benchmarking,2014-12,LDCF,24.8,100.0,3.9,1.0,24.8,1.0,Reasonable\\ Miss\\ Rate
0,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2014-12,Jaderberg et al.,88.5,90.86,88.5,100,97.4,0.91,Precision
1,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2016-04,Gupta et al.,92.0,94.46,3.5,0.3933,97.4,0.94,Precision
2,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2017-08,WordSup (VGG16-synth-icdar),93.34,95.83,1.34,0.1506,97.4,0.96,Precision
3,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2018-07,Mask TextSpotter,95.0,97.54,1.66,0.1865,97.4,0.98,Precision
4,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2019-04,CRAFT,97.4,100.0,2.4,0.2697,97.4,1.0,Precision
5,1,Pancreas Segmentation,CT-150 - Pancreas Segmentation benchmarking,2015-05,U-Net,0.848,99.88,0.848,100,0.849,0.01,Precision
6,1,Pancreas Segmentation,CT-150 - Pancreas Segmentation benchmarking,2018-04,Att U-Net,0.849,100.0,0.001,1.0,0.849,0.01,Precision
7,1,Gesture Recognition,Montalbano - Gesture Recognition benchmarking,2015-06,Temp Conv + LSTM,94.49,100.0,94.49,100,94.49,0.97,Precision
8,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2015-12,STAPLE_CA,46.72,67.34,46.72,100,69.38,0.48,Precision
9,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2016-11,ECO,48.86,70.42,2.14,0.0944,69.38,0.5,Precision
10,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2018-11,ATOM,64.84,93.46,15.98,0.7052,69.38,0.67,Precision
11,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2018-12,SiamRPN++,69.38,100.0,4.54,0.2004,69.38,0.71,Precision
12,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2016-04,MCLAB_FCN,70.8,76.42,70.8,100,92.65,0.73,Precision
13,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-03,SegLink,73.1,78.9,2.3,0.1053,92.65,0.75,Precision
14,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-04,EAST + PVANET2x RBOX (single-scale),83.6,90.23,10.5,0.4805,92.65,0.86,Precision
15,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-09,FTSN + MNMS,88.6,95.63,5.0,0.2288,92.65,0.91,Precision
16,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2018-01,FOTS MS,91.85,99.14,3.25,0.1487,92.65,0.94,Precision
17,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2019-10,CharNet H-88 (multi-scale),92.65,100.0,0.8,0.0366,92.65,0.95,Precision
18,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2016-06,Yao et al.,43.23,71.02,43.23,100,60.87,0.44,Precision
19,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2017-04,EAST + VGG16,50.39,82.78,7.16,0.4059,60.87,0.52,Precision
20,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2018-01,TextBoxes++_MS,60.87,100.0,10.48,0.5941,60.87,0.62,Precision
21,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2017-03,SegLink,86.0,93.99,86.0,100,91.5,0.88,Precision
22,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2017-04,EAST + PVANET2x,87.28,95.39,1.28,0.2327,91.5,0.9,Precision
23,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2017-09,FTSN + MNMS,87.6,95.74,0.32,0.0582,91.5,0.9,Precision
24,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2019-04,CRAFT,88.2,96.39,0.6,0.1091,91.5,0.91,Precision
25,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2019-11,DB-ResNet-50 (736),91.5,100.0,3.3,0.6,91.5,0.94,Precision
26,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2017-04,EAST,50.0,55.62,50.0,100,89.9,0.51,Precision
27,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2017-09,FTSN,84.7,94.22,34.7,0.8697,89.9,0.87,Precision
28,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2019-04,CRAFT,87.6,97.44,2.9,0.0727,89.9,0.9,Precision
29,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2019-10,CharNet H-88 (multi-scale),88.0,97.89,0.4,0.01,89.9,0.9,Precision
30,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2019-10,CharNet H-88,89.9,100.0,1.9,0.0476,89.9,0.92,Precision
31,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-01,FOTS,80.95,95.89,80.95,100,84.42,0.83,Precision
32,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-01,FOTS MS,81.86,96.97,0.91,0.2622,84.42,0.84,Precision
33,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-02,Corner Localization (single-scale),83.8,99.27,1.94,0.5591,84.42,0.86,Precision
34,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2019-03,PMTD*,84.42,100.0,0.62,0.1787,84.42,0.87,Precision
35,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-01,SLPR,80.1,92.28,80.1,100,86.8,0.82,Precision
36,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-06,PSENet-1s,82.5,95.05,2.4,0.3582,86.8,0.85,Precision
37,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-11,PAN,86.8,100.0,4.3,0.6418,86.8,0.89,Precision
38,1,Object Localization,Mall - Object Localization benchmarking,2018-06,Hausdorff Loss,88.1,100.0,88.1,100,88.1,0.9,Precision
39,1,Lung Nodule Segmentation,NIH - Lung Nodule Segmentation benchmarking,2019-04,U-Net+R+A4,0.969,100.0,0.969,100,0.969,0.01,Precision
40,1,Visual Object Tracking,OTB-2015 - Visual Object Tracking benchmarking,2019-09,GradNet,0.861,100.0,0.861,100,0.861,0.01,Precision
0,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2014-12,Jaderberg et al.,67.8,72.82,67.8,100,93.1,0.72,Recall
1,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2015-04,Neumann et al. *,72.4,77.77,4.6,0.1818,93.1,0.77,Recall
2,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2016-04,Gupta et al.,75.5,81.1,3.1,0.1225,93.1,0.8,Recall
3,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2017-03,SegLink,83.0,89.15,7.5,0.2964,93.1,0.88,Recall
4,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2017-08,WordSup (VGG16-synth-icdar),87.53,94.02,4.53,0.1791,93.1,0.93,Recall
5,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2018-07,Mask TextSpotter,88.6,95.17,1.07,0.0423,93.1,0.94,Recall
6,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2018-11,SPCNET,90.5,97.21,1.9,0.0751,93.1,0.96,Recall
7,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2019-04,CRAFT,93.1,100.0,2.6,0.1028,93.1,0.98,Recall
8,1,Pancreas Segmentation,CT-150 - Pancreas Segmentation benchmarking,2015-05,U-Net,0.806,95.84,0.806,100,0.841,0.01,Recall
9,1,Pancreas Segmentation,CT-150 - Pancreas Segmentation benchmarking,2018-04,Att U-Net,0.841,100.0,0.035,1.0,0.841,0.01,Recall
10,1,Gesture Recognition,Montalbano - Gesture Recognition benchmarking,2015-06,Temp Conv + LSTM,94.57,100.0,94.57,100,94.57,1.0,Recall
11,1,Temporal Action Localization,CrossTask - Temporal Action Localization benchmarking,2015-06,Alayrac,13.3,39.58,13.3,100,33.6,0.14,Recall
12,1,Temporal Action Localization,CrossTask - Temporal Action Localization benchmarking,2019-03,Fully-supervised upper-bound,31.6,94.05,18.3,0.9015,33.6,0.33,Recall
13,1,Temporal Action Localization,CrossTask - Temporal Action Localization benchmarking,2019-06,Text-Video Embedding,33.6,100.0,2.0,0.0985,33.6,0.36,Recall
14,1,Point Cloud Registration,3DMatch Benchmark - Point Cloud Registration benchmarking,2016-03,3DMatch + RANSAC,66.8,81.46,66.8,100,82.0,0.71,Recall
15,1,Point Cloud Registration,3DMatch Benchmark - Point Cloud Registration benchmarking,2019-10,FCGF + RANSAC,82.0,100.0,15.2,1.0,82.0,0.87,Recall
16,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2016-04,MCLAB_FCN,43.0,46.75,43.0,100,91.98,0.45,Recall
17,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-03,SegLink,76.8,83.5,33.8,0.6901,91.98,0.81,Recall
18,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-04,EAST + PVANET2x RBOX (multi-scale),78.3,85.13,1.5,0.0306,91.98,0.83,Recall
19,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-09,FTSN + MNMS,80.0,86.98,1.7,0.0347,91.98,0.85,Recall
20,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2018-01,PixelLink+VGG16 2s,82.0,89.15,2.0,0.0408,91.98,0.87,Recall
21,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2018-01,FOTS MS,87.92,95.59,5.92,0.1209,91.98,0.93,Recall
22,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2019-10,CharNet H-88 (multi-scale),90.47,98.36,2.55,0.0521,91.98,0.96,Recall
23,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2019-10,CharNet H-88 (single-scale),91.98,100.0,1.51,0.0308,91.98,0.97,Recall
24,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2016-06,Yao et al.,27.1,42.81,27.1,100,63.3,0.29,Recall
25,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2017-04,EAST + VGG16,32.4,51.18,5.3,0.1464,63.3,0.34,Recall
26,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2018-01,TextBoxes++_MS,56.7,89.57,24.3,0.6713,63.3,0.6,Recall
27,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2018-04,Corner-based Region Proposals,63.3,100.0,6.6,0.1823,63.3,0.67,Recall
28,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2017-03,SegLink,70.0,88.38,70.0,100,79.2,0.74,Recall
29,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2017-09,FTSN + MNMS,77.1,97.35,7.1,0.7717,79.2,0.82,Recall
30,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2019-04,CRAFT,78.2,98.74,1.1,0.1196,79.2,0.83,Recall
31,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2019-11,DB-ResNet-50 (736),79.2,100.0,1.0,0.1087,79.2,0.84,Recall
32,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2017-04,EAST,36.2,42.59,36.2,100,85.0,0.38,Recall
33,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2017-09,FTSN,78.0,91.76,41.8,0.8566,85.0,0.82,Recall
34,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2018-11,SPCNET,82.8,97.41,4.8,0.0984,85.0,0.88,Recall
35,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2019-10,CharNet H-88 (multi-scale),85.0,100.0,2.2,0.0451,85.0,0.9,Recall
36,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-01,FOTS,57.51,75.42,57.51,100,76.25,0.61,Recall
37,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-01,FOTS MS,62.3,81.7,4.79,0.2556,76.25,0.66,Recall
38,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-02,Corner Localization (multi-scale),70.6,92.59,8.3,0.4429,76.25,0.75,Recall
39,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2019-03,PMTD*,76.25,100.0,5.65,0.3015,76.25,0.81,Recall
40,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-01,SLPR,70.1,84.25,70.1,100,83.2,0.74,Recall
41,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-06,PSENet-1s,79.89,96.02,9.79,0.7473,83.2,0.84,Recall
42,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-11,PAN,83.2,100.0,3.31,0.2527,83.2,0.88,Recall
43,1,Object Localization,Pupil - Object Localization benchmarking,2018-06,Hausdorff Loss,89.2,100.0,89.2,100,89.2,0.94,Recall
44,1,Zero-Shot Object Detection,MS-COCO - Zero-Shot Object Detection benchmarking,2018-11,ZSD-Polarity Loss,43.56,100.0,43.56,100,43.56,0.46,Recall
45,1,Lung Nodule Segmentation,NIH - Lung Nodule Segmentation benchmarking,2019-04,U-Net+R+A4,0.956,100.0,0.956,100,0.956,0.01,Recall
0,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2014-12,Jaderberg et al.,76.8,83.39,76.8,100,92.1,0.82,F\\-Measure
1,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2015-04,Neumann et al. *,77.1,83.71,0.3,0.0196,92.1,0.83,F\\-Measure
2,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2016-04,Gupta et al.,83.0,90.12,5.9,0.3856,92.1,0.89,F\\-Measure
3,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2017-03,SegLink,85.3,92.62,2.3,0.1503,92.1,0.91,F\\-Measure
4,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2017-07,STN-OCR,90.3,98.05,5.0,0.3268,92.1,0.97,F\\-Measure
5,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2017-08,WordSup (VGG16-synth-icdar),90.34,98.09,0.04,0.0026,92.1,0.97,F\\-Measure
6,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2018-07,Mask TextSpotter,91.7,99.57,1.36,0.0889,92.1,0.98,F\\-Measure
7,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2018-11,SPCNET,92.1,100.0,0.4,0.0261,92.1,0.99,F\\-Measure
8,1,Curved Text Detection,SCUT-CTW1500 - Curved Text Detection benchmarking,2015-05,"CTD [[Noh et al.(2015)Noh, Hong, and Han]]",69.5,80.53,69.5,100,86.3,0.74,F\\-Measure
9,1,Curved Text Detection,SCUT-CTW1500 - Curved Text Detection benchmarking,2015-05,"CTD+TLOC [[Noh et al.(2015)Noh, Hong, and Han]]",73.4,85.05,3.9,0.2321,86.3,0.79,F\\-Measure
10,1,Curved Text Detection,SCUT-CTW1500 - Curved Text Detection benchmarking,2019-04,TextCohesion,86.3,100.0,12.9,0.7679,86.3,0.92,F\\-Measure
11,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2016-04,MCLAB_FCN,53.6,58.55,53.6,100,91.55,0.57,F\\-Measure
12,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-03,SegLink,75.0,81.92,21.4,0.5639,91.55,0.8,F\\-Measure
13,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-04,EAST + PVANET2x RBOX (single-scale),78.2,85.42,3.2,0.0843,91.55,0.84,F\\-Measure
14,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-04,EAST + PVANET2x RBOX (multi-scale),80.7,88.15,2.5,0.0659,91.55,0.86,F\\-Measure
15,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2017-09,FTSN + MNMS,84.1,91.86,3.4,0.0896,91.55,0.9,F\\-Measure
16,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2018-01,FOTS MS,89.84,98.13,5.74,0.1513,91.55,0.96,F\\-Measure
17,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2019-10,CharNet H-88 (multi-scale),91.55,100.0,1.71,0.0451,91.55,0.98,F\\-Measure
18,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2016-06,Yao et al.,33.31,56.36,33.31,100,59.1,0.36,F\\-Measure
19,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2017-04,EAST + VGG16,39.45,66.75,6.14,0.2381,59.1,0.42,F\\-Measure
20,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2018-01,TextBoxes++_MS,58.72,99.36,19.27,0.7472,59.1,0.63,F\\-Measure
21,1,Scene Text Detection,COCO-Text - Scene Text Detection benchmarking,2018-04,Corner-based Region Proposals,59.1,100.0,0.38,0.0147,59.1,0.63,F\\-Measure
22,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2017-03,SegLink,77.0,90.69,77.0,100,84.9,0.82,F\\-Measure
23,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2018-01,PixelLink + VGG16 2s,77.8,91.64,0.8,0.1013,84.9,0.83,F\\-Measure
24,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2018-02,Corner Localization,81.5,96.0,3.7,0.4684,84.9,0.87,F\\-Measure
25,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2019-11,DB-ResNet-50 (736),84.9,100.0,3.4,0.4304,84.9,0.91,F\\-Measure
26,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2017-04,EAST,42.0,48.55,42.0,100,86.5,0.45,F\\-Measure
27,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2017-09,FTSN,81.3,93.99,39.3,0.8831,86.5,0.87,F\\-Measure
28,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2018-11,SPCNET,82.9,95.84,1.6,0.036,86.5,0.89,F\\-Measure
29,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2019-04,TextCohesion,84.6,97.8,1.7,0.0382,86.5,0.91,F\\-Measure
30,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2019-10,CharNet H-88 (multi-scale),86.5,100.0,1.9,0.0427,86.5,0.93,F\\-Measure
31,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-01,FOTS,67.25,83.93,67.25,100,80.13,0.72,F\\-Measure
32,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-01,FOTS MS,70.75,88.29,3.5,0.2717,80.13,0.76,F\\-Measure
33,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-02,Corner Localization (multi-scale),72.4,90.35,1.65,0.1281,80.13,0.78,F\\-Measure
34,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-06,PSENet-1s,72.45,90.42,0.05,0.0039,80.13,0.78,F\\-Measure
35,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-11,SPCNET,74.1,92.47,1.65,0.1281,80.13,0.79,F\\-Measure
36,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2018-11,PAN,74.3,92.72,0.2,0.0155,80.13,0.8,F\\-Measure
37,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2019-03,PMTD*,80.13,100.0,5.83,0.4526,80.13,0.86,F\\-Measure
38,1,Object Skeleton Detection,SK-LARGE - Object Skeleton Detection benchmarking,2018-01,Hi-Fi,0.724,98.91,0.724,100,0.732,0.01,F\\-Measure
39,1,Object Skeleton Detection,SK-LARGE - Object Skeleton Detection benchmarking,2018-11,DeepFlux,0.732,100.0,0.008,1.0,0.732,0.01,F\\-Measure
40,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-06,PSENet-1s,81.17,95.49,81.17,100,85.0,0.87,F\\-Measure
41,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-11,PAN,85.0,100.0,3.83,1.0,85.0,0.91,F\\-Measure
42,1,Scene Text Detection,IC19-ReCTs - Scene Text Detection benchmarking,2019-06,BDN,93.36,100.0,93.36,100,93.36,1.0,F\\-Measure
0,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2014-12,Dual-Path (ResNet),25.3,65.54,25.3,100,38.6,0.52,Text\\-to\\-image\\ R@1
1,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2017-12,SCO (ResNet),33.1,85.75,7.8,0.5865,38.6,0.68,Text\\-to\\-image\\ R@1
2,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2018-03,SCAN,38.6,100.0,5.5,0.4135,38.6,0.79,Text\\-to\\-image\\ R@1
3,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2017-12,SCO\n  (ResNet),41.1,84.57,41.1,100,48.6,0.85,Text\\-to\\-image\\ R@1
4,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2018-03,SCAN,48.6,100.0,7.5,1.0,48.6,1.0,Text\\-to\\-image\\ R@1
0,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2014-12,Dual-Path (ResNet),53.4,77.06,53.4,100,69.3,0.69,Text\\-to\\-image\\ R@5
1,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2017-12,SCO (ResNet),62.9,90.76,9.5,0.5975,69.3,0.81,Text\\-to\\-image\\ R@5
2,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2018-03,SCAN,69.3,100.0,6.4,0.4025,69.3,0.89,Text\\-to\\-image\\ R@5
3,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2017-12,SCO\n  (ResNet),70.5,90.73,70.5,100,77.7,0.91,Text\\-to\\-image\\ R@5
4,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2018-03,SCAN,77.7,100.0,7.2,1.0,77.7,1.0,Text\\-to\\-image\\ R@5
0,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2014-12,Dual-Path (ResNet),66.4,82.59,66.4,100,80.4,0.78,Text\\-to\\-image\\ R@10
1,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2017-12,SCO (ResNet),75.5,93.91,9.1,0.65,80.4,0.89,Text\\-to\\-image\\ R@10
2,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2018-03,SCAN,80.4,100.0,4.9,0.35,80.4,0.94,Text\\-to\\-image\\ R@10
3,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2017-12,SCO\n  (ResNet),80.1,94.01,80.1,100,85.2,0.94,Text\\-to\\-image\\ R@10
4,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2018-03,SCAN,85.2,100.0,5.1,1.0,85.2,1.0,Text\\-to\\-image\\ R@10
0,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2014-12,"DVSA (R-CNN, AlexNet)",50.5,59.2,50.5,100,85.3,0.51,R\\-at\\-10
1,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2015-04,mCNN,69.6,81.59,19.1,0.5489,85.3,0.7,R\\-at\\-10
2,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2015-11,SPE,72.1,84.53,2.5,0.0718,85.3,0.73,R\\-at\\-10
3,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2016-11,DAN,79.1,92.73,7.0,0.2011,85.3,0.8,R\\-at\\-10
4,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2017-12,SCO,80.1,93.9,1.0,0.0287,85.3,0.81,R\\-at\\-10
5,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2018-03,SCAN i-t,82.6,96.83,2.5,0.0718,85.3,0.83,R\\-at\\-10
6,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2019-09,CAMP,85.3,100.0,2.7,0.0776,85.3,0.86,R\\-at\\-10
7,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,83.96,90.33,83.96,100,92.95,0.85,R\\-at\\-10
8,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-09,AMEM,87.43,94.06,3.47,0.386,92.95,0.88,R\\-at\\-10
9,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,88.81,95.55,1.38,0.1535,92.95,0.9,R\\-at\\-10
10,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,HACAN,90.68,97.56,1.87,0.208,92.95,0.92,R\\-at\\-10
11,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),92.95,100.0,2.27,0.2525,92.95,0.94,R\\-at\\-10
12,1,Sketch-Based Image Retrieval,Chairs - Sketch-Based Image Retrieval benchmarking,2016-06,Dense-HOG + rankSVM,93.8,94.75,93.8,100,99.0,0.95,R\\-at\\-10
13,1,Sketch-Based Image Retrieval,Chairs - Sketch-Based Image Retrieval benchmarking,2016-06,Chairs net +,99.0,100.0,5.2,1.0,99.0,1.0,R\\-at\\-10
14,1,Sketch-Based Image Retrieval,Handbags - Sketch-Based Image Retrieval benchmarking,2016-06,Chairs net +,58.3,68.03,58.3,100,85.7,0.59,R\\-at\\-10
15,1,Sketch-Based Image Retrieval,Handbags - Sketch-Based Image Retrieval benchmarking,2016-06,Shoes net +,59.5,69.43,1.2,0.0438,85.7,0.6,R\\-at\\-10
16,1,Sketch-Based Image Retrieval,Handbags - Sketch-Based Image Retrieval benchmarking,2017-09,EdgeMAC + whitening,85.7,100.0,26.2,0.9562,85.7,0.87,R\\-at\\-10
17,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,86.88,92.38,86.88,100,94.05,0.88,R\\-at\\-10
18,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),88.8,94.42,1.92,0.2678,94.05,0.9,R\\-at\\-10
19,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,89.35,95.0,0.55,0.0767,94.05,0.9,R\\-at\\-10
20,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,89.45,95.11,0.1,0.0139,94.05,0.9,R\\-at\\-10
21,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),94.05,100.0,4.6,0.6416,94.05,0.95,R\\-at\\-10
22,1,Sketch-Based Image Retrieval,Shoes - Sketch-Based Image Retrieval benchmarking,2017-09,EdgeMAC + whitening,92.2,100.0,92.2,100,92.2,0.93,R\\-at\\-10
23,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2018-05,BAN (Bottom-Up detector),86.35,99.82,86.35,100,86.51,0.87,R\\-at\\-10
24,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2019-08,VisualBERT,86.51,100.0,0.16,1.0,86.51,0.87,R\\-at\\-10
25,1,Phrase Grounding,Flickr30k Entities Dev - Phrase Grounding benchmarking,2019-08,VisualBERT,86.31,100.0,86.31,100,86.31,0.87,R\\-at\\-10
0,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2014-12,"DVSA (R-CNN, AlexNet)",15.2,29.51,15.2,100,51.5,0.16,R\\-at\\-1
1,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2015-04,mCNN,26.2,50.87,11.0,0.303,51.5,0.28,R\\-at\\-1
2,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2015-11,SPE,29.7,57.67,3.5,0.0964,51.5,0.31,R\\-at\\-1
3,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2016-08,2WayNet (VGG),36.0,69.9,6.3,0.1736,51.5,0.38,R\\-at\\-1
4,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2016-11,DAN,39.4,76.5,3.4,0.0937,51.5,0.42,R\\-at\\-1
5,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2017-12,SCO,41.1,79.81,1.7,0.0468,51.5,0.43,R\\-at\\-1
6,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2018-03,SCAN i-t,44.0,85.44,2.9,0.0799,51.5,0.46,R\\-at\\-1
7,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2019-09,CAMP,51.5,100.0,7.5,0.2066,51.5,0.54,R\\-at\\-1
8,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,43.51,78.88,43.51,100,55.16,0.46,R\\-at\\-1
9,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-09,AMEM,48.53,87.98,5.02,0.4309,55.16,0.51,R\\-at\\-1
10,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,50.29,91.17,1.76,0.1511,55.16,0.53,R\\-at\\-1
11,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),50.92,92.31,0.63,0.0541,55.16,0.54,R\\-at\\-1
12,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,HACAN,54.76,99.27,3.84,0.3296,55.16,0.58,R\\-at\\-1
13,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),55.16,100.0,0.4,0.0343,55.16,0.58,R\\-at\\-1
14,1,Sketch-Based Image Retrieval,Handbags - Sketch-Based Image Retrieval benchmarking,2016-06,Chairs net +,26.2,51.17,26.2,100,51.2,0.28,R\\-at\\-1
15,1,Sketch-Based Image Retrieval,Handbags - Sketch-Based Image Retrieval benchmarking,2017-09,EdgeMAC + whitening,51.2,100.0,25.0,1.0,51.2,0.54,R\\-at\\-1
16,1,Sketch-Based Image Retrieval,Chairs - Sketch-Based Image Retrieval benchmarking,2016-06,Dense-HOG + rankSVM,52.6,61.45,52.6,100,85.6,0.55,R\\-at\\-1
17,1,Sketch-Based Image Retrieval,Chairs - Sketch-Based Image Retrieval benchmarking,2016-06,Shoes net +,65.0,75.93,12.4,0.3758,85.6,0.69,R\\-at\\-1
18,1,Sketch-Based Image Retrieval,Chairs - Sketch-Based Image Retrieval benchmarking,2016-06,Chairs net +,72.2,84.35,7.2,0.2182,85.6,0.76,R\\-at\\-1
19,1,Sketch-Based Image Retrieval,Chairs - Sketch-Based Image Retrieval benchmarking,2017-09,EdgeMAC + whitening,85.6,100.0,13.4,0.4061,85.6,0.9,R\\-at\\-1
20,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2016-10,3D-VAE-GAN,0.02,3.77,0.02,100,0.53,0.0,R\\-at\\-1
21,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2017-11,MarrNet,0.42,79.25,0.4,0.7843,0.53,0.0,R\\-at\\-1
22,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2018-04,MarrNet extension (w/o Pose),0.53,100.0,0.11,0.2157,0.53,0.01,R\\-at\\-1
23,1,Metric Learning,CUB-200-2011 - Metric Learning benchmarking,2016-10,PDDM Quadruplet,58.3,87.28,58.3,100,66.8,0.61,R\\-at\\-1
24,1,Metric Learning,CUB-200-2011 - Metric Learning benchmarking,2016-11,HDC,60.7,90.87,2.4,0.2824,66.8,0.64,R\\-at\\-1
25,1,Metric Learning,CUB-200-2011 - Metric Learning benchmarking,2017-06,ResNet-50 + Margin,63.6,95.21,2.9,0.3412,66.8,0.67,R\\-at\\-1
26,1,Metric Learning,CUB-200-2011 - Metric Learning benchmarking,2019-08,ABE + HORDE,66.8,100.0,3.2,0.3765,66.8,0.7,R\\-at\\-1
27,1,Image Retrieval,SOP - Image Retrieval benchmarking,2016-11,HDC,69.5,82.54,69.5,100,84.2,0.73,R\\-at\\-1
28,1,Image Retrieval,SOP - Image Retrieval benchmarking,2018-01,A-BIER,74.2,88.12,4.7,0.3197,84.2,0.78,R\\-at\\-1
29,1,Image Retrieval,SOP - Image Retrieval benchmarking,2018-04,ABE-8,76.3,90.62,2.1,0.1429,84.2,0.8,R\\-at\\-1
30,1,Image Retrieval,SOP - Image Retrieval benchmarking,2018-11,NormSoftmax2048 (ResNet-50),79.5,94.42,3.2,0.2177,84.2,0.84,R\\-at\\-1
31,1,Image Retrieval,SOP - Image Retrieval benchmarking,2019-03,CGD (SG/GS),84.2,100.0,4.7,0.3197,84.2,0.89,R\\-at\\-1
32,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,44.15,79.34,44.15,100,55.65,0.47,R\\-at\\-1
33,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),47.55,85.44,3.4,0.2957,55.65,0.5,R\\-at\\-1
34,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,49.63,89.18,2.08,0.1809,55.65,0.52,R\\-at\\-1
35,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,50.88,91.43,1.25,0.1087,55.65,0.54,R\\-at\\-1
36,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),55.65,100.0,4.77,0.4148,55.65,0.59,R\\-at\\-1
37,1,Metric Learning,CARS196 - Metric Learning benchmarking,2017-06,ResNet-50 + Margin,79.6,90.45,79.6,100,88.0,0.84,R\\-at\\-1
38,1,Metric Learning,CARS196 - Metric Learning benchmarking,2018-04,ABE-8-512,85.2,96.82,5.6,0.6667,88.0,0.9,R\\-at\\-1
39,1,Metric Learning,CARS196 - Metric Learning benchmarking,2019-08,ABE + HORDE,88.0,100.0,2.8,0.3333,88.0,0.93,R\\-at\\-1
40,1,Image Retrieval,CARS196 - Image Retrieval benchmarking,2017-06,Margin,86.9,91.67,86.9,100,94.8,0.92,R\\-at\\-1
41,1,Image Retrieval,CARS196 - Image Retrieval benchmarking,2018-11,NormSoftmax2048 (ResNet-50),89.3,94.2,2.4,0.3038,94.8,0.94,R\\-at\\-1
42,1,Image Retrieval,CARS196 - Image Retrieval benchmarking,2019-03,CGD (MG/SG),94.8,100.0,5.5,0.6962,94.8,1.0,R\\-at\\-1
43,1,Sketch-Based Image Retrieval,Shoes - Sketch-Based Image Retrieval benchmarking,2017-09,EdgeMAC + whitening,54.8,100.0,54.8,100,54.8,0.58,R\\-at\\-1
44,1,Image Retrieval,In-Shop - Image Retrieval benchmarking,2018-04,ABE-8,87.3,94.99,87.3,100,91.9,0.92,R\\-at\\-1
45,1,Image Retrieval,In-Shop - Image Retrieval benchmarking,2018-11,NormSoftmax2048 (ResNet-50),89.4,97.28,2.1,0.4565,91.9,0.94,R\\-at\\-1
46,1,Image Retrieval,In-Shop - Image Retrieval benchmarking,2019-03,CGD (SG/GS),91.9,100.0,2.5,0.5435,91.9,0.97,R\\-at\\-1
47,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2018-05,BAN (Bottom-Up detector),69.69,97.7,69.69,100,71.33,0.74,R\\-at\\-1
48,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2019-08,VisualBERT,71.33,100.0,1.64,1.0,71.33,0.75,R\\-at\\-1
49,1,Image Retrieval,CUB-200-2011 - Image Retrieval benchmarking,2018-11,NormSoftmax2048 (ResNet-50),65.3,82.45,65.3,100,79.2,0.69,R\\-at\\-1
50,1,Image Retrieval,CUB-200-2011 - Image Retrieval benchmarking,2019-03,CGD (MG/SG),79.2,100.0,13.9,1.0,79.2,0.84,R\\-at\\-1
51,1,Phrase Grounding,Flickr30k Entities Dev - Phrase Grounding benchmarking,2019-08,VisualBERT,70.4,100.0,70.4,100,70.4,0.74,R\\-at\\-1
0,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2014-12,Dual-Path (ResNet),41.2,81.75,41.2,100,50.4,0.61,Image\\-to\\-text\\ R\\-at\\-1
1,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2017-12,SCO (ResNet),42.8,84.92,1.6,0.1739,50.4,0.64,Image\\-to\\-text\\ R\\-at\\-1
2,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2018-03,SCAN,50.4,100.0,7.6,0.8261,50.4,0.75,Image\\-to\\-text\\ R\\-at\\-1
3,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2017-12,SCO\n  (ResNet),55.5,82.34,55.5,100,67.4,0.82,Image\\-to\\-text\\ R\\-at\\-1
4,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2018-03,SCAN,67.4,100.0,11.9,1.0,67.4,1.0,Image\\-to\\-text\\ R\\-at\\-1
0,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2014-12,Dual-Path (ResNet),81.1,90.11,81.1,100,90.0,0.85,Image\\-to\\-text\\ R\\-at\\-10
1,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2017-12,SCO (ResNet),83.0,92.22,1.9,0.2135,90.0,0.87,Image\\-to\\-text\\ R\\-at\\-10
2,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2018-03,SCAN,90.0,100.0,7.0,0.7865,90.0,0.94,Image\\-to\\-text\\ R\\-at\\-10
3,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2017-12,SCO\n  (ResNet),89.3,93.22,89.3,100,95.8,0.93,Image\\-to\\-text\\ R\\-at\\-10
4,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2018-03,SCAN,95.8,100.0,6.5,1.0,95.8,1.0,Image\\-to\\-text\\ R\\-at\\-10
0,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2014-12,Dual-Path (ResNet),70.5,85.77,70.5,100,82.2,0.78,Image\\-to\\-text\\ R\\-at\\-5
1,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2017-12,SCO (ResNet),72.3,87.96,1.8,0.1538,82.2,0.8,Image\\-to\\-text\\ R\\-at\\-5
2,1,Cross-Modal Retrieval,COCO 2014 - Cross-Modal Retrieval benchmarking,2018-03,SCAN,82.2,100.0,9.9,0.8462,82.2,0.91,Image\\-to\\-text\\ R\\-at\\-5
3,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2017-12,SCO\n  (ResNet),82.0,90.81,82.0,100,90.3,0.91,Image\\-to\\-text\\ R\\-at\\-5
4,1,Cross-Modal Retrieval,Flickr30k - Cross-Modal Retrieval benchmarking,2018-03,SCAN,90.3,100.0,8.3,1.0,90.3,1.0,Image\\-to\\-text\\ R\\-at\\-5
0,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2014-12,DeepLab,203.0,89.43,203.0,100,227.0,0.05,Time\\ \\(ms\\)
1,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2015-11,SegNet,217.0,95.59,14.0,0.5833,227.0,0.05,Time\\ \\(ms\\)
2,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2015-11,Dilation10,227.0,100.0,10.0,0.4167,227.0,0.06,Time\\ \\(ms\\)
3,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2014-12,DeepLab,4000.0,100.0,4000.0,100,4000.0,1.0,Time\\ \\(ms\\)
0,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2014-12,DeepLab,63.1,74.94,63.1,100,84.2,0.75,Mean\\ IoU\\ \\(class\\)
1,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2015-04,Context,71.6,85.04,8.5,0.4028,84.2,0.85,Mean\\ IoU\\ \\(class\\)
2,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2016-05,LRR-4x,71.8,85.27,0.2,0.0095,84.2,0.85,Mean\\ IoU\\ \\(class\\)
3,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2016-11,RefineNet (ResNet-101),73.6,87.41,1.8,0.0853,84.2,0.87,Mean\\ IoU\\ \\(class\\)
4,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2016-11,ResNet-38,78.4,93.11,4.8,0.2275,84.2,0.93,Mean\\ IoU\\ \\(class\\)
5,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2017-06,"DeepLabv3 (ResNet-101, coarse)",81.3,96.56,2.9,0.1374,84.2,0.97,Mean\\ IoU\\ \\(class\\)
6,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2017-12,Mapillary,82.0,97.39,0.7,0.0332,84.2,0.97,Mean\\ IoU\\ \\(class\\)
7,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2018-02,DeepLabv3+ (Xception-JFT),82.1,97.51,0.1,0.0047,84.2,0.98,Mean\\ IoU\\ \\(class\\)
8,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2018-08,SSMA,82.3,97.74,0.2,0.0095,84.2,0.98,Mean\\ IoU\\ \\(class\\)
9,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2018-09,Dense Prediction Cell,82.7,98.22,0.4,0.019,84.2,0.98,Mean\\ IoU\\ \\(class\\)
10,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2018-12,DeepLabV3Plus + SDCNetAug,83.5,99.17,0.8,0.0379,84.2,0.99,Mean\\ IoU\\ \\(class\\)
11,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2019-11,Panoptic-DeepLab,84.2,100.0,0.7,0.0332,84.2,1.0,Mean\\ IoU\\ \\(class\\)
12,1,Semantic Segmentation,KITTI Semantic Segmentation - Semantic Segmentation benchmarking,2018-12,DeepLabV3Plus + SDCNetAug,72.8,100.0,72.8,100,72.8,0.86,Mean\\ IoU\\ \\(class\\)
0,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2014-12,DeepLab,4.9,3.94,4.9,100,124.5,0.03,Frame\\ \\(fps\\)
1,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet,5.4,4.34,0.5,0.0042,124.5,0.03,Frame\\ \\(fps\\)
2,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2017-04,ICNet,27.8,22.33,22.4,0.1873,124.5,0.18,Frame\\ \\(fps\\)
3,1,Real-Time Semantic Segmentation,CamVid - Real-Time Semantic Segmentation benchmarking,2020-04,BiSeNet V2,124.5,100.0,96.7,0.8085,124.5,0.8,Frame\\ \\(fps\\)
4,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2014-12,DeepLab,0.25,0.16,0.25,100,156.0,0.0,Frame\\ \\(fps\\)
5,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2015-02,CRF-RNN,1.4,0.9,1.15,0.0074,156.0,0.01,Frame\\ \\(fps\\)
6,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2015-11,SegNet,16.7,10.71,15.3,0.0982,156.0,0.11,Frame\\ \\(fps\\)
7,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2016-06,ENet,76.9,49.29,60.2,0.3865,156.0,0.49,Frame\\ \\(fps\\)
8,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2018-08,BiSeNet(Xception39),105.8,67.82,28.9,0.1856,156.0,0.68,Frame\\ \\(fps\\)
9,1,Real-Time Semantic Segmentation,Cityscapes test - Real-Time Semantic Segmentation benchmarking,2020-04,BiSeNet V2,156.0,100.0,50.2,0.3223,156.0,1.0,Frame\\ \\(fps\\)
10,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2020-01,BlendMask-512 (DLA_34),33.3,100.0,33.3,100,33.3,0.21,Frame\\ \\(fps\\)
0,1,Video Super-Resolution,Xiph HD - 4x upscaling - Video Super-Resolution benchmarking,2014-12,SRCNN,31.47,99.37,31.47,100,31.67,0.83,Average\\ PSNR
1,1,Video Super-Resolution,Xiph HD - 4x upscaling - Video Super-Resolution benchmarking,2016-09,ESPCN,31.67,100.0,0.2,1.0,31.67,0.84,Average\\ PSNR
2,1,Video Super-Resolution,Ultra Video Group HD - 4x upscaling - Video Super-Resolution benchmarking,2014-12,SRCNN,37.52,98.97,37.52,100,37.91,0.99,Average\\ PSNR
3,1,Video Super-Resolution,Ultra Video Group HD - 4x upscaling - Video Super-Resolution benchmarking,2016-09,ESPCN,37.91,100.0,0.39,1.0,37.91,1.0,Average\\ PSNR
4,1,Deblurring,REDS - Deblurring benchmarking,2017-11,DeblurGAN,24.09,69.22,24.09,100,34.8,0.64,Average\\ PSNR
5,1,Deblurring,REDS - Deblurring benchmarking,2019-05,EDVR_Deblur,34.8,100.0,10.71,1.0,34.8,0.92,Average\\ PSNR
6,1,Low-Light Image Enhancement,LOL - Low-Light Image Enhancement benchmarking,2019-05,KinD,20.8665,100.0,20.8665,100,20.8665,0.55,Average\\ PSNR
0,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2014-12,SRCNN,24.68,90.24,24.68,100,27.35,0.61,PSNR
1,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2016-09,ESPCN,25.06,91.63,0.38,0.1423,27.35,0.62,PSNR
2,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2016-11,VESPCN,25.35,92.69,0.29,0.1086,27.35,0.63,PSNR
3,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2017-04,DRDVSR,25.88,94.63,0.53,0.1985,27.35,0.64,PSNR
4,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2018-01,FRVSR,26.69,97.59,0.81,0.3034,27.35,0.66,PSNR
5,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2018-06,VSR-DUF,27.31,99.85,0.62,0.2322,27.35,0.68,PSNR
6,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2019-05,EDVR,27.35,100.0,0.04,0.015,27.35,0.68,PSNR
7,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,24.52,83.74,24.52,100,29.28,0.61,PSNR
8,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,DnCNN-3,25.2,86.07,0.68,0.1429,29.28,0.62,PSNR
9,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2016-11,Manifold Simplification,26.42,90.23,1.22,0.2563,29.28,0.65,PSNR
10,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,26.64,90.98,0.22,0.0462,29.28,0.66,PSNR
11,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2018-04,ProSR,26.89,91.84,0.25,0.0525,29.28,0.67,PSNR
12,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,SRGAN + Residual-in-Residual Dense Block,27.03,92.32,0.14,0.0294,29.28,0.67,PSNR
13,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,SAN,27.23,93.0,0.2,0.042,29.28,0.67,PSNR
14,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,27.3,93.24,0.07,0.0147,29.28,0.68,PSNR
15,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,29.28,100.0,1.98,0.416,29.28,0.73,PSNR
16,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,27.4,80.35,27.4,100,34.1,0.68,PSNR
17,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2016-12,EnhanceNet,29.42,86.28,2.02,0.3015,34.1,0.73,PSNR
18,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,34.1,100.0,4.68,0.6985,34.1,0.84,PSNR
19,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,27.58,86.76,27.58,100,31.79,0.68,PSNR
20,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,31.02,97.58,3.44,0.8171,31.79,0.77,PSNR
21,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2018-07,RCAN,31.22,98.21,0.2,0.0475,31.79,0.77,PSNR
22,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,SRGAN + Residual-in-Residual Dense Block,31.66,99.59,0.44,0.1045,31.79,0.78,PSNR
23,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,31.78,99.97,0.12,0.0285,31.79,0.79,PSNR
24,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,31.79,100.0,0.01,0.0024,31.79,0.79,PSNR
25,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,23.12,84.32,23.12,100,27.42,0.57,PSNR
26,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2016-12,EnhanceNet,23.64,86.21,0.52,0.1209,27.42,0.59,PSNR
27,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,27.42,100.0,3.78,0.8791,27.42,0.68,PSNR
28,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,26.9,92.28,26.9,100,29.15,0.67,PSNR
29,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,IA,27.16,93.17,0.26,0.1156,29.15,0.67,PSNR
30,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,27.21,93.34,0.05,0.0222,29.15,0.67,PSNR
31,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2016-03,RED30,27.4,94.0,0.19,0.0844,29.15,0.68,PSNR
32,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,27.58,94.61,0.18,0.08,29.15,0.68,PSNR
33,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2016-11,Manifold Simplification,27.66,94.89,0.08,0.0356,29.15,0.69,PSNR
34,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,27.71,95.06,0.05,0.0222,29.15,0.69,PSNR
35,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2018-02,RDN,27.72,95.09,0.01,0.0044,29.15,0.69,PSNR
36,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2018-04,ProSR,27.79,95.33,0.07,0.0311,29.15,0.69,PSNR
37,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,SRGAN + Residual-in-Residual Dense Block,27.85,95.54,0.06,0.0267,29.15,0.69,PSNR
38,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,SAN,27.86,95.57,0.01,0.0044,29.15,0.69,PSNR
39,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,27.87,95.61,0.01,0.0044,29.15,0.69,PSNR
40,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,29.15,100.0,1.28,0.5689,29.15,0.72,PSNR
41,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,30.49,93.13,30.49,100,32.74,0.76,PSNR
42,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2015-08,TNRD,30.85,94.23,0.36,0.16,32.74,0.76,PSNR
43,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,IA,31.1,94.99,0.25,0.1111,32.74,0.77,PSNR
44,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,31.52,96.27,0.42,0.1867,32.74,0.78,PSNR
45,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,32.05,97.89,0.53,0.2356,32.74,0.79,PSNR
46,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2016-11,Manifold Simplification,32.23,98.44,0.18,0.08,32.74,0.8,PSNR
47,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,32.46,99.14,0.23,0.1022,32.74,0.8,PSNR
48,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2018-02,RDN,32.47,99.18,0.01,0.0044,32.74,0.8,PSNR
49,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2018-07,RCAN,32.63,99.66,0.16,0.0711,32.74,0.81,PSNR
50,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,SRGAN + Residual-in-Residual Dense Block,32.73,99.97,0.1,0.0444,32.74,0.81,PSNR
51,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2018-11,PFF,32.74,100.0,0.01,0.0044,32.74,0.81,PSNR
52,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,27.5,90.73,27.5,100,30.31,0.68,PSNR
53,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2015-08,TNRD,27.68,91.32,0.18,0.0641,30.31,0.69,PSNR
54,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,IA,27.88,91.98,0.2,0.0712,30.31,0.69,PSNR
55,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,28.02,92.44,0.14,0.0498,30.31,0.69,PSNR
56,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,DnCNN-3,28.04,92.51,0.02,0.0071,30.31,0.69,PSNR
57,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,28.49,94.0,0.45,0.1601,30.31,0.71,PSNR
58,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2016-11,Manifold Simplification,28.8,95.02,0.31,0.1103,30.31,0.71,PSNR
59,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2018-02,RDN,28.81,95.05,0.01,0.0036,30.31,0.71,PSNR
60,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2018-03,D-DBPN,28.82,95.08,0.01,0.0036,30.31,0.71,PSNR
61,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2018-04,ProSR,28.94,95.48,0.12,0.0427,30.31,0.72,PSNR
62,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,SRGAN + Residual-in-Residual Dense Block,28.99,95.65,0.05,0.0178,30.31,0.72,PSNR
63,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,SAN,29.05,95.84,0.06,0.0214,30.31,0.72,PSNR
64,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,30.31,100.0,1.26,0.4484,30.31,0.75,PSNR
65,1,Denoising,Darmstadt Noise Dataset - Denoising benchmarking,2015-08,TNRD,33.65,87.63,33.65,100,38.4,0.83,PSNR
66,1,Denoising,Darmstadt Noise Dataset - Denoising benchmarking,2017-05,MCWNNM,37.38,97.34,3.73,0.7853,38.4,0.93,PSNR
67,1,Denoising,Darmstadt Noise Dataset - Denoising benchmarking,2018-07,TWSC,37.93,98.78,0.55,0.1158,38.4,0.94,PSNR
68,1,Denoising,Darmstadt Noise Dataset - Denoising benchmarking,2019-04,Pixel-shuffling Downsampling,38.4,100.0,0.47,0.0989,38.4,0.95,PSNR
69,1,Grayscale Image Denoising,BSD68 sigma15 - Grayscale Image Denoising benchmarking,2015-08,TNRD,31.42,98.56,31.42,100,31.88,0.78,PSNR
70,1,Grayscale Image Denoising,BSD68 sigma15 - Grayscale Image Denoising benchmarking,2017-04,Deep CNN Denoiser,31.63,99.22,0.21,0.4565,31.88,0.78,PSNR
71,1,Grayscale Image Denoising,BSD68 sigma15 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,31.86,99.94,0.23,0.5,31.88,0.79,PSNR
72,1,Grayscale Image Denoising,BSD68 sigma15 - Grayscale Image Denoising benchmarking,2018-06,NLRN,31.88,100.0,0.02,0.0435,31.88,0.79,PSNR
73,1,Grayscale Image Denoising,BSD68 sigma25 - Grayscale Image Denoising benchmarking,2015-08,TNRD,28.92,98.33,28.92,100,29.41,0.72,PSNR
74,1,Grayscale Image Denoising,BSD68 sigma25 - Grayscale Image Denoising benchmarking,2016-08,DnCNN,29.23,99.39,0.31,0.6327,29.41,0.72,PSNR
75,1,Grayscale Image Denoising,BSD68 sigma25 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,29.41,100.0,0.18,0.3673,29.41,0.73,PSNR
76,1,Grayscale Image Denoising,Urban100 sigma15 - Grayscale Image Denoising benchmarking,2015-08,TNRD,31.98,95.61,31.98,100,33.45,0.79,PSNR
77,1,Grayscale Image Denoising,Urban100 sigma15 - Grayscale Image Denoising benchmarking,2016-08,DnCNN,32.67,97.67,0.69,0.4694,33.45,0.81,PSNR
78,1,Grayscale Image Denoising,Urban100 sigma15 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,33.17,99.16,0.5,0.3401,33.45,0.82,PSNR
79,1,Grayscale Image Denoising,Urban100 sigma15 - Grayscale Image Denoising benchmarking,2018-06,NLRN,33.45,100.0,0.28,0.1905,33.45,0.83,PSNR
80,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2015-11,VDSR [[Kim et al.2016a]],30.76,87.29,30.76,100,35.24,0.76,PSNR
81,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,32.3,91.66,1.54,0.3438,35.24,0.8,PSNR
82,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,32.62,92.57,0.32,0.0714,35.24,0.81,PSNR
83,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,33.12,93.98,0.5,0.1116,35.24,0.82,PSNR
84,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,33.54,95.18,0.42,0.0937,35.24,0.83,PSNR
85,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,35.24,100.0,1.7,0.3795,35.24,0.87,PSNR
86,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN [[Kim et al.2016b]],33.04,92.78,33.04,100,35.61,0.82,PSNR
87,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2018-03,CARN-M [[Ahn et al.2018]],33.26,93.4,0.22,0.0856,35.61,0.82,PSNR
88,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2018-03,CARN [[Ahn et al.2018]],33.52,94.13,0.26,0.1012,35.61,0.83,PSNR
89,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,33.7,94.64,0.18,0.07,35.61,0.83,PSNR
90,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,33.82,94.97,0.12,0.0467,35.61,0.84,PSNR
91,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,34.43,96.69,0.61,0.2374,35.61,0.85,PSNR
92,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,35.61,100.0,1.18,0.4591,35.61,0.88,PSNR
93,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN [[Kim et al.2016b]],31.85,94.15,31.85,100,33.83,0.79,PSNR
94,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,31.99,94.56,0.14,0.0707,33.83,0.79,PSNR
95,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2018-03,CARN [[Ahn et al.2018]],32.09,94.86,0.1,0.0505,33.83,0.8,PSNR
96,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,32.23,95.27,0.14,0.0707,33.83,0.8,PSNR
97,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,32.29,95.45,0.06,0.0303,33.83,0.8,PSNR
98,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,32.33,95.57,0.04,0.0202,33.83,0.8,PSNR
99,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,32.47,95.98,0.14,0.0707,33.83,0.8,PSNR
100,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,33.83,100.0,1.36,0.6869,33.83,0.84,PSNR
101,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2015-11,VDSR [[Kim et al.2016a]],37.53,96.38,37.53,100,38.94,0.93,PSNR
102,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN [[Kim et al.2016b]],37.63,96.64,0.1,0.0709,38.94,0.93,PSNR
103,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,37.66,96.71,0.03,0.0213,38.94,0.93,PSNR
104,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2018-03,CARN [[Ahn et al.2018]],37.76,96.97,0.1,0.0709,38.94,0.94,PSNR
105,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,37.91,97.35,0.15,0.1064,38.94,0.94,PSNR
106,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,38.11,97.87,0.2,0.1418,38.94,0.94,PSNR
107,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,38.13,97.92,0.02,0.0142,38.94,0.94,PSNR
108,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,38.34,98.46,0.21,0.1489,38.94,0.95,PSNR
109,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,38.94,100.0,0.6,0.4255,38.94,0.96,PSNR
110,1,Image Super-Resolution,VggFace2 - 8x upscaling - Image Super-Resolution benchmarking,2015-11,VDSR,22.5,87.99,22.5,100,25.57,0.56,PSNR
111,1,Image Super-Resolution,VggFace2 - 8x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,23.01,89.99,0.51,0.1661,25.57,0.57,PSNR
112,1,Image Super-Resolution,VggFace2 - 8x upscaling - Image Super-Resolution benchmarking,2018-04,GFRNet,24.1,94.25,1.09,0.355,25.57,0.6,PSNR
113,1,Image Super-Resolution,VggFace2 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,Full-GWAInet,25.57,100.0,1.47,0.4788,25.57,0.63,PSNR
114,1,Image Super-Resolution,WebFace - 8x upscaling - Image Super-Resolution benchmarking,2015-11,VDSR,23.65,86.92,23.65,100,27.21,0.59,PSNR
115,1,Image Super-Resolution,WebFace - 8x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,24.49,90.0,0.84,0.236,27.21,0.61,PSNR
116,1,Image Super-Resolution,WebFace - 8x upscaling - Image Super-Resolution benchmarking,2018-04,GFRNet,27.21,100.0,2.72,0.764,27.21,0.67,PSNR
117,1,Image Super-Resolution,Set14 - 3x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,29.61,96.14,29.61,100,30.8,0.73,PSNR
118,1,Image Super-Resolution,Set14 - 3x upscaling - Image Super-Resolution benchmarking,2016-08,DnCNN-3,29.81,96.79,0.2,0.1681,30.8,0.74,PSNR
119,1,Image Super-Resolution,Set14 - 3x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,30.16,97.92,0.35,0.2941,30.8,0.75,PSNR
120,1,Image Super-Resolution,Set14 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,30.8,100.0,0.64,0.5378,30.8,0.76,PSNR
121,1,Image Super-Resolution,BSD100 - 3x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,28.93,98.4,28.93,100,29.4,0.72,PSNR
122,1,Image Super-Resolution,BSD100 - 3x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,29.12,99.05,0.19,0.4043,29.4,0.72,PSNR
123,1,Image Super-Resolution,BSD100 - 3x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,29.24,99.46,0.12,0.2553,29.4,0.72,PSNR
124,1,Image Super-Resolution,BSD100 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,29.4,100.0,0.16,0.3404,29.4,0.73,PSNR
125,1,Grayscale Image Denoising,BSD200 sigma70 - Grayscale Image Denoising benchmarking,2016-06,RED30,24.37,78.18,24.37,100,31.17,0.6,PSNR
126,1,Grayscale Image Denoising,BSD200 sigma70 - Grayscale Image Denoising benchmarking,2018-06,NLRN-MV,24.62,78.99,0.25,0.0368,31.17,0.61,PSNR
127,1,Grayscale Image Denoising,BSD200 sigma70 - Grayscale Image Denoising benchmarking,2019-10,RC-Net,31.17,100.0,6.55,0.9632,31.17,0.77,PSNR
128,1,Grayscale Image Denoising,BSD200 sigma10 - Grayscale Image Denoising benchmarking,2016-06,RED30,33.63,92.49,33.63,100,36.36,0.83,PSNR
129,1,Grayscale Image Denoising,BSD200 sigma10 - Grayscale Image Denoising benchmarking,2019-10,RC-Net,36.36,100.0,2.73,1.0,36.36,0.9,PSNR
130,1,Grayscale Image Denoising,BSD200 sigma50 - Grayscale Image Denoising benchmarking,2016-06,RED30,25.75,79.28,25.75,100,32.48,0.64,PSNR
131,1,Grayscale Image Denoising,BSD200 sigma50 - Grayscale Image Denoising benchmarking,2018-06,NLRN-MV,25.97,79.96,0.22,0.0327,32.48,0.64,PSNR
132,1,Grayscale Image Denoising,BSD200 sigma50 - Grayscale Image Denoising benchmarking,2019-10,RC-Net,32.48,100.0,6.51,0.9673,32.48,0.8,PSNR
133,1,Grayscale Image Denoising,BSD200 sigma30 - Grayscale Image Denoising benchmarking,2016-06,RED30,27.95,83.26,27.95,100,33.57,0.69,PSNR
134,1,Grayscale Image Denoising,BSD200 sigma30 - Grayscale Image Denoising benchmarking,2018-06,NLRN-MV,28.2,84.0,0.25,0.0445,33.57,0.7,PSNR
135,1,Grayscale Image Denoising,BSD200 sigma30 - Grayscale Image Denoising benchmarking,2019-10,RC-Net,33.57,100.0,5.37,0.9555,33.57,0.83,PSNR
136,1,Image Super-Resolution,Set5 - 3x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,33.82,97.02,33.82,100,34.86,0.84,PSNR
137,1,Image Super-Resolution,Set5 - 3x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,34.17,98.02,0.35,0.3365,34.86,0.85,PSNR
138,1,Image Super-Resolution,Set5 - 3x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,34.7,99.54,0.53,0.5096,34.86,0.86,PSNR
139,1,Image Super-Resolution,Set5 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,34.86,100.0,0.16,0.1538,34.86,0.86,PSNR
140,1,Image Super-Resolution,Urban100 - 3x upscaling - Image Super-Resolution benchmarking,2016-08,DnCNN-3,27.15,92.44,27.15,100,29.37,0.67,PSNR
141,1,Image Super-Resolution,Urban100 - 3x upscaling - Image Super-Resolution benchmarking,2018-05,MWCNN,28.13,95.78,0.98,0.4414,29.37,0.7,PSNR
142,1,Image Super-Resolution,Urban100 - 3x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,28.73,97.82,0.6,0.2703,29.37,0.71,PSNR
143,1,Image Super-Resolution,Urban100 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,29.37,100.0,0.64,0.2883,29.37,0.73,PSNR
144,1,Color Image Denoising,BSD68 sigma15 - Color Image Denoising benchmarking,2016-08,DnCNN-3,31.46,92.91,31.46,100,33.86,0.78,PSNR
145,1,Color Image Denoising,BSD68 sigma15 - Color Image Denoising benchmarking,2017-04,Deep CNN Denoiser,33.86,100.0,2.4,1.0,33.86,0.84,PSNR
146,1,Color Image Denoising,CBSD68 sigma35 - Color Image Denoising benchmarking,2016-08,DnCNN-B*,28.74,97.16,28.74,100,29.58,0.71,PSNR
147,1,Color Image Denoising,CBSD68 sigma35 - Color Image Denoising benchmarking,2017-10,FFDNet,29.58,100.0,0.84,1.0,29.58,0.73,PSNR
148,1,Color Image Denoising,BSD68 sigma25 - Color Image Denoising benchmarking,2016-08,DnCNN-3,29.02,93.13,29.02,100,31.16,0.72,PSNR
149,1,Color Image Denoising,BSD68 sigma25 - Color Image Denoising benchmarking,2017-04,Deep CNN Denoiser,31.16,100.0,2.14,1.0,31.16,0.77,PSNR
150,1,Grayscale Image Denoising,Urban100 sigma25 - Grayscale Image Denoising benchmarking,2016-08,DnCNN,29.97,96.86,29.97,100,30.94,0.74,PSNR
151,1,Grayscale Image Denoising,Urban100 sigma25 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,30.66,99.1,0.69,0.7113,30.94,0.76,PSNR
152,1,Grayscale Image Denoising,Urban100 sigma25 - Grayscale Image Denoising benchmarking,2018-06,NLRN,30.94,100.0,0.28,0.2887,30.94,0.77,PSNR
153,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,27.494,91.08,27.494,100,30.188,0.68,PSNR
154,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,30.188,100.0,2.694,1.0,30.188,0.75,PSNR
155,1,Color Image Denoising,CBSD68 sigma50 - Color Image Denoising benchmarking,2016-11,DnCNN,28.01,98.84,28.01,100,28.34,0.69,PSNR
156,1,Color Image Denoising,CBSD68 sigma50 - Color Image Denoising benchmarking,2018-02,Residual Dense Network +,28.34,100.0,0.33,1.0,28.34,0.7,PSNR
157,1,Multimodal Unsupervised Image-To-Image Translation,EPFL NIR-VIS - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,15.33,66.33,15.33,100,23.11,0.38,PSNR
158,1,Multimodal Unsupervised Image-To-Image Translation,EPFL NIR-VIS - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-11,In2I,23.11,100.0,7.78,1.0,23.11,0.57,PSNR
159,1,Unsupervised Image-To-Image Translation,Freiburg Forest Dataset - Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,9.42,43.51,9.42,100,21.65,0.23,PSNR
160,1,Unsupervised Image-To-Image Translation,Freiburg Forest Dataset - Unsupervised Image-To-Image Translation benchmarking,2017-11,In2I,21.65,100.0,12.23,1.0,21.65,0.54,PSNR
161,1,Color Image Denoising,BSD68 sigma5 - Color Image Denoising benchmarking,2017-04,Deep CNN Denoiser,40.36,100.0,40.36,100,40.36,1.0,PSNR
162,1,Color Image Denoising,BSD68 sigma35 - Color Image Denoising benchmarking,2017-04,Deep CNN Denoiser,29.5,100.0,29.5,100,29.5,0.73,PSNR
163,1,Grayscale Image Denoising,BSD68 sigma50 - Grayscale Image Denoising benchmarking,2017-04,Deep CNN Denoiser,26.19,98.72,26.19,100,26.53,0.65,PSNR
164,1,Grayscale Image Denoising,BSD68 sigma50 - Grayscale Image Denoising benchmarking,2017-10,FFDNet,26.29,99.1,0.1,0.2941,26.53,0.65,PSNR
165,1,Grayscale Image Denoising,BSD68 sigma50 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,26.53,100.0,0.24,0.7059,26.53,0.66,PSNR
166,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2017-05,PG2,28.2403,86.6,28.2403,100,32.6091,0.7,PSNR
167,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2017-12,DPIG,30.6487,93.99,2.4084,0.5513,32.6091,0.76,PSNR
168,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,32.6091,100.0,1.9604,0.4487,32.6091,0.81,PSNR
169,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-05,PG2,26.5138,94.78,26.5138,100,27.9749,0.66,PSNR
170,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-07,SAMG,26.9545,96.35,0.4407,0.3016,27.9749,0.67,PSNR
171,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-12,PoseGAN,27.3014,97.59,0.3469,0.2374,27.9749,0.68,PSNR
172,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,27.9749,100.0,0.6735,0.461,27.9749,0.69,PSNR
173,1,Video Frame Interpolation,Vimeo90k - Video Frame Interpolation benchmarking,2017-08,SepConv-L1,33.8,93.63,33.8,100,36.1,0.84,PSNR
174,1,Video Frame Interpolation,Vimeo90k - Video Frame Interpolation benchmarking,2018-10,MEMC-Net*,34.4,95.29,0.6,0.2609,36.1,0.85,PSNR
175,1,Video Frame Interpolation,Vimeo90k - Video Frame Interpolation benchmarking,2019-04,DAIN,34.71,96.15,0.31,0.1348,36.1,0.86,PSNR
176,1,Video Frame Interpolation,Vimeo90k - Video Frame Interpolation benchmarking,2020-03,SoftSplat,36.1,100.0,1.39,0.6043,36.1,0.89,PSNR
177,1,Color Image Denoising,CBSD68 sigma25 - Color Image Denoising benchmarking,2017-10,FFDNet,31.21,100.0,31.21,100,31.21,0.77,PSNR
178,1,Color Image Denoising,Kodak25 sigma15 - Color Image Denoising benchmarking,2017-10,FFDNet,34.63,100.0,34.63,100,34.63,0.86,PSNR
179,1,Color Image Denoising,Kodak25 sigma50 - Color Image Denoising benchmarking,2017-10,FFDNet,28.98,100.0,28.98,100,28.98,0.72,PSNR
180,1,Color Image Denoising,Kodak25 sigma25 - Color Image Denoising benchmarking,2017-10,FFDNet,32.13,100.0,32.13,100,32.13,0.8,PSNR
181,1,Color Image Denoising,McMaster sigma25 - Color Image Denoising benchmarking,2017-10,FFDNet,32.35,100.0,32.35,100,32.35,0.8,PSNR
182,1,Color Image Denoising,McMaster sigma35 - Color Image Denoising benchmarking,2017-10,FFDNet,30.81,100.0,30.81,100,30.81,0.76,PSNR
183,1,Color Image Denoising,McMaster sigma50 - Color Image Denoising benchmarking,2017-10,FFDNet,29.18,100.0,29.18,100,29.18,0.72,PSNR
184,1,Color Image Denoising,CBSD68 sigma75 - Color Image Denoising benchmarking,2017-10,FFDNet,26.24,99.58,26.24,100,26.35,0.65,PSNR
185,1,Color Image Denoising,CBSD68 sigma75 - Color Image Denoising benchmarking,2019-04,AdaFM-Net,26.35,100.0,0.11,1.0,26.35,0.65,PSNR
186,1,Color Image Denoising,McMaster sigma15 - Color Image Denoising benchmarking,2017-10,FFDNet,34.66,100.0,34.66,100,34.66,0.86,PSNR
187,1,Color Image Denoising,Kodak25 sigma35 - Color Image Denoising benchmarking,2017-10,FFDNet,30.57,100.0,30.57,100,30.57,0.76,PSNR
188,1,Grayscale Image Denoising,Clip300 sigma15 - Grayscale Image Denoising benchmarking,2017-10,FFDNet-Clip,31.68,100.0,31.68,100,31.68,0.78,PSNR
189,1,Grayscale Image Denoising,Clip300 sigma50 - Grayscale Image Denoising benchmarking,2017-10,FFDNet-Clip,26.25,100.0,26.25,100,26.25,0.65,PSNR
190,1,Color Image Denoising,Kodak25 sigma75 - Color Image Denoising benchmarking,2017-10,FFDNet,27.27,100.0,27.27,100,27.27,0.68,PSNR
191,1,Color Image Denoising,McMaster sigma75 - Color Image Denoising benchmarking,2017-10,FFDNet,27.33,100.0,27.33,100,27.33,0.68,PSNR
192,1,Color Image Denoising,CBSD68 sigma15 - Color Image Denoising benchmarking,2017-10,FFDNet,33.87,99.33,33.87,100,34.1,0.84,PSNR
193,1,Color Image Denoising,CBSD68 sigma15 - Color Image Denoising benchmarking,2019-04,AdaFM-Net,34.1,100.0,0.23,1.0,34.1,0.84,PSNR
194,1,Grayscale Image Denoising,BSD68 sigma35 - Grayscale Image Denoising benchmarking,2017-10,FFDNet,27.73,100.0,27.73,100,27.73,0.69,PSNR
195,1,Grayscale Image Denoising,Set12 sigma15 - Grayscale Image Denoising benchmarking,2017-10,FFDNet,25.49,76.87,25.49,100,33.16,0.63,PSNR
196,1,Grayscale Image Denoising,Set12 sigma15 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,33.15,99.97,7.66,0.9987,33.16,0.82,PSNR
197,1,Grayscale Image Denoising,Set12 sigma15 - Grayscale Image Denoising benchmarking,2018-06,NLRN,33.16,100.0,0.01,0.0013,33.16,0.82,PSNR
198,1,Grayscale Image Denoising,Clip300 sigma35 - Grayscale Image Denoising benchmarking,2017-10,FFDNet-Clip,27.75,100.0,27.75,100,27.75,0.69,PSNR
199,1,Grayscale Image Denoising,Clip300 sigma25 - Grayscale Image Denoising benchmarking,2017-10,FFDNet-Clip,29.25,100.0,29.25,100,29.25,0.72,PSNR
200,1,Grayscale Image Denoising,Clip300 sigma60 - Grayscale Image Denoising benchmarking,2017-10,FFDNet-Clip,25.51,100.0,25.51,100,25.51,0.63,PSNR
201,1,Grayscale Image Denoising,BSD68 sigma75 - Grayscale Image Denoising benchmarking,2017-10,FFDNet,24.79,100.0,24.79,100,24.79,0.61,PSNR
202,1,Grayscale Image Denoising,Set12 sigma50 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,27.74,100.0,27.74,100,27.74,0.69,PSNR
203,1,Grayscale Image Denoising,Set12 sigma25 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,30.79,100.0,30.79,100,30.79,0.76,PSNR
204,1,Grayscale Image Denoising,Urban100 sigma50 - Grayscale Image Denoising benchmarking,2018-05,MWCNN,27.42,99.75,27.42,100,27.49,0.68,PSNR
205,1,Grayscale Image Denoising,Urban100 sigma50 - Grayscale Image Denoising benchmarking,2018-06,NLRN,27.49,100.0,0.07,1.0,27.49,0.68,PSNR
206,1,Grayscale Image Denoising,Urban100 sigma70 - Grayscale Image Denoising benchmarking,2018-10,N3Net,25.15,100.0,25.15,100,25.15,0.62,PSNR
207,1,Grayscale Image Denoising,Set12 sigma70 - Grayscale Image Denoising benchmarking,2018-10,N3Net,25.9,100.0,25.9,100,25.9,0.64,PSNR
208,1,Grayscale Image Denoising,BSD68 sigma70 - Grayscale Image Denoising benchmarking,2018-10,N3Net,25.14,100.0,25.14,100,25.14,0.62,PSNR
209,1,Facial Inpainting,WebFace - Facial Inpainting benchmarking,2018-12,SymmFCNet (Full),27.22,100.0,27.22,100,27.22,0.67,PSNR
210,1,Facial Inpainting,VggFace2 - Facial Inpainting benchmarking,2018-12,SymmFCNet (Full),27.81,100.0,27.81,100,27.81,0.69,PSNR
211,1,Image Super-Resolution,Manga109 - 3x upscaling - Image Super-Resolution benchmarking,2019-02,LFFN-S,32.8,93.88,32.8,100,34.94,0.81,PSNR
212,1,Image Super-Resolution,Manga109 - 3x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,34.18,97.82,1.38,0.6449,34.94,0.85,PSNR
213,1,Image Super-Resolution,Manga109 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,34.94,100.0,0.76,0.3551,34.94,0.87,PSNR
214,1,Image Super-Resolution,Manga109 - 2x upscaling - Image Super-Resolution benchmarking,2019-02,LFFN-S,37.93,95.42,37.93,100,39.75,0.94,PSNR
215,1,Image Super-Resolution,Manga109 - 2x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,39.08,98.31,1.15,0.6319,39.75,0.97,PSNR
216,1,Image Super-Resolution,Manga109 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,39.3,98.87,0.22,0.1209,39.75,0.97,PSNR
217,1,Image Super-Resolution,Manga109 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,39.75,100.0,0.45,0.2473,39.75,0.98,PSNR
218,1,Image Super-Resolution,CUFED5 - 4x upscaling - Image Super-Resolution benchmarking,2019-03,SRNTT-l2,26.24,100.0,26.24,100,26.24,0.65,PSNR
219,1,Image Super-Resolution,Sun80 - 4x upscaling - Image Super-Resolution benchmarking,2019-03,SRNTT-l2,28.54,100.0,28.54,100,28.54,0.71,PSNR
220,1,Image Super-Resolution,KITTI 2015 - 4x upscaling - Image Super-Resolution benchmarking,2019-03,PASSRnet,25.43,100.0,25.43,100,25.43,0.63,PSNR
221,1,Image Super-Resolution,KITTI 2012 - 4x upscaling - Image Super-Resolution benchmarking,2019-03,PASSRnet,26.26,100.0,26.26,100,26.26,0.65,PSNR
222,1,Image Super-Resolution,KITTI 2012 - 2x upscaling - Image Super-Resolution benchmarking,2019-03,PASSRnet,30.65,100.0,30.65,100,30.65,0.76,PSNR
223,1,Image Super-Resolution,Middlebury - 4x upscaling - Image Super-Resolution benchmarking,2019-03,PASSRnet,28.63,100.0,28.63,100,28.63,0.71,PSNR
224,1,Image Super-Resolution,KITTI 2015 - 2x upscaling - Image Super-Resolution benchmarking,2019-03,PASSRnet,29.78,100.0,29.78,100,29.78,0.74,PSNR
225,1,Image Super-Resolution,Middlebury - 2x upscaling - Image Super-Resolution benchmarking,2019-03,PASSRnet,34.05,100.0,34.05,100,34.05,0.84,PSNR
226,1,Image Super-Resolution,Set5 - 8x upscaling - Image Super-Resolution benchmarking,2019-03,DeepRED,26.04,94.83,26.04,100,27.46,0.65,PSNR
227,1,Image Super-Resolution,Set5 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,27.17,98.94,1.13,0.7958,27.46,0.67,PSNR
228,1,Image Super-Resolution,Set5 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,27.46,100.0,0.29,0.2042,27.46,0.68,PSNR
229,1,Image Super-Resolution,Set14 - 8x upscaling - Image Super-Resolution benchmarking,2019-03,DeepRED,24.28,95.59,24.28,100,25.4,0.6,PSNR
230,1,Image Super-Resolution,Set14 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,24.96,98.27,0.68,0.6071,25.4,0.62,PSNR
231,1,Image Super-Resolution,Set14 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,25.4,100.0,0.44,0.3929,25.4,0.63,PSNR
232,1,Video Frame Interpolation,X4K1000FPS - Video Frame Interpolation benchmarking,2019-04,DAIN_f,27.52,100.0,27.52,100,27.52,0.68,PSNR
233,1,Video Frame Interpolation,UCF101 - Video Frame Interpolation benchmarking,2019-04,DAIN,34.99,98.87,34.99,100,35.39,0.87,PSNR
234,1,Video Frame Interpolation,UCF101 - Video Frame Interpolation benchmarking,2020-03,SoftSplat,35.39,100.0,0.4,1.0,35.39,0.88,PSNR
235,1,Video Denoising,Set8 sigma20 - Video Denoising benchmarking,2019-06,DVDnet,33.49,100.0,33.49,100,33.49,0.83,PSNR
236,1,Video Denoising,Set8 sigma50 - Video Denoising benchmarking,2019-06,DVDnet,29.56,100.0,29.56,100,29.56,0.73,PSNR
237,1,Video Denoising,DAVIS sigma50 - Video Denoising benchmarking,2019-06,DVDnet,31.85,100.0,31.85,100,31.85,0.79,PSNR
238,1,Video Denoising,DAVIS sigma10 - Video Denoising benchmarking,2019-06,DVDnet,38.13,97.84,38.13,100,38.97,0.94,PSNR
239,1,Video Denoising,DAVIS sigma10 - Video Denoising benchmarking,2019-07,FastDVDnet,38.97,100.0,0.84,1.0,38.97,0.97,PSNR
240,1,Video Denoising,Set8 sigma10 - Video Denoising benchmarking,2019-06,DVDnet,36.08,99.04,36.08,100,36.43,0.89,PSNR
241,1,Video Denoising,Set8 sigma10 - Video Denoising benchmarking,2019-07,FastDVDnet,36.43,100.0,0.35,1.0,36.43,0.9,PSNR
242,1,Video Denoising,DAVIS sigma40 - Video Denoising benchmarking,2019-06,DVDnet,32.86,100.0,32.86,100,32.86,0.81,PSNR
243,1,Video Denoising,Set8 sigma30 - Video Denoising benchmarking,2019-06,DVDnet,31.79,100.0,31.79,100,31.79,0.79,PSNR
244,1,Video Denoising,Set8 sigma40 - Video Denoising benchmarking,2019-06,DVDnet,30.55,100.0,30.55,100,30.55,0.76,PSNR
245,1,Video Denoising,DAVIS sigma20 - Video Denoising benchmarking,2019-06,DVDnet,35.7,99.55,35.7,100,35.86,0.88,PSNR
246,1,Video Denoising,DAVIS sigma20 - Video Denoising benchmarking,2019-07,FastDVDnet,35.86,100.0,0.16,1.0,35.86,0.89,PSNR
247,1,Video Denoising,DAVIS sigma30 - Video Denoising benchmarking,2019-06,DVDnet,34.08,100.0,34.08,100,34.08,0.84,PSNR
248,1,Image Super-Resolution,Manga109 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,25.24,98.79,25.24,100,25.55,0.63,PSNR
249,1,Image Super-Resolution,Manga109 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,25.55,100.0,0.31,1.0,25.55,0.63,PSNR
250,1,Image Super-Resolution,BSD100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,24.93,99.48,24.93,100,25.06,0.62,PSNR
251,1,Image Super-Resolution,BSD100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,25.06,100.0,0.13,1.0,25.06,0.62,PSNR
252,1,Image Super-Resolution,Urban100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,23.04,99.14,23.04,100,23.24,0.57,PSNR
253,1,Image Super-Resolution,Urban100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,23.24,100.0,0.2,1.0,23.24,0.58,PSNR
254,1,Image Super-Resolution,DIV2K val - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,38.26,100.0,38.26,100,38.26,0.95,PSNR
255,1,Image Super-Resolution,DIV2K val - 4x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,32.82,100.0,32.82,100,32.82,0.81,PSNR
256,1,Grayscale Image Denoising,Set12 sigma30 - Grayscale Image Denoising benchmarking,2019-08,Index Network,30.43,100.0,30.43,100,30.43,0.75,PSNR
257,1,Face Alignment,CelebA Aligned - Face Alignment benchmarking,2019-08,Progressive Face SR,22.66,100.0,22.66,100,22.66,0.56,PSNR
258,1,Image Super-Resolution,Celeb-HQ 4x upscaling - Image Super-Resolution benchmarking,2019-09,Edge-informed SR,28.23,100.0,28.23,100,28.23,0.7,PSNR
259,1,Image Super-Resolution,USR-248 - 4x upscaling - Image Super-Resolution benchmarking,2019-09,SRDRM-GAN,24.62,100.0,24.62,100,24.62,0.61,PSNR
260,1,Image Super-Resolution,DIV8K val - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,26.71,100.0,26.71,100,26.71,0.66,PSNR
261,1,Image Super-Resolution,BSD100 - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,22.72,100.0,22.72,100,22.72,0.56,PSNR
262,1,Image Super-Resolution,Urban100 - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,20.39,100.0,20.39,100,20.39,0.51,PSNR
263,1,Image Super-Resolution,Manga109 - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,21.25,100.0,21.25,100,21.25,0.53,PSNR
264,1,Image Super-Resolution,DIV2K val - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,24.38,100.0,24.38,100,24.38,0.6,PSNR
265,1,Image Dehazing,SOTS Indoor - Image Dehazing benchmarking,2019-11,FFA-Net,35.77,100.0,35.77,100,35.77,0.89,PSNR
266,1,Image Dehazing,SOTS Outdoor - Image Dehazing benchmarking,2019-11,FFA-Net,33.38,100.0,33.38,100,33.38,0.83,PSNR
267,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2019-12,LGGAN,22.5766,100.0,22.5766,100,22.5766,0.56,PSNR
268,1,Video Frame Interpolation,Middlebury - Video Frame Interpolation benchmarking,2020-03,SoftSplat,38.42,100.0,38.42,100,38.42,0.95,PSNR
0,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.688,84.31,0.688,100,0.816,0.01,SSIM
1,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,FSRCNN,0.709,86.89,0.021,0.1641,0.816,0.01,SSIM
2,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,0.816,100.0,0.107,0.8359,0.816,0.01,SSIM
3,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.8555,92.77,0.8555,100,0.9222,0.01,SSIM
4,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,0.9148,99.2,0.0593,0.8891,0.9222,0.01,SSIM
5,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2018-02,RDN,0.9151,99.23,0.0003,0.0045,0.9222,0.01,SSIM
6,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2018-07,RCAN,0.9173,99.47,0.0022,0.033,0.9222,0.01,SSIM
7,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,SRGAN + Residual-in-Residual Dense Block,0.9196,99.72,0.0023,0.0345,0.9222,0.01,SSIM
8,1,Image Super-Resolution,Manga109 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,SAN,0.9222,100.0,0.0026,0.039,0.9222,0.01,SSIM
9,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.801,88.41,0.801,100,0.906,0.01,SSIM
10,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,FSRCNN,0.804,88.74,0.003,0.0286,0.906,0.01,SSIM
11,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2016-12,EnhanceNet,0.832,91.83,0.028,0.2667,0.906,0.01,SSIM
12,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,0.906,100.0,0.074,0.7048,0.906,0.01,SSIM
13,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.8628,89.41,0.8628,100,0.965,0.01,SSIM
14,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,0.8938,92.62,0.031,0.3033,0.965,0.01,SSIM
15,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,0.9019,93.46,0.0081,0.0793,0.965,0.01,SSIM
16,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2018-11,PFF,0.9021,93.48,0.0002,0.002,0.965,0.01,SSIM
17,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.9278,96.15,0.0257,0.2515,0.965,0.01,SSIM
18,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2019-09,Edge-informed SR,0.965,100.0,0.0372,0.364,0.965,0.02,SSIM
19,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.7221,82.9,0.7221,100,0.8711,0.01,SSIM
20,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,DnCNN-3,0.7521,86.34,0.03,0.2013,0.8711,0.01,SSIM
21,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2016-11,Manifold Simplification,0.794,91.15,0.0419,0.2812,0.8711,0.01,SSIM
22,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,0.8033,92.22,0.0093,0.0624,0.8711,0.01,SSIM
23,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2018-07,RCAN,0.8087,92.84,0.0054,0.0362,0.8711,0.01,SSIM
24,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,SRGAN + Residual-in-Residual Dense Block,0.8153,93.59,0.0066,0.0443,0.8711,0.01,SSIM
25,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,SAN,0.8169,93.78,0.0016,0.0107,0.8711,0.01,SSIM
26,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.818,93.9,0.0011,0.0074,0.8711,0.01,SSIM
27,1,Image Super-Resolution,Urban100 - 4x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,0.8711,100.0,0.0531,0.3564,0.8711,0.01,SSIM
28,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.7513,84.04,0.7513,100,0.894,0.01,SSIM
29,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,0.8074,90.31,0.0561,0.3931,0.894,0.01,SSIM
30,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,0.8184,91.54,0.011,0.0771,0.894,0.01,SSIM
31,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,0.8382,93.76,0.0198,0.1388,0.894,0.01,SSIM
32,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2019-09,Edge-informed SR,0.894,100.0,0.0558,0.391,0.894,0.01,SSIM
33,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.7101,83.44,0.7101,100,0.851,0.01,SSIM
34,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,0.7493,88.05,0.0392,0.2782,0.851,0.01,SSIM
35,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,0.762,89.54,0.0127,0.0901,0.851,0.01,SSIM
36,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.8014,94.17,0.0394,0.2796,0.851,0.01,SSIM
37,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2019-09,Edge-informed SR,0.851,100.0,0.0496,0.352,0.851,0.01,SSIM
38,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2014-12,SRCNN,0.7158,86.03,0.7158,100,0.832,0.01,SSIM
39,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2016-09,ESPCN,0.7394,88.87,0.0236,0.2031,0.832,0.01,SSIM
40,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2016-11,VESPCN,0.7557,90.83,0.0163,0.1403,0.832,0.01,SSIM
41,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2017-04,DRDVSR,0.774,93.03,0.0183,0.1575,0.832,0.01,SSIM
42,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2018-01,FRVSR,0.822,98.8,0.048,0.4131,0.832,0.01,SSIM
43,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2018-06,VSR-DUF,0.832,100.0,0.01,0.0861,0.832,0.01,SSIM
44,1,Image Super-Resolution,BSD100 - 3x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,0.7994,94.12,0.7994,100,0.8493,0.01,SSIM
45,1,Image Super-Resolution,BSD100 - 3x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.8493,100.0,0.0499,1.0,0.8493,0.01,SSIM
46,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,0.8974,96.89,0.8974,100,0.9262,0.01,SSIM
47,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.9097,98.22,0.0123,0.4271,0.9262,0.01,SSIM
48,1,Image Super-Resolution,BSD100 - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,0.9262,100.0,0.0165,0.5729,0.9262,0.01,SSIM
49,1,Image Super-Resolution,Set5 - 3x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,0.923,97.98,0.923,100,0.942,0.01,SSIM
50,1,Image Super-Resolution,Set5 - 3x upscaling - Image Super-Resolution benchmarking,2019-02,LFFN-S,0.9233,98.01,0.0003,0.0158,0.942,0.01,SSIM
51,1,Image Super-Resolution,Set5 - 3x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.942,100.0,0.0187,0.9842,0.942,0.01,SSIM
52,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,0.9144,97.24,0.9144,100,0.9404,0.01,SSIM
53,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.921,97.94,0.0066,0.2538,0.9404,0.01,SSIM
54,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.9247,98.33,0.0037,0.1423,0.9404,0.01,SSIM
55,1,Image Super-Resolution,Set14 - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,0.9404,100.0,0.0157,0.6038,0.9404,0.01,SSIM
56,1,Grayscale Image Denoising,BSD200 sigma70 - Grayscale Image Denoising benchmarking,2016-06,RED30,0.6551,100.0,0.6551,100,0.6551,0.01,SSIM
57,1,Grayscale Image Denoising,BSD200 sigma30 - Grayscale Image Denoising benchmarking,2016-06,RED30,0.8019,100.0,0.8019,100,0.8019,0.01,SSIM
58,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,0.9599,99.39,0.9599,100,0.9658,0.02,SSIM
59,1,Image Super-Resolution,Set5 - 2x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.9658,100.0,0.0059,1.0,0.9658,0.02,SSIM
60,1,Image Super-Resolution,Set14 - 3x upscaling - Image Super-Resolution benchmarking,2016-06,RED30,0.8341,98.15,0.8341,100,0.8498,0.01,SSIM
61,1,Image Super-Resolution,Set14 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.8498,100.0,0.0157,1.0,0.8498,0.01,SSIM
62,1,Grayscale Image Denoising,BSD200 sigma10 - Grayscale Image Denoising benchmarking,2016-06,RED30,0.9319,100.0,0.9319,100,0.9319,0.01,SSIM
63,1,Grayscale Image Denoising,BSD200 sigma50 - Grayscale Image Denoising benchmarking,2016-06,RED30,0.7167,100.0,0.7167,100,0.7167,0.01,SSIM
64,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,0.735,88.88,0.735,100,0.827,0.01,SSIM
65,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,0.824,99.64,0.089,0.9674,0.827,0.01,SSIM
66,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2019-03,SRFBN,0.827,100.0,0.003,0.0326,0.827,0.01,SSIM
67,1,Cross-View Image-to-Image Translation,Ego2Top - Cross-View Image-to-Image Translation benchmarking,2016-11,Pix2pix,0.2213,36.74,0.2213,100,0.6024,0.0,SSIM
68,1,Cross-View Image-to-Image Translation,Ego2Top - Cross-View Image-to-Image Translation benchmarking,2018-03,X-Fork,0.274,45.48,0.0527,0.1383,0.6024,0.0,SSIM
69,1,Cross-View Image-to-Image Translation,Ego2Top - Cross-View Image-to-Image Translation benchmarking,2019-04,SelectionGAN,0.6024,100.0,0.3284,0.8617,0.6024,0.01,SSIM
70,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2016-11,Pix2pix,0.3923,73.7,0.3923,100,0.5323,0.01,SSIM
71,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2016-12,CrossNet,0.4147,77.91,0.0224,0.16,0.5323,0.01,SSIM
72,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2018-03,X-Seq,0.4231,79.49,0.0084,0.06,0.5323,0.01,SSIM
73,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2018-03,X-Fork,0.4356,81.83,0.0125,0.0893,0.5323,0.01,SSIM
74,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2019-04,SelectionGAN,0.5323,100.0,0.0967,0.6907,0.5323,0.01,SSIM
75,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2017-05,PG Squared,0.762,98.58,0.762,100,0.773,0.01,SSIM
76,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,0.773,100.0,0.011,1.0,0.773,0.01,SSIM
77,1,Face Sketch Synthesis,CUHK - Face Sketch Synthesis benchmarking,2017-10,PS2-MAN,61.56,97.28,61.56,100,63.28,0.97,SSIM
78,1,Face Sketch Synthesis,CUHK - Face Sketch Synthesis benchmarking,2018-12,Residual net + Pseudo Sketch Feature Loss + LSGAN,63.28,100.0,1.72,1.0,63.28,1.0,SSIM
79,1,Video Prediction,Human3.6M - Video Prediction benchmarking,2017-12,PredRNN,0.781,98.86,0.781,100,0.79,0.01,SSIM
80,1,Video Prediction,Human3.6M - Video Prediction benchmarking,2018-11,MIM,0.79,100.0,0.009,1.0,0.79,0.01,SSIM
81,1,Novel View Synthesis,ShapeNet Chair - Novel View Synthesis benchmarking,2018-10,Multi-view to Novel View,0.895,100.0,0.895,100,0.895,0.01,SSIM
82,1,Novel View Synthesis,ShapeNet Car - Novel View Synthesis benchmarking,2018-10,Multi-view to Novel View,0.923,100.0,0.923,100,0.923,0.01,SSIM
83,1,Novel View Synthesis,KITTI Novel View Synthesis - Novel View Synthesis benchmarking,2018-10,Multi-view to Novel View,0.626,100.0,0.626,100,0.626,0.01,SSIM
84,1,Novel View Synthesis,Synthia Novel View Synthesis - Novel View Synthesis benchmarking,2018-10,Multi-view to Novel View,0.697,100.0,0.697,100,0.697,0.01,SSIM
85,1,Grayscale Image Denoising,Urban100 sigma25 - Grayscale Image Denoising benchmarking,2018-10,N3Net,0.892,100.0,0.892,100,0.892,0.01,SSIM
86,1,Face Sketch Synthesis,CUFS - Face Sketch Synthesis benchmarking,2018-12,Residual net + Pseudo Sketch Feature Loss + LSGAN,54.63,100.0,54.63,100,54.63,0.86,SSIM
87,1,Face Sketch Synthesis,CUFSF - Face Sketch Synthesis benchmarking,2018-12,Residual net + Pseudo Sketch Feature Loss + LSGAN,40.85,100.0,40.85,100,40.85,0.65,SSIM
88,1,Image Super-Resolution,Manga109 - 3x upscaling - Image Super-Resolution benchmarking,2019-02,LFFN-S,0.9381,98.56,0.9381,100,0.9518,0.01,SSIM
89,1,Image Super-Resolution,Manga109 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.9518,100.0,0.0137,1.0,0.9518,0.02,SSIM
90,1,Image Super-Resolution,Manga109 - 2x upscaling - Image Super-Resolution benchmarking,2019-02,LFFN-S,0.9746,99.53,0.9746,100,0.9792,0.02,SSIM
91,1,Image Super-Resolution,Manga109 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.979,99.98,0.0044,0.9565,0.9792,0.02,SSIM
92,1,Image Super-Resolution,Manga109 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.9792,100.0,0.0002,0.0435,0.9792,0.02,SSIM
93,1,Video Frame Interpolation,X4K1000FPS - Video Frame Interpolation benchmarking,2019-04,DAIN_f,0.821,100.0,0.821,100,0.821,0.01,SSIM
94,1,Video Frame Interpolation,UCF101 - Video Frame Interpolation benchmarking,2019-04,DAIN,0.9683,100.0,0.9683,100,0.9683,0.02,SSIM
95,1,Video Frame Interpolation,Vimeo90k - Video Frame Interpolation benchmarking,2019-04,DAIN,0.9756,100.0,0.9756,100,0.9756,0.02,SSIM
96,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.8949,93.49,0.8949,100,0.9572,0.01,SSIM
97,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.938,97.99,0.0431,0.6918,0.9572,0.01,SSIM
98,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.9402,98.22,0.0022,0.0353,0.9572,0.01,SSIM
99,1,Image Super-Resolution,Urban100 - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,0.9572,100.0,0.017,0.2729,0.9572,0.02,SSIM
100,1,Image Super-Resolution,Urban100 - 3x upscaling - Image Super-Resolution benchmarking,2019-04,IKC,0.8165,93.36,0.8165,100,0.8746,0.01,SSIM
101,1,Image Super-Resolution,Urban100 - 3x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.8746,100.0,0.0581,1.0,0.8746,0.01,SSIM
102,1,Pose Transfer,Market-1501 - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,0.311,100.0,0.311,100,0.311,0.0,SSIM
103,1,Image Super-Resolution,Set14 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.642,98.06,0.642,100,0.6547,0.01,SSIM
104,1,Image Super-Resolution,Set14 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.6547,100.0,0.0127,1.0,0.6547,0.01,SSIM
105,1,Image Super-Resolution,BSD100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.602,99.18,0.602,100,0.607,0.01,SSIM
106,1,Image Super-Resolution,BSD100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.607,100.0,0.005,1.0,0.607,0.01,SSIM
107,1,Image Super-Resolution,Set5 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.785,99.17,0.785,100,0.7916,0.01,SSIM
108,1,Image Super-Resolution,Set5 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.7916,100.0,0.0066,1.0,0.7916,0.01,SSIM
109,1,Image Super-Resolution,Urban100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.647,99.19,0.647,100,0.6523,0.01,SSIM
110,1,Image Super-Resolution,Urban100 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.6523,100.0,0.0053,1.0,0.6523,0.01,SSIM
111,1,Image Super-Resolution,Manga109 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,HBPN,0.802,99.17,0.802,100,0.8087,0.01,SSIM
112,1,Image Super-Resolution,Manga109 - 8x upscaling - Image Super-Resolution benchmarking,2019-06,DRLN+,0.8087,100.0,0.0067,1.0,0.8087,0.01,SSIM
113,1,Image Super-Resolution,DIV2K val - 4x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,0.8837,100.0,0.8837,100,0.8837,0.01,SSIM
114,1,Image Super-Resolution,DIV2K val - 2x upscaling - Image Super-Resolution benchmarking,2019-07,CAR,0.9599,100.0,0.9599,100,0.9599,0.02,SSIM
115,1,Face Alignment,CelebA Aligned - Face Alignment benchmarking,2019-08,Progressive Face SR,0.685,100.0,0.685,100,0.685,0.01,SSIM
116,1,Image Super-Resolution,Celeb-HQ 4x upscaling - Image Super-Resolution benchmarking,2019-09,Edge-informed SR,0.912,100.0,0.912,100,0.912,0.01,SSIM
117,1,Image Super-Resolution,USR-248 - 4x upscaling - Image Super-Resolution benchmarking,2019-09,SRDRM-GAN,0.69,100.0,0.69,100,0.69,0.01,SSIM
118,1,Image Super-Resolution,Manga109 - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,0.673,100.0,0.673,100,0.673,0.01,SSIM
119,1,Image Super-Resolution,Urban100 - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,0.515,100.0,0.515,100,0.515,0.01,SSIM
120,1,Image Super-Resolution,DIV2K val - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,0.641,100.0,0.641,100,0.641,0.01,SSIM
121,1,Image Super-Resolution,DIV8K val - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,0.65,100.0,0.65,100,0.65,0.01,SSIM
122,1,Image Super-Resolution,BSD100 - 16x upscaling - Image Super-Resolution benchmarking,2019-10,ABPN,0.512,100.0,0.512,100,0.512,0.01,SSIM
123,1,Image Dehazing,SOTS Indoor - Image Dehazing benchmarking,2019-11,FFA-Net,0.9846,100.0,0.9846,100,0.9846,0.02,SSIM
124,1,Image Dehazing,SOTS Outdoor - Image Dehazing benchmarking,2019-11,FFA-Net,0.9804,100.0,0.9804,100,0.9804,0.02,SSIM
125,1,Video Frame Interpolation,Middlebury - Video Frame Interpolation benchmarking,2020-03,SoftSplat,0.971,100.0,0.971,100,0.971,0.02,SSIM
0,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,-147.21,197.78,-147.21,100,-74.43,9814.0,FID
1,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,FSRCNN,-139.78,187.8,7.43,0.1021,-74.43,9318.67,FID
2,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2016-12,EnhanceNet,-116.38,156.36,23.4,0.3215,-74.43,7758.67,FID
3,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,-74.43,100.0,41.95,0.5764,-74.43,4962.0,FID
4,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,-31.84,256.77,-31.84,100,-12.4,2122.67,FID
5,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,FSRCNN,-23.97,193.31,7.87,0.4048,-12.4,1598.0,FID
6,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2016-12,EnhanceNet,-19.07,153.79,4.9,0.2521,-12.4,1271.33,FID
7,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,-15.54,125.32,3.53,0.1816,-12.4,1036.0,FID
8,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,-12.4,100.0,3.14,0.1615,-12.4,826.67,FID
9,1,Image Generation,Stanford Dogs - Image Generation benchmarking,2016-06,InfoGAN,-29.34,114.34,-29.34,100,-25.66,1956.0,FID
10,1,Image Generation,Stanford Dogs - Image Generation benchmarking,2018-11,FineGAN,-25.66,100.0,3.68,1.0,-25.66,1710.67,FID
11,1,Image Generation,Stanford Cars - Image Generation benchmarking,2016-06,InfoGAN,-17.63,109.98,-17.63,100,-16.03,1175.33,FID
12,1,Image Generation,Stanford Cars - Image Generation benchmarking,2018-11,FineGAN,-16.03,100.0,1.6,1.0,-16.03,1068.67,FID
13,1,Image Generation,CUB 128 x 128 - Image Generation benchmarking,2016-06,InfoGAN,-13.2,117.33,-13.2,100,-11.25,880.0,FID
14,1,Image Generation,CUB 128 x 128 - Image Generation benchmarking,2018-11,FineGAN,-11.25,100.0,1.95,1.0,-11.25,750.0,FID
15,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,-4.396,125.49,-4.396,100,-3.503,293.07,FID
16,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,ESRGAN,-3.503,100.0,0.893,1.0,-3.503,233.53,FID
17,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2016-10,GAWWN,-67.22,439.35,-67.22,100,-15.3,4481.33,FID
18,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,-15.3,100.0,51.92,1.0,-15.3,1020.0,FID
19,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2016-11,pix2pix,-96.31,139579.71,-96.31,100,-0.069,6420.67,FID
20,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2017-06,Xian et al._,-60.848,88185.51,35.462,0.3685,-0.069,4056.53,FID
21,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2019-03,PI-REC,-0.069,100.0,60.779,0.6315,-0.069,4.6,FID
22,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2016-11,pix2pix,-197.492,1316613.33,-197.492,100,-0.015,13166.13,FID
23,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2017-06,Xian et al._,-44.762,298413.33,152.73,0.7734,-0.015,2984.13,FID
24,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2019-03,PI-REC,-0.015,100.0,44.747,0.2266,-0.015,1.0,FID
25,1,Fundus to Angiography Generation,Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking,2016-11,pix2pix,-48.6,198.37,-48.6,100,-24.5,3240.0,FID
26,1,Fundus to Angiography Generation,Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking,2017-11,pix2pixHD,-42.8,174.69,5.8,0.2407,-24.5,2853.33,FID
27,1,Fundus to Angiography Generation,Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking,2019-07,U-GAT-IT,-24.5,100.0,18.3,0.7593,-24.5,1633.33,FID
28,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2017-03,WGAN-GP,-29.3,257.02,-29.3,100,-11.4,1953.33,FID
29,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2017-06,WGAN-GP + TT Update Rule,-24.8,217.54,4.5,0.2514,-11.4,1653.33,FID
30,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2018-02,SN-GANs,-21.7,190.35,3.1,0.1732,-11.4,1446.67,FID
31,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2018-03,Dist-GAN,-17.61,154.47,4.09,0.2285,-11.4,1174.0,FID
32,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2018-09,BigGAN,-14.73,129.21,2.88,0.1609,-11.4,982.0,FID
33,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2019-07,WGAN-ALP,-12.96,113.68,1.77,0.0989,-11.4,864.0,FID
34,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2019-08,AutoGAN,-12.42,108.95,0.54,0.0302,-11.4,828.0,FID
35,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2019-11,MSGAN,-11.4,100.0,1.02,0.057,-11.4,760.0,FID
36,1,Image Generation,CAT 256x256 - Image Generation benchmarking,2017-03,WGAN-GP,-155.46,484.15,-155.46,100,-32.11,10364.0,FID
37,1,Image Generation,CAT 256x256 - Image Generation benchmarking,2018-07,RaSGAN,-32.11,100.0,123.35,1.0,-32.11,2140.67,FID
38,1,Image Generation,LSUN Bedroom 64 x 64 - Image Generation benchmarking,2017-06,WGAN-GP + TT Update Rule,-9.5,100.0,-9.5,100,-9.5,633.33,FID
39,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2017-07,CRN,-104.7,210.66,-104.7,100,-49.7,6980.0,FID
40,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,-95.0,191.15,9.7,0.1764,-49.7,6333.33,FID
41,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2018-04,SIMS,-49.7,100.0,45.3,0.8236,-49.7,3313.33,FID
42,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,-73.3,277.65,-73.3,100,-26.4,4886.67,FID
43,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,-33.9,128.41,39.4,0.8401,-26.4,2260.0,FID
44,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,-31.7,120.08,2.2,0.0469,-26.4,2113.33,FID
45,1,Image-to-Image Translation,ADE20K Labels-to-Photos - Image-to-Image Translation benchmarking,2020-04,CoCosNet,-26.4,100.0,5.3,0.113,-26.4,1760.0,FID
46,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,-99.0,233.49,-99.0,100,-42.4,6600.0,FID
47,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,-97.8,230.66,1.2,0.0212,-42.4,6520.0,FID
48,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2018-04,SIMS,-67.7,159.67,30.1,0.5318,-42.4,4513.33,FID
49,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,-63.3,149.29,4.4,0.0777,-42.4,4220.0,FID
50,1,Image-to-Image Translation,ADE20K-Outdoor Labels-to-Photos - Image-to-Image Translation benchmarking,2020-04,CoCosNet,-42.4,100.0,20.9,0.3693,-42.4,2826.67,FID
51,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2017-07,CRN,-70.4,366.67,-70.4,100,-19.2,4693.33,FID
52,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2019-03,SPADE,-22.6,117.71,47.8,0.9336,-19.2,1506.67,FID
53,1,Image-to-Image Translation,COCO-Stuff Labels-to-Photos - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,-19.2,100.0,3.4,0.0664,-19.2,1280.0,FID
54,1,Face Hallucination,FFHQ 512 x 512 - 16x upscaling - Face Hallucination benchmarking,2017-10,WaveletCNN,-60.916,119.68,-60.916,100,-50.901,4061.07,FID
55,1,Face Hallucination,FFHQ 512 x 512 - 16x upscaling - Face Hallucination benchmarking,2018-09,ESRGAN,-50.901,100.0,10.015,1.0,-50.901,3393.4,FID
56,1,Image Generation,LSUN Bedroom 256 x 256 - Image Generation benchmarking,2017-10,StackGAN-v2,-35.61,1343.77,-35.61,100,-2.65,2374.0,FID
57,1,Image Generation,LSUN Bedroom 256 x 256 - Image Generation benchmarking,2017-10,PGGAN,-8.34,314.72,27.27,0.8274,-2.65,556.0,FID
58,1,Image Generation,LSUN Bedroom 256 x 256 - Image Generation benchmarking,2018-12,StyleGAN,-2.65,100.0,5.69,0.1726,-2.65,176.67,FID
59,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2017-10,StackGAN-v1 ,-74.05,222.04,-74.05,100,-33.35,4936.67,FID
60,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-01,AttnGAN + OP,-33.35,100.0,40.7,1.0,-33.35,2223.33,FID
61,1,Text-to-Image Generation,Oxford 102 Flowers - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,-48.68,100.0,-48.68,100,-48.68,3245.33,FID
62,1,Image Generation,LSUN Cat 256 x 256 - Image Generation benchmarking,2017-10,PGGAN,-37.52,541.41,-37.52,100,-6.93,2501.33,FID
63,1,Image Generation,LSUN Cat 256 x 256 - Image Generation benchmarking,2019-12,StyleGAN2,-6.93,100.0,30.59,1.0,-6.93,462.0,FID
64,1,Image Generation,FFHQ - Image Generation benchmarking,2017-10,PGGAN,-8.04,283.1,-8.04,100,-2.84,536.0,FID
65,1,Image Generation,FFHQ - Image Generation benchmarking,2018-12,StyleGAN,-4.43,155.99,3.61,0.6942,-2.84,295.33,FID
66,1,Image Generation,FFHQ - Image Generation benchmarking,2019-04,StyleGAN (no instance norm),-4.16,146.48,0.27,0.0519,-2.84,277.33,FID
67,1,Image Generation,FFHQ - Image Generation benchmarking,2019-12,StyleGAN2,-2.84,100.0,1.32,0.2538,-2.84,189.33,FID
68,1,Image Generation,CelebA-HQ 1024x1024 - Image Generation benchmarking,2017-10,PGGAN,-7.3,144.27,-7.3,100,-5.06,486.67,FID
69,1,Image Generation,CelebA-HQ 1024x1024 - Image Generation benchmarking,2018-12,StyleGAN,-5.06,100.0,2.24,1.0,-5.06,337.33,FID
70,1,Image Generation,CelebA-HQ 256x256 - Image Generation benchmarking,2017-10,PGGAN,-8.03,100.0,-8.03,100,-8.03,535.33,FID
71,1,Image Generation,LSUN Churches 256 x 256 - Image Generation benchmarking,2017-10,PGGAN,-6.42,166.32,-6.42,100,-3.86,428.0,FID
72,1,Image Generation,LSUN Churches 256 x 256 - Image Generation benchmarking,2019-12,StyleGAN2,-3.86,100.0,2.56,1.0,-3.86,257.33,FID
73,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,-125.98,108.3,-125.98,100,-116.32,8398.67,FID
74,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2019-09,ControlGAN,-116.32,100.0,9.66,1.0,-116.32,7754.67,FID
75,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2018-02,Projection Discriminator,-17.5,723.14,-17.5,100,-2.42,1166.67,FID
76,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2018-09,BigGAN,-2.42,100.0,15.08,1.0,-2.42,161.33,FID
77,1,Conditional Image Generation,ImageNet 128x128 - Conditional Image Generation benchmarking,2018-02,Projection Discriminator,-27.62,484.56,-27.62,100,-5.7,1841.33,FID
78,1,Conditional Image Generation,ImageNet 128x128 - Conditional Image Generation benchmarking,2018-05,SAGAN,-18.65,327.19,8.97,0.4092,-5.7,1243.33,FID
79,1,Conditional Image Generation,ImageNet 128x128 - Conditional Image Generation benchmarking,2018-09,BigGAN-deep,-5.7,100.0,12.95,0.5908,-5.7,380.0,FID
80,1,Image Generation,STL-10 - Image Generation benchmarking,2018-02,SN-GAN,-40.1,129.31,-40.1,100,-31.01,2673.33,FID
81,1,Image Generation,STL-10 - Image Generation benchmarking,2018-03,Dist-GAN,-36.19,116.7,3.91,0.4301,-31.01,2412.67,FID
82,1,Image Generation,STL-10 - Image Generation benchmarking,2019-08,AutoGAN,-31.01,100.0,5.18,0.5699,-31.01,2067.33,FID
83,1,Layout-to-Image Generation,Visual Genome 64x64 - Layout-to-Image Generation benchmarking,2018-04,SG2Im,-74.61,238.75,-74.61,100,-31.25,4974.0,FID
84,1,Layout-to-Image Generation,Visual Genome 64x64 - Layout-to-Image Generation benchmarking,2018-11,Layout2Im,-31.25,100.0,43.36,1.0,-31.25,2083.33,FID
85,1,Layout-to-Image Generation,COCO-Stuff 64x64 - Layout-to-Image Generation benchmarking,2018-04,SG2Im,-67.96,198.08,-67.96,100,-34.31,4530.67,FID
86,1,Layout-to-Image Generation,COCO-Stuff 64x64 - Layout-to-Image Generation benchmarking,2018-11,Layout2Im,-38.14,111.16,29.82,0.8862,-34.31,2542.67,FID
87,1,Layout-to-Image Generation,COCO-Stuff 64x64 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,-34.31,100.0,3.83,0.1138,-34.31,2287.33,FID
88,1,Multimodal Unsupervised Image-To-Image Translation,CelebA-HQ - Multimodal Unsupervised Image-To-Image Translation benchmarking,2018-04,MUNIT,-31.4,228.7,-31.4,100,-13.73,2093.33,FID
89,1,Multimodal Unsupervised Image-To-Image Translation,CelebA-HQ - Multimodal Unsupervised Image-To-Image Translation benchmarking,2019-12,StarGAN v2,-13.73,100.0,17.67,1.0,-13.73,915.33,FID
90,1,Multimodal Unsupervised Image-To-Image Translation,AFHQ - Multimodal Unsupervised Image-To-Image Translation benchmarking,2018-04,MUNIT,-41.5,256.17,-41.5,100,-16.2,2766.67,FID
91,1,Multimodal Unsupervised Image-To-Image Translation,AFHQ - Multimodal Unsupervised Image-To-Image Translation benchmarking,2019-12,StarGAN v2,-16.2,100.0,25.3,1.0,-16.2,1080.0,FID
92,1,Image Generation,ImageNet 256x256 - Image Generation benchmarking,2018-09,BigGAN-deep,-8.1,100.0,-8.1,100,-8.1,540.0,FID
93,1,Image Generation,ImageNet 128x128 - Image Generation benchmarking,2018-09,BigGAN-deep,-5.7,100.0,-5.7,100,-5.7,380.0,FID
94,1,Image Generation,CelebA-HQ 128x128 - Image Generation benchmarking,2018-11,SS-GAN (sBN),-24.36,424.39,-24.36,100,-5.74,1624.0,FID
95,1,Image Generation,CelebA-HQ 128x128 - Image Generation benchmarking,2019-03,COCO-GAN,-5.74,100.0,18.62,1.0,-5.74,382.67,FID
96,1,Image Generation,CelebA-HQ 64x64 - Image Generation benchmarking,2019-03,COCO-GAN,-4.0,100.0,-4,100,-4,266.67,FID
97,1,Video Generation,TrailerFaces - Video Generation benchmarking,2019-04,PG-SWGAN-3D,-404.1,100.0,-404.1,100,-404.1,26940.0,FID
98,1,Talking Head Generation,VoxCeleb1 - 1-shot learning - Talking Head Generation benchmarking,2019-05,Few-shot Adversarial Model,-43.0,100.0,-43,100,-43,2866.67,FID
99,1,Talking Head Generation,VoxCeleb2 - 1-shot learning - Talking Head Generation benchmarking,2019-05,Few-shot Adversarial Model,-48.5,100.0,-48.5,100,-48.5,3233.33,FID
100,1,Talking Head Generation,VoxCeleb2 - 32-shot learning - Talking Head Generation benchmarking,2019-05,Few-shot Adversarial Model,-30.6,100.0,-30.6,100,-30.6,2040.0,FID
101,1,Talking Head Generation,VoxCeleb2 - 8-shot learning - Talking Head Generation benchmarking,2019-05,Few-shot Adversarial Model,-42.2,100.0,-42.2,100,-42.2,2813.33,FID
102,1,Talking Head Generation,VoxCeleb1 - 32-shot learning - Talking Head Generation benchmarking,2019-05,Few-shot Adversarial Model,-29.5,100.0,-29.5,100,-29.5,1966.67,FID
103,1,Talking Head Generation,VoxCeleb1 - 8-shot learning - Talking Head Generation benchmarking,2019-05,Few-shot Adversarial Model,-38.0,100.0,-38,100,-38,2533.33,FID
104,1,Image Generation,MNIST - Image Generation benchmarking,2019-05,GLF+perceptual loss (ours),-5.8,100.0,-5.8,100,-5.8,386.67,FID
105,1,Image Generation,Fashion-MNIST - Image Generation benchmarking,2019-05,GLF+perceptual loss (ours),-10.3,100.0,-10.3,100,-10.3,686.67,FID
106,1,Image Generation,CelebA 256x256 - Image Generation benchmarking,2019-05,GLF+perceptual loss (ours),-41.8,100.0,-41.8,100,-41.8,2786.67,FID
107,1,Video Generation,"Kinetics-600 48 frames, 64x64 - Video Generation benchmarking",2019-07,DVD-GAN,-12.92,100.0,-12.92,100,-12.92,861.33,FID
108,1,Video Generation,"Kinetics-600 12 frames, 128x128 - Video Generation benchmarking",2019-07,DVD-GAN,-2.16,100.0,-2.16,100,-2.16,144.0,FID
109,1,Video Generation,"Kinetics-600 12 frames, 64x64 - Video Generation benchmarking",2019-07,DVD-GAN,-0.91,100.0,-0.91,100,-0.91,60.67,FID
110,1,Layout-to-Image Generation,COCO-Stuff 128x128 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,-29.65,100.0,-29.65,100,-29.65,1976.67,FID
111,1,Layout-to-Image Generation,Visual Genome 128x128 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,-29.36,100.0,-29.36,100,-29.36,1957.33,FID
112,1,Image Generation,Stacked MNIST - Image Generation benchmarking,2019-10,PresGAN,-23.965,100.0,-23.965,100,-23.965,1597.67,FID
113,1,Image Generation,CelebA 128 x 128 - Image Generation benchmarking,2019-10,PresGAN,-29.115,100.0,-29.115,100,-29.115,1941.0,FID
114,1,Image Reconstruction,Edge-to-Clothes - Image Reconstruction benchmarking,2019-10,bFT,-58.4,100.0,-58.4,100,-58.4,3893.33,FID
115,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2019-10,bFT,-12.266,100.0,-12.266,100,-12.266,817.73,FID
116,1,Video-to-Video Synthesis,YouTube Dancing - Video-to-Video Synthesis benchmarking,2019-10,Few-shot Video-to-Video,-80.44,100.0,-80.44,100,-80.44,5362.67,FID
117,1,Video-to-Video Synthesis,Street Scene - Video-to-Video Synthesis benchmarking,2019-10,Few-shot Video-to-Video,-144.24,100.0,-144.24,100,-144.24,9616.0,FID
118,1,Image Generation,CIFAR-100 - Image Generation benchmarking,2019-11,MSGAN,-19.74,273.41,-19.74,100,-7.22,1316.0,FID
119,1,Image Generation,CIFAR-100 - Image Generation benchmarking,2019-12,TAC-GAN,-7.22,100.0,12.52,1.0,-7.22,481.33,FID
120,1,Image Generation,ImageNet 32x32 - Image Generation benchmarking,2019-11,MSGAN,-12.3,100.0,-12.3,100,-12.3,820.0,FID
121,1,Image-to-Image Translation,ADE-Indoor Labels-to-Photo - Image-to-Image Translation benchmarking,2019-11,SB-GAN,-48.15,100.0,-48.15,100,-48.15,3210.0,FID
122,1,Image Generation,Cityscapes-25K 256x512 - Image Generation benchmarking,2019-11,SB-GAN,-62.97,100.0,-62.97,100,-62.97,4198.0,FID
123,1,Image Generation,ADE-Indoor - Image Generation benchmarking,2019-11,SB-GAN,-85.27,100.0,-85.27,100,-85.27,5684.67,FID
124,1,Image Generation,Cityscapes-5K 256x512 - Image Generation benchmarking,2019-11,SB-GAN,-65.49,100.0,-65.49,100,-65.49,4366.0,FID
125,1,Image Generation,LSUN Car 512 x 384 - Image Generation benchmarking,2019-12,StyleGAN2,-2.32,100.0,-2.32,100,-2.32,154.67,FID
126,1,Image Generation,LSUN Horse 256 x 256 - Image Generation benchmarking,2019-12,StyleGAN2,-3.43,100.0,-3.43,100,-3.43,228.67,FID
127,1,Image-to-Image Translation,CelebA-HQ - Image-to-Image Translation benchmarking,2019-12,StarGAN v2,-13.73,100.0,-13.73,100,-13.73,915.33,FID
128,1,Image-to-Image Translation,AFHQ - Image-to-Image Translation benchmarking,2019-12,StarGAN v2,-24.4,100.0,-24.4,100,-24.4,1626.67,FID
129,1,Image-to-Image Translation,Deep-Fashion - Image-to-Image Translation benchmarking,2020-04,CoCosNet,-14.4,100.0,-14.4,100,-14.4,960.0,FID
0,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2014-12,SRCNN,6.9,74.11,6.9,100,9.31,0.74,MOVIE
1,1,Video Super-Resolution,Vid4 - 4x upscaling - Video Super-Resolution benchmarking,2016-11,bicubic,9.31,100.0,2.41,1.0,9.31,1.0,MOVIE
0,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.9,93.95,0.9,100,0.958,0.93,MS\\-SSIM
1,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,FSRCNN,0.93,97.08,0.03,0.5172,0.958,0.96,MS\\-SSIM
2,1,Image Super-Resolution,FFHQ 256 x 256 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,0.958,100.0,0.028,0.4828,0.958,0.99,MS\\-SSIM
3,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2014-12,SRCNN,0.924,95.16,0.924,100,0.971,0.95,MS\\-SSIM
4,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2016-08,FSRCNN,0.951,97.94,0.027,0.5745,0.971,0.98,MS\\-SSIM
5,1,Image Super-Resolution,FFHQ 1024 x 1024 - 4x upscaling - Image Super-Resolution benchmarking,2019-10,CAGFace,0.971,100.0,0.02,0.4255,0.971,1.0,MS\\-SSIM
6,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,0.935,97.29,0.935,100,0.961,0.96,MS\\-SSIM
7,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,0.961,100.0,0.026,1.0,0.961,0.99,MS\\-SSIM
8,1,Face Alignment,CelebA Aligned - Face Alignment benchmarking,2019-08,Progressive Face SR,0.902,100.0,0.902,100,0.902,0.93,MS\\-SSIM
0,1,Face Verification,IJB-C - Face Verification benchmarking,2015-03,FaceNet,66.5,68.44,66.5,100,97.17,0.68,TAR\\ at\\ FAR=0\\.01
1,1,Face Verification,IJB-C - Face Verification benchmarking,2017-10,VGGFace2_ft,96.7,99.52,30.2,0.9847,97.17,0.98,TAR\\ at\\ FAR=0\\.01
2,1,Face Verification,IJB-C - Face Verification benchmarking,2019-04,PFEfuse + match,97.17,100.0,0.47,0.0153,97.17,0.99,TAR\\ at\\ FAR=0\\.01
3,1,Face Verification,IJB-A - Face Verification benchmarking,2015-07,Deep CNN + COTS matcher,73.3,75.1,73.3,100,97.6,0.74,TAR\\ at\\ FAR=0\\.01
4,1,Face Verification,IJB-A - Face Verification benchmarking,2015-08,DCNN,83.8,85.86,10.5,0.4321,97.6,0.85,TAR\\ at\\ FAR=0\\.01
5,1,Face Verification,IJB-A - Face Verification benchmarking,2016-03,Template adaptation,93.9,96.21,10.1,0.4156,97.6,0.95,TAR\\ at\\ FAR=0\\.01
6,1,Face Verification,IJB-A - Face Verification benchmarking,2016-03,NAN,94.1,96.41,0.2,0.0082,97.6,0.96,TAR\\ at\\ FAR=0\\.01
7,1,Face Verification,IJB-A - Face Verification benchmarking,2017-03,L2-constrained softmax loss,97.0,99.39,2.9,0.1193,97.6,0.98,TAR\\ at\\ FAR=0\\.01
8,1,Face Verification,IJB-A - Face Verification benchmarking,2017-12,Dual-Agent GANs,97.6,100.0,0.6,0.0247,97.6,0.99,TAR\\ at\\ FAR=0\\.01
9,1,Face Verification,Oulu-CASIA NIR-VIS - Face Verification benchmarking,2017-08,W-CNN He et al. (2018),81.5,82.74,81.5,100,98.5,0.83,TAR\\ at\\ FAR=0\\.01
10,1,Face Verification,Oulu-CASIA NIR-VIS - Face Verification benchmarking,2018-09,DVR Wu et al. (2019),97.2,98.68,15.7,0.9235,98.5,0.99,TAR\\ at\\ FAR=0\\.01
11,1,Face Verification,Oulu-CASIA NIR-VIS - Face Verification benchmarking,2019-03,LightCNN-29 + DVG,98.5,100.0,1.3,0.0765,98.5,1.0,TAR\\ at\\ FAR=0\\.01
12,1,Face Verification,BUAA-VisNir - Face Verification benchmarking,2017-08,W-CNN He et al. (2018),96.0,97.46,96.0,100,98.5,0.97,TAR\\ at\\ FAR=0\\.01
13,1,Face Verification,BUAA-VisNir - Face Verification benchmarking,2018-09,DVR Wu et al. (2019),98.5,100.0,2.5,1.0,98.5,1.0,TAR\\ at\\ FAR=0\\.01
14,1,Face Verification,IJB-B - Face Verification benchmarking,2017-08,FPN,96.5,100.0,96.5,100,96.5,0.98,TAR\\ at\\ FAR=0\\.01
15,1,Face Verification,IIIT-D Viewed Sketch - Face Verification benchmarking,2019-03,LightCNN-29 + DVG,97.86,100.0,97.86,100,97.86,0.99,TAR\\ at\\ FAR=0\\.01
0,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2015-04,iRNN,82.0,84.36,82.0,100,97.2,0.84,Permuted\\ Accuracy
1,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2015-11,LSTM,88.0,90.53,6.0,0.3947,97.2,0.91,Permuted\\ Accuracy
2,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2016-03,BN LSTM,95.4,98.15,7.4,0.4868,97.2,0.98,Permuted\\ Accuracy
3,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2018-03,Temporal Convolutional Network,97.2,100.0,1.8,0.1184,97.2,1.0,Permuted\\ Accuracy
0,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2015-04,iRNN,97.0,97.78,97.0,100,99.2,0.98,Unpermuted\\ Accuracy
1,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2015-11,LSTM,98.2,98.99,1.2,0.5455,99.2,0.99,Unpermuted\\ Accuracy
2,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2016-03,BN LSTM,99.0,99.8,0.8,0.3636,99.2,1.0,Unpermuted\\ Accuracy
3,1,Sequential Image Classification,Sequential MNIST - Sequential Image Classification benchmarking,2017-10,Dilated GRU,99.2,100.0,0.2,0.0909,99.2,1.0,Unpermuted\\ Accuracy
4,1,Sequential Image Classification,Sequential CIFAR-10 - Sequential Image Classification benchmarking,2018-03,"Transformer (self-attention) (Trinh et al., 2018)",62.2,84.72,62.2,100,73.42,0.63,Unpermuted\\ Accuracy
5,1,Sequential Image Classification,Sequential CIFAR-10 - Sequential Image Classification benchmarking,2018-10,Trellis Network,73.42,100.0,11.22,1.0,73.42,0.74,Unpermuted\\ Accuracy
0,1,Lane Detection,Caltech Lanes Washington - Lane Detection benchmarking,2015-04,Overfeat CNN detector + DBSCAN,0.861,99.08,0.861,100,0.869,0.01,F1
1,1,Lane Detection,Caltech Lanes Washington - Lane Detection benchmarking,2017-10,VPGNet,0.869,100.0,0.008,1.0,0.869,0.01,F1
2,1,Lane Detection,Caltech Lanes Cordova - Lane Detection benchmarking,2015-04,Overfeat CNN detector + DBSCAN,0.866,97.96,0.866,100,0.884,0.01,F1
3,1,Lane Detection,Caltech Lanes Cordova - Lane Detection benchmarking,2017-10,VPGNet,0.884,100.0,0.018,1.0,0.884,0.01,F1
4,1,3D Point Cloud Classification,Sydney Urban Objects - 3D Point Cloud Classification benchmarking,2016-04,ORION,77.8,99.23,77.8,100,78.4,0.79,F1
5,1,3D Point Cloud Classification,Sydney Urban Objects - 3D Point Cloud Classification benchmarking,2017-04,ECC,78.4,100.0,0.6,1.0,78.4,0.79,F1
6,1,Facial Action Unit Detection,BP4D - Facial Action Unit Detection benchmarking,2017-02,Baseline,45.2,78.34,45.2,100,57.7,0.46,F1
7,1,Facial Action Unit Detection,BP4D - Facial Action Unit Detection benchmarking,2017-04,Multi-View,57.7,100.0,12.5,1.0,57.7,0.58,F1
8,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (T+V),0.756,98.44,0.756,100,0.768,0.01,F1
9,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (A+T+V),0.768,100.0,0.012,1.0,0.768,0.01,F1
10,1,Iris Segmentation,CASIA - Iris Segmentation benchmarking,2019-01,IrisParseNet (ASPP) CASIA,94.3,100.0,94.3,100,94.3,0.95,F1
11,1,Iris Segmentation,UBIRIS - Iris Segmentation benchmarking,2019-01,IrisParseNet (ASPP),91.82,100.0,91.82,100,91.82,0.93,F1
12,1,Iris Segmentation,MICHE - Iris Segmentation benchmarking,2019-01,IrisParseNet (PSP),91.5,100.0,91.5,100,91.5,0.93,F1
13,1,Speech Emotion Recognition,IEMOCAP - Speech Emotion Recognition benchmarking,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression),0.718,100.0,0.718,100,0.718,0.01,F1
14,1,Edge Detection,BSDS500 - Edge Detection benchmarking,2019-04,RCN,0.824,100.0,0.824,100,0.824,0.01,F1
15,1,Head Detection,Rebar Head - Head Detection benchmarking,2019-04,"WSMA-Seg (stack=2 ,base=40, depth=5)",98.83,100.0,98.83,100,98.83,1.0,F1
0,1,Multi-Object Tracking,MOT16 - Multi-Object Tracking benchmarking,2015-04,NOMT,46.4,84.67,46.4,100,54.8,0.55,MOTA
1,1,Multi-Object Tracking,MOT16 - Multi-Object Tracking benchmarking,2018-04,GCRA,48.2,87.96,1.8,0.2143,54.8,0.57,MOTA
2,1,Multi-Object Tracking,MOT16 - Multi-Object Tracking benchmarking,2018-11,TNT,49.2,89.78,1.0,0.119,54.8,0.58,MOTA
3,1,Multi-Object Tracking,MOT16 - Multi-Object Tracking benchmarking,2019-06,DeepMOT-Tracktor,54.8,100.0,5.6,0.6667,54.8,0.65,MOTA
4,1,Multiple Object Tracking,KITTI Tracking test - Multiple Object Tracking benchmarking,2015-04,NOMT,78.15,92.19,78.15,100,84.77,0.92,MOTA
5,1,Multiple Object Tracking,KITTI Tracking test - Multiple Object Tracking benchmarking,2018-02,RRC-IIITH,84.24,99.37,6.09,0.9199,84.77,0.99,MOTA
6,1,Multiple Object Tracking,KITTI Tracking test - Multiple Object Tracking benchmarking,2018-11,3DT,84.52,99.71,0.28,0.0423,84.77,1.0,MOTA
7,1,Multiple Object Tracking,KITTI Tracking test - Multiple Object Tracking benchmarking,2019-09,mmMOT-normal,84.77,100.0,0.25,0.0378,84.77,1.0,MOTA
8,1,Pose Tracking,Multi-Person PoseTrack - Pose Tracking benchmarking,2016-11,PoseTrack,28.2,100.0,28.2,100,28.2,0.33,MOTA
9,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2017-10,PoseTrack,48.37,83.38,48.37,100,58.01,0.57,MOTA
10,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2017-12,ProTracker,51.82,89.33,3.45,0.3579,58.01,0.61,MOTA
11,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2018-04,MSRA (FlowTrack),57.81,99.66,5.99,0.6214,58.01,0.68,MOTA
12,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2019-02,HRNet-W48 COCO,57.93,99.86,0.12,0.0124,58.01,0.68,MOTA
13,1,Pose Tracking,PoseTrack2017 - Pose Tracking benchmarking,2019-05,LightTrack,58.01,100.0,0.08,0.0083,58.01,0.68,MOTA
14,1,3D Multi-Object Tracking,KITTI - 3D Multi-Object Tracking benchmarking,2018-02,BeyondPixels,84.24,100.0,84.24,100,84.24,0.99,MOTA
15,1,Pose Tracking,PoseTrack2018 - Pose Tracking benchmarking,2018-04,MSRA,61.37,100.0,61.37,100,61.37,0.72,MOTA
16,1,Multi-Object Tracking,MOT17 - Multi-Object Tracking benchmarking,2018-09,MOTDT17,50.9,94.79,50.9,100,53.7,0.6,MOTA
17,1,Multi-Object Tracking,MOT17 - Multi-Object Tracking benchmarking,2018-11,TNT,51.9,96.65,1.0,0.3571,53.7,0.61,MOTA
18,1,Multi-Object Tracking,MOT17 - Multi-Object Tracking benchmarking,2019-06,DeepMOT-Tracktor,53.7,100.0,1.8,0.6429,53.7,0.63,MOTA
19,1,Online Multi-Object Tracking,MOT16 - Online Multi-Object Tracking benchmarking,2019-02,DMAN,46.1,84.74,46.1,100,54.4,0.54,MOTA
20,1,Online Multi-Object Tracking,MOT16 - Online Multi-Object Tracking benchmarking,2019-03,Tracktor++,54.4,100.0,8.3,1.0,54.4,0.64,MOTA
21,1,Online Multi-Object Tracking,MOT17 - Online Multi-Object Tracking benchmarking,2019-03,Tracktor++,53.5,100.0,53.5,100,53.5,0.63,MOTA
22,1,Online Multi-Object Tracking,2D MOT 2015 - Online Multi-Object Tracking benchmarking,2019-03,Tracktor++,44.1,100.0,44.1,100,44.1,0.52,MOTA
23,1,Multi-Object Tracking,2D MOT 2015 - Multi-Object Tracking benchmarking,2019-06,DeepMOT-Tracktor,44.1,100.0,44.1,100,44.1,0.52,MOTA
24,1,Online Multi-Object Tracking,MOT15 - Online Multi-Object Tracking benchmarking,2019-07,GMPHD Filter (Occlusion Group Management),30.7,100.0,30.7,100,30.7,0.36,MOTA
0,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2015-04,mCNN,56.3,73.02,56.3,100,77.1,0.65,R\\-at\\-5
1,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2015-11,SPE,60.1,77.95,3.8,0.1827,77.1,0.69,R\\-at\\-5
2,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2016-11,DAN,69.2,89.75,9.1,0.4375,77.1,0.8,R\\-at\\-5
3,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2017-12,SCO,70.5,91.44,1.3,0.0625,77.1,0.81,R\\-at\\-5
4,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2018-03,SCAN i-t,74.2,96.24,3.7,0.1779,77.1,0.86,R\\-at\\-5
5,1,Image Retrieval,Flickr30K 1K test - Image Retrieval benchmarking,2019-09,CAMP,77.1,100.0,2.9,0.1394,77.1,0.89,R\\-at\\-5
6,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,74.49,86.36,74.49,100,86.26,0.86,R\\-at\\-5
7,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-09,AMEM,78.66,91.19,4.17,0.3543,86.26,0.91,R\\-at\\-5
8,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,80.71,93.57,2.05,0.1742,86.26,0.93,R\\-at\\-5
9,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,HACAN,83.03,96.26,2.32,0.1971,86.26,0.96,R\\-at\\-5
10,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),86.26,100.0,3.23,0.2744,86.26,0.99,R\\-at\\-5
11,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,76.88,88.64,76.88,100,86.73,0.89,R\\-at\\-5
12,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),78.1,90.05,1.22,0.1239,86.73,0.9,R\\-at\\-5
13,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,79.75,91.95,1.65,0.1675,86.73,0.92,R\\-at\\-5
14,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,80.63,92.97,0.88,0.0893,86.73,0.93,R\\-at\\-5
15,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),86.73,100.0,6.1,0.6193,86.73,1.0,R\\-at\\-5
16,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2018-05,BAN (Bottom-Up detector),84.22,99.11,84.22,100,84.98,0.97,R\\-at\\-5
17,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2019-08,VisualBERT,84.98,100.0,0.76,1.0,84.98,0.98,R\\-at\\-5
18,1,Phrase Grounding,Flickr30k Entities Dev - Phrase Grounding benchmarking,2019-08,VisualBERT,84.49,100.0,84.49,100,84.49,0.97,R\\-at\\-5
0,1,Brain Tumor Segmentation,BRATS-2013 - Brain Tumor Segmentation benchmarking,2015-05,InputCascadeCNN,0.88,95.05,0.88,100,0.9258,0.01,Dice\\ Score
1,1,Brain Tumor Segmentation,BRATS-2013 - Brain Tumor Segmentation benchmarking,2019-08,ModelGenesis,0.9258,100.0,0.0458,1.0,0.9258,0.01,Dice\\ Score
2,1,Brain Tumor Segmentation,BRATS-2013 leaderboard - Brain Tumor Segmentation benchmarking,2015-05,InputCascadeCNN,0.84,100.0,0.84,100,0.84,0.01,Dice\\ Score
3,1,Pancreas Segmentation,CT-150 - Pancreas Segmentation benchmarking,2015-05,U-Net,0.814,96.9,0.814,100,0.84,0.01,Dice\\ Score
4,1,Pancreas Segmentation,CT-150 - Pancreas Segmentation benchmarking,2018-04,Att U-Net,0.84,100.0,0.026,1.0,0.84,0.01,Dice\\ Score
5,1,Pancreas Segmentation,TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking,2015-05,U-Net,0.82,93.61,0.82,100,0.876,0.01,Dice\\ Score
6,1,Pancreas Segmentation,TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking,2017-09,Recurrent Saliency Transformation Network,0.876,100.0,0.056,1.0,0.876,0.01,Dice\\ Score
7,1,Lesion Segmentation,ISLES-2015 - Lesion Segmentation benchmarking,2016-03,3D CNN + CRF,59.0,100.0,59,100,59,0.64,Dice\\ Score
8,1,Brain Tumor Segmentation,BRATS-2015 - Brain Tumor Segmentation benchmarking,2016-03,3D CNN + CRF,85.0,97.7,85,100,87,0.93,Dice\\ Score
9,1,Brain Tumor Segmentation,BRATS-2015 - Brain Tumor Segmentation benchmarking,2019-06,OM-Net + CGAp,87.0,100.0,2,1.0,87,0.95,Dice\\ Score
10,1,Volumetric Medical Image Segmentation,PROMISE 2012 - Volumetric Medical Image Segmentation benchmarking,2016-06,V-Net + Dice-based loss,0.869,100.0,0.869,100,0.869,0.01,Dice\\ Score
11,1,3D Medical Imaging Segmentation,TCIA Pancreas-CT - 3D Medical Imaging Segmentation benchmarking,2017-01,Holistic-nested CNN,81.3,100.0,81.3,100,81.3,0.89,Dice\\ Score
12,1,Brain Tumor Segmentation,BRATS-2017 val - Brain Tumor Segmentation benchmarking,2017-09,Wang et al.,0.905,99.77,0.905,100,0.9071,0.01,Dice\\ Score
13,1,Brain Tumor Segmentation,BRATS-2017 val - Brain Tumor Segmentation benchmarking,2019-06,OM-Net + CGAp,0.9071,100.0,0.0021,1.0,0.9071,0.01,Dice\\ Score
14,1,Brain Tumor Segmentation,BRATS-2014 - Brain Tumor Segmentation benchmarking,2017-09,Cascaded Anisotropic CNNs,0.8739,100.0,0.8739,100,0.8739,0.01,Dice\\ Score
15,1,Infant Brain MRI Segmentation,iSEG 2017 Challenge - Infant Brain MRI Segmentation benchmarking,2017-12,LiviaNet (SemiDenseNet),0.9243,100.0,0.9243,100,0.9243,0.01,Dice\\ Score
16,1,Medical Image Segmentation,iSEG 2017 Challenge - Medical Image Segmentation benchmarking,2018-04,HyperDenseNet,0.9257,100.0,0.9257,100,0.9257,0.01,Dice\\ Score
17,1,Lesion Segmentation,BUS 2017 Dataset B - Lesion Segmentation benchmarking,2018-10,Attn U-Net + Multi-Input + FTL,0.804,100.0,0.804,100,0.804,0.01,Dice\\ Score
18,1,Lesion Segmentation,ISIC 2018 - Lesion Segmentation benchmarking,2018-10,U-Net + FTL,0.829,92.63,0.829,100,0.895,0.01,Dice\\ Score
19,1,Lesion Segmentation,ISIC 2018 - Lesion Segmentation benchmarking,2018-10,Attn U-Net + Multi-Input + FTL,0.856,95.64,0.027,0.4091,0.895,0.01,Dice\\ Score
20,1,Lesion Segmentation,ISIC 2018 - Lesion Segmentation benchmarking,2020-03,MCGU-Net,0.895,100.0,0.039,0.5909,0.895,0.01,Dice\\ Score
21,1,Brain Tumor Segmentation,BRATS 2018 - Brain Tumor Segmentation benchmarking,2018-10,NVDLMED,0.87049,100.0,0.87049,100,0.87049,0.01,Dice\\ Score
22,1,Brain Image Segmentation,T1-weighted MRI - Brain Image Segmentation benchmarking,2019-02,Learned Transformations (random augmentaiton),81.5,100.0,81.5,100,81.5,0.89,Dice\\ Score
23,1,Lung Nodule Segmentation,NIH - Lung Nodule Segmentation benchmarking,2019-04,U-Net+R+A4,0.962,100.0,0.962,100,0.962,0.01,Dice\\ Score
24,1,Brain Tumor Segmentation,BRATS 2018 val - Brain Tumor Segmentation benchmarking,2019-06,OM-Net + CGAp,91.59,100.0,91.59,100,91.59,1.0,Dice\\ Score
25,1,Medical Image Segmentation,HSVM - Medical Image Segmentation benchmarking,2019-06,MS-Dual-Guided,83.2,100.0,83.2,100,83.2,0.91,Dice\\ Score
26,1,Medical Image Segmentation,CHAOS MRI Dataset - Medical Image Segmentation benchmarking,2019-06,MS-Dual-Guided,86.75,100.0,86.75,100,86.75,0.95,Dice\\ Score
27,1,Brain Image Segmentation,Brain MRI segmentation - Brain Image Segmentation benchmarking,2019-06,U-Net,0.82,100.0,0.82,100,0.82,0.01,Dice\\ Score
28,1,Lung Nodule Segmentation,Lung Nodule  - Lung Nodule Segmentation benchmarking,2019-08,BCDU-net,0.994,100.0,0.994,100,0.994,0.01,Dice\\ Score
0,1,Medical Image Segmentation,ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking,2015-05,U-Net,0.000353,100.0,0.000353,100,0.000353,1.0,Warping\\ Error
0,1,Semantic Segmentation,Kvasir-Instrument - Semantic Segmentation benchmarking,2015-05,UNet,0.9158,100.0,0.9158,100,0.9158,1.0,DSC
0,1,Medical Image Segmentation,Kvasir-SEG - Medical Image Segmentation benchmarking,2015-05,U-Net,0.818,99.63,0.818,100,0.821,0.99,mean\\ Dice
1,1,Medical Image Segmentation,Kvasir-SEG - Medical Image Segmentation benchmarking,2018-07,U-Net++,0.821,100.0,0.003,1.0,0.821,1.0,mean\\ Dice
2,1,Medical Image Segmentation,CVC-ClinicDB - Medical Image Segmentation benchmarking,2015-05,U-Net,0.823,100.0,0.823,100,0.823,1.0,mean\\ Dice
0,1,Medical Image Segmentation,Kvasir-SEG - Medical Image Segmentation benchmarking,2015-05,U-Net,0.055,100.0,0.055,100,0.055,0.41,Average\\ MAE
1,1,RGB Salient Object Detection,SOC - RGB Salient Object Detection benchmarking,2017-07,NLDF,0.106,79.7,0.106,100,0.133,0.8,Average\\ MAE
2,1,RGB Salient Object Detection,SOC - RGB Salient Object Detection benchmarking,2017-08,PiCANet,0.133,100.0,0.027,1.0,0.133,1.0,Average\\ MAE
0,1,Medical Image Segmentation,Kvasir-SEG - Medical Image Segmentation benchmarking,2015-05,U-Net,0.858,99.54,0.858,100,0.862,0.01,S\\-Measure
1,1,Medical Image Segmentation,Kvasir-SEG - Medical Image Segmentation benchmarking,2018-07,U-Net++,0.862,100.0,0.004,1.0,0.862,0.01,S\\-Measure
2,1,RGB Salient Object Detection,SOC - RGB Salient Object Detection benchmarking,2017-07,NLDF,0.816,97.03,0.816,100,0.841,0.01,S\\-Measure
3,1,RGB Salient Object Detection,SOC - RGB Salient Object Detection benchmarking,2019-06,BASNet,0.841,100.0,0.025,1.0,0.841,0.01,S\\-Measure
4,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2017-08,PiCANet,0.842,96.12,0.842,100,0.876,0.01,S\\-Measure
5,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2018-06,DGRL,0.846,96.58,0.004,0.1176,0.876,0.01,S\\-Measure
6,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2019-06,BASNet,0.876,100.0,0.03,0.8824,0.876,0.01,S\\-Measure
7,1,Camouflaged Object Segmentation,COD - Camouflaged Object Segmentation benchmarking,2019-04,CPD,74.7,100.0,74.7,100,74.7,1.0,S\\-Measure
8,1,Camouflaged Object Segmentation,CAMO - Camouflaged Object Segmentation benchmarking,2019-06,BASNet,61.8,100.0,61.8,100,61.8,0.83,S\\-Measure
0,1,Medical Image Segmentation,Kvasir-SEG - Medical Image Segmentation benchmarking,2015-05,U-Net,0.893,98.13,0.893,100,0.91,0.98,max\\ E\\-Measure
1,1,Medical Image Segmentation,Kvasir-SEG - Medical Image Segmentation benchmarking,2018-07,U-Net++,0.91,100.0,0.017,1.0,0.91,1.0,max\\ E\\-Measure
0,1,Skin Cancer Segmentation,Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking,2015-05,U-Net,0.9371,99.49,0.9371,100,0.9419,0.01,AUC
1,1,Skin Cancer Segmentation,Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking,2017-11,Residual U-Net,0.9396,99.76,0.0025,0.5208,0.9419,0.01,AUC
2,1,Skin Cancer Segmentation,Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking,2018-02,R2U-Net,0.9419,100.0,0.0023,0.4792,0.9419,0.01,AUC
3,1,Retinal Vessel Segmentation,STARE - Retinal Vessel Segmentation benchmarking,2015-05,U-Net,0.9898,99.84,0.9898,100,0.9914,0.01,AUC
4,1,Retinal Vessel Segmentation,STARE - Retinal Vessel Segmentation benchmarking,2018-02,R2U-Net,0.9914,100.0,0.0016,1.0,0.9914,0.01,AUC
5,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2015-05,U-Net,0.9772,99.2,0.9772,100,0.9851,0.01,AUC
6,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2017-11,Residual U-Net,0.9779,99.27,0.0007,0.0886,0.9851,0.01,AUC
7,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2018-02,R2U-Net,0.9815,99.63,0.0036,0.4557,0.9851,0.01,AUC
8,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2018-06,VGN,0.983,99.79,0.0015,0.1899,0.9851,0.01,AUC
9,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2018-10,LadderNet,0.9839,99.88,0.0009,0.1139,0.9851,0.01,AUC
10,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2019-12,IterNet,0.9851,100.0,0.0012,0.1519,0.9851,0.01,AUC
11,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2015-05,U-Net,0.9784,98.37,0.9784,100,0.9946,0.01,AUC
12,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2017-11,Residual U-Net,0.9849,99.02,0.0065,0.4012,0.9946,0.01,AUC
13,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2019-08,BCDU-Net (d=3),0.9946,100.0,0.0097,0.5988,0.9946,0.01,AUC
14,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2015-05,U-Net,0.8676,96.91,0.8676,100,0.8953,0.01,AUC
15,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2019-08,DTN,0.8953,100.0,0.0277,1.0,0.8953,0.01,AUC
16,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2015-05,U-Net,0.9755,99.38,0.9755,100,0.9816,0.01,AUC
17,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2017-11,Residual U-Net,0.9779,99.62,0.0024,0.3934,0.9816,0.01,AUC
18,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2018-06,VGN,0.9802,99.86,0.0023,0.377,0.9816,0.01,AUC
19,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2019-12,IterNet,0.9816,100.0,0.0014,0.2295,0.9816,0.01,AUC
20,1,Breast Tumour Classification,PCam - Breast Tumour Classification benchmarking,2015-12,ResNet-34 (e),0.942,99.37,0.942,100,0.948,0.01,AUC
21,1,Breast Tumour Classification,PCam - Breast Tumour Classification benchmarking,2015-12,ResNet-50 (e),0.948,100.0,0.006,1.0,0.948,0.01,AUC
22,1,Visual Object Tracking,OTB-50 - Visual Object Tracking benchmarking,2016-06,SiamFC-3s,0.516,84.59,0.516,100,0.61,0.01,AUC
23,1,Visual Object Tracking,OTB-50 - Visual Object Tracking benchmarking,2017-04,CFNet,0.53,86.89,0.014,0.1489,0.61,0.01,AUC
24,1,Visual Object Tracking,OTB-50 - Visual Object Tracking benchmarking,2018-02,SA-Siam,0.61,100.0,0.08,0.8511,0.61,0.01,AUC
25,1,Visual Object Tracking,OTB-2013 - Visual Object Tracking benchmarking,2016-06,SiamFC-3s,0.607,89.66,0.607,100,0.677,0.01,AUC
26,1,Visual Object Tracking,OTB-2013 - Visual Object Tracking benchmarking,2017-04,CFNet,0.611,90.25,0.004,0.0571,0.677,0.01,AUC
27,1,Visual Object Tracking,OTB-2013 - Visual Object Tracking benchmarking,2017-10,DSiam,0.656,96.9,0.045,0.6429,0.677,0.01,AUC
28,1,Visual Object Tracking,OTB-2013 - Visual Object Tracking benchmarking,2018-02,SA-Siam,0.677,100.0,0.021,0.3,0.677,0.01,AUC
29,1,Visual Object Tracking,OTB-2015 - Visual Object Tracking benchmarking,2017-04,CFNet,0.568,82.08,0.568,100,0.692,0.01,AUC
30,1,Visual Object Tracking,OTB-2015 - Visual Object Tracking benchmarking,2018-02,SA-Siam,0.657,94.94,0.089,0.7177,0.692,0.01,AUC
31,1,Visual Object Tracking,OTB-2015 - Visual Object Tracking benchmarking,2019-06,ASRCF,0.692,100.0,0.035,0.2823,0.692,0.01,AUC
32,1,3D Human Pose Estimation,MPI-INF-3DHP - 3D Human Pose Estimation benchmarking,2017-05,VNect (Augm.),42.0,67.31,42.0,100,62.4,0.43,AUC
33,1,3D Human Pose Estimation,MPI-INF-3DHP - 3D Human Pose Estimation benchmarking,2018-06,"MargiPose (Procrustes alignment, PA, multi-crop)",62.2,99.68,20.2,0.9902,62.4,0.64,AUC
34,1,3D Human Pose Estimation,MPI-INF-3DHP - 3D Human Pose Estimation benchmarking,2020-02,Explicit Compositional Depth Maps,62.4,100.0,0.2,0.0098,62.4,0.64,AUC
35,1,Abnormal Event Detection In Video,UBI-Fights - Abnormal Event Detection In Video benchmarking,2017-08,Adversarial Generator,0.523,100.0,0.523,100,0.523,0.01,AUC
36,1,Semi-supervised Anomaly Detection,UBI-Fights - Semi-supervised Anomaly Detection benchmarking,2017-08,Adversarial Generator,0.533,100.0,0.533,100,0.533,0.01,AUC
37,1,Abnormal Event Detection In Video,UCSD - Abnormal Event Detection In Video benchmarking,2017-08,Adversarial Generator,97.4,100.0,97.4,100,97.4,1.0,AUC
38,1,Retinal Vessel Segmentation,HRF - Retinal Vessel Segmentation benchmarking,2018-06,VGN,0.9838,100.0,0.9838,100,0.9838,0.01,AUC
39,1,Visual Object Tracking,LaSOT - Visual Object Tracking benchmarking,2018-11,ATOM,51.4,100.0,51.4,100,51.4,0.53,AUC
40,1,Horizon Line Estimation,KITTI Horizon - Horizon Line Estimation benchmarking,2019-07,"ConvLSTM (Huber Loss, naive residual path)",74.55,100.0,74.55,100,74.55,0.77,AUC
0,1,3D Shape Reconstruction,Pix3D - 3D Shape Reconstruction benchmarking,2018-04,MarrNet extension (w/ Pose),0.282,100.0,0.282,100,0.282,0.0,IoU
1,1,Lung Nodule Segmentation,LIDC-IDRI - Lung Nodule Segmentation benchmarking,2019-08,ModelGenesis,77.62,100.0,77.62,100,77.62,0.83,IoU
2,1,Liver Segmentation,LiTS2017 - Liver Segmentation benchmarking,2019-08,ModelGenesis,79.52,100.0,79.52,100,79.52,0.85,IoU
3,1,Skin Cancer Segmentation,PH2 - Skin Cancer Segmentation benchmarking,2019-11,SegNet,93.61,100.0,93.61,100,93.61,1.0,IoU
4,1,Medical Image Segmentation,Cell - Medical Image Segmentation benchmarking,2019-12,UNet++,91.21,100.0,91.21,100,91.21,0.97,IoU
5,1,Medical Image Segmentation,EM - Medical Image Segmentation benchmarking,2019-12,UNet++,89.33,100.0,89.33,100,89.33,0.95,IoU
0,1,Colorectal Gland Segmentation:,CRAG - Colorectal Gland Segmentation: benchmarking,2015-05,FCN8 (e),0.796,96.25,0.796,100,0.827,0.01,F1\\-score
1,1,Colorectal Gland Segmentation:,CRAG - Colorectal Gland Segmentation: benchmarking,2015-05,U-Net (e),0.827,100.0,0.031,1.0,0.827,0.01,F1\\-score
2,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2016-03,FnsNet,0.7413,90.23,0.7413,100,0.8216,0.01,F1\\-score
3,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2017-03,Mask R-CNN,0.8004,97.42,0.0591,0.736,0.8216,0.01,F1\\-score
4,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2018-09,Cell R-CNN,0.8216,100.0,0.0212,0.264,0.8216,0.01,F1\\-score
5,1,Unsupervised Video Summarization,TvSum - Unsupervised Video Summarization benchmarking,2017-12,DR-DSN,57.6,97.96,57.6,100,58.8,0.98,F1\\-score
6,1,Unsupervised Video Summarization,TvSum - Unsupervised Video Summarization benchmarking,2018-11,CSNet,58.8,100.0,1.2,1.0,58.8,1.0,F1\\-score
7,1,Unsupervised Video Summarization,SumMe - Unsupervised Video Summarization benchmarking,2017-12,DR-DSN,41.4,80.7,41.4,100,51.3,0.7,F1\\-score
8,1,Unsupervised Video Summarization,SumMe - Unsupervised Video Summarization benchmarking,2018-11,CSNet,51.3,100.0,9.9,1.0,51.3,0.87,F1\\-score
0,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2015-05,U-Net,0.7783,96.41,0.7783,100,0.8073,0.01,F1\\ score
1,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2017-11,Residual U-Net,0.78,96.62,0.0017,0.0586,0.8073,0.01,F1\\ score
2,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2018-02,R2U-Net,0.7928,98.2,0.0128,0.4414,0.8073,0.01,F1\\ score
3,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2018-06,VGN,0.8034,99.52,0.0106,0.3655,0.8073,0.01,F1\\ score
4,1,Retinal Vessel Segmentation,CHASE_DB1 - Retinal Vessel Segmentation benchmarking,2019-12,IterNet,0.8073,100.0,0.0039,0.1345,0.8073,0.01,F1\\ score
5,1,Retinal Vessel Segmentation,STARE - Retinal Vessel Segmentation benchmarking,2015-05,U-Net,0.8373,98.8,0.8373,100,0.8475,0.01,F1\\ score
6,1,Retinal Vessel Segmentation,STARE - Retinal Vessel Segmentation benchmarking,2017-11,Residual U-Net,0.8388,98.97,0.0015,0.1471,0.8475,0.01,F1\\ score
7,1,Retinal Vessel Segmentation,STARE - Retinal Vessel Segmentation benchmarking,2018-02,R2U-Net,0.8475,100.0,0.0087,0.8529,0.8475,0.01,F1\\ score
8,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2015-05,U-Net,0.9658,97.52,0.9658,100,0.9904,0.01,F1\\ score
9,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2017-11,Residual U-Net,0.969,97.84,0.0032,0.1301,0.9904,0.01,F1\\ score
10,1,Lung Nodule Segmentation,LUNA - Lung Nodule Segmentation benchmarking,2019-08,BCDU-Net (d=3),0.9904,100.0,0.0214,0.8699,0.9904,0.01,F1\\ score
11,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2015-05,U-Net,0.8142,98.54,0.8142,100,0.8263,0.01,F1\\ score
12,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2017-11,Residual U-Net,0.8149,98.62,0.0007,0.0579,0.8263,0.01,F1\\ score
13,1,Retinal Vessel Segmentation,DRIVE - Retinal Vessel Segmentation benchmarking,2018-06,VGN,0.8263,100.0,0.0114,0.9421,0.8263,0.01,F1\\ score
14,1,Skin Cancer Segmentation,Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking,2015-05,U-Net,0.8682,97.33,0.8682,100,0.892,0.01,F1\\ score
15,1,Skin Cancer Segmentation,Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking,2017-11,Residual U-Net,0.8799,98.64,0.0117,0.4916,0.892,0.01,F1\\ score
16,1,Skin Cancer Segmentation,Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking,2018-02,R2U-Net,0.892,100.0,0.0121,0.5084,0.892,0.01,F1\\ score
17,1,Lane Detection,CULane - Lane Detection benchmarking,2017-12,SCNN,71.6,100.0,71.6,100,71.6,0.74,F1\\ score
18,1,Lane Detection,TuSimple - Lane Detection benchmarking,2017-12,Spatial CNN,95.97,99.7,95.97,100,96.26,1.0,F1\\ score
19,1,Lane Detection,TuSimple - Lane Detection benchmarking,2018-06,EL-GAN,96.26,100.0,0.29,1.0,96.26,1.0,F1\\ score
20,1,Retinal Vessel Segmentation,HRF - Retinal Vessel Segmentation benchmarking,2018-06,VGN,0.8151,100.0,0.8151,100,0.8151,0.01,F1\\ score
21,1,Medical Image Segmentation,DRIVE - Medical Image Segmentation benchmarking,2019-08,BCDU-net,0.8222,100.0,0.8222,100,0.8222,0.01,F1\\ score
0,1,Medical Image Segmentation,RITE - Medical Image Segmentation benchmarking,2015-05,U-Net,31.11,79.48,31.11,100,39.14,0.79,Jaccard\\ Index
1,1,Medical Image Segmentation,RITE - Medical Image Segmentation benchmarking,2015-11,SegNet,39.14,100.0,8.03,1.0,39.14,1.0,Jaccard\\ Index
0,1,Video Retrieval,YouCook2 - Video Retrieval benchmarking,2015-06,HGLMM FV CCA,75.0,100.0,75,100,75,1.0,text\\-to\\-video\\ Median\\ Rank
1,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2016-09,C+LSTM+SA+FC7,55.0,100.0,55.0,100,55.0,0.73,text\\-to\\-video\\ Median\\ Rank
2,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2016-10,CT-SAN,46.0,88.46,46,100,52,0.61,text\\-to\\-video\\ Median\\ Rank
3,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2017-07,Large-Scale Discriminative Clustering,52.0,100.0,6,1.0,52,0.69,text\\-to\\-video\\ Median\\ Rank
4,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2018-08,JSFusion,13.0,100.0,13,100,13,0.17,text\\-to\\-video\\ Median\\ Rank
5,1,Video Retrieval,MSVD - Video Retrieval benchmarking,2019-07,Collaborative Experts,6.0,100.0,6,100,6,0.08,text\\-to\\-video\\ Median\\ Rank
6,1,Video Retrieval,DiDeMo - Video Retrieval benchmarking,2019-07,Collaborative Experts,8.3,100.0,8.3,100,8.3,0.11,text\\-to\\-video\\ Median\\ Rank
7,1,Video Retrieval,ActivityNet - Video Retrieval benchmarking,2019-07,Collaborative Experts,6.0,100.0,6,100,6,0.08,text\\-to\\-video\\ Median\\ Rank
0,1,Video Retrieval,YouCook2 - Video Retrieval benchmarking,2015-06,HGLMM FV CCA,4.6,56.1,4.6,100,8.2,0.22,text\\-to\\-video\\ R\\-at\\-1
1,1,Video Retrieval,YouCook2 - Video Retrieval benchmarking,2019-06,Text-Video Embedding,8.2,100.0,3.6,1.0,8.2,0.39,text\\-to\\-video\\ R\\-at\\-1
2,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2016-09,C+LSTM+SA+FC7,4.2,28.19,4.2,100,14.9,0.2,text\\-to\\-video\\ R\\-at\\-1
3,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2016-12,Kaufman,4.7,31.54,0.5,0.0467,14.9,0.22,text\\-to\\-video\\ R\\-at\\-1
4,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,7.0,46.98,2.3,0.215,14.9,0.33,text\\-to\\-video\\ R\\-at\\-1
5,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-08,JSFusion,10.2,68.46,3.2,0.2991,14.9,0.49,text\\-to\\-video\\ R\\-at\\-1
6,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2019-06,Text-Video Embedding,14.9,100.0,4.7,0.4393,14.9,0.71,text\\-to\\-video\\ R\\-at\\-1
7,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2016-10,CT-SAN,5.1,45.54,5.1,100,11.2,0.24,text\\-to\\-video\\ R\\-at\\-1
8,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2017-07,Large-Scale Discriminative Clustering,7.3,65.18,2.2,0.3607,11.2,0.35,text\\-to\\-video\\ R\\-at\\-1
9,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2018-04,MoEE,10.1,90.18,2.8,0.459,11.2,0.48,text\\-to\\-video\\ R\\-at\\-1
10,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2019-07,Collaborative Experts,11.2,100.0,1.1,0.1803,11.2,0.54,text\\-to\\-video\\ R\\-at\\-1
11,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2018-08,JSFusion,10.2,48.8,10.2,100,20.9,0.49,text\\-to\\-video\\ R\\-at\\-1
12,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2019-06,HT-Pretrained,14.9,71.29,4.7,0.4393,20.9,0.71,text\\-to\\-video\\ R\\-at\\-1
13,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2019-07,Collaborative Experts,20.9,100.0,6.0,0.5607,20.9,1.0,text\\-to\\-video\\ R\\-at\\-1
14,1,Video Retrieval,MSVD - Video Retrieval benchmarking,2019-07,Collaborative Experts,19.8,100.0,19.8,100,19.8,0.95,text\\-to\\-video\\ R\\-at\\-1
15,1,Video Retrieval,DiDeMo - Video Retrieval benchmarking,2019-07,Collaborative Experts,16.1,100.0,16.1,100,16.1,0.77,text\\-to\\-video\\ R\\-at\\-1
16,1,Video Retrieval,ActivityNet - Video Retrieval benchmarking,2019-07,Collaborative Experts,20.5,100.0,20.5,100,20.5,0.98,text\\-to\\-video\\ R\\-at\\-1
0,1,Video Retrieval,YouCook2 - Video Retrieval benchmarking,2015-06,HGLMM FV CCA,14.3,58.37,14.3,100,24.5,0.29,text\\-to\\-video\\ R\\-at\\-5
1,1,Video Retrieval,YouCook2 - Video Retrieval benchmarking,2019-06,Text-Video Embedding,24.5,100.0,10.2,1.0,24.5,0.5,text\\-to\\-video\\ R\\-at\\-5
2,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2016-10,CT-SAN,16.3,60.59,16.3,100,26.9,0.33,text\\-to\\-video\\ R\\-at\\-5
3,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2017-07,Large-Scale Discriminative Clustering,19.2,71.38,2.9,0.2736,26.9,0.39,text\\-to\\-video\\ R\\-at\\-5
4,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2018-04,MoEE,25.6,95.17,6.4,0.6038,26.9,0.52,text\\-to\\-video\\ R\\-at\\-5
5,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2019-07,Collaborative Experts,26.9,100.0,1.3,0.1226,26.9,0.55,text\\-to\\-video\\ R\\-at\\-5
6,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,20.9,72.07,20.9,100,29.0,0.43,text\\-to\\-video\\ R\\-at\\-5
7,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2019-07,Collaborative Experts,29.0,100.0,8.1,1.0,29.0,0.59,text\\-to\\-video\\ R\\-at\\-5
8,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2018-08,JSFusion,31.2,63.93,31.2,100,48.8,0.64,text\\-to\\-video\\ R\\-at\\-5
9,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2019-06,HT-Pretrained,40.2,82.38,9.0,0.5114,48.8,0.82,text\\-to\\-video\\ R\\-at\\-5
10,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2019-07,Collaborative Experts,48.8,100.0,8.6,0.4886,48.8,1.0,text\\-to\\-video\\ R\\-at\\-5
11,1,Video Retrieval,DiDeMo - Video Retrieval benchmarking,2019-07,Collaborative Experts,41.1,100.0,41.1,100,41.1,0.84,text\\-to\\-video\\ R\\-at\\-5
12,1,Video Retrieval,ActivityNet - Video Retrieval benchmarking,2019-07,Collaborative Experts,47.7,100.0,47.7,100,47.7,0.97,text\\-to\\-video\\ R\\-at\\-5
13,1,Video Retrieval,MSVD - Video Retrieval benchmarking,2019-07,Collaborative Experts,49.0,100.0,49,100,49,1.0,text\\-to\\-video\\ R\\-at\\-5
0,1,Video Retrieval,YouCook2 - Video Retrieval benchmarking,2015-06,HGLMM FV CCA,21.6,61.19,21.6,100,35.3,0.34,text\\-to\\-video\\ R\\-at\\-10
1,1,Video Retrieval,YouCook2 - Video Retrieval benchmarking,2019-06,Text-Video Embedding,35.3,100.0,13.7,1.0,35.3,0.55,text\\-to\\-video\\ R\\-at\\-10
2,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2016-09,C+LSTM+SA+FC7,19.9,37.69,19.9,100,52.8,0.31,text\\-to\\-video\\ R\\-at\\-10
3,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2016-12,Kaufman,24.1,45.64,4.2,0.1277,52.8,0.38,text\\-to\\-video\\ R\\-at\\-10
4,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,29.7,56.25,5.6,0.1702,52.8,0.46,text\\-to\\-video\\ R\\-at\\-10
5,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-08,JSFusion,43.2,81.82,13.5,0.4103,52.8,0.68,text\\-to\\-video\\ R\\-at\\-10
6,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2019-06,Text-Video Embedding,52.8,100.0,9.6,0.2918,52.8,0.83,text\\-to\\-video\\ R\\-at\\-10
7,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2016-10,CT-SAN,25.2,72.41,25.2,100,34.8,0.39,text\\-to\\-video\\ R\\-at\\-10
8,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2017-07,Large-Scale Discriminative Clustering,27.1,77.87,1.9,0.1979,34.8,0.42,text\\-to\\-video\\ R\\-at\\-10
9,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2018-04,MoEE,34.6,99.43,7.5,0.7813,34.8,0.54,text\\-to\\-video\\ R\\-at\\-10
10,1,Video Retrieval,LSMDC - Video Retrieval benchmarking,2019-07,Collaborative Experts,34.8,100.0,0.2,0.0208,34.8,0.54,text\\-to\\-video\\ R\\-at\\-10
11,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2018-08,JSFusion,43.2,69.23,43.2,100,62.4,0.68,text\\-to\\-video\\ R\\-at\\-10
12,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2019-06,HT-Pretrained,52.8,84.62,9.6,0.5,62.4,0.83,text\\-to\\-video\\ R\\-at\\-10
13,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2019-07,Collaborative Experts,62.4,100.0,9.6,0.5,62.4,0.98,text\\-to\\-video\\ R\\-at\\-10
14,1,Video Retrieval,MSVD - Video Retrieval benchmarking,2019-07,Collaborative Experts,63.8,100.0,63.8,100,63.8,1.0,text\\-to\\-video\\ R\\-at\\-10
15,1,Video Retrieval,DiDeMo - Video Retrieval benchmarking,2019-07,Collaborative Experts,54.4,100.0,54.4,100,54.4,0.85,text\\-to\\-video\\ R\\-at\\-10
16,1,Video Retrieval,ActivityNet - Video Retrieval benchmarking,2019-07,Collaborative Experts,63.9,100.0,63.9,100,63.9,1.0,text\\-to\\-video\\ R\\-at\\-10
0,1,Object Counting,CARPK - Object Counting benchmarking,2015-06,Faster R-CNN (2015),-39.88,589.07,-39.88,100,-6.77,1329.33,MAE
1,1,Object Counting,CARPK - Object Counting benchmarking,2016-09,One-Look Regression (2016),-21.88,323.19,18.0,0.5436,-6.77,729.33,MAE
2,1,Object Counting,CARPK - Object Counting benchmarking,2017-07,RetinaNet (2018),-16.62,245.49,5.26,0.1589,-6.77,554.0,MAE
3,1,Object Counting,CARPK - Object Counting benchmarking,2019-04,Soft-IoU + EM-Merger unit,-6.77,100.0,9.85,0.2975,-6.77,225.67,MAE
4,1,Crowd Counting,UCF-QNRF - Crowd Counting benchmarking,2015-11,Encoder-Decoder,-270.0,165.64,-270,100,-163,9000.0,MAE
5,1,Crowd Counting,UCF-QNRF - Crowd Counting benchmarking,2015-12,Resnet101,-190.0,116.56,80,0.7477,-163,6333.33,MAE
6,1,Crowd Counting,UCF-QNRF - Crowd Counting benchmarking,2016-08,Densenet201,-163.0,100.0,27,0.2523,-163,5433.33,MAE
7,1,Head Pose Estimation,AFLW2000 - Head Pose Estimation benchmarking,2015-11,3DDFA,-7.393,145.82,-7.393,100,-5.07,246.43,MAE
8,1,Head Pose Estimation,AFLW2000 - Head Pose Estimation benchmarking,2017-10,Multi-Loss ResNet50 (a=2),-6.155,121.4,1.238,0.5329,-5.07,205.17,MAE
9,1,Head Pose Estimation,AFLW2000 - Head Pose Estimation benchmarking,2019-01,Hybrid Coarse-Fine,-5.395,106.41,0.76,0.3272,-5.07,179.83,MAE
10,1,Head Pose Estimation,AFLW2000 - Head Pose Estimation benchmarking,2019-06,FSA-Net (Caps-Fusion),-5.07,100.0,0.325,0.1399,-5.07,169.0,MAE
11,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2016-03,DCL,-0.081,225.0,-0.081,100,-0.036,2.7,MAE
12,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2016-06,DHS,-0.065,180.56,0.016,0.3556,-0.036,2.17,MAE
13,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2016-11,DSS,-0.065,180.56,0.0,0.0,-0.036,2.17,MAE
14,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2017-04,MSR,-0.062,172.22,0.003,0.0667,-0.036,2.07,MAE
15,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2017-08,PiCANet,-0.05,138.89,0.012,0.2667,-0.036,1.67,MAE
16,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2018-06,DGRL,-0.049,136.11,0.001,0.0222,-0.036,1.63,MAE
17,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),-0.036,100.0,0.013,0.2889,-0.036,1.2,MAE
18,1,Age Estimation,MORPH Album2 - Age Estimation benchmarking,2016-11,"DLDL+VGG-Face (KL, Max)3",-2.42,100.0,-2.42,100,-2.42,80.67,MAE
19,1,Head Pose Estimation,AFLW - Head Pose Estimation benchmarking,2016-11,DLDL (KL),-9.78,240.89,-9.78,100,-4.06,326.0,MAE
20,1,Head Pose Estimation,AFLW - Head Pose Estimation benchmarking,2017-10,Ruiz et al.,-5.324,131.13,4.456,0.779,-4.06,177.47,MAE
21,1,Head Pose Estimation,AFLW - Head Pose Estimation benchmarking,2018-12,CNN + Heatmap,-4.06,100.0,1.264,0.221,-4.06,135.33,MAE
22,1,Age Estimation,ChaLearn 2015 - Age Estimation benchmarking,2016-11,DLDL+VGG-Face,-3.51,100.0,-3.51,100,-3.51,117.0,MAE
23,1,Head Pose Estimation,Pointing'04 - Head Pose Estimation benchmarking,2016-11,Ours DLDL (KL),-4.64,100.0,-4.64,100,-4.64,154.67,MAE
24,1,Head Pose Estimation,BJUT-3D - Head Pose Estimation benchmarking,2016-11,Ours DLDL (KL),-0.09,100.0,-0.09,100,-0.09,3.0,MAE
25,1,Formation Energy,QM9 - Formation Energy benchmarking,2017-02,HDAD+KRR,-0.58,239.67,-0.58,100,-0.242,19.33,MAE
26,1,Formation Energy,QM9 - Formation Energy benchmarking,2017-04,MPNN,-0.45,185.95,0.13,0.3846,-0.242,15.0,MAE
27,1,Formation Energy,QM9 - Formation Energy benchmarking,2017-06,SchNet,-0.31,128.1,0.14,0.4142,-0.242,10.33,MAE
28,1,Formation Energy,QM9 - Formation Energy benchmarking,2017-09,HIP-NN,-0.256,105.79,0.054,0.1598,-0.242,8.53,MAE
29,1,Formation Energy,QM9 - Formation Energy benchmarking,2018-06,SchNet-edge-update,-0.242,100.0,0.014,0.0414,-0.242,8.07,MAE
30,1,Crowd Counting,UCF CC 50 - Crowd Counting benchmarking,2017-07,Cascaded-MTL,-322.8,143.09,-322.8,100,-225.6,10760.0,MAE
31,1,Crowd Counting,UCF CC 50 - Crowd Counting benchmarking,2019-06,LSC-CNN,-225.6,100.0,97.2,1.0,-225.6,7520.0,MAE
32,1,Crowd Counting,ShanghaiTech B - Crowd Counting benchmarking,2017-07,Cascaded-MTL,-20.0,298.51,-20.0,100,-6.7,666.67,MAE
33,1,Crowd Counting,ShanghaiTech B - Crowd Counting benchmarking,2019-06,LSC-CNN,-8.1,120.9,11.9,0.8947,-6.7,270.0,MAE
34,1,Crowd Counting,ShanghaiTech B - Crowd Counting benchmarking,2019-08,S-DCNet,-6.7,100.0,1.4,0.1053,-6.7,223.33,MAE
35,1,Crowd Counting,ShanghaiTech A - Crowd Counting benchmarking,2017-07,Cascaded-MTL,-101.3,173.76,-101.3,100,-58.3,3376.67,MAE
36,1,Crowd Counting,ShanghaiTech A - Crowd Counting benchmarking,2019-06,LSC-CNN,-66.4,113.89,34.9,0.8116,-58.3,2213.33,MAE
37,1,Crowd Counting,ShanghaiTech A - Crowd Counting benchmarking,2019-08,S-DCNet,-58.3,100.0,8.1,0.1884,-58.3,1943.33,MAE
38,1,Saliency Detection,DUT-OMRON - Saliency Detection benchmarking,2017-08,UCF,-0.1203,290.58,-0.1203,100,-0.0414,4.01,MAE
39,1,Saliency Detection,DUT-OMRON - Saliency Detection benchmarking,2019-03,Pyramid Feature Attention,-0.0414,100.0,0.0789,1.0,-0.0414,1.38,MAE
40,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2017-08,SparseConvs,-481.0,231.25,-481.0,100,-208.0,16033.33,MAE
41,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-08,Spade-sD,-248.0,119.23,233.0,0.8535,-208.0,8266.67,MAE
42,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-08,Spade-RGBsD,-235.0,112.98,13.0,0.0476,-208.0,7833.33,MAE
43,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-11,NConv-CNN-L2,-233.0,112.02,2.0,0.0073,-208.0,7766.67,MAE
44,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-11,NConv-CNN-L1,-208.0,100.0,25.0,0.0916,-208.0,6933.33,MAE
45,1,Video Prediction,Human3.6M - Video Prediction benchmarking,2017-12,PredRNN,-1895.2,106.3,-1895.2,100,-1782.8,63173.33,MAE
46,1,Video Prediction,Human3.6M - Video Prediction benchmarking,2018-11,MIM,-1782.8,100.0,112.4,1.0,-1782.8,59426.67,MAE
47,1,Facial Beauty Prediction,SCUT-FBP - Facial Beauty Prediction benchmarking,2018-01,Combined Features + Gaussian Reg,-0.3931,151.48,-0.3931,100,-0.2595,13.1,MAE
48,1,Facial Beauty Prediction,SCUT-FBP - Facial Beauty Prediction benchmarking,2018-03,CNN features + Bayesian ridge regression,-0.2595,100.0,0.1336,1.0,-0.2595,8.65,MAE
49,1,Age Estimation,MORPH - Age Estimation benchmarking,2018-04,CMAAE-OR,-1.48,100.0,-1.48,100,-1.48,49.33,MAE
50,1,Age Estimation,FGNET - Age Estimation benchmarking,2018-04,CMAAE-OR,-3.62,122.71,-3.62,100,-2.95,120.67,MAE
51,1,Age Estimation,FGNET - Age Estimation benchmarking,2019-04,C3AE (WIKI-IMDB),-2.95,100.0,0.67,1.0,-2.95,98.33,MAE
52,1,RGB Salient Object Detection,SOD - RGB Salient Object Detection benchmarking,2018-06,BMPM,-0.108,105.88,-0.108,100,-0.102,3.6,MAE
53,1,RGB Salient Object Detection,SOD - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),-0.102,100.0,0.006,1.0,-0.102,3.4,MAE
54,1,RGB Salient Object Detection,PASCAL-S - RGB Salient Object Detection benchmarking,2018-06,BMPM,-0.074,113.85,-0.074,100,-0.065,2.47,MAE
55,1,RGB Salient Object Detection,PASCAL-S - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),-0.072,110.77,0.002,0.2222,-0.065,2.4,MAE
56,1,RGB Salient Object Detection,PASCAL-S - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),-0.065,100.0,0.007,0.7778,-0.065,2.17,MAE
57,1,Formation Energy,Materials Project - Formation Energy benchmarking,2018-06,SchNet,-31.8,140.09,-31.8,100,-22.7,1060.0,MAE
58,1,Formation Energy,Materials Project - Formation Energy benchmarking,2018-06,SchNet-edge-update,-22.7,100.0,9.1,1.0,-22.7,756.67,MAE
59,1,Steering Control,Comma.ai - Steering Control benchmarking,2018-11,FM-Net,-0.7048,100.0,-0.7048,100,-0.7048,23.49,MAE
60,1,Steering Control,Udacity - Steering Control benchmarking,2018-11,FM-Net,-1.6236,100.0,-1.6236,100,-1.6236,54.12,MAE
61,1,Saliency Detection,DUTS-test - Saliency Detection benchmarking,2019-03,Pyramid Feature Attention,-0.0405,100.0,-0.0405,100,-0.0405,1.35,MAE
62,1,Saliency Detection,PASCAL-S - Saliency Detection benchmarking,2019-03,Pyramid Feature Attention,-0.0677,100.0,-0.0677,100,-0.0677,2.26,MAE
63,1,Saliency Detection,HKU-IS - Saliency Detection benchmarking,2019-03,Pyramid Feature Attention,-0.0324,100.0,-0.0324,100,-0.0324,1.08,MAE
64,1,Saliency Detection,ECSSD - Saliency Detection benchmarking,2019-03,Pyramid Feature Attention,-0.0328,100.0,-0.0328,100,-0.0328,1.09,MAE
65,1,RGB Salient Object Detection,DUTS-test - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),-0.043,100.0,-0.043,100,-0.043,1.43,MAE
66,1,RGB Salient Object Detection,HKU-IS - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),-0.034,113.33,-0.034,100,-0.03,1.13,MAE
67,1,RGB Salient Object Detection,HKU-IS - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),-0.03,100.0,0.004,1.0,-0.03,1.0,MAE
68,1,Camouflaged Object Segmentation,COD - Camouflaged Object Segmentation benchmarking,2019-04,CPD,-0.059,100.0,-0.059,100,-0.059,1.97,MAE
69,1,RGB Salient Object Detection,ECSSD - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),-0.037,100.0,-0.037,100,-0.037,1.23,MAE
70,1,RGB Salient Object Detection,ECSSD - RGB Salient Object Detection benchmarking,2019-06,BASNet,-0.037,100.0,0.0,nan,-0.037,1.23,MAE
71,1,RGB Salient Object Detection,DUT-OMRON - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),-0.056,105.66,-0.056,100,-0.053,1.87,MAE
72,1,RGB Salient Object Detection,DUT-OMRON - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),-0.053,100.0,0.003,1.0,-0.053,1.77,MAE
73,1,Depth Completion,VOID - Depth Completion benchmarking,2019-05,VOICED,-85.05,100.0,-85.05,100,-85.05,2835.0,MAE
74,1,Formation Energy,OQMD v1.2 - Formation Energy benchmarking,2019-05,CGNN-192,-34.6,113.44,-34.6,100,-30.5,1153.33,MAE
75,1,Formation Energy,OQMD v1.2 - Formation Energy benchmarking,2019-05,CGNN Ensemble,-30.5,100.0,4.1,1.0,-30.5,1016.67,MAE
76,1,Salient Object Detection,DUTS-TE - Salient Object Detection benchmarking,2019-06,BASNET (ResNet-34),-0.047,100.0,-0.047,100,-0.047,1.57,MAE
77,1,Salient Object Detection,ECSSD - Salient Object Detection benchmarking,2019-06,BASNet (ResNet-34),-0.037,100.0,-0.037,100,-0.037,1.23,MAE
78,1,Camouflaged Object Segmentation,CAMO - Camouflaged Object Segmentation benchmarking,2019-06,BASNet,-0.159,100.0,-0.159,100,-0.159,5.3,MAE
79,1,Age Estimation,CACD - Age Estimation benchmarking,2019-08,RNDF,-4.6,100.0,-4.6,100,-4.6,153.33,MAE
0,1,Real-Time Object Detection,PASCAL VOC 2007 - Real-Time Object Detection benchmarking,2015-06,Faster R-CNN,7.0,15.22,7.0,100,46.0,0.07,FPS
1,1,Real-Time Object Detection,PASCAL VOC 2007 - Real-Time Object Detection benchmarking,2015-06,YOLO,46.0,100.0,39.0,1.0,46.0,0.44,FPS
2,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2016-12,AlphaPose,23.0,100.0,23,100,23,0.22,FPS
3,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2017-03,Mask R-CNN X-152-32x8d,3.0,3.61,3.0,100,83.0,0.03,FPS
4,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2018-04,YOLOv3-320,45.0,54.22,42.0,0.525,83.0,0.43,FPS
5,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2019-09,TTFNet,54.4,65.54,9.4,0.1175,83.0,0.52,FPS
6,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2019-11,CSPResNeXt50-PANet-SPP,58.0,69.88,3.6,0.045,83.0,0.55,FPS
7,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2020-04,YOLOv4-512,83.0,100.0,25.0,0.3125,83.0,0.79,FPS
8,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2018-12,Joint-candidate SPPE +,10.1,100.0,10.1,100,10.1,0.1,FPS
9,1,Hand Pose Estimation,ICVL Hands - Hand Pose Estimation benchmarking,2019-08,A2J,105.06,100.0,105.06,100,105.06,1.0,FPS
10,1,3D Pose Estimation,K2HPD - 3D Pose Estimation benchmarking,2019-08,A2J,93.78,100.0,93.78,100,93.78,0.89,FPS
11,1,Hand Pose Estimation,NYU Hands - Hand Pose Estimation benchmarking,2019-08,A2J,105.06,100.0,105.06,100,105.06,1.0,FPS
0,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2015-06,Faster-RCNN,0.01,1.8,0.01,100,0.556,0.0,AP75
1,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2016-12,YOLO9000opt,0.073,13.13,0.063,0.1154,0.556,0.0,AP75
2,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2017-08,RetinaNet,0.389,69.96,0.316,0.5788,0.556,0.0,AP75
3,1,Dense Object Detection,SKU-110K - Dense Object Detection benchmarking,2019-04,Soft-IoU + EM-Merger unit,0.556,100.0,0.167,0.3059,0.556,0.01,AP75
4,1,Object Detection,COCO test-dev - Object Detection benchmarking,2015-12,SSD512,30.3,51.79,30.3,100,58.5,0.36,AP75
5,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),43.4,74.19,13.1,0.4645,58.5,0.51,AP75
6,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-08,RetinaNet (ResNeXt-101-FPN),44.1,75.38,0.7,0.0248,58.5,0.52,AP75
7,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,RefineDet512+ (ResNet-101),45.7,78.12,1.6,0.0567,58.5,0.54,AP75
8,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (ResNet-101, multi-scale)",48.4,82.74,2.7,0.0957,58.5,0.57,AP75
9,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (DPN-98 with flip, multi-scale)",51.1,87.35,2.7,0.0957,58.5,0.6,AP75
10,1,Object Detection,COCO test-dev - Object Detection benchmarking,2018-03,"PANet (ResNeXt-101, multi-scale)",51.8,88.55,0.7,0.0248,58.5,0.61,AP75
11,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-01,"TridentNet (ResNet-101-Deformable, Image Pyramid)",53.5,91.45,1.7,0.0603,58.5,0.63,AP75
12,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-09,"Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)",58.5,100.0,5.0,0.1773,58.5,0.69,AP75
13,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,72.3,85.56,72.3,100,84.5,0.85,AP75
14,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,80.0,94.67,7.7,0.6311,84.5,0.95,AP75
15,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN+,80.9,95.74,0.9,0.0738,84.5,0.96,AP75
16,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base,81.1,95.98,0.2,0.0164,84.5,0.96,AP75
17,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,84.0,99.41,2.9,0.2377,84.5,0.99,AP75
18,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-02,HRNet*,84.5,100.0,0.5,0.041,84.5,1.0,AP75
19,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2016-11,CMU-Pose,67.5,87.44,67.5,100,77.2,0.8,AP75
20,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2017-01,G-RMI,71.3,92.36,3.8,0.3918,77.2,0.84,AP75
21,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2018-03,PersonLab,75.4,97.67,4.1,0.4227,77.2,0.89,AP75
22,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet (HR-Net-48),77.2,100.0,1.8,0.1856,77.2,0.91,AP75
23,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-11,CMU-Pose,67.5,79.79,67.5,100,84.6,0.8,AP75
24,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-12,RMPE,69.8,82.51,2.3,0.1345,84.6,0.83,AP75
25,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-12,RMPE++,79.1,93.5,9.3,0.5439,84.6,0.93,AP75
26,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2017-11,"CPN+ [6, 9]",80.9,95.63,1.8,0.1053,84.6,0.96,AP75
27,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-04,Flow-based (ResNet-152),81.1,95.86,0.2,0.0117,84.6,0.96,AP75
28,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-12,PoseFix,81.9,96.81,0.8,0.0468,84.6,0.97,AP75
29,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-01,MSPN,83.8,99.05,1.9,0.1111,84.6,0.99,AP75
30,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-02,HRNet-W48,84.5,99.88,0.7,0.0409,84.6,1.0,AP75
31,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-10,HRNet-W48+DARK,84.6,100.0,0.1,0.0058,84.6,1.0,AP75
32,1,Object Detection,COCO minival - Object Detection benchmarking,2016-12,FPN+,43.3,81.85,43.3,100,52.9,0.51,AP75
33,1,Object Detection,COCO minival - Object Detection benchmarking,2017-11,Mask R-CNN (ResNet-101 + 1 NL),44.5,84.12,1.2,0.125,52.9,0.53,AP75
34,1,Object Detection,COCO minival - Object Detection benchmarking,2017-11,Mask R-CNN (ResNeXt-152 + 1 NL),48.9,92.44,4.4,0.4583,52.9,0.58,AP75
35,1,Object Detection,COCO minival - Object Detection benchmarking,2018-11,Mask R-CNN (ResNeXt-152-FPN),51.1,96.6,2.2,0.2292,52.9,0.6,AP75
36,1,Object Detection,COCO minival - Object Detection benchmarking,2018-11,"Mask R-CNN (ResNeXt-152-FPN, cascade)",52.9,100.0,1.8,0.1875,52.9,0.63,AP75
37,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,75.2,91.04,75.2,100,82.6,0.89,AP75
38,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,78.9,95.52,3.7,0.5,82.6,0.93,AP75
39,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,80.8,97.82,1.9,0.2568,82.6,0.96,AP75
40,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,82.6,100.0,1.8,0.2432,82.6,0.98,AP75
41,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),39.4,83.12,39.4,100,47.4,0.47,AP75
42,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-08,"Cascade R-CNN (ResNet-101-FPN, map-guided)",42.9,90.51,3.5,0.4375,47.4,0.51,AP75
43,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-11,CenterMask + VoVNetV2-99 (multi-scale),47.4,100.0,4.5,0.5625,47.4,0.56,AP75
44,1,Video Instance Segmentation,YouTube-VIS validation - Video Instance Segmentation benchmarking,2018-02,OSMN,29.1,100.0,29.1,100,29.1,0.34,AP75
45,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT,25.4,86.99,25.4,100,29.2,0.3,AP75
46,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT-550 (ResNet-101-FPN),29.2,100.0,3.8,1.0,29.2,0.35,AP75
47,1,3D Object Detection,nuScenes-F - 3D Object Detection benchmarking,2019-05,RRPN + R101 - F,48.5,100.0,48.5,100,48.5,0.57,AP75
48,1,3D Object Detection,nuScenes-FB - 3D Object Detection benchmarking,2019-05,RRPN + R101 - FB,37.0,100.0,37,100,37,0.44,AP75
0,1,Gesture Recognition,Montalbano - Gesture Recognition benchmarking,2015-06,Temp Conv + LSTM,90.6,100.0,90.6,100,90.6,1.0,Jaccard\\ \\(Mean\\)
1,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-06,OFL,68.0,78.52,68.0,100,86.6,0.75,Jaccard\\ \\(Mean\\)
2,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,79.8,92.15,11.8,0.6344,86.6,0.88,Jaccard\\ \\(Mean\\)
3,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2017-06,OnAVOS,86.1,99.42,6.3,0.3387,86.6,0.95,Jaccard\\ \\(Mean\\)
4,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2019-08,RANet+ (online learning),86.6,100.0,0.5,0.0269,86.6,0.96,Jaccard\\ \\(Mean\\)
5,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2017-09,SFL,67.4,83.73,67.4,100,80.5,0.74,Jaccard\\ \\(Mean\\)
6,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2020-01,COSNet,80.5,100.0,13.1,1.0,80.5,0.89,Jaccard\\ \\(Mean\\)
7,1,Unsupervised Video Object Segmentation,FBMS - Unsupervised Video Object Segmentation benchmarking,2020-01,COSNet,75.6,100.0,75.6,100,75.6,0.83,Jaccard\\ \\(Mean\\)
0,1,Face Alignment,WFLW - Face Alignment benchmarking,2015-06,CFSS,0.366,61.9,0.366,100,0.5913,0.62,AUC\\-at\\-0\\.1\\ \\(all\\)
1,1,Face Alignment,WFLW - Face Alignment benchmarking,2017-07,DVLN,0.456,77.12,0.09,0.3995,0.5913,0.77,AUC\\-at\\-0\\.1\\ \\(all\\)
2,1,Face Alignment,WFLW - Face Alignment benchmarking,2017-11,Wing,0.554,93.69,0.098,0.435,0.5913,0.94,AUC\\-at\\-0\\.1\\ \\(all\\)
3,1,Face Alignment,WFLW - Face Alignment benchmarking,2019-02,3DDE (Inter-ocular Norm),0.5544,93.76,0.0004,0.0018,0.5913,0.94,AUC\\-at\\-0\\.1\\ \\(all\\)
4,1,Face Alignment,WFLW - Face Alignment benchmarking,2019-04,DeCaFA,0.563,95.21,0.0086,0.0382,0.5913,0.95,AUC\\-at\\-0\\.1\\ \\(all\\)
5,1,Face Alignment,WFLW - Face Alignment benchmarking,2019-06,GRegNet + LRefNet,0.584,98.77,0.021,0.0932,0.5913,0.99,AUC\\-at\\-0\\.1\\ \\(all\\)
6,1,Face Alignment,WFLW - Face Alignment benchmarking,2019-08,AVS,0.5913,100.0,0.0073,0.0324,0.5913,1.0,AUC\\-at\\-0\\.1\\ \\(all\\)
0,1,Face Alignment,WFLW - Face Alignment benchmarking,2015-06,CFSS,20.56,100.0,20.56,100,20.56,1.0,"FR\\-at\\-0\\.1\\(%,\\ all\\)"
0,1,Face Alignment,WFLW - Face Alignment benchmarking,2015-06,CFSS,9.07,83.67,9.07,100,10.84,0.84,"ME\\ \\(%,\\ all\\)"
1,1,Face Alignment,WFLW - Face Alignment benchmarking,2017-07,DVLN,10.84,100.0,1.77,1.0,10.84,1.0,"ME\\ \\(%,\\ all\\)"
0,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2015-06,DeepMatching*,5.84,15.81,5.84,100,36.94,0.16,Viewpoint\\ I\\ AEPE
1,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2016-11,SPyNet,36.94,100.0,31.1,1.0,36.94,1.0,Viewpoint\\ I\\ AEPE
0,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2015-06,DeepMatching*,4.63,9.09,4.63,100,50.92,0.09,Viewpoint\\ II\\ AEPE
1,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2016-11,SPyNet,50.92,100.0,46.29,1.0,50.92,1.0,Viewpoint\\ II\\ AEPE
0,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2015-06,DeepMatching*,12.43,22.9,12.43,100,54.29,0.23,Viewpoint\\ III\\ AEPE
1,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2016-11,SPyNet,54.29,100.0,41.86,1.0,54.29,1.0,Viewpoint\\ III\\ AEPE
0,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2015-06,DeepMatching*,12.17,19.44,12.17,100,62.6,0.19,Viewpoint\\ IV\\ AEPE
1,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2016-11,SPyNet,62.6,100.0,50.43,1.0,62.6,1.0,Viewpoint\\ IV\\ AEPE
0,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2015-06,DeepMatching*,22.55,31.07,22.55,100,72.57,0.31,Viewpoint\\ V\\ AEPE
1,1,Dense Pixel Correspondence Estimation,HPatches - Dense Pixel Correspondence Estimation benchmarking,2016-11,SPyNet,72.57,100.0,50.02,1.0,72.57,1.0,Viewpoint\\ V\\ AEPE
0,1,Human Pose Forecasting,Human3.6M - Human Pose Forecasting benchmarking,2015-08,ERD,1.78,100.0,1.78,100,1.78,1.0,"MAR,\\ walking,\\ 400ms"
0,1,Human Pose Forecasting,Human3.6M - Human Pose Forecasting benchmarking,2015-08,ERD,2.38,100.0,2.38,100,2.38,1.0,"MAR,\\ walking,\\ 1,000ms"
0,1,Color Image Denoising,Darmstadt Noise Dataset - Color Image Denoising benchmarking,2015-08,TNRD,0.8306,88.16,0.8306,100,0.9421,0.88,SSIM\\ \\(sRGB\\)
1,1,Color Image Denoising,Darmstadt Noise Dataset - Color Image Denoising benchmarking,2018-07,TWSC,0.9403,99.81,0.1097,0.9839,0.9421,1.0,SSIM\\ \\(sRGB\\)
2,1,Color Image Denoising,Darmstadt Noise Dataset - Color Image Denoising benchmarking,2018-07,CBDNet (Blind),0.9421,100.0,0.0018,0.0161,0.9421,1.0,SSIM\\ \\(sRGB\\)
3,1,Image Denoising,DND - Image Denoising benchmarking,2018-07,CBDNet,0.942,100.0,0.942,100,0.942,1.0,SSIM\\ \\(sRGB\\)
4,1,Image Denoising,SIDD - Image Denoising benchmarking,2018-07,CBDNet,0.801,100.0,0.801,100,0.801,0.85,SSIM\\ \\(sRGB\\)
0,1,Color Image Denoising,Darmstadt Noise Dataset - Color Image Denoising benchmarking,2015-08,TNRD,33.65,88.41,33.65,100,38.06,0.88,PSNR\\ \\(sRGB\\)
1,1,Color Image Denoising,Darmstadt Noise Dataset - Color Image Denoising benchmarking,2018-07,TWSC,37.94,99.68,4.29,0.9728,38.06,1.0,PSNR\\ \\(sRGB\\)
2,1,Color Image Denoising,Darmstadt Noise Dataset - Color Image Denoising benchmarking,2018-07,CBDNet (Blind),38.06,100.0,0.12,0.0272,38.06,1.0,PSNR\\ \\(sRGB\\)
3,1,Image Denoising,DND - Image Denoising benchmarking,2018-07,CBDNet,38.06,100.0,38.06,100,38.06,1.0,PSNR\\ \\(sRGB\\)
4,1,Image Denoising,SIDD - Image Denoising benchmarking,2018-07,CBDNet,30.78,100.0,30.78,100,30.78,0.81,PSNR\\ \\(sRGB\\)
0,1,3D Object Detection,SUN-RGBD val - 3D Object Detection benchmarking,2015-11,DSS,42.1,71.24,42.1,100,59.1,0.66,mAP\\-at\\-0\\.25
1,1,3D Object Detection,SUN-RGBD val - 3D Object Detection benchmarking,2016-06,COG,47.6,80.54,5.5,0.3235,59.1,0.74,mAP\\-at\\-0\\.25
2,1,3D Object Detection,SUN-RGBD val - 3D Object Detection benchmarking,2017-11,F-PointNet,54.0,91.37,6.4,0.3765,59.1,0.84,mAP\\-at\\-0\\.25
3,1,3D Object Detection,SUN-RGBD val - 3D Object Detection benchmarking,2019-04,VoteNet (Geo only),59.1,100.0,5.1,0.3,59.1,0.92,mAP\\-at\\-0\\.25
4,1,3D Object Detection,SUN-RGBD - 3D Object Detection benchmarking,2017-11,Frustum PointNets,54.0,85.17,54.0,100,63.4,0.84,mAP\\-at\\-0\\.25
5,1,3D Object Detection,SUN-RGBD - 3D Object Detection benchmarking,2020-01,ImVoteNet,63.4,100.0,9.4,1.0,63.4,0.99,mAP\\-at\\-0\\.25
6,1,3D Semantic Instance Segmentation,ScanNetV1 - 3D Semantic Instance Segmentation benchmarking,2017-11,SGPN,35.1,100.0,35.1,100,35.1,0.55,mAP\\-at\\-0\\.25
7,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2017-11,SGPN,20.7,32.24,20.7,100,64.2,0.32,mAP\\-at\\-0\\.25
8,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2018-12,GSPN,30.6,47.66,9.9,0.2276,64.2,0.48,mAP\\-at\\-0\\.25
9,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2018-12,3D-SIS,40.2,62.62,9.6,0.2207,64.2,0.63,mAP\\-at\\-0\\.25
10,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2019-04,VoteNet,58.6,91.28,18.4,0.423,64.2,0.91,mAP\\-at\\-0\\.25
11,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2020-03,3D-MPA,64.2,100.0,5.6,0.1287,64.2,1.0,mAP\\-at\\-0\\.25
0,1,Weakly Supervised Object Detection,COCO test-dev - Weakly Supervised Object Detection benchmarking,2015-11,WSDDN,11.5,84.56,11.5,100,13.6,0.12,AP50
1,1,Weakly Supervised Object Detection,COCO test-dev - Weakly Supervised Object Detection benchmarking,2016-11,WCCN,12.3,90.44,0.8,0.381,13.6,0.13,AP50
2,1,Weakly Supervised Object Detection,COCO test-dev - Weakly Supervised Object Detection benchmarking,2017-11,WSGARN+SSD,13.6,100.0,1.3,0.619,13.6,0.15,AP50
3,1,Object Detection,COCO test-dev - Object Detection benchmarking,2015-12,SSD512,48.5,67.45,48.5,100,71.9,0.52,AP50
4,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,DeformConv-R-FCN (Aligned-Inception-ResNet),58.0,80.67,9.5,0.406,71.9,0.62,AP50
5,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),62.3,86.65,4.3,0.1838,71.9,0.67,AP50
6,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,RefineDet512+ (ResNet-101),62.9,87.48,0.6,0.0256,71.9,0.67,AP50
7,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (ResNet-101, multi-scale)",65.5,91.1,2.6,0.1111,71.9,0.7,AP50
8,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (DPN-98 with flip, multi-scale)",67.3,93.6,1.8,0.0769,71.9,0.72,AP50
9,1,Object Detection,COCO test-dev - Object Detection benchmarking,2018-11,"DCNv2 (ResNet-101, multi-scale)",67.9,94.44,0.6,0.0256,71.9,0.73,AP50
10,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-01,"TridentNet (ResNet-101-Deformable, Image Pyramid)",69.7,96.94,1.8,0.0769,71.9,0.75,AP50
11,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-09,"Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)",71.9,100.0,2.2,0.094,71.9,0.77,AP50
12,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2015-12,MNC,44.3,66.92,44.3,100,66.2,0.47,AP50
13,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2016-11,FCIS  +OHEM,49.5,74.77,5.2,0.2374,66.2,0.53,AP50
14,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2016-11,FCIS+++  +OHEM,54.5,82.33,5.0,0.2283,66.2,0.58,AP50
15,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),60.0,90.63,5.5,0.2511,66.2,0.64,AP50
16,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-08,"Cascade R-CNN (ResNet-101-FPN, map-guided)",61.4,92.75,1.4,0.0639,66.2,0.66,AP50
17,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-11,CenterMask + VoVNetV2-99 (multi-scale),66.2,100.0,4.8,0.2192,66.2,0.71,AP50
18,1,Few-Shot Image Classification,CUB-200-2011 - 0-Shot - Few-Shot Image Classification benchmarking,2016-05,Word CNN-RNN (DS-SJE Embedding),48.7,100.0,48.7,100,48.7,0.52,AP50
19,1,Few-Shot Image Classification,Flowers-102 - 0-Shot - Few-Shot Image Classification benchmarking,2016-05,Word CNN-RNN (DS-SJE Embedding),59.6,100.0,59.6,100,59.6,0.64,AP50
20,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,86.8,92.93,86.8,100,93.4,0.93,AP50
21,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-03,Mask R-CNN,87.3,93.47,0.5,0.0758,93.4,0.93,AP50
22,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,91.4,97.86,4.1,0.6212,93.4,0.98,AP50
23,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN+,91.7,98.18,0.3,0.0455,93.4,0.98,AP50
24,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base,91.9,98.39,0.2,0.0303,93.4,0.98,AP50
25,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,92.4,98.93,0.5,0.0758,93.4,0.99,AP50
26,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-01,MSPN,93.4,100.0,1.0,0.1515,93.4,1.0,AP50
27,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-11,CMU-Pose,84.9,90.9,84.9,100,93.4,0.91,AP50
28,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-12,RMPE++,89.2,95.5,4.3,0.5059,93.4,0.96,AP50
29,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2017-11,"CPN+ [6, 9]",91.7,98.18,2.5,0.2941,93.4,0.98,AP50
30,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-04,Flow-based (ResNet-152),91.9,98.39,0.2,0.0235,93.4,0.98,AP50
31,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-01,MSPN,93.4,100.0,1.5,0.1765,93.4,1.0,AP50
32,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2016-11,CMU-Pose,84.9,95.07,84.9,100,89.3,0.91,AP50
33,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2017-01,G-RMI,85.5,95.74,0.6,0.1364,89.3,0.92,AP50
34,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2018-03,PersonLab,89.0,99.66,3.5,0.7955,89.3,0.95,AP50
35,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet (HR-Net-48),89.3,100.0,0.3,0.0682,89.3,0.96,AP50
36,1,Object Detection,COCO minival - Object Detection benchmarking,2016-12,FPN+,61.3,90.41,61.3,100,67.8,0.66,AP50
37,1,Object Detection,COCO minival - Object Detection benchmarking,2017-11,Mask R-CNN (ResNet-101 + 1 NL),63.1,93.07,1.8,0.2769,67.8,0.68,AP50
38,1,Object Detection,COCO minival - Object Detection benchmarking,2017-11,Mask R-CNN (ResNeXt-152 + 1 NL),67.8,100.0,4.7,0.7231,67.8,0.73,AP50
39,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,85.9,92.47,85.9,100,92.9,0.92,AP50
40,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-03,Mask R-CNN*,89.2,96.02,3.3,0.4714,92.9,0.96,AP50
41,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,90.5,97.42,1.3,0.1857,92.9,0.97,AP50
42,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,90.9,97.85,0.4,0.0571,92.9,0.97,AP50
43,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,92.9,100.0,2.0,0.2857,92.9,0.99,AP50
44,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT,42.0,90.13,42.0,100,46.6,0.45,AP50
45,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT-550 (ResNet-101-FPN),46.6,100.0,4.6,1.0,46.6,0.5,AP50
46,1,3D Object Detection,nuScenes-FB - 3D Object Detection benchmarking,2019-05,RRPN + R101 - FB,59.0,100.0,59,100,59,0.63,AP50
47,1,3D Object Detection,nuScenes-F - 3D Object Detection benchmarking,2019-05,RRPN + R101 - F,64.9,100.0,64.9,100,64.9,0.69,AP50
0,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,3.26,91.06,3.26,100,3.58,0.87,MOS
1,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,3.37,94.13,0.11,0.3437,3.58,0.9,MOS
2,1,Image Super-Resolution,Set5 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,3.58,100.0,0.21,0.6562,3.58,0.96,MOS
3,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,2.84,76.34,2.84,100,3.72,0.76,MOS
4,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRResNet,2.98,80.11,0.14,0.1591,3.72,0.8,MOS
5,1,Image Super-Resolution,Set14 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,3.72,100.0,0.74,0.8409,3.72,1.0,MOS
6,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2015-11,DRCN,2.12,59.55,2.12,100,3.56,0.57,MOS
7,1,Image Super-Resolution,BSD100 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,3.56,100.0,1.44,1.0,3.56,0.95,MOS
8,1,Face Alignment,CelebA Aligned - Face Alignment benchmarking,2019-08,Progressive Face SR,3.73,100.0,3.73,100,3.73,1.0,MOS
0,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2015-11,DCGAN,6.58,71.37,6.58,100,9.22,0.05,Inception\\ score
1,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2016-06,Improved GAN,8.09,87.74,1.51,0.572,9.22,0.06,Inception\\ score
2,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2016-10,AC-GAN,8.25,89.48,0.16,0.0606,9.22,0.07,Inception\\ score
3,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2016-12,SGAN,8.59,93.17,0.34,0.1288,9.22,0.07,Inception\\ score
4,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2017-03,WGAN-GP,8.67,94.03,0.08,0.0303,9.22,0.07,Inception\\ score
5,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2017-09,Splitting GAN,8.87,96.2,0.2,0.0758,9.22,0.07,Inception\\ score
6,1,Conditional Image Generation,CIFAR-10 - Conditional Image Generation benchmarking,2018-09,BigGAN,9.22,100.0,0.35,0.1326,9.22,0.07,Inception\\ score
7,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2016-06,ALI,5.34,57.92,5.34,100,9.22,0.04,Inception\\ score
8,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2016-06,Improved GAN,6.86,74.4,1.52,0.3918,9.22,0.06,Inception\\ score
9,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2017-02,CEGAN-Ent-VI,7.07,76.68,0.21,0.0541,9.22,0.06,Inception\\ score
10,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2017-03,LR-GAN,7.17,77.77,0.1,0.0258,9.22,0.06,Inception\\ score
11,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2017-03,WGAN-GP,7.86,85.25,0.69,0.1778,9.22,0.06,Inception\\ score
12,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2017-09,Splitting GAN,7.9,85.68,0.04,0.0103,9.22,0.06,Inception\\ score
13,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2017-10,PGGAN,8.8,95.44,0.9,0.232,9.22,0.07,Inception\\ score
14,1,Image Generation,CIFAR-10 - Image Generation benchmarking,2018-09,BigGAN,9.22,100.0,0.42,0.1082,9.22,0.07,Inception\\ score
15,1,Image Generation,Stanford Cars - Image Generation benchmarking,2016-06,InfoGAN,28.62,87.74,28.62,100,32.62,0.23,Inception\\ score
16,1,Image Generation,Stanford Cars - Image Generation benchmarking,2018-11,FineGAN,32.62,100.0,4.0,1.0,32.62,0.26,Inception\\ score
17,1,Image Generation,Stanford Dogs - Image Generation benchmarking,2016-06,InfoGAN,43.16,91.99,43.16,100,46.92,0.35,Inception\\ score
18,1,Image Generation,Stanford Dogs - Image Generation benchmarking,2018-11,FineGAN,46.92,100.0,3.76,1.0,46.92,0.38,Inception\\ score
19,1,Image Generation,CUB 128 x 128 - Image Generation benchmarking,2016-06,InfoGAN,47.32,90.08,47.32,100,52.53,0.38,Inception\\ score
20,1,Image Generation,CUB 128 x 128 - Image Generation benchmarking,2018-11,FineGAN,52.53,100.0,5.21,1.0,52.53,0.42,Inception\\ score
21,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2016-10,GAWWN,3.62,76.21,3.62,100,4.75,0.03,Inception\\ score
22,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2016-12,StackGAN,3.7,77.89,0.08,0.0708,4.75,0.03,Inception\\ score
23,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,3.82,80.42,0.12,0.1062,4.75,0.03,Inception\\ score
24,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2017-11,AttnGAN,4.36,91.79,0.54,0.4779,4.75,0.04,Inception\\ score
25,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2019-03,MirrorGAN,4.56,96.0,0.2,0.177,4.75,0.04,Inception\\ score
26,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2019-04,DM-GAN,4.75,100.0,0.19,0.1681,4.75,0.04,Inception\\ score
27,1,Conditional Image Generation,ImageNet 128x128 - Conditional Image Generation benchmarking,2016-10,AC-GAN,28.5,22.89,28.5,100,124.5,0.23,Inception\\ score
28,1,Conditional Image Generation,ImageNet 128x128 - Conditional Image Generation benchmarking,2018-02,Projection Discriminator,36.8,29.56,8.3,0.0865,124.5,0.3,Inception\\ score
29,1,Conditional Image Generation,ImageNet 128x128 - Conditional Image Generation benchmarking,2018-05,SAGAN,52.52,42.18,15.72,0.1638,124.5,0.42,Inception\\ score
30,1,Conditional Image Generation,ImageNet 128x128 - Conditional Image Generation benchmarking,2018-09,BigGAN-deep,124.5,100.0,71.98,0.7498,124.5,1.0,Inception\\ score
31,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2016-12,StackGAN,8.45,27.71,8.45,100,30.49,0.07,Inception\\ score
32,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2017-11,AttnGAN,25.89,84.91,17.44,0.7913,30.49,0.21,Inception\\ score
33,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-03,MirrorGAN,26.47,86.82,0.58,0.0263,30.49,0.21,Inception\\ score
34,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-04,DM-GAN,30.49,100.0,4.02,0.1824,30.49,0.24,Inception\\ score
35,1,Text-to-Image Generation,Oxford 102 Flowers - Text-to-Image Generation benchmarking,2016-12,StackGAN,3.2,98.16,3.2,100,3.26,0.03,Inception\\ score
36,1,Text-to-Image Generation,Oxford 102 Flowers - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,3.26,100.0,0.06,1.0,3.26,0.03,Inception\\ score
37,1,Image Generation,STL-10 - Image Generation benchmarking,2017-09,D2GAN,7.98,85.44,7.98,100,9.34,0.06,Inception\\ score
38,1,Image Generation,STL-10 - Image Generation benchmarking,2018-02,SN-GAN,9.1,97.43,1.12,0.8235,9.34,0.07,Inception\\ score
39,1,Image Generation,STL-10 - Image Generation benchmarking,2018-12,Improving MMD GAN,9.34,100.0,0.24,0.1765,9.34,0.08,Inception\\ score
0,1,Unsupervised Image Classification,SVHN - Unsupervised Image Classification benchmarking,2015-11,DEC,10.0,100.0,10,100,10,1.0,\\#\\ of\\ clusters\\ \\(k\\)
0,1,Unsupervised Image Classification,SVHN - Unsupervised Image Classification benchmarking,2015-11,DEC,11.9,15.49,11.9,100,76.8,0.12,Acc
1,1,Unsupervised Image Classification,SVHN - Unsupervised Image Classification benchmarking,2017-02,IMSAT,57.3,74.61,45.4,0.6995,76.8,0.58,Acc
2,1,Unsupervised Image Classification,SVHN - Unsupervised Image Classification benchmarking,2018-02,ACOL-GAR,76.8,100.0,19.5,0.3005,76.8,0.77,Acc
3,1,Retinal OCT Disease Classification,OCT2017 - Retinal OCT Disease Classification benchmarking,2015-12,InceptionV3,96.6,97.18,96.6,100,99.4,0.97,Acc
4,1,Retinal OCT Disease Classification,OCT2017 - Retinal OCT Disease Classification benchmarking,2015-12,ResNet50-v1,99.3,99.9,2.7,0.9643,99.4,1.0,Acc
5,1,Retinal OCT Disease Classification,OCT2017 - Retinal OCT Disease Classification benchmarking,2018-01,MobileNet-v2,99.4,100.0,0.1,0.0357,99.4,1.0,Acc
6,1,Retinal OCT Disease Classification,Srinivasan2014 - Retinal OCT Disease Classification benchmarking,2015-12,ResNet50-v1,94.92,97.39,94.92,100,97.46,0.95,Acc
7,1,Retinal OCT Disease Classification,Srinivasan2014 - Retinal OCT Disease Classification benchmarking,2018-01,MobileNet-v2,97.46,100.0,2.54,1.0,97.46,0.98,Acc
8,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-02,ST-CNN,60.6,75.94,60.6,100,79.8,0.61,Acc
9,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-11,ED-TCN,64.0,80.2,3.4,0.1771,79.8,0.64,Acc
10,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2018-06,TDRN,70.1,87.84,6.1,0.3177,79.8,0.71,Acc
11,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2019-03,MS-TCN,79.2,99.25,9.1,0.474,79.8,0.8,Acc
12,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2020-03,SSTDA,79.8,100.0,0.6,0.0313,79.8,0.8,Acc
13,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,13.0,79.27,13.0,100,16.4,0.13,Acc
14,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2019-04,DM-GAN,16.4,100.0,3.4,1.0,16.4,0.16,Acc
15,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2019-03,MS-TCN (IDT),65.1,92.74,65.1,100,70.2,0.65,Acc
16,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2019-03,MS-TCN (I3D),66.3,94.44,1.2,0.2353,70.2,0.67,Acc
17,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2020-03,SSTDA,70.2,100.0,3.9,0.7647,70.2,0.71,Acc
18,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2019-03,MS-TCN,80.7,97.0,80.7,100,83.2,0.81,Acc
19,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2020-03,SSTDA,83.2,100.0,2.5,1.0,83.2,0.84,Acc
0,1,Face Anti-Spoofing,Replay-Attack - Face Anti-Spoofing benchmarking,2015-11,YCbCr+HSV-LBP,2.9,100.0,2.9,100,2.9,1.0,HTER
0,1,Face Anti-Spoofing,MSU-MFSD - Face Anti-Spoofing benchmarking,2015-11,Color LBP,10.8,100.0,10.8,100,10.8,1.0,Equal\\ Error\\ Rate
0,1,Multi-Person Pose Estimation,WAF - Multi-Person Pose Estimation benchmarking,2015-11,DeepCut,86.5,98.18,86.5,100,88.1,0.98,AOP
1,1,Multi-Person Pose Estimation,WAF - Multi-Person Pose Estimation benchmarking,2016-05,DeeperCut,88.1,100.0,1.6,1.0,88.1,1.0,AOP
0,1,Semantic Segmentation,CamVid - Semantic Segmentation benchmarking,2015-11,ReSeg,88.7,96.94,88.7,100,91.5,0.97,Global\\ Accuracy
1,1,Semantic Segmentation,CamVid - Semantic Segmentation benchmarking,2016-11,FC-DenseNet103,91.5,100.0,2.8,1.0,91.5,1.0,Global\\ Accuracy
0,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2015-11,Yeung et. al.,17.1,44.07,17.1,100,38.8,0.21,mAP\\-at\\-0\\.5
1,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2016-01,Shou et. al.,19.0,48.97,1.9,0.0876,38.8,0.23,mAP\\-at\\-0\\.5
2,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,TURN,24.5,63.14,5.5,0.2535,38.8,0.3,mAP\\-at\\-0\\.5
3,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,Single-stream R-C3D  (one-way buffer),27.0,69.59,2.5,0.1152,38.8,0.33,mAP\\-at\\-0\\.5
4,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,Single-stream R-C3D  (two-way buffer),28.9,74.48,1.9,0.0876,38.8,0.35,mAP\\-at\\-0\\.5
5,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-04,SSN,29.8,76.8,0.9,0.0415,38.8,0.36,mAP\\-at\\-0\\.5
6,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2018-06,BSN,36.9,95.1,7.1,0.3272,38.8,0.45,mAP\\-at\\-0\\.5
7,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2018-11,MGG UNet,37.4,96.39,0.5,0.023,38.8,0.46,mAP\\-at\\-0\\.5
8,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2019-07,BMN,38.8,100.0,1.4,0.0645,38.8,0.47,mAP\\-at\\-0\\.5
9,1,Keypoint Detection,MPII Multi-Person - Keypoint Detection benchmarking,2016-05,DeeperCut,59.4,72.35,59.4,100,82.1,0.72,mAP\\-at\\-0\\.5
10,1,Keypoint Detection,MPII Multi-Person - Keypoint Detection benchmarking,2016-08,Local Joint-to-Person Association,62.2,75.76,2.8,0.1233,82.1,0.76,mAP\\-at\\-0\\.5
11,1,Keypoint Detection,MPII Multi-Person - Keypoint Detection benchmarking,2016-11,Associative Embedding,77.5,94.4,15.3,0.674,82.1,0.94,mAP\\-at\\-0\\.5
12,1,Keypoint Detection,MPII Multi-Person - Keypoint Detection benchmarking,2016-12,AlphaPose,82.1,100.0,4.6,0.2026,82.1,1.0,mAP\\-at\\-0\\.5
13,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2017-03,UntrimmedNets,13.7,50.74,13.7,100,27.0,0.17,mAP\\-at\\-0\\.5
14,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2017-12,STPN,16.9,62.59,3.2,0.2406,27.0,0.21,mAP\\-at\\-0\\.5
15,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2018-07,W-TALC,22.8,84.44,5.9,0.4436,27.0,0.28,mAP\\-at\\-0\\.5
16,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2019-06,CMCS,23.1,85.56,0.3,0.0226,27.0,0.28,mAP\\-at\\-0\\.5
17,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2019-08,3C-Net,26.6,98.52,3.5,0.2632,27.0,0.32,mAP\\-at\\-0\\.5
18,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2019-11,BaS-Net,27.0,100.0,0.4,0.0301,27.0,0.33,mAP\\-at\\-0\\.5
19,1,Instance Segmentation,NYU Depth v2 - Instance Segmentation benchmarking,2017-11,SGPN-CNN,30.5,100.0,30.5,100,30.5,0.37,mAP\\-at\\-0\\.5
20,1,Weakly Supervised Action Localization,ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking,2017-12,STPN,29.3,84.93,29.3,100,34.5,0.36,mAP\\-at\\-0\\.5
21,1,Weakly Supervised Action Localization,ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking,2019-05,MAAN,33.7,97.68,4.4,0.8462,34.5,0.41,mAP\\-at\\-0\\.5
22,1,Weakly Supervised Action Localization,ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking,2019-06,CMCS,34.0,98.55,0.3,0.0577,34.5,0.41,mAP\\-at\\-0\\.5
23,1,Weakly Supervised Action Localization,ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking,2019-11,BaS-Net,34.5,100.0,0.5,0.0962,34.5,0.42,mAP\\-at\\-0\\.5
24,1,Unsupervised Domain Adaptation,Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking,2018-03,DA-Faster,26.1,75.0,26.1,100,34.8,0.32,mAP\\-at\\-0\\.5
25,1,Unsupervised Domain Adaptation,Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking,2018-12,SWDA,34.8,100.0,8.7,1.0,34.8,0.42,mAP\\-at\\-0\\.5
26,1,Weakly Supervised Action Localization,ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking,2018-07,W-TALC,37.0,96.1,37.0,100,38.5,0.45,mAP\\-at\\-0\\.5
27,1,Weakly Supervised Action Localization,ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking,2019-08,3C-Net,37.2,96.62,0.2,0.1333,38.5,0.45,mAP\\-at\\-0\\.5
28,1,Weakly Supervised Action Localization,ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking,2019-11,BaS-Net,38.5,100.0,1.3,0.8667,38.5,0.47,mAP\\-at\\-0\\.5
29,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2018-12,GSPN,17.7,35.98,17.7,100,49.2,0.22,mAP\\-at\\-0\\.5
30,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2018-12,3D-SIS,22.5,45.73,4.8,0.1524,49.2,0.27,mAP\\-at\\-0\\.5
31,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2019-04,VoteNet,33.5,68.09,11.0,0.3492,49.2,0.41,mAP\\-at\\-0\\.5
32,1,3D Object Detection,ScanNetV2 - 3D Object Detection benchmarking,2020-03,3D-MPA,49.2,100.0,15.7,0.4984,49.2,0.6,mAP\\-at\\-0\\.5
33,1,Unsupervised Domain Adaptation,SIM10K to BDD100K - Unsupervised Domain Adaptation benchmarking,2018-12,SWDA,42.9,100.0,42.9,100,42.9,0.52,mAP\\-at\\-0\\.5
34,1,3D Instance Segmentation,SceneNN - 3D Instance Segmentation benchmarking,2019-04,MLS-CRF,12.1,25.69,12.1,100,47.1,0.15,mAP\\-at\\-0\\.5
35,1,3D Instance Segmentation,SceneNN - 3D Instance Segmentation benchmarking,2020-03,OccuSeg,47.1,100.0,35.0,1.0,47.1,0.57,mAP\\-at\\-0\\.5
36,1,3D Object Detection,SUN-RGBD val - 3D Object Detection benchmarking,2019-04,VoteNet (Geo only),35.8,100.0,35.8,100,35.8,0.44,mAP\\-at\\-0\\.5
37,1,Object Detection,BDD100K - Object Detection benchmarking,2019-09,hybrid incremental net,45.7,100.0,45.7,100,45.7,0.56,mAP\\-at\\-0\\.5
38,1,Object Detection,India Driving Dataset - Object Detection benchmarking,2019-09,hybrid incremental net,31.57,100.0,31.57,100,31.57,0.38,mAP\\-at\\-0\\.5
39,1,Weakly Supervised Action Localization,THUMOS’14 - Weakly Supervised Action Localization benchmarking,2019-11,BasNet,27.0,100.0,27,100,27,0.33,mAP\\-at\\-0\\.5
0,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2015-11,Yeung et. al.,48.9,74.09,48.9,100,66.0,0.74,mAP\\-at\\-0\\.1
1,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,Single-stream R-C3D  (one-way buffer),51.6,78.18,2.7,0.1579,66.0,0.78,mAP\\-at\\-0\\.1
2,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,Single-stream R-C3D  (two-way buffer),54.5,82.58,2.9,0.1696,66.0,0.83,mAP\\-at\\-0\\.1
3,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-04,SSN,66.0,100.0,11.5,0.6725,66.0,1.0,mAP\\-at\\-0\\.1
0,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2015-11,Yeung et. al.,44.0,74.07,44.0,100,59.4,0.74,mAP\\-at\\-0\\.2
1,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,Single-stream R-C3D  (one-way buffer),49.2,82.83,5.2,0.3377,59.4,0.83,mAP\\-at\\-0\\.2
2,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,Single-stream R-C3D  (two-way buffer),51.5,86.7,2.3,0.1494,59.4,0.87,mAP\\-at\\-0\\.2
3,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-04,SSN,59.4,100.0,7.9,0.513,59.4,1.0,mAP\\-at\\-0\\.2
0,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2015-11,Yeung et. al.,36.0,64.29,36.0,100,56.0,0.64,mAP\\-at\\-0\\.3
1,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2016-01,Shou et. al.,36.3,64.82,0.3,0.015,56.0,0.65,mAP\\-at\\-0\\.3
2,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,TURN,46.3,82.68,10.0,0.5,56.0,0.83,mAP\\-at\\-0\\.3
3,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-04,SSN,51.9,92.68,5.6,0.28,56.0,0.93,mAP\\-at\\-0\\.3
4,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2018-06,BSN,53.5,95.54,1.6,0.08,56.0,0.96,mAP\\-at\\-0\\.3
5,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2018-11,MGG UNet,53.9,96.25,0.4,0.02,56.0,0.96,mAP\\-at\\-0\\.3
6,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2019-07,BMN,56.0,100.0,2.1,0.105,56.0,1.0,mAP\\-at\\-0\\.3
0,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2015-11,Yeung et. al.,26.4,55.7,26.4,100,47.4,0.56,mAP\\-at\\-0\\.4
1,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2016-01,Shou et. al.,28.7,60.55,2.3,0.1095,47.4,0.61,mAP\\-at\\-0\\.4
2,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,TURN,35.3,74.47,6.6,0.3143,47.4,0.74,mAP\\-at\\-0\\.4
3,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-03,Single-stream R-C3D  (two-way buffer),35.6,75.11,0.3,0.0143,47.4,0.75,mAP\\-at\\-0\\.4
4,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2017-04,SSN,41.0,86.5,5.4,0.2571,47.4,0.86,mAP\\-at\\-0\\.4
5,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2018-06,BSN,45.0,94.94,4.0,0.1905,47.4,0.95,mAP\\-at\\-0\\.4
6,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2018-11,MGG UNet,46.8,98.73,1.8,0.0857,47.4,0.99,mAP\\-at\\-0\\.4
7,1,Action Recognition,THUMOS’14 - Action Recognition benchmarking,2019-07,BMN,47.4,100.0,0.6,0.0286,47.4,1.0,mAP\\-at\\-0\\.4
0,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2015-11,Yeung et al.,17.1,34.83,17.1,100,49.1,0.34,mAP\\ IOU\\-at\\-0\\.5
1,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2016-01,S-CNN,19.0,38.7,1.9,0.0594,49.1,0.38,mAP\\ IOU\\-at\\-0\\.5
2,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,CDC,23.3,47.45,4.3,0.1344,49.1,0.46,mAP\\ IOU\\-at\\-0\\.5
3,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,TURN-FL-16 + S-CNN,25.6,52.14,2.3,0.0719,49.1,0.51,mAP\\ IOU\\-at\\-0\\.5
4,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,R-C3D,28.9,58.86,3.3,0.1031,49.1,0.57,mAP\\ IOU\\-at\\-0\\.5
5,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-05,CBR-TS,31.0,63.14,2.1,0.0656,49.1,0.62,mAP\\ IOU\\-at\\-0\\.5
6,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2018-04,TAL-Net,42.8,87.17,11.8,0.3688,49.1,0.85,mAP\\ IOU\\-at\\-0\\.5
7,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-04,Decouple-SSAD,44.2,90.02,1.4,0.0438,49.1,0.88,mAP\\ IOU\\-at\\-0\\.5
8,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-09,P-GCN,49.1,100.0,4.9,0.1531,49.1,0.97,mAP\\ IOU\\-at\\-0\\.5
9,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2017-03,SSN,39.12,77.68,39.12,100,50.36,0.78,mAP\\ IOU\\-at\\-0\\.5
10,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2018-06,BSN,46.45,92.24,7.33,0.6521,50.36,0.92,mAP\\ IOU\\-at\\-0\\.5
11,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2019-07,BMN,50.07,99.42,3.62,0.3221,50.36,0.99,mAP\\ IOU\\-at\\-0\\.5
12,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2019-11,G-TAD,50.36,100.0,0.29,0.0258,50.36,1.0,mAP\\ IOU\\-at\\-0\\.5
13,1,Temporal Action Localization,ActivityNet-1.2 - Temporal Action Localization benchmarking,2020-01,DeepMetricLearner,35.2,100.0,35.2,100,35.2,0.7,mAP\\ IOU\\-at\\-0\\.5
0,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2015-11,Yeung et al.,48.9,70.36,48.9,100,69.5,0.7,mAP\\ IOU\\-at\\-0\\.1
1,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,TURN-FL-16 + S-CNN,54.0,77.7,5.1,0.2476,69.5,0.78,mAP\\ IOU\\-at\\-0\\.1
2,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,R-C3D,54.5,78.42,0.5,0.0243,69.5,0.78,mAP\\ IOU\\-at\\-0\\.1
3,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-05,CBR-TS,60.1,86.47,5.6,0.2718,69.5,0.86,mAP\\ IOU\\-at\\-0\\.1
4,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-09,P-GCN,69.5,100.0,9.4,0.4563,69.5,1.0,mAP\\ IOU\\-at\\-0\\.1
5,1,Temporal Action Localization,ActivityNet-1.2 - Temporal Action Localization benchmarking,2020-01,DeepMetricLearner,60.5,100.0,60.5,100,60.5,0.87,mAP\\ IOU\\-at\\-0\\.1
0,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2015-11,Yeung et al.,44.0,64.9,44.0,100,67.8,0.65,mAP\\ IOU\\-at\\-0\\.2
1,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,TURN-FL-16 + S-CNN,50.9,75.07,6.9,0.2899,67.8,0.75,mAP\\ IOU\\-at\\-0\\.2
2,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,R-C3D,51.5,75.96,0.6,0.0252,67.8,0.76,mAP\\ IOU\\-at\\-0\\.2
3,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-05,CBR-TS,56.7,83.63,5.2,0.2185,67.8,0.84,mAP\\ IOU\\-at\\-0\\.2
4,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2018-04,TAL-Net,57.1,84.22,0.4,0.0168,67.8,0.84,mAP\\ IOU\\-at\\-0\\.2
5,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-09,P-GCN,67.8,100.0,10.7,0.4496,67.8,1.0,mAP\\ IOU\\-at\\-0\\.2
0,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2015-11,Yeung et al.,36.0,56.6,36.0,100,63.6,0.57,mAP\\ IOU\\-at\\-0\\.3
1,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2016-01,S-CNN,36.3,57.08,0.3,0.0109,63.6,0.57,mAP\\ IOU\\-at\\-0\\.3
2,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,CDC,40.1,63.05,3.8,0.1377,63.6,0.63,mAP\\ IOU\\-at\\-0\\.3
3,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,TURN-FL-16 + S-CNN,44.1,69.34,4.0,0.1449,63.6,0.69,mAP\\ IOU\\-at\\-0\\.3
4,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,R-C3D,44.8,70.44,0.7,0.0254,63.6,0.7,mAP\\ IOU\\-at\\-0\\.3
5,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-05,CBR-TS,50.1,78.77,5.3,0.192,63.6,0.79,mAP\\ IOU\\-at\\-0\\.3
6,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2018-04,TAL-Net,53.2,83.65,3.1,0.1123,63.6,0.84,mAP\\ IOU\\-at\\-0\\.3
7,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2018-06,BSN UNet,53.5,84.12,0.3,0.0109,63.6,0.84,mAP\\ IOU\\-at\\-0\\.3
8,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-04,Decouple-SSAD,60.2,94.65,6.7,0.2428,63.6,0.95,mAP\\ IOU\\-at\\-0\\.3
9,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-09,P-GCN,63.6,100.0,3.4,0.1232,63.6,1.0,mAP\\ IOU\\-at\\-0\\.3
10,1,Temporal Action Localization,ActivityNet-1.2 - Temporal Action Localization benchmarking,2020-01,DeepMetricLearner,48.4,100.0,48.4,100,48.4,0.76,mAP\\ IOU\\-at\\-0\\.3
0,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2015-11,Yeung et al.,26.4,45.67,26.4,100,57.8,0.46,mAP\\ IOU\\-at\\-0\\.4
1,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2016-01,S-CNN,28.7,49.65,2.3,0.0732,57.8,0.5,mAP\\ IOU\\-at\\-0\\.4
2,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,CDC,29.4,50.87,0.7,0.0223,57.8,0.51,mAP\\ IOU\\-at\\-0\\.4
3,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,TURN-FL-16 + S-CNN,34.9,60.38,5.5,0.1752,57.8,0.6,mAP\\ IOU\\-at\\-0\\.4
4,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,R-C3D,35.6,61.59,0.7,0.0223,57.8,0.62,mAP\\ IOU\\-at\\-0\\.4
5,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-05,CBR-TS,41.3,71.45,5.7,0.1815,57.8,0.71,mAP\\ IOU\\-at\\-0\\.4
6,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2018-04,TAL-Net,48.5,83.91,7.2,0.2293,57.8,0.84,mAP\\ IOU\\-at\\-0\\.4
7,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-04,Decouple-SSAD,54.1,93.6,5.6,0.1783,57.8,0.94,mAP\\ IOU\\-at\\-0\\.4
8,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2019-09,P-GCN,57.8,100.0,3.7,0.1178,57.8,1.0,mAP\\ IOU\\-at\\-0\\.4
0,1,Head Pose Estimation,BIWI - Head Pose Estimation benchmarking,2015-11,3DDFA,19.068,100.0,19.068,100,19.068,1.0,MAE\\ \\(trained\\ with\\ other\\ data\\)
0,1,Face Alignment,AFLW2000-3D - Face Alignment benchmarking,2015-11,3DDFA + SDM,4.94,100.0,4.94,100,4.94,0.77,Mean\\ NME
1,1,3D Face Reconstruction,AFLW2000-3D - 3D Face Reconstruction benchmarking,2015-11,3DDFA,5.3695,95.11,5.3695,100,5.6454,0.84,Mean\\ NME
2,1,3D Face Reconstruction,AFLW2000-3D - 3D Face Reconstruction benchmarking,2017-09,DeFA,5.6454,100.0,0.2759,1.0,5.6454,0.88,Mean\\ NME
3,1,3D Face Reconstruction,Florence - 3D Face Reconstruction benchmarking,2015-11,3DDFA,6.3833,100.0,6.3833,100,6.3833,1.0,Mean\\ NME
4,1,Face Alignment,AFLW-Full - Face Alignment benchmarking,2017-03,Binary Face Alignment,2.85,100.0,2.85,100,2.85,0.45,Mean\\ NME
5,1,Face Alignment,AFLW-LFPA - Face Alignment benchmarking,2017-09,DeFA,3.86,100.0,3.86,100,3.86,0.6,Mean\\ NME
6,1,Facial Landmark Detection,AFLW-Full - Facial Landmark Detection benchmarking,2018-03,SAN,1.91,95.02,1.91,100,2.01,0.3,Mean\\ NME
7,1,Facial Landmark Detection,AFLW-Full - Facial Landmark Detection benchmarking,2019-02,"3DDE (Box height Norm, 19 landmarks - no earlobs)",2.01,100.0,0.1,1.0,2.01,0.31,Mean\\ NME
8,1,Facial Landmark Detection,AFLW-Front - Facial Landmark Detection benchmarking,2018-03,SAN,1.85,100.0,1.85,100,1.85,0.29,Mean\\ NME
9,1,Face Alignment,AFLW - Face Alignment benchmarking,2018-04,3DDFA,4.55,100.0,4.55,100,4.55,0.71,Mean\\ NME
0,1,Monocular 3D Human Pose Estimation,Human3.6M - Monocular 3D Human Pose Estimation benchmarking,2015-11,Sparseness Meets Deepness,300.0,100.0,300,100,300,1.0,Frames\\ Needed
0,1,Vehicle Pose Estimation,KITTI Cars Hard - Vehicle Pose Estimation benchmarking,2015-12,3DOP,76.52,97.25,76.52,100,78.68,0.97,Average\\ Orientation\\ Similarity
1,1,Vehicle Pose Estimation,KITTI Cars Hard - Vehicle Pose Estimation benchmarking,2016-04,SubCNN,78.68,100.0,2.16,1.0,78.68,1.0,Average\\ Orientation\\ Similarity
0,1,Retinal OCT Disease Classification,OCT2017 - Retinal OCT Disease Classification benchmarking,2015-12,InceptionV3,97.8,98.39,97.8,100,99.4,0.98,Sensitivity
1,1,Retinal OCT Disease Classification,OCT2017 - Retinal OCT Disease Classification benchmarking,2015-12,ResNet50-v1,99.3,99.9,1.5,0.9375,99.4,1.0,Sensitivity
2,1,Retinal OCT Disease Classification,OCT2017 - Retinal OCT Disease Classification benchmarking,2018-01,MobileNet-v2,99.4,100.0,0.1,0.0625,99.4,1.0,Sensitivity
0,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2015-12,STAPLE_CA,60.84,76.07,60.84,100,79.98,0.76,Normalized\\ Precision
1,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2016-11,ECO,62.14,77.69,1.3,0.0679,79.98,0.78,Normalized\\ Precision
2,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2018-11,ATOM,77.11,96.41,14.97,0.7821,79.98,0.96,Normalized\\ Precision
3,1,Visual Object Tracking,TrackingNet - Visual Object Tracking benchmarking,2018-12,SiamRPN++,79.98,100.0,2.87,0.1499,79.98,1.0,Normalized\\ Precision
0,1,Object Detection,COCO test-dev - Object Detection benchmarking,2015-12,SSD512,28.8,54.03,28.8,100,53.3,0.31,box\\ AP
1,1,Object Detection,COCO test-dev - Object Detection benchmarking,2015-12,"Faster R-CNN (box refinement, context, multi-scale testing)",34.9,65.48,6.1,0.249,53.3,0.37,box\\ AP
2,1,Object Detection,COCO test-dev - Object Detection benchmarking,2016-12,Faster R-CNN + FPN,36.2,67.92,1.3,0.0531,53.3,0.39,box\\ AP
3,1,Object Detection,COCO test-dev - Object Detection benchmarking,2016-12,Faster R-CNN + TDM,36.8,69.04,0.6,0.0245,53.3,0.39,box\\ AP
4,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,DeformConv-R-FCN (Aligned-Inception-ResNet),37.5,70.36,0.7,0.0286,53.3,0.4,box\\ AP
5,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,Mask R-CNN (ResNet-101-FPN),38.2,71.67,0.7,0.0286,53.3,0.41,box\\ AP
6,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),39.8,74.67,1.6,0.0653,53.3,0.42,box\\ AP
7,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-08,RetinaNet (ResNeXt-101-FPN),40.8,76.55,1.0,0.0408,53.3,0.43,box\\ AP
8,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,RefineDet512+ (ResNet-101),41.8,78.42,1.0,0.0408,53.3,0.44,box\\ AP
9,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (DPN-98 with flip, multi-scale)",45.7,85.74,3.9,0.1592,53.3,0.49,box\\ AP
10,1,Object Detection,COCO test-dev - Object Detection benchmarking,2018-03,"PANet (ResNeXt-101, multi-scale)",47.4,88.93,1.7,0.0694,53.3,0.5,box\\ AP
11,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-01,"TridentNet (ResNet-101-Deformable, Image Pyramid)",48.4,90.81,1.0,0.0408,53.3,0.51,box\\ AP
12,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-06,"NAS-FPN (AmoebaNet-D, learned aug)",50.7,95.12,2.3,0.0939,53.3,0.54,box\\ AP
13,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-09,"Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)",53.3,100.0,2.6,0.1061,53.3,0.57,box\\ AP
14,1,Object Detection,COCO minival - Object Detection benchmarking,2016-12,FPN+,39.8,81.89,39.8,100,48.6,0.42,box\\ AP
15,1,Object Detection,COCO minival - Object Detection benchmarking,2017-03,Mask R-CNN (ResNet-101-FPN),40.0,82.3,0.2,0.0227,48.6,0.43,box\\ AP
16,1,Object Detection,COCO minival - Object Detection benchmarking,2017-11,Mask R-CNN (ResNet-101 + 1 NL),40.8,83.95,0.8,0.0909,48.6,0.43,box\\ AP
17,1,Object Detection,COCO minival - Object Detection benchmarking,2017-11,Mask R-CNN (ResNeXt-152 + 1 NL),45.0,92.59,4.2,0.4773,48.6,0.48,box\\ AP
18,1,Object Detection,COCO minival - Object Detection benchmarking,2018-11,Mask R-CNN (ResNeXt-152-FPN),46.4,95.47,1.4,0.1591,48.6,0.49,box\\ AP
19,1,Object Detection,COCO minival - Object Detection benchmarking,2018-11,"Mask R-CNN (ResNet-101-FPN, GN, Cascade)",47.4,97.53,1.0,0.1136,48.6,0.5,box\\ AP
20,1,Object Detection,COCO minival - Object Detection benchmarking,2018-11,"Mask R-CNN (ResNeXt-152-FPN, cascade)",48.6,100.0,1.2,0.1364,48.6,0.52,box\\ AP
21,1,3D Shape Modeling,Pix3D S1 - 3D Shape Modeling benchmarking,2019-06,Mesh R-CNN,94.0,100.0,94,100,94,1.0,box\\ AP
22,1,3D Shape Modeling,Pix3D S2 - 3D Shape Modeling benchmarking,2019-06,Mesh R-CNN,72.2,100.0,72.2,100,72.2,0.77,box\\ AP
0,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2015-12,ResNet,63.5,99.37,63.5,100,63.9,0.99,Backpack
1,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2016-08,DenseNet,63.9,100.0,0.4,1.0,63.9,1.0,Backpack
0,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2015-12,ResNet,49.7,91.03,49.7,100,54.6,0.91,LCC
1,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2016-08,DenseNet,54.6,100.0,4.9,1.0,54.6,1.0,LCC
0,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2015-12,ResNet,-69.3,100.58,-69.3,100,-68.9,1.01,LCS
1,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2016-08,DenseNet,-68.9,100.0,0.4,1.0,-68.9,1.0,LCS
0,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2015-12,ResNet,44.4,89.16,44.4,100,49.8,0.89,UCC
1,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2016-08,DenseNet,49.8,100.0,5.4,1.0,49.8,1.0,UCC
0,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2015-12,ResNet,68.9,94.38,68.9,100,73.0,0.94,UCS
1,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2016-08,DenseNet,73.0,100.0,4.1,1.0,73.0,1.0,UCS
0,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2015-12,ResNet,74.7,99.6,74.7,100,75.0,1.0,Gender
1,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2016-08,DenseNet,75.0,100.0,0.3,1.0,75.0,1.0,Gender
0,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2015-12,ResNet,65.2,97.02,65.2,100,67.2,0.97,Hat
1,1,Pedestrian Attribute Recognition,UAV-Human - Pedestrian Attribute Recognition benchmarking,2016-08,DenseNet,67.2,100.0,2.0,1.0,67.2,1.0,Hat
0,1,Domain Generalization,ImageNet-A - Domain Generalization benchmarking,2015-12,ResNet-50 (300 Epochs),4.2,57.53,4.2,100,7.3,0.08,Top\\-1\\ accuracy\\ %
1,1,Domain Generalization,ImageNet-A - Domain Generalization benchmarking,2017-08,Cutout (ResNet-50),4.4,60.27,0.2,0.0645,7.3,0.08,Top\\-1\\ accuracy\\ %
2,1,Domain Generalization,ImageNet-A - Domain Generalization benchmarking,2017-10,Mixup (ResNet-50),6.6,90.41,2.2,0.7097,7.3,0.13,Top\\-1\\ accuracy\\ %
3,1,Domain Generalization,ImageNet-A - Domain Generalization benchmarking,2019-05,CutMix (ResNet-50),7.3,100.0,0.7,0.2258,7.3,0.14,Top\\-1\\ accuracy\\ %
4,1,Compositional Zero-Shot Learning,UT-Zappos - Compositional Zero-Shot Learning benchmarking,2020-04,SymNet,52.1,100.0,52.1,100,52.1,1.0,Top\\-1\\ accuracy\\ %
5,1,Compositional Zero-Shot Learning,MIT-States - Compositional Zero-Shot Learning benchmarking,2020-04,SymNet,19.9,100.0,19.9,100,19.9,0.38,Top\\-1\\ accuracy\\ %
0,1,Domain Generalization,ImageNet-R - Domain Generalization benchmarking,2015-12,ResNet-50,63.9,100.0,63.9,100,63.9,0.95,Top\\-1\\ Error\\ Rate
1,1,Weakly-Supervised Object Localization,ILSVRC 2015 - Weakly-Supervised Object Localization benchmarking,2015-12,AlexNet-GAP,67.19,100.0,67.19,100,67.19,1.0,Top\\-1\\ Error\\ Rate
2,1,Fine-Grained Image Classification,FGVC Aircraft - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,7.33,100.0,7.33,100,7.33,0.11,Top\\-1\\ Error\\ Rate
3,1,Fine-Grained Image Classification,Oxford-IIIT Pets - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,11.02,100.0,11.02,100,11.02,0.16,Top\\-1\\ Error\\ Rate
4,1,Fine-Grained Image Classification,Oxford 102 Flowers - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,4.64,100.0,4.64,100,4.64,0.07,Top\\-1\\ Error\\ Rate
5,1,Fine-Grained Image Classification,Caltech-101 - Fine-Grained Image Classification benchmarking,2018-05,AutoAugment,13.07,100.0,13.07,100,13.07,0.19,Top\\-1\\ Error\\ Rate
6,1,Weakly-Supervised Object Localization,CUB-200-2011 - Weakly-Supervised Object Localization benchmarking,2018-07,SPG,53.36,100.0,53.36,100,53.36,0.79,Top\\-1\\ Error\\ Rate
7,1,Image Recognition,ImageNet - Image Recognition benchmarking,2020-03,LIO/ResNet-50 (multi-stage),22.87,100.0,22.87,100,22.87,0.34,Top\\-1\\ Error\\ Rate
0,1,Weakly-Supervised Object Localization,Tiny ImageNet - Weakly-Supervised Object Localization benchmarking,2015-12,CAM,40.55,100.0,40.55,100,40.55,1.0,Top\\-1\\ Localization\\ Accuracy
0,1,Weakly-Supervised Object Localization,ILSVRC 2016 - Weakly-Supervised Object Localization benchmarking,2015-12,VGGnet-GAP,45.14,86.54,45.14,100,52.16,0.87,Top\\-5\\ Error
1,1,Weakly-Supervised Object Localization,ILSVRC 2016 - Weakly-Supervised Object Localization benchmarking,2015-12,AlexNet-GAP,52.16,100.0,7.02,1.0,52.16,1.0,Top\\-5\\ Error
2,1,Weakly-Supervised Object Localization,CUB-200-2011 - Weakly-Supervised Object Localization benchmarking,2018-07,SPG,42.28,100.0,42.28,100,42.28,0.81,Top\\-5\\ Error
0,1,Multi-Human Parsing,PASCAL-Part - Multi-Human Parsing benchmarking,2015-12,MNC,38.8,64.99,38.8,100,59.7,0.65,AP\\ 0\\.5
1,1,Multi-Human Parsing,PASCAL-Part - Multi-Human Parsing benchmarking,2017-09,Holistic instance-level,40.6,68.01,1.8,0.0861,59.7,0.68,AP\\ 0\\.5
2,1,Multi-Human Parsing,PASCAL-Part - Multi-Human Parsing benchmarking,2018-04,NAN,59.7,100.0,19.1,0.9139,59.7,1.0,AP\\ 0\\.5
3,1,Multi-Human Parsing,MHP v2.0 - Multi-Human Parsing benchmarking,2017-03,Mask R-CNN,14.9,59.27,14.9,100,25.14,0.25,AP\\ 0\\.5
4,1,Multi-Human Parsing,MHP v2.0 - Multi-Human Parsing benchmarking,2017-05,MH-Parser,17.99,71.56,3.09,0.3018,25.14,0.3,AP\\ 0\\.5
5,1,Multi-Human Parsing,MHP v2.0 - Multi-Human Parsing benchmarking,2018-04,NAN,25.14,100.0,7.15,0.6982,25.14,0.42,AP\\ 0\\.5
6,1,Multi-Human Parsing,MHP v1.0 - Multi-Human Parsing benchmarking,2017-03,Mask R-CNN,52.68,92.28,52.68,100,57.09,0.88,AP\\ 0\\.5
7,1,Multi-Human Parsing,MHP v1.0 - Multi-Human Parsing benchmarking,2018-04,NAN,57.09,100.0,4.41,1.0,57.09,0.96,AP\\ 0\\.5
8,1,One-Shot Object Detection,COCO - One-Shot Object Detection benchmarking,2018-11,Siamese Mask R-CNN,16.3,74.09,16.3,100,22.0,0.27,AP\\ 0\\.5
9,1,One-Shot Object Detection,COCO - One-Shot Object Detection benchmarking,2019-11,One-Shot Object Detection,22.0,100.0,5.7,1.0,22.0,0.37,AP\\ 0\\.5
10,1,One-Shot Instance Segmentation,COCO - One-Shot Instance Segmentation benchmarking,2018-11,Siamese Mask R-CNN,14.5,100.0,14.5,100,14.5,0.24,AP\\ 0\\.5
0,1,Person Identification,BioEye - Person Identification benchmarking,2016-01,RBFN,98.69,100.0,98.69,100,98.69,1.0,R1
0,1,Image Generation,Binarized MNIST - Image Generation benchmarking,2016-01,PixelCNN,-81.3,102.65,-81.3,100,-79.2,1.03,nats
1,1,Image Generation,Binarized MNIST - Image Generation benchmarking,2016-01,PixelRNN,-79.2,100.0,2.1,1.0,-79.2,1.0,nats
0,1,Image Generation,ImageNet 32x32 - Image Generation benchmarking,2016-01,PixelRNN,-3.86,102.39,-3.86,100,-3.77,6.33,bpd
1,1,Image Generation,ImageNet 32x32 - Image Generation benchmarking,2016-06,Gated PixelCNN,-3.83,101.59,0.03,0.3333,-3.77,6.28,bpd
2,1,Image Generation,ImageNet 32x32 - Image Generation benchmarking,2018-02,Image Transformer,-3.77,100.0,0.06,0.6667,-3.77,6.18,bpd
3,1,Image Generation,CelebA 256x256 - Image Generation benchmarking,2018-07,"Glow (Kingma and Dhariwal, 2018)",-1.03,168.85,-1.03,100,-0.61,1.69,bpd
4,1,Image Generation,CelebA 256x256 - Image Generation benchmarking,2018-12,SPN Menick and Kalchbrenner (2019),-0.61,100.0,0.42,1.0,-0.61,1.0,bpd
0,1,Pose Estimation,FLIC Elbows - Pose Estimation benchmarking,2016-01,Convolutional Pose Machines,97.59,98.58,97.59,100,99.0,0.99,PCK\\-at\\-0\\.2
1,1,Pose Estimation,FLIC Elbows - Pose Estimation benchmarking,2016-03,Stacked Hourglass Networks,99.0,100.0,1.41,1.0,99.0,1.0,PCK\\-at\\-0\\.2
2,1,Pose Estimation,FLIC Wrists - Pose Estimation benchmarking,2016-01,Convolutional Pose Machines,95.03,97.97,95.03,100,97.0,0.96,PCK\\-at\\-0\\.2
3,1,Pose Estimation,FLIC Wrists - Pose Estimation benchmarking,2016-03,Stacked Hourglass Networks,97.0,100.0,1.97,1.0,97.0,0.98,PCK\\-at\\-0\\.2
4,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2016-12,FlowNet2,62.9,80.54,62.9,100,78.1,0.64,PCK\\-at\\-0\\.2
5,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2018-06,ColorPointer,69.6,89.12,6.7,0.4408,78.1,0.7,PCK\\-at\\-0\\.2
6,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2019-04,mgPFF+ft 1st,78.1,100.0,8.5,0.5592,78.1,0.79,PCK\\-at\\-0\\.2
0,1,Pose Estimation,J-HMDB - Pose Estimation benchmarking,2016-01,CPM,91.9,98.18,91.9,100,93.6,0.93,Mean\\ PCK\\-at\\-0\\.2
1,1,Pose Estimation,J-HMDB - Pose Estimation benchmarking,2017-12,LSTM PM,93.6,100.0,1.7,1.0,93.6,0.94,Mean\\ PCK\\-at\\-0\\.2
2,1,Pose Estimation,UPenn Action - Pose Estimation benchmarking,2017-12,LSTM PM,97.7,98.39,97.7,100,99.3,0.98,Mean\\ PCK\\-at\\-0\\.2
3,1,Pose Estimation,UPenn Action - Pose Estimation benchmarking,2020-01,UniPose-LSTM,99.3,100.0,1.6,1.0,99.3,1.0,Mean\\ PCK\\-at\\-0\\.2
0,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-02,ST-CNN,41.9,53.72,41.9,100,78.0,0.54,F1@50%
1,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-11,ED-TCN,56.0,71.79,14.1,0.3906,78.0,0.72,F1@50%
2,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2018-06,TDRN,62.7,80.38,6.7,0.1856,78.0,0.8,F1@50%
3,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2019-03,MS-TCN,74.6,95.64,11.9,0.3296,78.0,0.96,F1@50%
4,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2020-03,SSTDA,78.0,100.0,3.4,0.0942,78.0,1.0,F1@50%
5,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2019-03,MS-TCN (IDT),40.8,73.91,40.8,100,55.2,0.52,F1@50%
6,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2020-03,SSTDA,55.2,100.0,14.4,1.0,55.2,0.71,F1@50%
7,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2019-03,MS-TCN,64.5,87.4,64.5,100,73.8,0.83,F1@50%
8,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2020-03,SSTDA,73.8,100.0,9.3,1.0,73.8,0.95,F1@50%
0,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-02,ST-CNN,54.4,61.05,54.4,100,89.1,0.61,F1@25%
1,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-11,ED-TCN,69.3,77.78,14.9,0.4294,89.1,0.78,F1@25%
2,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2018-06,TDRN,74.4,83.5,5.1,0.147,89.1,0.84,F1@25%
3,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2019-03,MS-TCN,85.4,95.85,11.0,0.317,89.1,0.96,F1@25%
4,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2020-03,SSTDA,89.1,100.0,3.7,0.1066,89.1,1.0,F1@25%
5,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2019-03,MS-TCN (IDT),52.9,76.56,52.9,100,69.1,0.59,F1@25%
6,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2020-03,SSTDA,69.1,100.0,16.2,1.0,69.1,0.78,F1@25%
7,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2019-03,MS-TCN,74.0,90.8,74.0,100,81.5,0.83,F1@25%
8,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2020-03,SSTDA,81.5,100.0,7.5,1.0,81.5,0.91,F1@25%
0,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-02,ST-CNN,58.7,65.22,58.7,100,90.0,0.65,F1@10%
1,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2016-11,ED-TCN,72.2,80.22,13.5,0.4313,90.0,0.8,F1@10%
2,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2018-06,TDRN,79.2,88.0,7.0,0.2236,90.0,0.88,F1@10%
3,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2019-03,MS-TCN,87.5,97.22,8.3,0.2652,90.0,0.97,F1@10%
4,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2020-03,SSTDA,90.0,100.0,2.5,0.0799,90.0,1.0,F1@10%
5,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2019-03,MS-TCN (IDT),58.2,77.6,58.2,100,75.0,0.65,F1@10%
6,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2020-03,SSTDA,75.0,100.0,16.8,1.0,75.0,0.83,F1@10%
7,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2019-03,MS-TCN,76.3,91.93,76.3,100,83.0,0.85,F1@10%
8,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2020-03,SSTDA,83.0,100.0,6.7,1.0,83.0,0.92,F1@10%
0,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2016-03,DCL,0.786,88.12,0.786,100,0.892,0.01,F\\-measure
1,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2016-06,DHS,0.815,91.37,0.029,0.2736,0.892,0.01,F\\-measure
2,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2017-04,MSR,0.824,92.38,0.009,0.0849,0.892,0.01,F\\-measure
3,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2017-08,PiCANet,0.863,96.75,0.039,0.3679,0.892,0.01,F\\-measure
4,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),0.892,100.0,0.029,0.2736,0.892,0.01,F\\-measure
5,1,RGB Salient Object Detection,HKU-IS - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),0.891,95.29,0.891,100,0.935,0.01,F\\-measure
6,1,RGB Salient Object Detection,HKU-IS - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),0.935,100.0,0.044,1.0,0.935,0.01,F\\-measure
7,1,RGB Salient Object Detection,DUTS-test - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),80.5,100.0,80.5,100,80.5,1.0,F\\-measure
8,1,RGB Salient Object Detection,PASCAL-S - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),0.824,93.64,0.824,100,0.88,0.01,F\\-measure
9,1,RGB Salient Object Detection,PASCAL-S - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),0.88,100.0,0.056,1.0,0.88,0.01,F\\-measure
10,1,RGB Salient Object Detection,ECSSD - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),0.917,97.04,0.917,100,0.945,0.01,F\\-measure
11,1,RGB Salient Object Detection,ECSSD - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),0.945,100.0,0.028,1.0,0.945,0.01,F\\-measure
12,1,RGB Salient Object Detection,DUT-OMRON - RGB Salient Object Detection benchmarking,2019-04,CPD-R (ResNet50),0.747,89.68,0.747,100,0.833,0.01,F\\-measure
13,1,RGB Salient Object Detection,DUT-OMRON - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),0.833,100.0,0.086,1.0,0.833,0.01,F\\-measure
14,1,RGB Salient Object Detection,SOD - RGB Salient Object Detection benchmarking,2019-04,PoolNet (VGG-16),0.882,100.0,0.882,100,0.882,0.01,F\\-measure
0,1,Pose Estimation,ITOP top-view - Pose Estimation benchmarking,2016-03,Multi-task learning + viewpoint invariance,75.5,90.48,75.5,100,83.44,0.02,Mean\\ mAP
1,1,Pose Estimation,ITOP top-view - Pose Estimation benchmarking,2017-11,V2V-PoseNet,83.44,100.0,7.94,1.0,83.44,0.03,Mean\\ mAP
2,1,Pose Estimation,ITOP front-view - Pose Estimation benchmarking,2016-03,Multi-task learning + viewpoint invariance,77.4,87.22,77.4,100,88.74,0.02,Mean\\ mAP
3,1,Pose Estimation,ITOP front-view - Pose Estimation benchmarking,2017-07,REN,84.9,95.67,7.5,0.6614,88.74,0.03,Mean\\ mAP
4,1,Pose Estimation,ITOP front-view - Pose Estimation benchmarking,2017-11,V2V-PoseNet,88.74,100.0,3.84,0.3386,88.74,0.03,Mean\\ mAP
5,1,Multi-Person Pose Estimation,Multi-Person PoseTrack - Multi-Person Pose Estimation benchmarking,2016-11,PoseTrack,38.2,100.0,38.2,100,38.2,0.01,Mean\\ mAP
6,1,Multi-Person Pose Estimation,PoseTrack2017 - Multi-Person Pose Estimation benchmarking,2017-10,PoseTrack,59.4,76.21,59.4,100,77.94,0.02,Mean\\ mAP
7,1,Multi-Person Pose Estimation,PoseTrack2017 - Multi-Person Pose Estimation benchmarking,2019-06,PoseWarper,77.9,99.95,18.5,0.9978,77.94,0.02,Mean\\ mAP
8,1,Multi-Person Pose Estimation,PoseTrack2017 - Multi-Person Pose Estimation benchmarking,2019-06,PoseWarper,77.94,100.0,0.04,0.0022,77.94,0.02,Mean\\ mAP
9,1,Image Retrieval,INRIA Holidays - Image Retrieval benchmarking,2019-02,MultiGrain R50 @ 500,91.8,99.24,91.8,100,92.5,0.03,Mean\\ mAP
10,1,Image Retrieval,INRIA Holidays - Image Retrieval benchmarking,2019-02,MultiGrain R50 @ 800,92.5,100.0,0.7,1.0,92.5,0.03,Mean\\ mAP
11,1,Multi-Person Pose Estimation,PoseTrack2018 - Multi-Person Pose Estimation benchmarking,2019-06,PoseWarper,78.0,100.0,78,100,78,0.02,Mean\\ mAP
12,1,Weakly Supervised Action Localization,ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking,2019-08,3C-Net,21.7,100.0,21.7,100,21.7,0.01,Mean\\ mAP
13,1,Object Detection,COCO 2017 - Object Detection benchmarking,2019-12,retinanet,3153.0,100.0,3153,100,3153,1.0,Mean\\ mAP
0,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2016-03,FnsNet,-25.9102,229.01,-25.9102,100,-11.3141,2.29,Hausdorff
1,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2016-11,Pix2Pix,-19.1441,169.21,6.7661,0.4636,-11.3141,1.69,Hausdorff
2,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2017-03,Mask R-CNN,-12.6723,112.0,6.4718,0.4434,-11.3141,1.12,Hausdorff
3,1,Nuclear Segmentation,Cell17 - Nuclear Segmentation benchmarking,2018-09,Cell R-CNN,-11.3141,100.0,1.3582,0.0931,-11.3141,1.0,Hausdorff
0,1,3D Object Reconstruction,Data3D−R2N2 - 3D Object Reconstruction benchmarking,2016-04,3D-R2N2,0.56,84.72,0.56,100,0.661,0.01,3DIoU
1,1,3D Object Reconstruction,Data3D−R2N2 - 3D Object Reconstruction benchmarking,2016-12,PSGN,0.64,96.82,0.08,0.7921,0.661,0.01,3DIoU
2,1,3D Object Reconstruction,Data3D−R2N2 - 3D Object Reconstruction benchmarking,2019-01,Pix2Vox-A,0.661,100.0,0.021,0.2079,0.661,0.01,3DIoU
3,1,3D Reconstruction,Data3D−R2N2 - 3D Reconstruction benchmarking,2016-04,3D-R2N2,0.56,87.23,0.56,100,0.642,0.01,3DIoU
4,1,3D Reconstruction,Data3D−R2N2 - 3D Reconstruction benchmarking,2016-12,PSGN,0.64,99.69,0.08,0.9756,0.642,0.01,3DIoU
5,1,3D Reconstruction,Data3D−R2N2 - 3D Reconstruction benchmarking,2018-08,AttSets,0.642,100.0,0.002,0.0244,0.642,0.01,3DIoU
6,1,Semantic Segmentation,ScanNet - Semantic Segmentation benchmarking,2017-02,ScanNet,0.306,41.69,0.306,100,0.734,0.0,3DIoU
7,1,Semantic Segmentation,ScanNet - Semantic Segmentation benchmarking,2017-06,PointNet++,0.339,46.19,0.033,0.0771,0.734,0.0,3DIoU
8,1,Semantic Segmentation,ScanNet - Semantic Segmentation benchmarking,2017-11,SparseConvNet,0.725,98.77,0.386,0.9019,0.734,0.01,3DIoU
9,1,Semantic Segmentation,ScanNet - Semantic Segmentation benchmarking,2019-04,MinkowskiNet,0.734,100.0,0.009,0.021,0.734,0.01,3DIoU
10,1,3D Room Layouts From A Single RGB Panorama,Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking,2018-03,LayoutNet,76.33,95.66,76.33,100,79.79,0.93,3DIoU
11,1,3D Room Layouts From A Single RGB Panorama,Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking,2018-11,DuLa-Net,79.36,99.46,3.03,0.8757,79.79,0.97,3DIoU
12,1,3D Room Layouts From A Single RGB Panorama,Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking,2019-01,HorizonNet,79.79,100.0,0.43,0.1243,79.79,0.97,3DIoU
13,1,3D Room Layouts From A Single RGB Panorama,Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking,2018-03,LayoutNet,62.77,81.31,62.77,100,77.2,0.76,3DIoU
14,1,3D Room Layouts From A Single RGB Panorama,Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking,2018-11,DuLa-Net,77.2,100.0,14.43,1.0,77.2,0.94,3DIoU
15,1,3D Room Layouts From A Single RGB Panorama,PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking,2018-03,LayoutNet,74.48,90.64,74.48,100,82.17,0.91,3DIoU
16,1,3D Room Layouts From A Single RGB Panorama,PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking,2018-11,DuLa-Net,77.42,94.22,2.94,0.3823,82.17,0.94,3DIoU
17,1,3D Room Layouts From A Single RGB Panorama,PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking,2019-01,HorizonNet,82.17,100.0,4.75,0.6177,82.17,1.0,3DIoU
18,1,Single-View 3D Reconstruction,ShapeNet - Single-View 3D Reconstruction benchmarking,2019-04,SoftRas,64.64,100.0,64.64,100,64.64,0.79,3DIoU
19,1,Scene Segmentation,ScanNet - Scene Segmentation benchmarking,2019-04,KPConv,68.6,100.0,68.6,100,68.6,0.83,3DIoU
0,1,3D Object Reconstruction,Data3D−R2N2 - 3D Object Reconstruction benchmarking,2016-04,3D-R2N2,39.01,57.9,39.01,100,67.37,0.58,Avg\\ F1
1,1,3D Object Reconstruction,Data3D−R2N2 - 3D Object Reconstruction benchmarking,2016-12,PSGN,48.58,72.11,9.57,0.3374,67.37,0.72,Avg\\ F1
2,1,3D Object Reconstruction,Data3D−R2N2 - 3D Object Reconstruction benchmarking,2018-02,MVD,66.39,98.55,17.81,0.628,67.37,0.99,Avg\\ F1
3,1,3D Object Reconstruction,Data3D−R2N2 - 3D Object Reconstruction benchmarking,2019-01,GEOMetrics,67.37,100.0,0.98,0.0346,67.37,1.0,Avg\\ F1
4,1,Action Unit Detection,BP4D - Action Unit Detection benchmarking,2018-12,AU R-CNN,63.1,100.0,63.1,100,63.1,0.94,Avg\\ F1
0,1,Horizon Line Estimation,Horizon Lines in the Wild - Horizon Line Estimation benchmarking,2016-04,"GoogleNet (Huber Loss, horizon line projection)",71.16,94.63,71.16,100,75.2,0.75,AUC\\ \\(horizon\\ error\\)
1,1,Horizon Line Estimation,Horizon Lines in the Wild - Horizon Line Estimation benchmarking,2019-05,NG-DSAC,75.2,100.0,4.04,1.0,75.2,0.79,AUC\\ \\(horizon\\ error\\)
2,1,Horizon Line Estimation,Eurasian Cities Dataset - Horizon Line Estimation benchmarking,2016-04,"GoogleNet (Huber Loss, horizon line projection)",83.6,92.07,83.6,100,90.8,0.88,AUC\\ \\(horizon\\ error\\)
3,1,Horizon Line Estimation,Eurasian Cities Dataset - Horizon Line Estimation benchmarking,2016-08,CNN+FULL,90.8,100.0,7.2,1.0,90.8,0.96,AUC\\ \\(horizon\\ error\\)
4,1,Horizon Line Estimation,York Urban Dataset - Horizon Line Estimation benchmarking,2016-04,"GoogleNet (Huber Loss, horizon line projection)",86.41,91.17,86.41,100,94.78,0.91,AUC\\ \\(horizon\\ error\\)
5,1,Horizon Line Estimation,York Urban Dataset - Horizon Line Estimation benchmarking,2016-08,CNN+FULL,94.78,100.0,8.37,1.0,94.78,1.0,AUC\\ \\(horizon\\ error\\)
0,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2016-04,MultiPath Network,25.0,57.74,25.0,100,43.3,0.28,mask\\ AP
1,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),37.1,85.68,12.1,0.6612,43.3,0.42,mask\\ AP
2,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2017-12,"MaskLab+ (ResNet-101, JFT)",38.1,87.99,1.0,0.0546,43.3,0.43,mask\\ AP
3,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2018-03,PANet,42.0,97.0,3.9,0.2131,43.3,0.48,mask\\ AP
4,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-09,"Cascade Mask R-CNN (ResNeXt152, CBNet)",43.3,100.0,1.3,0.071,43.3,0.49,mask\\ AP
5,1,Instance Segmentation,COCO minival - Instance Segmentation benchmarking,2017-11,"Mask R-CNN (ResNet-50, +1 NL)",35.5,83.53,35.5,100,42.5,0.4,mask\\ AP
6,1,Instance Segmentation,COCO minival - Instance Segmentation benchmarking,2017-11,"Mask R-CNN (ResNext-152, +1 NL)",40.3,94.82,4.8,0.6857,42.5,0.46,mask\\ AP
7,1,Instance Segmentation,COCO minival - Instance Segmentation benchmarking,2019-02,HTC (HRNetV2p-W48),41.0,96.47,0.7,0.1,42.5,0.46,mask\\ AP
8,1,Instance Segmentation,COCO minival - Instance Segmentation benchmarking,2019-11,CenterMask-VoVNetV2-99 (multi-scale),42.5,100.0,1.5,0.2143,42.5,0.48,mask\\ AP
9,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT,24.9,70.74,24.9,100,35.2,0.28,mask\\ AP
10,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT-550 (ResNet-101-FPN),28.2,80.11,3.3,0.3204,35.2,0.32,mask\\ AP
11,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-11,CenterMask-Lite (ResNet-50-FPN),32.9,93.47,4.7,0.4563,35.2,0.37,mask\\ AP
12,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2020-01,BlendMask-512 (DLA_34),35.2,100.0,2.3,0.2233,35.2,0.4,mask\\ AP
13,1,3D Shape Modeling,Pix3D S1 - 3D Shape Modeling benchmarking,2019-06,Mesh R-CNN,88.4,100.0,88.4,100,88.4,1.0,mask\\ AP
14,1,3D Shape Modeling,Pix3D S2 - 3D Shape Modeling benchmarking,2019-06,Mesh R-CNN,63.9,100.0,63.9,100,63.9,0.72,mask\\ AP
15,1,Instance Segmentation,LVIS v1.0 - Instance Segmentation benchmarking,2019-12,"PointRend (MaskR-CNN, ResNet-50-FPN)",39.7,100.0,39.7,100,39.7,0.45,mask\\ AP
0,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-04,P-LSTM,13.0,44.83,13,100,29,0.45,Accuracy\\ \\(CV\\ I\\)
1,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-04,LSTM,16.0,55.17,3,0.1875,29,0.55,Accuracy\\ \\(CV\\ I\\)
2,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2017-08,SK-CNN,26.0,89.66,10,0.625,29,0.9,Accuracy\\ \\(CV\\ I\\)
3,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2019-04,VS-CNN,29.0,100.0,3,0.1875,29,1.0,Accuracy\\ \\(CV\\ I\\)
0,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-04,P-LSTM,33.0,46.48,33,100,71,0.46,Accuracy\\ \\(CV\\ II\\)
1,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-11,TCN,43.0,60.56,10,0.2632,71,0.61,Accuracy\\ \\(CV\\ II\\)
2,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2017-04,Res-TCN,48.0,67.61,5,0.1316,71,0.68,Accuracy\\ \\(CV\\ II\\)
3,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2017-08,SK-CNN,68.0,95.77,20,0.5263,71,0.96,Accuracy\\ \\(CV\\ II\\)
4,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2019-04,VS-CNN,71.0,100.0,3,0.0789,71,1.0,Accuracy\\ \\(CV\\ II\\)
0,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-04,P-LSTM,33.0,57.89,33,100,57,0.58,Accuracy\\ \\(AV\\ I\\)
1,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-11,TCN,43.0,75.44,10,0.4167,57,0.75,Accuracy\\ \\(AV\\ I\\)
2,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2017-04,Res-TCN,48.0,84.21,5,0.2083,57,0.84,Accuracy\\ \\(AV\\ I\\)
3,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2018-01,ST-GCN,53.0,92.98,5,0.2083,57,0.93,Accuracy\\ \\(AV\\ I\\)
4,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2019-04,VS-CNN,57.0,100.0,4,0.1667,57,1.0,Accuracy\\ \\(AV\\ I\\)
0,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-04,P-LSTM,50.0,64.94,50,100,77,0.65,Accuracy\\ \\(AV\\ II\\)
1,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2016-04,LSTM,68.0,88.31,18,0.6667,77,0.88,Accuracy\\ \\(AV\\ II\\)
2,1,Skeleton Based Action Recognition,Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking,2017-08,SK-CNN,77.0,100.0,9,0.3333,77,1.0,Accuracy\\ \\(AV\\ II\\)
0,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,glance-ft-2L,0.42,85.71,0.42,100,0.49,0.84,mRMSE
1,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,Fast-RCNN,0.49,100.0,0.07,1.0,0.49,0.98,mRMSE
2,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,ens,0.42,84.0,0.42,100,0.5,0.84,mRMSE
3,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,Fast-RCNN,0.5,100.0,0.08,1.0,0.5,1.0,mRMSE
0,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,ens,1.68,87.5,1.68,100,1.92,0.6,mRMSE\\-nz
1,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,Fast-RCNN,1.92,100.0,0.24,1.0,1.92,0.69,mRMSE\\-nz
2,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,glance-ft-2L,2.25,80.94,2.25,100,2.78,0.81,mRMSE\\-nz
3,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,Fast-RCNN,2.78,100.0,0.53,1.0,2.78,1.0,mRMSE\\-nz
0,1,3D Point Cloud Classification,ModelNet40 - 3D Point Cloud Classification benchmarking,2016-04,Subvolume,89.2,96.02,89.2,100,92.9,0.89,Overall\\ Accuracy
1,1,3D Point Cloud Classification,ModelNet40 - 3D Point Cloud Classification benchmarking,2017-04,Kd-Net,91.8,98.82,2.6,0.7027,92.9,0.92,Overall\\ Accuracy
2,1,3D Point Cloud Classification,ModelNet40 - 3D Point Cloud Classification benchmarking,2017-06,PointNet++,91.9,98.92,0.1,0.027,92.9,0.92,Overall\\ Accuracy
3,1,3D Point Cloud Classification,ModelNet40 - 3D Point Cloud Classification benchmarking,2018-03,SpiderCNN,92.4,99.46,0.5,0.1351,92.9,0.92,Overall\\ Accuracy
4,1,3D Point Cloud Classification,ModelNet40 - 3D Point Cloud Classification benchmarking,2018-12,PointCNN,92.5,99.57,0.1,0.027,92.9,0.92,Overall\\ Accuracy
5,1,3D Point Cloud Classification,ModelNet40 - 3D Point Cloud Classification benchmarking,2019-04,KPConv,92.9,100.0,0.4,0.1081,92.9,0.93,Overall\\ Accuracy
6,1,Depiction Invariant Object Recognition,Photo-Art-50 - Depiction Invariant Object Recognition benchmarking,2016-07,SwiDeN,93.02,100.0,93.02,100,93.02,0.93,Overall\\ Accuracy
7,1,Hyperspectral Image Classification,Pavia University - Hyperspectral Image Classification benchmarking,2016-12,BASSNet,97.48,98.04,97.48,100,99.43,0.97,Overall\\ Accuracy
8,1,Hyperspectral Image Classification,Pavia University - Hyperspectral Image Classification benchmarking,2018-07,WCRN,99.43,100.0,1.95,1.0,99.43,0.99,Overall\\ Accuracy
9,1,Hyperspectral Image Classification,Indian Pines - Hyperspectral Image Classification benchmarking,2016-12,BASSNet,96.77,96.95,96.77,100,99.81,0.97,Overall\\ Accuracy
10,1,Hyperspectral Image Classification,Indian Pines - Hyperspectral Image Classification benchmarking,2019-02,HybridSN,99.81,100.0,3.04,1.0,99.81,1.0,Overall\\ Accuracy
11,1,3D Point Cloud Classification,ScanObjectNN - 3D Point Cloud Classification benchmarking,2016-12,PointNet,68.2,86.88,68.2,100,78.5,0.68,Overall\\ Accuracy
12,1,3D Point Cloud Classification,ScanObjectNN - 3D Point Cloud Classification benchmarking,2017-06,PointNet++,77.9,99.24,9.7,0.9417,78.5,0.78,Overall\\ Accuracy
13,1,3D Point Cloud Classification,ScanObjectNN - 3D Point Cloud Classification benchmarking,2018-01,PointCNN,78.5,100.0,0.6,0.0583,78.5,0.78,Overall\\ Accuracy
14,1,Hyperspectral Image Classification,Salinas Scene - Hyperspectral Image Classification benchmarking,2019-02,HybridSN,100.0,100.0,100,100,100,1.0,Overall\\ Accuracy
15,1,Facial Expression Recognition,RAF-DB - Facial Expression Recognition benchmarking,2019-05,RAN (ResNet-18),86.9,100.0,86.9,100,86.9,0.87,Overall\\ Accuracy
0,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,glance-ft-2L,0.23,95.83,0.23,100,0.24,0.96,m\\-reIRMSE
1,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,Aso-sub-ft-3x3,0.24,100.0,0.01,1.0,0.24,1.0,m\\-reIRMSE
0,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,glance-ft-2L,0.91,80.53,0.91,100,1.13,0.81,m\\-reIRMSE\\-nz
1,1,Object Counting,COCO count-test - Object Counting benchmarking,2016-04,Fast-RCNN,1.13,100.0,0.22,1.0,1.13,1.0,m\\-reIRMSE\\-nz
2,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,ens,0.65,76.47,0.65,100,0.85,0.58,m\\-reIRMSE\\-nz
3,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,Fast-RCNN,0.85,100.0,0.2,1.0,0.85,0.75,m\\-reIRMSE\\-nz
0,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,ens,0.2,74.07,0.2,100,0.27,0.74,m\\-relRMSE
1,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,Fast-RCNN,0.26,96.3,0.06,0.8571,0.27,0.96,m\\-relRMSE
2,1,Object Counting,Pascal VOC 2007 count-test - Object Counting benchmarking,2016-04,glance-noft-2L,0.27,100.0,0.01,0.1429,0.27,1.0,m\\-relRMSE
0,1,Continuous Control,Cart-Pole Balancing - Continuous Control benchmarking,2016-04,TRPO,4869.8,100.0,4869.8,100,4869.8,1.0,Score
1,1,Continuous Control,2D Walker - Continuous Control benchmarking,2016-04,TRPO,1353.8,100.0,1353.8,100,1353.8,0.28,Score
2,1,Continuous Control,Full Humanoid - Continuous Control benchmarking,2016-04,TRPO,287.0,100.0,287,100,287,0.06,Score
3,1,Continuous Control,Inverted Pendulum - Continuous Control benchmarking,2016-04,TRPO,247.2,100.0,247.2,100,247.2,0.05,Score
4,1,Continuous Control,Simple Humanoid - Continuous Control benchmarking,2016-04,TRPO,269.7,100.0,269.7,100,269.7,0.06,Score
5,1,Continuous Control,Hopper - Continuous Control benchmarking,2016-04,TRPO,1183.3,100.0,1183.3,100,1183.3,0.24,Score
6,1,Continuous Control,Swimmer - Continuous Control benchmarking,2016-04,TRPO,96.0,100.0,96,100,96,0.02,Score
7,1,Continuous Control,Half-Cheetah - Continuous Control benchmarking,2016-04,TRPO,1914.0,100.0,1914,100,1914,0.39,Score
8,1,Continuous Control,Ant - Continuous Control benchmarking,2016-04,TRPO,730.2,100.0,730.2,100,730.2,0.15,Score
9,1,Continuous Control,Double Inverted Pendulum - Continuous Control benchmarking,2016-04,TRPO,4412.4,100.0,4412.4,100,4412.4,0.91,Score
10,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2018-08,HAN,28.65,55.04,28.65,100,52.05,0.01,Score
11,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2019-02,MuRel,39.54,75.97,10.89,0.4654,52.05,0.01,Score
12,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2019-05,UpDn+SCR (VQA-X),49.45,95.0,9.91,0.4235,52.05,0.01,Score
13,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2019-09,Learned-Mixin +H,52.05,100.0,2.6,0.1111,52.05,0.01,Score
0,1,Temporal Action Localization,J-HMDB-21 - Temporal Action Localization benchmarking,2016-04,Actionness,39.9,54.43,39.9,100,73.3,0.52,Frame\\-mAP
1,1,Temporal Action Localization,J-HMDB-21 - Temporal Action Localization benchmarking,2016-09,Peng w/o MR,56.9,77.63,17.0,0.509,73.3,0.75,Frame\\-mAP
2,1,Temporal Action Localization,J-HMDB-21 - Temporal Action Localization benchmarking,2016-09,Peng w/ MR,58.5,79.81,1.6,0.0479,73.3,0.77,Frame\\-mAP
3,1,Temporal Action Localization,J-HMDB-21 - Temporal Action Localization benchmarking,2017-05,ACT,65.7,89.63,7.2,0.2156,73.3,0.86,Frame\\-mAP
4,1,Temporal Action Localization,J-HMDB-21 - Temporal Action Localization benchmarking,2017-05,Faster-RCNN + two-stream I3D conv,73.3,100.0,7.6,0.2275,73.3,0.96,Frame\\-mAP
5,1,Temporal Action Localization,UCF101-24 - Temporal Action Localization benchmarking,2016-09,Peng w/ MR,65.7,86.11,65.7,100,76.3,0.86,Frame\\-mAP
6,1,Temporal Action Localization,UCF101-24 - Temporal Action Localization benchmarking,2017-05,ACT,69.5,91.09,3.8,0.3585,76.3,0.91,Frame\\-mAP
7,1,Temporal Action Localization,UCF101-24 - Temporal Action Localization benchmarking,2017-05,Faster-RCNN + two-stream I3D conv,76.3,100.0,6.8,0.6415,76.3,1.0,Frame\\-mAP
0,1,Playing Game of Doom,ViZDoom Basic Scenario - Playing Game of Doom benchmarking,2016-05,DQN,82.2,100.0,82.2,100,82.2,1.0,Average\\ Score
0,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,57.88,83.98,57.88,100,68.92,0.84,MRR
1,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,63.98,92.83,6.1,0.5525,68.92,0.93,MRR
2,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),64.1,93.01,0.12,0.0109,68.92,0.93,MRR
3,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,DAN,66.38,96.31,2.28,0.2065,68.92,0.96,MRR
4,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),68.92,100.0,2.54,0.2301,68.92,1.0,MRR
0,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,5.84,100.0,5.84,100,5.84,0.49,Mean\\ Rank
1,1,Few-Shot Image Classification,Meta-Dataset Rank - Few-Shot Image Classification benchmarking,2016-06,Matching Networks,10.5,88.98,10.5,100,11.8,0.89,Mean\\ Rank
2,1,Few-Shot Image Classification,Meta-Dataset Rank - Few-Shot Image Classification benchmarking,2017-11,Relation Networks,11.8,100.0,1.3,1.0,11.8,1.0,Mean\\ Rank
0,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-06,OFL,65.7,75.43,65.7,100,87.1,0.75,J\\&F
1,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,80.2,92.08,14.5,0.6776,87.1,0.92,J\\&F
2,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2017-06,OnAVOS,85.5,98.16,5.3,0.2477,87.1,0.98,J\\&F
3,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2019-08,RANet+ (online learning),87.1,100.0,1.6,0.0748,87.1,1.0,J\\&F
4,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2017-09,SFL,67.05,83.81,67.05,100,80.0,0.77,J\\&F
5,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2020-01,COSNet,80.0,100.0,12.95,1.0,80.0,0.92,J\\&F
0,1,Low-Light Image Enhancement,VV - Low-Light Image Enhancement benchmarking,2016-06,SRIE,2.8,86.42,2.8,100,3.24,0.68,User\\ Study\\ Score
1,1,Low-Light Image Enhancement,VV - Low-Light Image Enhancement benchmarking,2019-06,Wang et al.,2.95,91.05,0.15,0.3409,3.24,0.71,User\\ Study\\ Score
2,1,Low-Light Image Enhancement,VV - Low-Light Image Enhancement benchmarking,2020-01,Zero-DCE,3.24,100.0,0.29,0.6591,3.24,0.78,User\\ Study\\ Score
3,1,Low-Light Image Enhancement,DICM - Low-Light Image Enhancement benchmarking,2016-06,SRIE,3.42,97.16,3.42,100,3.52,0.83,User\\ Study\\ Score
4,1,Low-Light Image Enhancement,DICM - Low-Light Image Enhancement benchmarking,2019-06,Wang et al.,3.44,97.73,0.02,0.2,3.52,0.83,User\\ Study\\ Score
5,1,Low-Light Image Enhancement,DICM - Low-Light Image Enhancement benchmarking,2020-01,Zero-DCE,3.52,100.0,0.08,0.8,3.52,0.85,User\\ Study\\ Score
6,1,Low-Light Image Enhancement,MEF - Low-Light Image Enhancement benchmarking,2016-06,SRIE,3.22,77.97,3.22,100,4.13,0.78,User\\ Study\\ Score
7,1,Low-Light Image Enhancement,MEF - Low-Light Image Enhancement benchmarking,2020-01,Zero-DCE,4.13,100.0,0.91,1.0,4.13,1.0,User\\ Study\\ Score
8,1,Low-Light Image Enhancement,LIME - Low-Light Image Enhancement benchmarking,2020-01,Zero-DCE,3.8,100.0,3.8,100,3.8,0.92,User\\ Study\\ Score
9,1,Low-Light Image Enhancement,NPE - Low-Light Image Enhancement benchmarking,2020-01,Zero-DCE,3.81,100.0,3.81,100,3.81,0.92,User\\ Study\\ Score
0,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-06,OFL,70.4,73.26,70.4,100,96.1,0.73,F\\-measure\\ \\(Recall\\)
1,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,92.6,96.36,22.2,0.8638,96.1,0.96,F\\-measure\\ \\(Recall\\)
2,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2018-04,PML,93.4,97.19,0.8,0.0311,96.1,0.97,F\\-measure\\ \\(Recall\\)
3,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2019-08,RANet,94.9,98.75,1.5,0.0584,96.1,0.99,F\\-measure\\ \\(Recall\\)
4,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2019-08,RANet+ (online learning),96.1,100.0,1.2,0.0467,96.1,1.0,F\\-measure\\ \\(Recall\\)
5,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2017-09,SFL,77.1,86.15,77.1,100,89.5,0.8,F\\-measure\\ \\(Recall\\)
6,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2020-01,COSNet,89.5,100.0,12.4,1.0,89.5,0.93,F\\-measure\\ \\(Recall\\)
0,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-06,OFL,27.2,100.0,27.2,100,27.2,1.0,F\\-measure\\ \\(Decay\\)
1,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2017-09,SFL,5.1,100.0,5.1,100,5.1,0.19,F\\-measure\\ \\(Decay\\)
0,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-06,OFL,63.4,72.37,63.4,100,87.6,0.72,F\\-measure\\ \\(Mean\\)
1,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,80.6,92.01,17.2,0.7107,87.6,0.92,F\\-measure\\ \\(Mean\\)
2,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2017-06,OnAVOS,84.9,96.92,4.3,0.1777,87.6,0.97,F\\-measure\\ \\(Mean\\)
3,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2018-03,CINM,85.0,97.03,0.1,0.0041,87.6,0.97,F\\-measure\\ \\(Mean\\)
4,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2019-08,RANet,85.4,97.49,0.4,0.0165,87.6,0.97,F\\-measure\\ \\(Mean\\)
5,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2019-08,RANet+ (online learning),87.6,100.0,2.2,0.0909,87.6,1.0,F\\-measure\\ \\(Mean\\)
6,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2017-09,SFL,66.7,83.9,66.7,100,79.5,0.76,F\\-measure\\ \\(Mean\\)
7,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2020-01,COSNet,79.5,100.0,12.8,1.0,79.5,0.91,F\\-measure\\ \\(Mean\\)
0,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-06,OFL,26.4,100.0,26.4,100,26.4,1.0,Jaccard\\ \\(Decay\\)
1,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2017-09,SFL,6.2,100.0,6.2,100,6.2,0.23,Jaccard\\ \\(Decay\\)
0,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-06,OFL,75.6,77.78,75.6,100,97.2,0.78,Jaccard\\ \\(Recall\\)
1,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,93.6,96.3,18.0,0.8333,97.2,0.96,Jaccard\\ \\(Recall\\)
2,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2017-06,OnAVOS,96.1,98.87,2.5,0.1157,97.2,0.99,Jaccard\\ \\(Recall\\)
3,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2018-06,FAVOS,96.5,99.28,0.4,0.0185,97.2,0.99,Jaccard\\ \\(Recall\\)
4,1,Semi-Supervised Video Object Segmentation,DAVIS 2016 - Semi-Supervised Video Object Segmentation benchmarking,2019-08,RANet,97.2,100.0,0.7,0.0324,97.2,1.0,Jaccard\\ \\(Recall\\)
5,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2017-09,SFL,81.4,87.43,81.4,100,93.1,0.84,Jaccard\\ \\(Recall\\)
6,1,Unsupervised Video Object Segmentation,DAVIS 2016 - Unsupervised Video Object Segmentation benchmarking,2020-01,COSNet,93.1,100.0,11.7,1.0,93.1,0.96,Jaccard\\ \\(Recall\\)
0,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-06,BiGAN,0.02,11.11,0.02,100,0.18,0.06,Class\\ IOU
1,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-06,CoGAN,0.06,33.33,0.04,0.25,0.18,0.19,Class\\ IOU
2,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-11,pix2pix,0.18,100.0,0.12,0.75,0.18,0.56,Class\\ IOU
3,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-06,BiGAN,0.07,21.88,0.07,100,0.32,0.22,Class\\ IOU
4,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-06,CoGAN,0.08,25.0,0.01,0.04,0.32,0.25,Class\\ IOU
5,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-11,pix2pix,0.32,100.0,0.24,0.96,0.32,1.0,Class\\ IOU
6,1,Image-to-Image Translation,Aerial-to-Map - Image-to-Image Translation benchmarking,2016-11,cGAN,0.26,100.0,0.26,100,0.26,0.81,Class\\ IOU
0,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-06,BiGAN,6.0,24.0,6,100,25,0.13,Per\\-class\\ Accuracy
1,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-06,CoGAN,10.0,40.0,4,0.2105,25,0.22,Per\\-class\\ Accuracy
2,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-11,pix2pix,25.0,100.0,15,0.7895,25,0.54,Per\\-class\\ Accuracy
3,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-06,BiGAN,13.0,32.5,13,100,40,0.28,Per\\-class\\ Accuracy
4,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-11,pix2pix,40.0,100.0,27,1.0,40,0.87,Per\\-class\\ Accuracy
5,1,Image-to-Image Translation,Aerial-to-Map - Image-to-Image Translation benchmarking,2016-11,cGAN,46.0,100.0,46,100,46,1.0,Per\\-class\\ Accuracy
0,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-06,BiGAN,19.0,23.09,19.0,100,82.3,0.21,Per\\-pixel\\ Accuracy
1,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-06,CoGAN,40.0,48.6,21.0,0.3318,82.3,0.43,Per\\-pixel\\ Accuracy
2,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2016-11,pix2pix,71.0,86.27,31.0,0.4897,82.3,0.77,Per\\-pixel\\ Accuracy
3,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2017-07,CRN,77.1,93.68,6.1,0.0964,82.3,0.84,Per\\-pixel\\ Accuracy
4,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2017-11,pix2pixHD,81.4,98.91,4.3,0.0679,82.3,0.88,Per\\-pixel\\ Accuracy
5,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2019-03,SPADE,81.9,99.51,0.5,0.0079,82.3,0.89,Per\\-pixel\\ Accuracy
6,1,Image-to-Image Translation,Cityscapes Labels-to-Photo - Image-to-Image Translation benchmarking,2019-10,CC-FPSE,82.3,100.0,0.4,0.0063,82.3,0.89,Per\\-pixel\\ Accuracy
7,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-06,BiGAN,41.0,48.24,41,100,85,0.45,Per\\-pixel\\ Accuracy
8,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-06,CoGAN,45.0,52.94,4,0.0909,85,0.49,Per\\-pixel\\ Accuracy
9,1,Image-to-Image Translation,Cityscapes Photo-to-Labels - Image-to-Image Translation benchmarking,2016-11,pix2pix,85.0,100.0,40,0.9091,85,0.92,Per\\-pixel\\ Accuracy
10,1,Image-to-Image Translation,Aerial-to-Map - Image-to-Image Translation benchmarking,2016-11,cGAN,70.0,100.0,70,100,70,0.76,Per\\-pixel\\ Accuracy
11,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2017-11,CyCADA pixel+feat,82.3,100.0,82.3,100,82.3,0.89,Per\\-pixel\\ Accuracy
12,1,Image-to-Image Translation,SYNTHIA Fall-to-Winter - Image-to-Image Translation benchmarking,2017-11,CyCADA,92.1,100.0,92.1,100,92.1,1.0,Per\\-pixel\\ Accuracy
0,1,Multivariate Time Series Imputation,MuJoCo - Multivariate Time Series Imputation benchmarking,2016-06,RNN GRU-D,0.748,12.26,0.748,100,6.1,0.12,"MSE\\ \\(10\\^2,\\ 50%\\ missing\\)"
1,1,Multivariate Time Series Imputation,MuJoCo - Multivariate Time Series Imputation benchmarking,2018-06,RNN-VAE,6.1,100.0,5.352,1.0,6.1,1.0,"MSE\\ \\(10\\^2,\\ 50%\\ missing\\)"
0,1,Image Generation,ImageNet 64x64 - Image Generation benchmarking,2016-06,"Gated PixelCNN (van den Oord et al., [2016c])",3.57,93.7,3.57,100,3.81,0.94,Bits\\ per\\ dim
1,1,Image Generation,ImageNet 64x64 - Image Generation benchmarking,2017-03,Parallel Multiscale,3.7,97.11,0.13,0.5417,3.81,0.97,Bits\\ per\\ dim
2,1,Image Generation,ImageNet 64x64 - Image Generation benchmarking,2018-07,"Glow (Kingma and Dhariwal, 2018)",3.81,100.0,0.11,0.4583,3.81,1.0,Bits\\ per\\ dim
0,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2016-06,3D-UNet [Cicek:2016un],84.6,97.47,84.6,100,86.8,0.97,Instance\\ Average\\ IoU
1,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2016-12,SSCNN,84.7,97.58,0.1,0.0455,86.8,0.98,Instance\\ Average\\ IoU
2,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2017-06,SSCN,86.0,99.08,1.3,0.5909,86.8,0.99,Instance\\ Average\\ IoU
3,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2018-01,PointCNN,86.14,99.24,0.14,0.0636,86.8,0.99,Instance\\ Average\\ IoU
4,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2019-04,KPConv,86.4,99.54,0.26,0.1182,86.8,1.0,Instance\\ Average\\ IoU
5,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2019-09,Spherical Kernel,86.8,100.0,0.4,0.1818,86.8,1.0,Instance\\ Average\\ IoU
0,1,Facial Expression Recognition,Oulu-CASIA - Facial Expression Recognition benchmarking,2016-07,PPDN,84.59,94.41,84.59,100,89.6,0.94,Accuracy\\ \\(10\\-fold\\)
1,1,Facial Expression Recognition,Oulu-CASIA - Facial Expression Recognition benchmarking,2019-11,Dynamic MTL,89.6,100.0,5.01,1.0,89.6,1.0,Accuracy\\ \\(10\\-fold\\)
0,1,Scene Graph Generation,VRD - Scene Graph Generation benchmarking,2016-07,VRD,18.16,99.13,18.16,100,18.32,0.57,Recall\\-at\\-50
1,1,Scene Graph Generation,VRD - Scene Graph Generation benchmarking,2018-06,FactorizableNet,18.32,100.0,0.16,1.0,18.32,0.57,Recall\\-at\\-50
2,1,Scene Graph Generation,Visual Genome - Scene Graph Generation benchmarking,2017-07,MSDN,10.72,33.57,10.72,100,31.93,0.34,Recall\\-at\\-50
3,1,Scene Graph Generation,Visual Genome - Scene Graph Generation benchmarking,2018-08,Graph-RCNN,11.4,35.7,0.68,0.0321,31.93,0.36,Recall\\-at\\-50
4,1,Scene Graph Generation,Visual Genome - Scene Graph Generation benchmarking,2018-12,VCTree,27.9,87.38,16.5,0.7779,31.93,0.87,Recall\\-at\\-50
5,1,Scene Graph Generation,Visual Genome - Scene Graph Generation benchmarking,2020-02,Causal-TDE,31.93,100.0,4.03,0.19,31.93,1.0,Recall\\-at\\-50
0,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2016-08,TSN,73.9,89.25,73.9,100,82.8,0.89,Vid\\ acc@1
1,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2017-11,I3D + NL,77.7,93.84,3.8,0.427,82.8,0.94,Vid\\ acc@1
2,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2018-10,"RepFlow-50 ([2+1]D CNN, FcF, Non-local block)",77.9,94.08,0.2,0.0225,82.8,0.94,Vid\\ acc@1
3,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2018-12,SlowFast 16x8 (ResNet-101),78.9,95.29,1.0,0.1124,82.8,0.95,Vid\\ acc@1
4,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2018-12,SlowFast (ResNet-101 + NL),79.8,96.38,0.9,0.1011,82.8,0.96,Vid\\ acc@1
5,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2019-04,R[2+1]D-152 (IG-65M pretraining),81.3,98.19,1.5,0.1685,82.8,0.98,Vid\\ acc@1
6,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2019-04,ir-CSN-152 (IG-65M pretraining),82.6,99.76,1.3,0.1461,82.8,1.0,Vid\\ acc@1
7,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2019-05,irCSN-152 (IG-Kinetics-65M pretrain),82.8,100.0,0.2,0.0225,82.8,1.0,Vid\\ acc@1
0,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2016-08,TSN,91.1,95.59,91.1,100,95.3,0.96,Vid\\ acc@5
1,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2017-11,I3D + NL,93.3,97.9,2.2,0.5238,95.3,0.98,Vid\\ acc@5
2,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2017-12,"S3D-G (RGB, ImageNet pretrained)",93.4,98.01,0.1,0.0238,95.3,0.98,Vid\\ acc@5
3,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2018-12,SlowFast 16x8 (ResNet-101),93.5,98.11,0.1,0.0238,95.3,0.98,Vid\\ acc@5
4,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2018-12,SlowFast 16x8 (ResNet-101 + NL),93.9,98.53,0.4,0.0952,95.3,0.99,Vid\\ acc@5
5,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2019-04,R[2+1]D-152 (IG-65M pretraining),95.1,99.79,1.2,0.2857,95.3,1.0,Vid\\ acc@5
6,1,Action Classification,Kinetics-400 - Action Classification benchmarking,2019-04,ip-CSN-152 (IG-65M pretraining),95.3,100.0,0.2,0.0476,95.3,1.0,Vid\\ acc@5
0,1,Domain Adaptation,Synth Objects-to-LINEMOD - Domain Adaptation benchmarking,2016-08,DSN (DANN),53.27,100.0,53.27,100,53.27,1.0,Mean\\ Angle\\ Error
0,1,Video Generation,"UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking",2016-09,VGAN,8.18,35.7,8.18,100,22.91,0.04,Inception\\ Score
1,1,Video Generation,"UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking",2016-11,TGAN-SVC,11.85,51.72,3.67,0.2492,22.91,0.05,Inception\\ Score
2,1,Video Generation,"UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking",2017-07,MoCoGAN,12.42,54.21,0.57,0.0387,22.91,0.06,Inception\\ Score
3,1,Video Generation,"UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking",2019-12,TGAN-F,22.91,100.0,10.49,0.7122,22.91,0.1,Inception\\ Score
4,1,Layout-to-Image Generation,Visual Genome 64x64 - Layout-to-Image Generation benchmarking,2018-04,SG2Im,6.3,72.41,6.3,100,8.7,0.03,Inception\\ Score
5,1,Layout-to-Image Generation,Visual Genome 64x64 - Layout-to-Image Generation benchmarking,2018-11,Layout2Im,8.1,93.1,1.8,0.75,8.7,0.04,Inception\\ Score
6,1,Layout-to-Image Generation,Visual Genome 64x64 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,8.7,100.0,0.6,0.25,8.7,0.04,Inception\\ Score
7,1,Layout-to-Image Generation,COCO-Stuff 64x64 - Layout-to-Image Generation benchmarking,2018-04,SG2Im,7.3,70.87,7.3,100,10.3,0.03,Inception\\ Score
8,1,Layout-to-Image Generation,COCO-Stuff 64x64 - Layout-to-Image Generation benchmarking,2018-11,Layout2Im,9.1,88.35,1.8,0.6,10.3,0.04,Inception\\ Score
9,1,Layout-to-Image Generation,COCO-Stuff 64x64 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,9.8,95.15,0.7,0.2333,10.3,0.04,Inception\\ Score
10,1,Layout-to-Image Generation,COCO-Stuff 64x64 - Layout-to-Image Generation benchmarking,2019-09,SOARISG,10.3,100.0,0.5,0.1667,10.3,0.05,Inception\\ Score
11,1,Video Generation,"Kinetics-600 48 frames, 64x64 - Video Generation benchmarking",2019-07,DVD-GAN,219.05,100.0,219.05,100,219.05,1.0,Inception\\ Score
12,1,Video Generation,"Kinetics-600 12 frames, 64x64 - Video Generation benchmarking",2019-07,DVD-GAN,129.9,100.0,129.9,100,129.9,0.59,Inception\\ Score
13,1,Layout-to-Image Generation,COCO-Stuff 128x128 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,13.8,100.0,13.8,100,13.8,0.06,Inception\\ Score
14,1,Layout-to-Image Generation,Visual Genome 128x128 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,11.1,100.0,11.1,100,11.1,0.05,Inception\\ Score
15,1,Video Generation,"UCF-101 16 frames, 64x64, Unconditional - Video Generation benchmarking",2019-12,TGAN-F,13.62,100.0,13.62,100,13.62,0.06,Inception\\ Score
0,1,Monocular Depth Estimation,Mid-Air Dataset - Monocular Depth Estimation benchmarking,2016-09,Monodepth,0.3136,43.74,0.3136,100,0.717,0.44,Abs\\ Rel
1,1,Monocular Depth Estimation,Mid-Air Dataset - Monocular Depth Estimation benchmarking,2018-06,Monodepth2,0.717,100.0,0.4034,1.0,0.717,1.0,Abs\\ Rel
2,1,Monocular Depth Estimation,Make3D - Monocular Depth Estimation benchmarking,2018-06,Monodepth2,0.322,100.0,0.322,100,0.322,0.45,Abs\\ Rel
0,1,Monocular Depth Estimation,Mid-Air Dataset - Monocular Depth Estimation benchmarking,2016-09,Monodepth,8.7127,23.44,8.7127,100,37.164,0.23,SQ\\ Rel
1,1,Monocular Depth Estimation,Mid-Air Dataset - Monocular Depth Estimation benchmarking,2018-06,Monodepth2,37.164,100.0,28.4513,1.0,37.164,1.0,SQ\\ Rel
0,1,Monocular Depth Estimation,Mid-Air Dataset - Monocular Depth Estimation benchmarking,2016-09,Monodepth,0.438,49.66,0.438,100,0.882,0.5,RMSE\\ log
1,1,Monocular Depth Estimation,Mid-Air Dataset - Monocular Depth Estimation benchmarking,2018-06,Monodepth2,0.882,100.0,0.444,1.0,0.882,1.0,RMSE\\ log
0,1,Monocular Depth Estimation,KITTI Eigen split unsupervised - Monocular Depth Estimation benchmarking,2016-09,Monodepth S,0.133,100.0,0.133,100,0.133,0.69,absolute\\ relative\\ error
1,1,Monocular Depth Estimation,KITTI Eigen split - Monocular Depth Estimation benchmarking,2018-03,CFA,0.096,49.74,0.096,100,0.193,0.5,absolute\\ relative\\ error
2,1,Monocular Depth Estimation,KITTI Eigen split - Monocular Depth Estimation benchmarking,2018-05,CC,0.14,72.54,0.044,0.4536,0.193,0.73,absolute\\ relative\\ error
3,1,Monocular Depth Estimation,KITTI Eigen split - Monocular Depth Estimation benchmarking,2019-03,VDA,0.193,100.0,0.053,0.5464,0.193,1.0,absolute\\ relative\\ error
0,1,Image Super-Resolution,PIRM-test - Image Super-Resolution benchmarking,2016-09,SRGAN,-2.71,106.27,-2.71,100,-2.55,1.06,NIQE
1,1,Image Super-Resolution,PIRM-test - Image Super-Resolution benchmarking,2018-09,ESRGAN,-2.55,100.0,0.16,1.0,-2.55,1.0,NIQE
2,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,-7.378,105.64,-7.378,100,-6.984,2.89,NIQE
3,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,ESRGAN,-6.984,100.0,0.394,1.0,-6.984,2.74,NIQE
4,1,Face Hallucination,FFHQ 512 x 512 - 16x upscaling - Face Hallucination benchmarking,2017-10,WaveletCNN,-11.45,100.0,-11.45,100,-11.45,4.49,NIQE
0,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,2.269,83.97,2.269,100,2.702,0.84,LLE
1,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2017-10,WaveletCNN,2.702,100.0,0.433,1.0,2.702,1.0,LLE
0,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,-0.1097,130.13,-0.1097,100,-0.0843,1.3,FED
1,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2017-07,EDSR,-0.0843,100.0,0.0254,1.0,-0.0843,1.0,FED
0,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2016-09,SRGAN,-0.1313,107.53,-0.1313,100,-0.1221,1.54,LPIPS
1,1,Image Super-Resolution,FFHQ 512 x 512 - 4x upscaling - Image Super-Resolution benchmarking,2018-09,ESRGAN,-0.1221,100.0,0.0092,1.0,-0.1221,1.44,LPIPS
2,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2016-11,pix2pix,-0.234,139.29,-0.234,100,-0.168,2.75,LPIPS
3,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2017-06,Xian et al._,-0.171,101.79,0.063,0.9545,-0.168,2.01,LPIPS
4,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2019-03,PI-REC,-0.168,100.0,0.003,0.0455,-0.168,1.98,LPIPS
5,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2016-11,pix2pix,-0.238,280.0,-0.238,100,-0.085,2.8,LPIPS
6,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2017-06,Xian et al._,-0.124,145.88,0.114,0.7451,-0.085,1.46,LPIPS
7,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2019-03,PI-REC,-0.085,100.0,0.039,0.2549,-0.085,1.0,LPIPS
8,1,Face Hallucination,FFHQ 512 x 512 - 16x upscaling - Face Hallucination benchmarking,2017-10,WaveletCNN,-0.4909,124.97,-0.4909,100,-0.3928,5.78,LPIPS
9,1,Face Hallucination,FFHQ 512 x 512 - 16x upscaling - Face Hallucination benchmarking,2018-09,ESRGAN,-0.3928,100.0,0.0981,1.0,-0.3928,4.62,LPIPS
10,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,-0.512,100.0,-0.512,100,-0.512,6.02,LPIPS
11,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2017-12,Deformable GAN,-0.233,100.0,-0.233,100,-0.233,2.74,LPIPS
12,1,Image Reconstruction,Edge-to-Clothes - Image Reconstruction benchmarking,2019-10,bFT,-0.1,100.0,-0.1,100,-0.1,1.18,LPIPS
13,1,Image-to-Image Translation,AFHQ - Image-to-Image Translation benchmarking,2019-12,StarGAN v2,-0.524,100.0,-0.524,100,-0.524,6.16,LPIPS
14,1,Image-to-Image Translation,CelebA-HQ - Image-to-Image Translation benchmarking,2019-12,StarGAN v2,-0.428,100.0,-0.428,100,-0.428,5.04,LPIPS
0,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2016-09,C+LSTM+SA+FC7,12.9,31.54,12.9,100,40.9,0.32,video\\-to\\-text\\ R\\-at\\-5
1,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2016-12,Kaufman,16.6,40.59,3.7,0.1321,40.9,0.41,video\\-to\\-text\\ R\\-at\\-5
2,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,32.1,78.48,15.5,0.5536,40.9,0.78,video\\-to\\-text\\ R\\-at\\-5
3,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2019-06,Text-Video Embedding,40.2,98.29,8.1,0.2893,40.9,0.98,video\\-to\\-text\\ R\\-at\\-5
4,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2019-07,Collaborative Experts,40.9,100.0,0.7,0.025,40.9,1.0,video\\-to\\-text\\ R\\-at\\-5
0,1,Video Classification,YouTube-8M - Video Classification benchmarking,2016-09,Mixture-of-2-Experts,70.1,79.93,70.1,100,87.7,0.8,Hit\\-at\\-1
1,1,Video Classification,YouTube-8M - Video Classification benchmarking,2019-06,DCGN (self-attention graph pooling),87.7,100.0,17.6,1.0,87.7,1.0,Hit\\-at\\-1
0,1,Video Classification,YouTube-8M - Video Classification benchmarking,2016-09,Mixture-of-2-Experts,84.8,100.0,84.8,100,84.8,1.0,Hit\\-at\\-5
0,1,Video Classification,YouTube-8M - Video Classification benchmarking,2016-09,Mixture-of-2-Experts,29.1,100.0,29.1,100,29.1,1.0,PERR
0,1,Face Alignment,3DFAW - Face Alignment benchmarking,2016-09,3D Face alignment,3.4767,100.0,3.4767,100,3.4767,1.0,CVGTCE
0,1,Face Alignment,3DFAW - Face Alignment benchmarking,2016-09,3D Face alignment,4.5623,100.0,4.5623,100,4.5623,1.0,GTE
0,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2016-10,3D-VAE-GAN,0.21,24.71,0.21,100,0.85,0.25,R\\-at\\-16
1,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2017-11,MarrNet,0.71,83.53,0.5,0.7812,0.85,0.84,R\\-at\\-16
2,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2018-04,MarrNet extension (w/o Pose),0.85,100.0,0.14,0.2188,0.85,1.0,R\\-at\\-16
0,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2016-10,3D-VAE-GAN,0.03,4.84,0.03,100,0.62,0.05,R\\-at\\-2
1,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2017-11,MarrNet,0.51,82.26,0.48,0.8136,0.62,0.82,R\\-at\\-2
2,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2018-04,MarrNet extension (w/o Pose),0.62,100.0,0.11,0.1864,0.62,1.0,R\\-at\\-2
0,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2016-10,3D-VAE-GAN,0.34,37.78,0.34,100,0.9,0.38,R\\-at\\-32
1,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2017-11,MarrNet,0.78,86.67,0.44,0.7857,0.9,0.87,R\\-at\\-32
2,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2018-04,MarrNet extension (w/o Pose),0.9,100.0,0.12,0.2143,0.9,1.0,R\\-at\\-32
0,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2016-10,3D-VAE-GAN,0.07,9.86,0.07,100,0.71,0.1,R\\-at\\-4
1,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2017-11,MarrNet,0.57,80.28,0.5,0.7813,0.71,0.8,R\\-at\\-4
2,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2018-04,MarrNet extension (w/o Pose),0.71,100.0,0.14,0.2188,0.71,1.0,R\\-at\\-4
0,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2016-10,3D-VAE-GAN,0.12,15.38,0.12,100,0.78,0.15,R\\-at\\-8
1,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2017-11,MarrNet,0.64,82.05,0.52,0.7879,0.78,0.82,R\\-at\\-8
2,1,3D Shape Classification,Pix3D - 3D Shape Classification benchmarking,2018-04,MarrNet extension (w/o Pose),0.78,100.0,0.14,0.2121,0.78,1.0,R\\-at\\-8
0,1,Optical Flow Estimation,Sintel-final - Optical Flow Estimation benchmarking,2016-11,Spynet,8.36,100.0,8.36,100,8.36,1.0,Average\\ End\\-Point\\ Error
1,1,Optical Flow Estimation,Sintel-clean - Optical Flow Estimation benchmarking,2016-11,Spynet,6.64,100.0,6.64,100,6.64,0.79,Average\\ End\\-Point\\ Error
2,1,Optical Flow Estimation,KITTI 2012 - Optical Flow Estimation benchmarking,2018-09,PWC-Net + ft - axXiv,1.5,93.75,1.5,100,1.6,0.18,Average\\ End\\-Point\\ Error
3,1,Optical Flow Estimation,KITTI 2012 - Optical Flow Estimation benchmarking,2019-04,IRR-PWC,1.6,100.0,0.1,1.0,1.6,0.19,Average\\ End\\-Point\\ Error
0,1,RGB Salient Object Detection,UCF - RGB Salient Object Detection benchmarking,2016-11,DSS,10.56,91.83,10.56,100,11.5,0.92,Balanced\\ Error\\ Rate
1,1,RGB Salient Object Detection,UCF - RGB Salient Object Detection benchmarking,2017-10,scGAN,11.5,100.0,0.94,1.0,11.5,1.0,Balanced\\ Error\\ Rate
2,1,RGB Salient Object Detection,SBU - RGB Salient Object Detection benchmarking,2016-11,DSS,7.0,76.92,7.0,100,9.1,0.61,Balanced\\ Error\\ Rate
3,1,RGB Salient Object Detection,SBU - RGB Salient Object Detection benchmarking,2017-07,NLDF,7.02,77.14,0.02,0.0095,9.1,0.61,Balanced\\ Error\\ Rate
4,1,RGB Salient Object Detection,SBU - RGB Salient Object Detection benchmarking,2017-10,scGAN,9.1,100.0,2.08,0.9905,9.1,0.79,Balanced\\ Error\\ Rate
5,1,RGB Salient Object Detection,ISTD - RGB Salient Object Detection benchmarking,2016-11,DSS,10.48,100.0,10.48,100,10.48,0.91,Balanced\\ Error\\ Rate
0,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,0.1,1.4,0.1,100,7.14,0.0,Speed\\ \\ \\(FPS\\)
1,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2018-02,OSMN,7.14,100.0,7.04,1.0,7.14,0.02,Speed\\ \\ \\(FPS\\)
2,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-01,MFA-Net,361.0,100.0,361,100,361,1.0,Speed\\ \\ \\(FPS\\)
0,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,58.8,100.0,58.8,100,58.8,1.0,Overall
0,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2016-11,OSVOS,60.5,96.49,60.5,100,62.7,0.96,F\\-Measure\\ \\(Seen\\)
1,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2017-06,OnAVOS,62.7,100.0,2.2,1.0,62.7,1.0,F\\-Measure\\ \\(Seen\\)
2,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,60.5,100.0,60.5,100,60.5,0.96,F\\-Measure\\ \\(Seen\\)
3,1,One-shot visual object segmentation,YouTube-VOS - One-shot visual object segmentation benchmarking,2016-11,OSVOS,60.5,100.0,60.5,100,60.5,0.96,F\\-Measure\\ \\(Seen\\)
0,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,60.7,100.0,60.7,100,60.7,1.0,F\\-Measure\\ \\(Unseen\\)
1,1,One-shot visual object segmentation,YouTube-VOS - One-shot visual object segmentation benchmarking,2016-11,OSVOS,60.7,100.0,60.7,100,60.7,1.0,F\\-Measure\\ \\(Unseen\\)
2,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2016-11,OSVOS,60.7,100.0,60.7,100,60.7,1.0,F\\-Measure\\ \\(Unseen\\)
0,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,72.6,87.36,72.6,100,83.1,0.82,APL
1,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-12,AlphaPose,81.5,98.07,8.9,0.8476,83.1,0.92,APL
2,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,82.7,99.52,1.2,0.1143,83.1,0.93,APL
3,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-02,HRNet*,83.1,100.0,0.4,0.0381,83.1,0.94,APL
4,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2016-11,FCIS  +OHEM,50.0,91.74,50.0,100,54.5,0.56,APL
5,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),53.5,98.17,3.5,0.7778,54.5,0.6,APL
6,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-11,CenterMask + VoVNet99,54.3,99.63,0.8,0.1778,54.5,0.61,APL
7,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2020-01,BlendMask (ResNet-101 + DCN interval=3),54.5,100.0,0.2,0.0444,54.5,0.62,APL
8,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2016-11,CMU-Pose,68.2,89.97,68.2,100,75.8,0.77,APL
9,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2017-01,G-RMI,70.0,92.35,1.8,0.2368,75.8,0.79,APL
10,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2018-03,PersonLab,75.5,99.6,5.5,0.7237,75.8,0.85,APL
11,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet (HR-Net-48),75.8,100.0,0.3,0.0395,75.8,0.86,APL
12,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-11,CMU-Pose,68.2,81.48,68.2,100,83.7,0.77,APL
13,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-12,RMPE++,78.6,93.91,10.4,0.671,83.7,0.89,APL
14,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-04,Flow-based (ResNet-152),80.0,95.58,1.4,0.0903,83.7,0.9,APL
15,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-12,PoseFix,81.2,97.01,1.2,0.0774,83.7,0.92,APL
16,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-01,MSPN,81.5,97.37,0.3,0.0194,83.7,0.92,APL
17,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-02,HRNet-W48,83.1,99.28,1.6,0.1032,83.7,0.94,APL
18,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-10,HRNet-W48+DARK,83.7,100.0,0.6,0.0387,83.7,0.94,APL
19,1,Object Detection,COCO minival - Object Detection benchmarking,2016-12,FPN+,52.6,84.57,52.6,100,62.2,0.59,APL
20,1,Object Detection,COCO minival - Object Detection benchmarking,2017-12,"Cascade R-CNN (ResNet-101-FPN+, cascade)",57.4,92.28,4.8,0.5,62.2,0.65,APL
21,1,Object Detection,COCO minival - Object Detection benchmarking,2018-11,"Faster R-CNN (ResNet-101, DCNv2)",58.7,94.37,1.3,0.1354,62.2,0.66,APL
22,1,Object Detection,COCO minival - Object Detection benchmarking,2019-01,"ExtremeNet (Hourglass-104, multi-scale)",59.4,95.5,0.7,0.0729,62.2,0.67,APL
23,1,Object Detection,COCO minival - Object Detection benchmarking,2019-08,"Mask R-CNN (HRNetV2p-W48, cascade)",60.1,96.62,0.7,0.0729,62.2,0.68,APL
24,1,Object Detection,COCO minival - Object Detection benchmarking,2019-08,HTC (HRNetV2p-W48),62.2,100.0,2.1,0.2187,62.2,0.7,APL
25,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,82.4,93.0,82.4,100,88.6,0.93,APL
26,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-03,Mask R-CNN*,82.6,93.23,0.2,0.0323,88.6,0.93,APL
27,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,84.7,95.6,2.1,0.3387,88.6,0.96,APL
28,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,87.5,98.76,2.8,0.4516,88.6,0.99,APL
29,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,88.6,100.0,1.1,0.1774,88.6,1.0,APL
30,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,DeformConv-R-FCN (Aligned-Inception-ResNet),52.5,78.71,52.5,100,66.7,0.59,APL
31,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,RefineDet512+ (ResNet-101),54.1,81.11,1.6,0.1127,66.7,0.61,APL
32,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (ResNet-101, multi-scale)",54.9,82.31,0.8,0.0563,66.7,0.62,APL
33,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (DPN-98 with flip, multi-scale)",57.1,85.61,2.2,0.1549,66.7,0.64,APL
34,1,Object Detection,COCO test-dev - Object Detection benchmarking,2018-03,"PANet (ResNeXt-101, multi-scale)",60.0,89.96,2.9,0.2042,66.7,0.68,APL
35,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-01,"TridentNet (ResNet-101-Deformable, Image Pyramid)",60.3,90.4,0.3,0.0211,66.7,0.68,APL
36,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-06,"NAS-FPN (AmoebaNet-D, learned aug)",64.5,96.7,4.2,0.2958,66.7,0.73,APL
37,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-09,"Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)",66.7,100.0,2.2,0.1549,66.7,0.75,APL
38,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT,45.0,92.4,45.0,100,48.7,0.51,APL
39,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-11,CenterMask-Lite (ResNet-50-FPN),48.7,100.0,3.7,1.0,48.7,0.55,APL
0,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,59.8,99.67,59.8,100,60.0,0.81,Jaccard\\ \\(Seen\\)
1,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2018-02,OSMN,60.0,100.0,0.2,1.0,60.0,0.82,Jaccard\\ \\(Seen\\)
2,1,One-shot visual object segmentation,YouTube-VOS - One-shot visual object segmentation benchmarking,2016-11,OSVOS,59.8,99.67,59.8,100,60.0,0.81,Jaccard\\ \\(Seen\\)
3,1,One-shot visual object segmentation,YouTube-VOS - One-shot visual object segmentation benchmarking,2018-02,OSMN,60.0,100.0,0.2,1.0,60.0,0.82,Jaccard\\ \\(Seen\\)
4,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2017-06,OnAVOS,60.1,81.77,60.1,100,73.5,0.82,Jaccard\\ \\(Seen\\)
5,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2019-07,PTSNet,73.5,100.0,13.4,1.0,73.5,1.0,Jaccard\\ \\(Seen\\)
0,1,Semi-Supervised Video Object Segmentation,YouTube-VOS - Semi-Supervised Video Object Segmentation benchmarking,2016-11,OSVOS,54.2,100.0,54.2,100,54.2,0.84,Jaccard\\ \\(Unseen\\)
1,1,One-shot visual object segmentation,YouTube-VOS - One-shot visual object segmentation benchmarking,2016-11,OSVOS,54.2,100.0,54.2,100,54.2,0.84,Jaccard\\ \\(Unseen\\)
2,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2017-06,OnAVOS,46.6,72.47,46.6,100,64.3,0.72,Jaccard\\ \\(Unseen\\)
3,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2019-07,PTSNet,64.3,100.0,17.7,1.0,64.3,1.0,Jaccard\\ \\(Unseen\\)
0,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,60.6,82.56,60.6,100,73.4,0.82,APM
1,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-01,G-RMI,62.3,84.88,1.7,0.1328,73.4,0.85,APM
2,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,68.7,93.6,6.4,0.5,73.4,0.93,APM
3,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN+,69.5,94.69,0.8,0.0625,73.4,0.94,APM
4,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base,70.3,95.78,0.8,0.0625,73.4,0.96,APM
5,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,73.0,99.46,2.7,0.2109,73.4,0.99,APM
6,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-02,HRNet*,73.4,100.0,0.4,0.0312,73.4,1.0,APM
7,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2016-11,FCIS  +OHEM,31.3,70.5,31.3,100,44.4,0.43,APM
8,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),39.9,89.86,8.6,0.6565,44.4,0.54,APM
9,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-08,"Cascade R-CNN (ResNet-101-FPN, map-guided)",42.5,95.72,2.6,0.1985,44.4,0.58,APM
10,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-11,CenterMask + VoVNet99,44.4,100.0,1.9,0.145,44.4,0.6,APM
11,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2016-11,CMU-Pose,57.1,85.48,57.1,100,66.8,0.78,APM
12,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2017-01,G-RMI,62.3,93.26,5.2,0.5361,66.8,0.85,APM
13,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2018-03,PersonLab,64.1,95.96,1.8,0.1856,66.8,0.87,APM
14,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet (HR-Net-48),66.6,99.7,2.5,0.2577,66.8,0.9,APM
15,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-11,Identity Mapping Hourglass,66.8,100.0,0.2,0.0206,66.8,0.91,APM
16,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-12,RMPE,58.6,79.62,58.6,100,73.6,0.8,APM
17,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-12,RMPE++,68.0,92.39,9.4,0.6267,73.6,0.92,APM
18,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-04,Flow-based (ResNet-152),70.3,95.52,2.3,0.1533,73.6,0.96,APM
19,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-12,PoseFix,71.1,96.6,0.8,0.0533,73.6,0.97,APM
20,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-01,MSPN,72.3,98.23,1.2,0.08,73.6,0.98,APM
21,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-02,HRNet-W48,73.4,99.73,1.1,0.0733,73.6,1.0,APM
22,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-10,HRNet-W48+DARK,73.6,100.0,0.2,0.0133,73.6,1.0,APM
23,1,Object Detection,COCO minival - Object Detection benchmarking,2016-12,FPN+,43.3,86.08,43.3,100,50.3,0.59,APM
24,1,Object Detection,COCO minival - Object Detection benchmarking,2017-12,"Cascade R-CNN (ResNet-101-FPN+, cascade)",46.2,91.85,2.9,0.4143,50.3,0.63,APM
25,1,Object Detection,COCO minival - Object Detection benchmarking,2019-01,TridentNet (ResNet-101),47.0,93.44,0.8,0.1143,50.3,0.64,APM
26,1,Object Detection,COCO minival - Object Detection benchmarking,2019-08,"Mask R-CNN (HRNetV2p-W32, cascade)",47.9,95.23,0.9,0.1286,50.3,0.65,APM
27,1,Object Detection,COCO minival - Object Detection benchmarking,2019-08,Cascade R-CNN (HRNetV2p-W48),48.1,95.63,0.2,0.0286,50.3,0.65,APM
28,1,Object Detection,COCO minival - Object Detection benchmarking,2019-08,HTC (HRNetV2p-W48),50.3,100.0,2.2,0.3143,50.3,0.68,APM
29,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,DeformConv-R-FCN (Aligned-Inception-ResNet),40.1,71.86,40.1,100,55.8,0.54,APM
30,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,Mask R-CNN (ResNet-101-FPN),41.1,73.66,1.0,0.0637,55.8,0.56,APM
31,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),43.2,77.42,2.1,0.1338,55.8,0.59,APM
32,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-08,RetinaNet (ResNeXt-101-FPN),44.2,79.21,1.0,0.0637,55.8,0.6,APM
33,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,RefineDet512+ (ResNet-101),45.1,80.82,0.9,0.0573,55.8,0.61,APM
34,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (ResNet-101, multi-scale)",46.5,83.33,1.4,0.0892,55.8,0.63,APM
35,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (DPN-98 with flip, multi-scale)",48.8,87.46,2.3,0.1465,55.8,0.66,APM
36,1,Object Detection,COCO test-dev - Object Detection benchmarking,2018-03,"PANet (ResNeXt-101, multi-scale)",51.7,92.65,2.9,0.1847,55.8,0.7,APM
37,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-06,"NAS-FPN (AmoebaNet-D, learned aug)",55.5,99.46,3.8,0.242,55.8,0.75,APM
38,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-09,"Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)",55.8,100.0,0.3,0.0191,55.8,0.76,APM
39,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT,25.3,72.91,25.3,100,34.7,0.34,APM
40,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT-550 (ResNet-101-FPN),29.3,84.44,4.0,0.4255,34.7,0.4,APM
41,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-11,CenterMask-Lite (ResNet-50-FPN),34.7,100.0,5.4,0.5745,34.7,0.47,APM
0,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2016-11,Pose-AE,62.8,81.88,62.8,100,76.7,0.82,Test\\ AP
1,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2016-12,AlphaPose,73.3,95.57,10.5,0.7554,76.7,0.96,Test\\ AP
2,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2018-12,PoseFix,76.7,100.0,3.4,0.2446,76.7,1.0,Test\\ AP
0,1,Visual Object Tracking,YouTube-VOS - Visual Object Tracking benchmarking,2016-11,OSVOS,58.8,100.0,58.8,100,58.8,1.0,O\\ \\(Average\\ of\\ Measures\\)
0,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,70.2,85.61,70.2,100,82.0,0.85,AR
1,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,78.5,95.73,8.3,0.7034,82.0,0.95,AR
2,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN+,79.0,96.34,0.5,0.0424,82.0,0.96,AR
3,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,81.5,99.39,2.5,0.2119,82.0,0.99,AR
4,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-01,MSPN,81.6,99.51,0.1,0.0085,82.0,0.99,AR
5,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-02,HRNet*,82.0,100.0,0.4,0.0339,82.0,1.0,AR
6,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2016-11,CMU-Pose,66.5,80.8,66.5,100,82.3,0.81,AR
7,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2017-01,G-RMI,69.7,84.69,3.2,0.2025,82.3,0.85,AR
8,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2017-11,"CPN+ [6, 9]",79.0,95.99,9.3,0.5886,82.3,0.96,AR
9,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2018-12,PoseFix,79.9,97.08,0.9,0.057,82.3,0.97,AR
10,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-01,MSPN,81.6,99.15,1.7,0.1076,82.3,0.99,AR
11,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-02,HRNet-W48,82.0,99.64,0.4,0.0253,82.3,1.0,AR
12,1,Pose Estimation,COCO test-dev - Pose Estimation benchmarking,2019-10,HRNet-W48+DARK,82.3,100.0,0.3,0.019,82.3,1.0,AR
13,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,75.1,91.36,75.1,100,82.2,0.91,AR
14,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-03,Mask R-CNN*,75.4,91.73,0.3,0.0423,82.2,0.92,AR
15,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,78.7,95.74,3.3,0.4648,82.2,0.96,AR
16,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,80.5,97.93,1.8,0.2535,82.2,0.98,AR
17,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,82.2,100.0,1.7,0.2394,82.2,1.0,AR
18,1,3D Object Detection,nuScenes-FB - 3D Object Detection benchmarking,2019-05,RRPN + R101 - FB,42.1,100.0,42.1,100,42.1,0.51,AR
19,1,3D Object Detection,nuScenes-F - 3D Object Detection benchmarking,2019-05,RRPN + R101 - F,48.6,100.0,48.6,100,48.6,0.59,AR
20,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-11,Identity Mapping Hourglass,72.1,100.0,72.1,100,72.1,0.88,AR
0,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,89.5,92.94,89.5,100,96.3,0.93,AR50
1,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,95.1,98.75,5.6,0.8235,96.3,0.99,AR50
2,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,95.8,99.48,0.7,0.1029,96.3,0.99,AR50
3,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-01,MSPN,96.3,100.0,0.5,0.0735,96.3,1.0,AR50
4,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,90.7,94.48,90.7,100,96.0,0.94,AR50
5,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-03,Mask R-CNN*,93.2,97.08,2.5,0.4717,96.0,0.97,AR50
6,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,94.7,98.65,1.5,0.283,96.0,0.98,AR50
7,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,95.1,99.06,0.4,0.0755,96.0,0.99,AR50
8,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,96.0,100.0,0.9,0.1698,96.0,1.0,AR50
9,1,Multi-Person Pose Estimation,COCO test-dev - Multi-Person Pose Estimation benchmarking,2019-11,Identity Mapping Hourglass,88.2,100.0,88.2,100,88.2,0.92,AR50
0,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,76.0,86.17,76.0,100,88.2,0.86,AR75
1,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,85.3,96.71,9.3,0.7623,88.2,0.97,AR75
2,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN+,85.9,97.39,0.6,0.0492,88.2,0.97,AR75
3,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,88.2,100.0,2.3,0.1885,88.2,1.0,AR75
4,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,80.7,92.02,80.7,100,87.7,0.91,AR75
5,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-03,Mask R-CNN*,81.2,92.59,0.5,0.0714,87.7,0.92,AR75
6,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,84.8,96.69,3.6,0.5143,87.7,0.96,AR75
7,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,86.3,98.4,1.5,0.2143,87.7,0.98,AR75
8,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,87.7,100.0,1.4,0.2,87.7,0.99,AR75
0,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,78.1,89.56,78.1,100,87.2,0.9,ARL
1,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,84.3,96.67,6.2,0.6813,87.2,0.97,ARL
2,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN+,84.6,97.02,0.3,0.033,87.2,0.97,ARL
3,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,87.2,100.0,2.6,0.2857,87.2,1.0,ARL
4,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,74.5,89.54,74.5,100,83.2,0.85,ARL
5,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-03,Mask R-CNN*,76.8,92.31,2.3,0.2644,83.2,0.88,ARL
6,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,78.1,93.87,1.3,0.1494,83.2,0.9,ARL
7,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,82.9,99.64,4.8,0.5517,83.2,0.95,ARL
8,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,83.2,100.0,0.3,0.0345,83.2,0.95,ARL
0,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2016-11,AE,64.6,83.35,64.6,100,77.5,0.83,ARM
1,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN,74.2,95.74,9.6,0.7442,77.5,0.96,ARM
2,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2017-11,CPN+,74.8,96.52,0.6,0.0465,77.5,0.97,ARM
3,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2018-04,Simple Base+*,77.4,99.87,2.6,0.2016,77.5,1.0,ARM
4,1,Keypoint Detection,COCO test-dev - Keypoint Detection benchmarking,2019-01,MSPN,77.5,100.0,0.1,0.0078,77.5,1.0,ARM
5,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-01,G-RMI*,69.7,89.94,69.7,100,77.5,0.9,ARM
6,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-03,Mask R-CNN*,70.2,90.58,0.5,0.0641,77.5,0.91,ARM
7,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2017-11,CPN+,74.3,95.87,4.1,0.5256,77.5,0.96,ARM
8,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2018-04,Simple Base+*,75.3,97.16,1.0,0.1282,77.5,0.97,ARM
9,1,Keypoint Detection,COCO test-challenge - Keypoint Detection benchmarking,2019-01,MSPN+*,77.5,100.0,2.2,0.2821,77.5,1.0,ARM
0,1,Outdoor Light Source Estimation,SUN360 - Outdoor Light Source Estimation benchmarking,2016-11,ÇuNNy,1.25,100.0,1.25,100,1.25,1.0,Median\\ Relighting\\ Error
0,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2016-11,FCIS  +OHEM,7.1,26.1,7.1,100,27.2,0.2,APS
1,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),16.9,62.13,9.8,0.4876,27.2,0.48,APS
2,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-08,"Cascade R-CNN (ResNet-101-FPN, map-guided)",21.2,77.94,4.3,0.2139,27.2,0.6,APS
3,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-11,CenterMask + VoVNet99,24.4,89.71,3.2,0.1592,27.2,0.69,APS
4,1,Instance Segmentation,COCO test-dev - Instance Segmentation benchmarking,2019-11,CenterMask + VoVNetV2-99 (multi-scale),27.2,100.0,2.8,0.1393,27.2,0.77,APS
5,1,Object Detection,COCO minival - Object Detection benchmarking,2016-12,FPN+,22.9,78.42,22.9,100,29.2,0.65,APS
6,1,Object Detection,COCO minival - Object Detection benchmarking,2017-12,"Cascade R-CNN (ResNet-101-FPN+, cascade)",23.8,81.51,0.9,0.1429,29.2,0.67,APS
7,1,Object Detection,COCO minival - Object Detection benchmarking,2019-01,TridentNet (ResNet-101),24.9,85.27,1.1,0.1746,29.2,0.7,APS
8,1,Object Detection,COCO minival - Object Detection benchmarking,2019-01,"ExtremeNet (Hourglass-104, multi-scale)",25.7,88.01,0.8,0.127,29.2,0.72,APS
9,1,Object Detection,COCO minival - Object Detection benchmarking,2019-08,HTC (HRNetV2p-W32),27.0,92.47,1.3,0.2063,29.2,0.76,APS
10,1,Object Detection,COCO minival - Object Detection benchmarking,2019-08,HTC (HRNetV2p-W48),28.8,98.63,1.8,0.2857,29.2,0.81,APS
11,1,Object Detection,COCO minival - Object Detection benchmarking,2019-11,CenterMask+VoVNetV2-99 (single-scale),29.2,100.0,0.4,0.0635,29.2,0.82,APS
12,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,DeformConv-R-FCN (Aligned-Inception-ResNet),19.4,54.65,19.4,100,35.5,0.55,APS
13,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-03,Mask R-CNN (ResNeXt-101-FPN),22.1,62.25,2.7,0.1677,35.5,0.62,APS
14,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-08,RetinaNet (ResNeXt-101-FPN),24.1,67.89,2.0,0.1242,35.5,0.68,APS
15,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,RefineDet512+ (ResNet-101),25.6,72.11,1.5,0.0932,35.5,0.72,APS
16,1,Object Detection,COCO test-dev - Object Detection benchmarking,2017-11,"D-RFCN + SNIP (DPN-98 with flip, multi-scale)",29.3,82.54,3.7,0.2298,35.5,0.83,APS
17,1,Object Detection,COCO test-dev - Object Detection benchmarking,2018-03,"PANet (ResNeXt-101, multi-scale)",30.1,84.79,0.8,0.0497,35.5,0.85,APS
18,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-01,"TridentNet (ResNet-101-Deformable, Image Pyramid)",31.8,89.58,1.7,0.1056,35.5,0.9,APS
19,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-06,"NAS-FPN (AmoebaNet-D, learned aug)",34.2,96.34,2.4,0.1491,35.5,0.96,APS
20,1,Object Detection,COCO test-dev - Object Detection benchmarking,2019-09,"Cascade Mask R-CNN (Triple-ResNeXt152, multi-scale)",35.5,100.0,1.3,0.0807,35.5,1.0,APS
21,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT,5.0,38.76,5.0,100,12.9,0.14,APS
22,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-04,YOLACT-550 (ResNet-101-FPN),9.2,71.32,4.2,0.5316,12.9,0.26,APS
23,1,Real-time Instance Segmentation,MSCOCO - Real-time Instance Segmentation benchmarking,2019-11,CenterMask-Lite (ResNet-50-FPN),12.9,100.0,3.7,0.4684,12.9,0.36,APS
0,1,Pose Tracking,Multi-Person PoseTrack - Pose Tracking benchmarking,2016-11,PoseTrack,55.7,100.0,55.7,100,55.7,0.65,MOTP
1,1,3D Multi-Object Tracking,KITTI - 3D Multi-Object Tracking benchmarking,2018-02,BeyondPixels,85.73,100.0,85.73,100,85.73,1.0,MOTP
0,1,Instance Segmentation,Cityscapes test - Instance Segmentation benchmarking,2016-11,Deep Watershed Transform,19.4,49.74,19.4,100,39.0,0.33,Average\\ Precision
1,1,Instance Segmentation,Cityscapes test - Instance Segmentation benchmarking,2017-04,Dynamically Instantiated Network,23.4,60.0,4.0,0.2041,39.0,0.4,Average\\ Precision
2,1,Instance Segmentation,Cityscapes test - Instance Segmentation benchmarking,2019-06,Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth,27.7,71.03,4.3,0.2194,39.0,0.47,Average\\ Precision
3,1,Instance Segmentation,Cityscapes test - Instance Segmentation benchmarking,2019-11,Panoptic-DeepLab [Cityscapes-fine],34.6,88.72,6.9,0.352,39.0,0.59,Average\\ Precision
4,1,Instance Segmentation,Cityscapes test - Instance Segmentation benchmarking,2019-11,Panoptic-DeepLab [Mapillary Vistas],39.0,100.0,4.4,0.2245,39.0,0.66,Average\\ Precision
5,1,Object Detection,iSAID - Object Detection benchmarking,2017-03,Mask-RCNN+,37.18,89.25,37.18,100,41.66,0.63,Average\\ Precision
6,1,Object Detection,iSAID - Object Detection benchmarking,2018-03,PANet,41.66,100.0,4.48,1.0,41.66,0.71,Average\\ Precision
7,1,Long-tail Learning,EGTEA - Long-tail Learning benchmarking,2017-08,Focal loss (3D- ResNeXt101),59.09,100.0,59.09,100,59.09,1.0,Average\\ Precision
8,1,Instance Segmentation,iSAID - Instance Segmentation benchmarking,2018-03,PANet,34.17,100.0,34.17,100,34.17,0.58,Average\\ Precision
0,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2016-11,Part Affinity Fields,60.5,78.27,60.5,100,77.3,0.78,Validation\\ AP
1,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2017-03,Mask R-CNN,69.2,89.52,8.7,0.5179,77.3,0.9,Validation\\ AP
2,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2018-04,ResNet-50,72.2,93.4,3.0,0.1786,77.3,0.93,Validation\\ AP
3,1,Keypoint Detection,COCO - Keypoint Detection benchmarking,2018-12,PoseFix,77.3,100.0,5.1,0.3036,77.3,1.0,Validation\\ AP
0,1,Visual Object Tracking,VOT2017/18 - Visual Object Tracking benchmarking,2016-11,CSRDCF,0.263,58.97,0.263,100,0.446,0.56,Expected\\ Average\\ Overlap\\ \\(EAO\\)
1,1,Visual Object Tracking,VOT2017/18 - Visual Object Tracking benchmarking,2016-11,ECO,0.28,62.78,0.017,0.0929,0.446,0.6,Expected\\ Average\\ Overlap\\ \\(EAO\\)
2,1,Visual Object Tracking,VOT2017/18 - Visual Object Tracking benchmarking,2017-06,LSART,0.323,72.42,0.043,0.235,0.446,0.69,Expected\\ Average\\ Overlap\\ \\(EAO\\)
3,1,Visual Object Tracking,VOT2017/18 - Visual Object Tracking benchmarking,2018-03,STRCF,0.345,77.35,0.022,0.1202,0.446,0.74,Expected\\ Average\\ Overlap\\ \\(EAO\\)
4,1,Visual Object Tracking,VOT2017/18 - Visual Object Tracking benchmarking,2018-06,SiamRPN,0.383,85.87,0.038,0.2077,0.446,0.82,Expected\\ Average\\ Overlap\\ \\(EAO\\)
5,1,Visual Object Tracking,VOT2017/18 - Visual Object Tracking benchmarking,2018-12,SiamRPN++,0.414,92.83,0.031,0.1694,0.446,0.89,Expected\\ Average\\ Overlap\\ \\(EAO\\)
6,1,Visual Object Tracking,VOT2017/18 - Visual Object Tracking benchmarking,2019-07,SiamMask_E,0.446,100.0,0.032,0.1749,0.446,0.96,Expected\\ Average\\ Overlap\\ \\(EAO\\)
7,1,Visual Object Tracking,VOT2016 - Visual Object Tracking benchmarking,2017-04,CFCF,0.3903,83.76,0.3903,100,0.466,0.84,Expected\\ Average\\ Overlap\\ \\(EAO\\)
8,1,Visual Object Tracking,VOT2016 - Visual Object Tracking benchmarking,2019-07,SiamMask_E,0.466,100.0,0.0757,1.0,0.466,1.0,Expected\\ Average\\ Overlap\\ \\(EAO\\)
9,1,Visual Object Tracking,VOT2017 - Visual Object Tracking benchmarking,2019-01,SiamRPN+,0.3,75.57,0.3,100,0.397,0.64,Expected\\ Average\\ Overlap\\ \\(EAO\\)
10,1,Visual Object Tracking,VOT2017 - Visual Object Tracking benchmarking,2019-07,GFS-DCF,0.397,100.0,0.097,1.0,0.397,0.85,Expected\\ Average\\ Overlap\\ \\(EAO\\)
11,1,Visual Object Tracking,VOT2019 - Visual Object Tracking benchmarking,2019-07,SiamMask_E,0.309,100.0,0.309,100,0.309,0.66,Expected\\ Average\\ Overlap\\ \\(EAO\\)
0,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2016-12,PointNet,80.4,94.48,80.4,100,85.1,0.94,Class\\ Average\\ IoU
1,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2016-12,SSCNN,82.0,96.36,1.6,0.3404,85.1,0.96,Class\\ Average\\ IoU
2,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2017-11,SGPN,82.8,97.3,0.8,0.1702,85.1,0.97,Class\\ Average\\ IoU
3,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2018-01,PointCNN,84.6,99.41,1.8,0.383,85.1,0.99,Class\\ Average\\ IoU
4,1,3D Part Segmentation,ShapeNet-Part - 3D Part Segmentation benchmarking,2019-04,KPConv,85.1,100.0,0.5,0.1064,85.1,1.0,Class\\ Average\\ IoU
0,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2016-12,"MCB [11, 12]",62.27,85.89,62.27,100,72.5,0.86,overall
1,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2017-05,MUTAN,67.4,92.97,5.13,0.5015,72.5,0.93,overall
2,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2017-07,Up-Down,70.34,97.02,2.94,0.2874,72.5,0.97,overall
3,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2018-05,BAN+Glove+Counter,70.4,97.1,0.06,0.0059,72.5,0.97,overall
4,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2019-06,MCANed-6,70.9,97.79,0.5,0.0489,72.5,0.98,overall
5,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2019-08,VisualBERT,71.0,97.93,0.1,0.0098,72.5,0.98,overall
6,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2019-08,LXMERT,72.5,100.0,1.5,0.1466,72.5,1.0,overall
7,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-04,Pythia v0.3 ,54.72,98.77,54.72,100,55.4,0.75,overall
8,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",55.4,100.0,0.68,1.0,55.4,0.76,overall
0,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2016-12,PointNet,66.2,80.73,66.2,100,82.0,0.76,mAcc
1,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2017-11,SPG,73.0,89.02,6.8,0.4304,82.0,0.84,mAcc
2,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2018-12,PointCNN,75.6,92.2,2.6,0.1646,82.0,0.87,mAcc
3,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2019-04,KPConv,79.1,96.46,3.5,0.2215,82.0,0.91,mAcc
4,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2019-11,RandLA-Net,82.0,100.0,2.9,0.1835,82.0,0.94,mAcc
5,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2016-12,PointNet,49.0,67.31,49.0,100,72.8,0.56,mAcc
6,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2017-10,SegCloud,57.4,78.85,8.4,0.3529,72.8,0.66,mAcc
7,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2017-11,SPG,66.5,91.35,9.1,0.3824,72.8,0.76,mAcc
8,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2019-04,MinkowskiNet,71.7,98.49,5.2,0.2185,72.8,0.82,mAcc
9,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2019-04,KPConv,72.8,100.0,1.1,0.0462,72.8,0.84,mAcc
10,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2018-01,PointCNN,75.61,100.0,75.61,100,75.61,0.87,mAcc
11,1,3D Semantic Segmentation,S3DIS - 3D Semantic Segmentation benchmarking,2019-07,PVCNN++ (1xC) volumetric,87.12,100.0,87.12,100,87.12,1.0,mAcc
0,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2016-12,PointNet,78.5,88.4,78.5,100,88.8,0.83,oAcc
1,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2017-11,SPG,85.5,96.28,7.0,0.6796,88.8,0.9,oAcc
2,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2018-12,PointCNN,88.1,99.21,2.6,0.2524,88.8,0.93,oAcc
3,1,Semantic Segmentation,S3DIS - Semantic Segmentation benchmarking,2019-04,ConvPoint,88.8,100.0,0.7,0.068,88.8,0.94,oAcc
4,1,Semantic Segmentation,S3DIS Area5 - Semantic Segmentation benchmarking,2017-11,SPG,86.38,100.0,86.38,100,86.38,0.91,oAcc
5,1,Semantic Segmentation,Semantic3D - Semantic Segmentation benchmarking,2017-11,SPG,92.9,98.0,92.9,100,94.8,0.98,oAcc
6,1,Semantic Segmentation,Semantic3D - Semantic Segmentation benchmarking,2019-11,RandLA-Net,94.8,100.0,1.9,1.0,94.8,1.0,oAcc
0,1,3D Point Cloud Classification,ModelNet40 - 3D Point Cloud Classification benchmarking,2016-12,PointNet,86.0,100.0,86,100,86,1.0,Mean\\ Accuracy
1,1,Medical Object Detection,Barrett’s Esophagus - Medical Object Detection benchmarking,2017-03,Sliding Window,74.0,91.36,74,100,81,0.86,Mean\\ Accuracy
2,1,Medical Object Detection,Barrett’s Esophagus - Medical Object Detection benchmarking,2018-11,Attention-based model,81.0,100.0,7,1.0,81,0.94,Mean\\ Accuracy
0,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2016-12,PSPNet,55.38,98.17,55.38,100,56.41,0.98,Test\\ Score
1,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2018-03,EncNet,55.67,98.69,0.29,0.2816,56.41,0.99,Test\\ Score
2,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2019-03,EncNet + JPU,55.84,98.99,0.17,0.165,56.41,0.99,Test\\ Score
3,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2019-11,LaU-regression-loss,56.32,99.84,0.48,0.466,56.41,1.0,Test\\ Score
4,1,Semantic Segmentation,ADE20K - Semantic Segmentation benchmarking,2019-11,LaU-offset-loss,56.41,100.0,0.09,0.0874,56.41,1.0,Test\\ Score
0,1,Real-Time Semantic Segmentation,NYU Depth v2 - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet18,19.0,26.39,19,100,72,0.26,Speed\\(ms/f\\)
1,1,Real-Time Semantic Segmentation,NYU Depth v2 - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet50,47.0,65.28,28,0.5283,72,0.65,Speed\\(ms/f\\)
2,1,Real-Time Semantic Segmentation,NYU Depth v2 - Real-Time Semantic Segmentation benchmarking,2016-12,PSPNet101,72.0,100.0,25,0.4717,72,1.0,Speed\\(ms/f\\)
0,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2016-12,FlowNet2,45.2,77.4,45.2,100,58.4,0.77,PCK\\-at\\-0\\.1
1,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2019-04,mgPFF+ft 1st,58.4,100.0,13.2,1.0,58.4,1.0,PCK\\-at\\-0\\.1
0,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2016-12,FlowNet2,73.5,85.56,73.5,100,85.9,0.86,PCK\\-at\\-0\\.3
1,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2018-06,ColorPointer,80.8,94.06,7.3,0.5887,85.9,0.94,PCK\\-at\\-0\\.3
2,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2019-04,mgPFF+ft 1st,85.9,100.0,5.1,0.4113,85.9,1.0,PCK\\-at\\-0\\.3
0,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2016-12,FlowNet2,80.6,89.76,80.6,100,89.8,0.9,PCK\\-at\\-0\\.4
1,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2018-06,ColorPointer,87.5,97.44,6.9,0.75,89.8,0.97,PCK\\-at\\-0\\.4
2,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2019-04,mgPFF+ft 1st,89.8,100.0,2.3,0.25,89.8,1.0,PCK\\-at\\-0\\.4
0,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2016-12,FlowNet2,85.5,92.53,85.5,100,92.4,0.93,PCK\\-at\\-0\\.5
1,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2018-06,ColorPointer,91.4,98.92,5.9,0.8551,92.4,0.99,PCK\\-at\\-0\\.5
2,1,Skeleton Based Action Recognition,JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking,2019-04,mgPFF+ft 1st,92.4,100.0,1.0,0.1449,92.4,1.0,PCK\\-at\\-0\\.5
0,1,Image-to-Image Translation,SYNTHIA-to-Cityscapes - Image-to-Image Translation benchmarking,2016-12,FCNs in the wild,20.2,37.9,20.2,100,53.3,0.38,mIoU\\ \\(13\\ classes\\)
1,1,Image-to-Image Translation,SYNTHIA-to-Cityscapes - Image-to-Image Translation benchmarking,2017-07,CDA,29.0,54.41,8.8,0.2659,53.3,0.54,mIoU\\ \\(13\\ classes\\)
2,1,Image-to-Image Translation,SYNTHIA-to-Cityscapes - Image-to-Image Translation benchmarking,2018-11,ADVENT,48.0,90.06,19.0,0.574,53.3,0.9,mIoU\\ \\(13\\ classes\\)
3,1,Image-to-Image Translation,SYNTHIA-to-Cityscapes - Image-to-Image Translation benchmarking,2019-03,SWD,48.1,90.24,0.1,0.003,53.3,0.9,mIoU\\ \\(13\\ classes\\)
4,1,Image-to-Image Translation,SYNTHIA-to-Cityscapes - Image-to-Image Translation benchmarking,2019-04,DADA (ResNet-101),49.8,93.43,1.7,0.0514,53.3,0.93,mIoU\\ \\(13\\ classes\\)
5,1,Image-to-Image Translation,SYNTHIA-to-Cityscapes - Image-to-Image Translation benchmarking,2019-04,Bidirectional Learning (ResNet-101),51.4,96.44,1.6,0.0483,53.3,0.96,mIoU\\ \\(13\\ classes\\)
6,1,Image-to-Image Translation,SYNTHIA-to-Cityscapes - Image-to-Image Translation benchmarking,2019-08,PyCDA (ResNet-101),53.3,100.0,1.9,0.0574,53.3,1.0,mIoU\\ \\(13\\ classes\\)
0,1,Rotated MNIST,Rotated MNIST - Rotated MNIST benchmarking,2016-12,H-net,1.69,100.0,1.69,100,1.69,1.0,Test\\ error
0,1,3D Face Reconstruction,Florence - 3D Face Reconstruction benchmarking,2016-12,3DMM-CNN,1.93,100.0,1.93,100,1.93,0.12,Average\\ 3D\\ Error
1,1,Hand Pose Estimation,NYU Hands - Hand Pose Estimation benchmarking,2017-02,REN,12.7,81.41,12.7,100,15.6,0.81,Average\\ 3D\\ Error
2,1,Hand Pose Estimation,NYU Hands - Hand Pose Estimation benchmarking,2017-07,REN,15.6,100.0,2.9,1.0,15.6,1.0,Average\\ 3D\\ Error
3,1,Hand Pose Estimation,ICVL Hands - Hand Pose Estimation benchmarking,2017-02,REN,7.5,92.59,7.5,100,8.1,0.48,Average\\ 3D\\ Error
4,1,Hand Pose Estimation,ICVL Hands - Hand Pose Estimation benchmarking,2017-08,DeepPrior++,8.1,100.0,0.6,1.0,8.1,0.52,Average\\ 3D\\ Error
5,1,Hand Pose Estimation,MSRA Hands - Hand Pose Estimation benchmarking,2017-02,REN,9.8,100.0,9.8,100,9.8,0.63,Average\\ 3D\\ Error
6,1,Hand Pose Estimation,HANDS 2017 - Hand Pose Estimation benchmarking,2017-08,THU VCLab,11.7,98.24,11.7,100,11.91,0.75,Average\\ 3D\\ Error
7,1,Hand Pose Estimation,HANDS 2017 - Hand Pose Estimation benchmarking,2017-12,Vanora,11.91,100.0,0.21,1.0,11.91,0.76,Average\\ 3D\\ Error
8,1,Hand Pose Estimation,HANDS 2019 - Hand Pose Estimation benchmarking,2020-01,HandAugment,13.66,100.0,13.66,100,13.66,0.88,Average\\ 3D\\ Error
0,1,Weakly-supervised 3D Human Pose Estimation,Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking,2017-01,Tome et al.,1.0,50.0,1,100,2,0.5,Number\\ of\\ Views
1,1,Weakly-supervised 3D Human Pose Estimation,Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking,2019-03,Kocabas et al.,2.0,100.0,1,1.0,2,1.0,Number\\ of\\ Views
0,1,Weakly-supervised 3D Human Pose Estimation,Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking,2017-01,Tome et al.,1.0,0.41,1,100,243,0.0,Number\\ of\\ Frames\\ Per\\ View
1,1,Weakly-supervised 3D Human Pose Estimation,Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking,2018-11,Pavllo et al.,243.0,100.0,242,1.0,243,1.0,Number\\ of\\ Frames\\ Per\\ View
0,1,Optical Character Recognition,FSNS - Test - Optical Character Recognition benchmarking,2017-02,STREET,27.54,100.0,27.54,100,27.54,1.0,Sequence\\ error
0,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-02,FRCNN+Seg,22.6,88.28,22.6,100,25.6,0.88,Small\\ MR\\^\\-2
1,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-02,FRCNN,25.6,100.0,3.0,1.0,25.6,1.0,Small\\ MR\\^\\-2
0,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-02,FRCNN+Seg,6.7,93.06,6.7,100,7.2,0.93,Medium\\ MR\\^\\-2
1,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-02,FRCNN,7.2,100.0,0.5,1.0,7.2,1.0,Medium\\ MR\\^\\-2
0,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-02,FRCNN+Seg,8.0,100.0,8.0,100,8.0,1.0,Large\\ MR\\^\\-2
0,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-02,FRCNN+Seg,14.8,95.48,14.8,100,15.5,0.95,Reasonable\\ MR\\^\\-2
1,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-02,FRCNN,15.4,99.35,0.6,0.8571,15.5,0.99,Reasonable\\ MR\\^\\-2
2,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2018-07,TLL,15.5,100.0,0.1,0.1429,15.5,1.0,Reasonable\\ MR\\^\\-2
0,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Shoes - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,0.011,10.09,0.011,100,0.109,0.06,Diversity
1,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Shoes - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-11,BicycleGAN,0.104,95.41,0.093,0.949,0.109,0.59,Diversity
2,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Shoes - Multimodal Unsupervised Image-To-Image Translation benchmarking,2018-04,MUNIT,0.109,100.0,0.005,0.051,0.109,0.62,Diversity
3,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Handbags - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,0.023,13.14,0.023,100,0.175,0.13,Diversity
4,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Handbags - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-11,BicycleGAN,0.14,80.0,0.117,0.7697,0.175,0.8,Diversity
5,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Handbags - Multimodal Unsupervised Image-To-Image Translation benchmarking,2018-04,MUNIT,0.175,100.0,0.035,0.2303,0.175,1.0,Diversity
0,1,Multimodal Unsupervised Image-To-Image Translation,Cats-and-Dogs - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,0.826,78.67,0.826,100,1.05,0.01,IS
1,1,Multimodal Unsupervised Image-To-Image Translation,Cats-and-Dogs - Multimodal Unsupervised Image-To-Image Translation benchmarking,2018-04,MUNIT,1.05,100.0,0.224,1.0,1.05,0.01,IS
2,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2017-05,PG2,2.4152,94.6,2.4152,100,2.5532,0.02,IS
3,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2017-07,SAMG,2.4919,97.6,0.0767,0.5558,2.5532,0.02,IS
4,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,2.5532,100.0,0.0613,0.4442,2.5532,0.02,IS
5,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-05,PG2,3.3699,98.8,3.3699,100,3.4107,0.03,IS
6,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-12,DPIG,3.3874,99.32,0.0175,0.4289,3.4107,0.03,IS
7,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,3.4107,100.0,0.0233,0.5711,3.4107,0.03,IS
8,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2017-05,PG Squared,3.09,89.85,3.09,100,3.439,0.02,IS
9,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2017-12,Disentangled PG,3.228,93.86,0.138,0.3954,3.439,0.03,IS
10,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2017-12,Deformable GAN,3.439,100.0,0.211,0.6046,3.439,0.03,IS
11,1,Image Generation,ImageNet 128x128 - Image Generation benchmarking,2018-09,BigGAN-deep,124.5,100.0,124.5,100,124.5,1.0,IS
12,1,Pose Transfer,Market-1501 - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,3.323,100.0,3.323,100,3.323,0.03,IS
0,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Handbags - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,-37.3,100.0,-37.3,100,-37.3,1.0,Quality
1,1,Multimodal Unsupervised Image-To-Image Translation,Edge-to-Shoes - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,-37.4,100.0,-37.4,100,-37.4,1.0,Quality
0,1,Multimodal Unsupervised Image-To-Image Translation,Cats-and-Dogs - Multimodal Unsupervised Image-To-Image Translation benchmarking,2017-03,UNIT,0.115,11.07,0.115,100,1.039,0.11,CIS
1,1,Multimodal Unsupervised Image-To-Image Translation,Cats-and-Dogs - Multimodal Unsupervised Image-To-Image Translation benchmarking,2018-04,MUNIT,1.039,100.0,0.924,1.0,1.039,1.0,CIS
0,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,CDC,7.9,37.98,7.9,100,20.8,0.38,mAP\\ IOU\\-at\\-0\\.7
1,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-05,CBR-TS,9.9,47.6,2.0,0.155,20.8,0.48,mAP\\ IOU\\-at\\-0\\.7
2,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2018-04,TAL-Net,20.8,100.0,10.9,0.845,20.8,1.0,mAP\\ IOU\\-at\\-0\\.7
3,1,Temporal Action Localization,ActivityNet-1.2 - Temporal Action Localization benchmarking,2020-01,DeepMetricLearner,16.3,100.0,16.3,100,16.3,0.78,mAP\\ IOU\\-at\\-0\\.7
0,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-03,CDC,13.1,38.76,13.1,100,33.8,0.39,mAP\\ IOU\\-at\\-0\\.6
1,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2017-05,CBR-TS,19.1,56.51,6.0,0.2899,33.8,0.57,mAP\\ IOU\\-at\\-0\\.6
2,1,Temporal Action Localization,THUMOS’14 - Temporal Action Localization benchmarking,2018-04,TAL-Net,33.8,100.0,14.7,0.7101,33.8,1.0,mAP\\ IOU\\-at\\-0\\.6
0,1,Image Compression,ImageNet32 - Image Compression benchmarking,2017-03,MS-PixelCNN,3.95,61.53,3.95,100,6.42,0.62,bpsp
1,1,Image Compression,ImageNet32 - Image Compression benchmarking,2018-11,L3C,4.76,74.14,0.81,0.3279,6.42,0.74,bpsp
2,1,Image Compression,ImageNet32 - Image Compression benchmarking,2018-11,PNG,6.42,100.0,1.66,0.6721,6.42,1.0,bpsp
0,1,Real-Time Object Detection,COCO - Real-Time Object Detection benchmarking,2017-03,Mask R-CNN X-152-32x8d,333.0,100.0,333.0,100,333.0,1.0,inference\\ time\\ \\(ms\\)
0,1,Real-Time Object Detection,COCO minival - Real-Time Object Detection benchmarking,2017-03,Mask R-CNN X-152-32x8d,45.2,100.0,45.2,100,45.2,1.0,APbb75
0,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2017-03,Mask R-CNN+COCO,54.0,91.99,54.0,100,58.7,0.92,PQth
1,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-12,TASCNet (ResNet-50),56.0,95.4,2.0,0.4255,58.7,0.95,PQth
2,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-12,"TASCNet (ResNet-50, multi-scale)",56.1,95.57,0.1,0.0213,58.7,0.96,PQth
3,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2019-01,"UPSNet (ResNet-101, multiscale)",57.6,98.13,1.5,0.3191,58.7,0.98,PQth
4,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2019-09,AdaptIS (ResNeXt-101),58.7,100.0,1.1,0.234,58.7,1.0,PQth
5,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2018-09,JSIS-Net (ResNet-50),29.6,53.05,29.6,100,55.8,0.5,PQth
6,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2018-12,AUNet (ResNeXt-152-FPN),55.8,100.0,26.2,1.0,55.8,0.95,PQth
0,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2017-03,Mask R-CNN,60.3,89.2,60.3,100,67.6,0.71,mAP\\ at\\-0\\.5:0\\.95
1,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2018-12,Joint-candidate SPPE +,66.0,97.63,5.7,0.7808,67.6,0.78,mAP\\ at\\-0\\.5:0\\.95
2,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet(HR-Net-48),67.6,100.0,1.6,0.2192,67.6,0.8,mAP\\ at\\-0\\.5:0\\.95
3,1,Traffic Sign Recognition,DFG traffic-sign dataset - Traffic Sign Recognition benchmarking,2019-04,Mask R-CNN (ResNet50),82.3,97.51,82.3,100,84.4,0.98,mAP\\ at\\-0\\.5:0\\.95
4,1,Traffic Sign Recognition,DFG traffic-sign dataset - Traffic Sign Recognition benchmarking,2019-04,Mask R-CNN with adaptations for traffic sings and augmentations (ResNet50),84.4,100.0,2.1,1.0,84.4,1.0,mAP\\ at\\-0\\.5:0\\.95
0,1,Face Alignment,LS3D-W Balanced - Face Alignment benchmarking,2017-03,3D-FAN,72.3,100.0,72.3,100,72.3,1.0,AUC0\\.07
0,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2017-03,TriNet,99.01,99.41,99.01,100,99.6,0.99,Rank\\-5
1,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2017-11,AlignedReID (RK),99.6,100.0,0.59,1.0,99.6,1.0,Rank\\-5
2,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2017-03,TriNet (RK),90.76,96.86,90.76,100,93.7,0.91,Rank\\-5
3,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2017-03,TriNet,91.36,97.5,0.6,0.2041,93.7,0.92,Rank\\-5
4,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2019-08,TKP,93.7,100.0,2.34,0.7959,93.7,0.94,Rank\\-5
5,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,LuNet (RK),91.89,92.91,91.89,100,98.9,0.92,Rank\\-5
6,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,LuNet,92.34,93.37,0.45,0.0642,98.9,0.93,Rank\\-5
7,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,TriNet (RK),93.38,94.42,1.04,0.1484,98.9,0.94,Rank\\-5
8,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2017-03,TriNet,94.21,95.26,0.83,0.1184,98.9,0.95,Rank\\-5
9,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2018-12,"st-ReID(RE, RK)",98.9,100.0,4.69,0.669,98.9,0.99,Rank\\-5
10,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,60.7,63.76,60.7,100,95.2,0.61,Rank\\-5
11,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,75.8,79.62,15.1,0.4377,95.2,0.76,Rank\\-5
12,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),95.2,100.0,19.4,0.5623,95.2,0.96,Rank\\-5
13,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,43.4,51.67,43.4,100,84.0,0.44,Rank\\-5
14,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,62.3,74.17,18.9,0.4655,84.0,0.63,Rank\\-5
15,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),84.0,100.0,21.7,0.5345,84.0,0.84,Rank\\-5
16,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-09,DGM+IDE+,81.3,81.87,81.3,100,99.3,0.82,Rank\\-5
17,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-09,DGM+MLAPG+,92.5,93.15,11.2,0.6222,99.3,0.93,Rank\\-5
18,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-10,SMP*,95.6,96.27,3.1,0.1722,99.3,0.96,Rank\\-5
19,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2018-06,Snippet (Supervised),99.3,100.0,3.7,0.2056,99.3,1.0,Rank\\-5
20,1,Unsupervised Person Re-Identification,MSMT17->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN,62.3,100.0,62.3,100,62.3,0.63,Rank\\-5
21,1,Person Re-Identification,UAV-Human - Person Re-Identification benchmarking,2017-11,PCB,83.9,99.43,83.9,100,84.38,0.84,Rank\\-5
22,1,Person Re-Identification,UAV-Human - Person Re-Identification benchmarking,2019-03,Tricks,84.38,100.0,0.48,1.0,84.38,0.85,Rank\\-5
23,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2018-09,TAUDL,42.0,66.88,42.0,100,62.8,0.42,Rank\\-5
24,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2019-03,UTAL,62.8,100.0,20.8,1.0,62.8,0.63,Rank\\-5
25,1,Person Re-Identification,iLIDS-VID - Person Re-Identification benchmarking,2019-03,UTAL,59.0,74.31,59.0,100,79.4,0.59,Rank\\-5
26,1,Person Re-Identification,iLIDS-VID - Person Re-Identification benchmarking,2019-08,TKP,79.4,100.0,20.4,1.0,79.4,0.8,Rank\\-5
27,1,Unsupervised Person Re-Identification,DukeMTMC-reID->MSMT17 - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,41.5,100.0,41.5,100,41.5,0.42,Rank\\-5
28,1,Unsupervised Person Re-Identification,Market-1501->MSMT17 - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,36.3,100.0,36.3,100,36.3,0.36,Rank\\-5
29,1,Unsupervised Person Re-Identification,DukeMTMC-reID->Market-1501 - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,87.6,100.0,87.6,100,87.6,0.88,Rank\\-5
30,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,75.8,81.95,75.8,100,92.5,0.76,Rank\\-5
31,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,92.5,100.0,16.7,1.0,92.5,0.93,Rank\\-5
32,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2019-06,Dispersion based Clustering,64.6,66.94,64.6,100,96.5,0.65,Rank\\-5
33,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2019-10,P2-Net (triplet loss),93.1,96.48,28.5,0.8934,96.5,0.93,Rank\\-5
34,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2019-12,Viewpoint-Aware Loss(RK),96.5,100.0,3.4,0.1066,96.5,0.97,Rank\\-5
0,1,Video Object Detection,ImageNet VID - Video Object Detection benchmarking,2017-03,FGFA + Seq-NMS,954.0,100.0,954,100,954,1.0,runtime\\ \\(ms\\)
0,1,Object Proposal Generation,"PASCAL VOC 2012, 60 proposals per image - Object Proposal Generation benchmarking",2017-03,inst-DML,0.667,81.94,0.667,100,0.814,0.01,Average\\ Recall
1,1,Object Proposal Generation,"PASCAL VOC 2012, 60 proposals per image - Object Proposal Generation benchmarking",2017-12,Recurrent Pixel Embedding,0.814,100.0,0.147,1.0,0.814,0.01,Average\\ Recall
2,1,Long-tail Learning,EGTEA - Long-tail Learning benchmarking,2017-08,Focal loss (3D- ResNeXt101),59.17,100.0,59.17,100,59.17,1.0,Average\\ Recall
3,1,Scene Recognition,ScanNet - Scene Recognition benchmarking,2018-08,SSMA,54.28,100.0,54.28,100,54.28,0.92,Average\\ Recall
4,1,3D Feature Matching,3DMatch Benchmark - 3D Feature Matching benchmarking,2019-10,FCGF,0.9578,100.0,0.9578,100,0.9578,0.02,Average\\ Recall
0,1,Action Detection,UCF101-24 - Action Detection benchmarking,2017-03,T-CNN,73.1,95.43,73.1,100,76.6,0.95,Video\\-mAP\\ 0\\.2
1,1,Action Detection,UCF101-24 - Action Detection benchmarking,2019-04,STEP,76.6,100.0,3.5,1.0,76.6,1.0,Video\\-mAP\\ 0\\.2
0,1,Action Detection,UCF101-24 - Action Detection benchmarking,2017-03,T-CNN,77.9,93.74,77.9,100,83.1,0.94,Video\\-mAP\\ 0\\.1
1,1,Action Detection,UCF101-24 - Action Detection benchmarking,2019-04,STEP,83.1,100.0,5.2,1.0,83.1,1.0,Video\\-mAP\\ 0\\.1
0,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2017-03,BB8,43.6,45.8,43.6,100,95.2,0.44,Mean\\ ADD
1,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2017-11,Single-shot Deep CNN,55.95,58.77,12.35,0.2393,95.2,0.56,Mean\\ ADD
2,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2017-11,SSD-6D,76.3,80.15,20.35,0.3944,95.2,0.77,Mean\\ ADD
3,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2018-03,PoseCNN + DeepIM,88.6,93.07,12.3,0.2384,95.2,0.89,Mean\\ ADD
4,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2019-02,DPOD,95.2,100.0,6.6,0.1279,95.2,0.96,Mean\\ ADD
5,1,6D Pose Estimation using RGBD,YCB-Video - 6D Pose Estimation using RGBD benchmarking,2017-11,PoseCNN (ICP),79.3,84.99,79.3,100,93.3,0.8,Mean\\ ADD
6,1,6D Pose Estimation using RGBD,YCB-Video - 6D Pose Estimation using RGBD benchmarking,2018-03,PoseCNN + DeepIM,80.6,86.39,1.3,0.0929,93.3,0.81,Mean\\ ADD
7,1,6D Pose Estimation using RGBD,YCB-Video - 6D Pose Estimation using RGBD benchmarking,2019-11,MaskedFusion,93.3,100.0,12.7,0.9071,93.3,0.94,Mean\\ ADD
8,1,6D Pose Estimation using RGB,YCB-Video - 6D Pose Estimation using RGB benchmarking,2017-11,PoseCNN,53.7,76.6,53.7,100,70.1,0.54,Mean\\ ADD
9,1,6D Pose Estimation using RGB,YCB-Video - 6D Pose Estimation using RGB benchmarking,2018-03,PoseCNN + DeepIM,70.1,100.0,16.4,1.0,70.1,0.71,Mean\\ ADD
10,1,6D Pose Estimation using RGBD,LineMOD - 6D Pose Estimation using RGBD benchmarking,2017-11,SSD-6D,90.9,91.45,90.9,100,99.4,0.91,Mean\\ ADD
11,1,6D Pose Estimation using RGBD,LineMOD - 6D Pose Estimation using RGBD benchmarking,2019-01,DeepFusion,94.3,94.87,3.4,0.4,99.4,0.95,Mean\\ ADD
12,1,6D Pose Estimation using RGBD,LineMOD - 6D Pose Estimation using RGBD benchmarking,2019-11,PVN3D,99.4,100.0,5.1,0.6,99.4,1.0,Mean\\ ADD
13,1,6D Pose Estimation using RGB,Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking,2018-03,PoseCNN + DeepIM (Trained on the Occlusion LineMOD),55.5,100.0,55.5,100,55.5,0.56,Mean\\ ADD
0,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2017-03,BB8,43.6,45.82,43.6,100,95.15,0.44,Accuracy\\ \\(ADD\\)
1,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2018-03,PoseCNN + DeepIM,88.1,92.59,44.5,0.8632,95.15,0.89,Accuracy\\ \\(ADD\\)
2,1,6D Pose Estimation using RGB,LineMOD - 6D Pose Estimation using RGB benchmarking,2019-02,DPOD,95.15,100.0,7.05,0.1368,95.15,0.96,Accuracy\\ \\(ADD\\)
3,1,6D Pose Estimation using RGB,YCB-Video - 6D Pose Estimation using RGB benchmarking,2017-11,PoseCNN,21.3,54.62,21.3,100,39.0,0.21,Accuracy\\ \\(ADD\\)
4,1,6D Pose Estimation using RGB,YCB-Video - 6D Pose Estimation using RGB benchmarking,2018-12,SegDriven,39.0,100.0,17.7,1.0,39.0,0.39,Accuracy\\ \\(ADD\\)
5,1,6D Pose Estimation,LineMOD - 6D Pose Estimation benchmarking,2019-01,DenseFusion,94.3,94.87,94.3,100,99.4,0.95,Accuracy\\ \\(ADD\\)
6,1,6D Pose Estimation,LineMOD - 6D Pose Estimation benchmarking,2019-11,PVN3D,99.4,100.0,5.1,1.0,99.4,1.0,Accuracy\\ \\(ADD\\)
0,1,Depth Estimation,NYU-Depth V2 - Depth Estimation benchmarking,2017-04,MS-CRF,0.586,100.0,0.586,100,0.586,1.0,RMS
0,1,Panoptic Segmentation,Cityscapes test - Panoptic Segmentation benchmarking,2017-04,Dynamically Instantiated Network,55.4,84.58,55.4,100,65.5,0.85,PQ
1,1,Panoptic Segmentation,Cityscapes test - Panoptic Segmentation benchmarking,2019-11,SOGNet (ResNet-50),60.0,91.6,4.6,0.4554,65.5,0.92,PQ
2,1,Panoptic Segmentation,Cityscapes test - Panoptic Segmentation benchmarking,2019-11,Panoptic-Deeplab,65.5,100.0,5.5,0.5446,65.5,1.0,PQ
3,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-01,MRCNN + PSPNet (ResNet-101),61.2,95.48,61.2,100,64.1,0.93,PQ
4,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2019-01,"UPSNet (ResNet-101, multiscale)",61.8,96.41,0.6,0.2069,64.1,0.94,PQ
5,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2019-09,AdaptIS (ResNeXt-101),62.0,96.72,0.2,0.069,64.1,0.95,PQ
6,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2019-11,Panoptic-DeepLab (X71),64.1,100.0,2.1,0.7241,64.1,0.98,PQ
7,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2018-09,JSIS-Net (ResNet-50),27.2,56.9,27.2,100,47.8,0.42,PQ
8,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2018-12,AUNet (ResNeXt-152-FPN),46.5,97.28,19.3,0.9369,47.8,0.71,PQ
9,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2019-01,UPSNet (ResNet-101-FPN),46.6,97.49,0.1,0.0049,47.8,0.71,PQ
10,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2019-11,SOGNet (ResNet-101-FPN),47.8,100.0,1.2,0.0583,47.8,0.73,PQ
11,1,Panoptic Segmentation,Mapillary val - Panoptic Segmentation benchmarking,2018-09,JSIS-Net (ResNet-50),35.9,88.64,35.9,100,40.5,0.55,PQ
12,1,Panoptic Segmentation,Mapillary val - Panoptic Segmentation benchmarking,2019-09,AdaptIS (ResNeXt-101),40.3,99.51,4.4,0.9565,40.5,0.62,PQ
13,1,Panoptic Segmentation,Mapillary val - Panoptic Segmentation benchmarking,2019-11,Panoptic-DeepLab (X71),40.5,100.0,0.2,0.0435,40.5,0.62,PQ
14,1,Panoptic Segmentation,KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking,2019-01,Panoptic FPN,39.3,93.13,39.3,100,42.2,0.6,PQ
15,1,Panoptic Segmentation,KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking,2019-01,UPSNet,39.9,94.55,0.6,0.2069,42.2,0.61,PQ
16,1,Panoptic Segmentation,KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking,2019-05,Seamless,42.2,100.0,2.3,0.7931,42.2,0.64,PQ
17,1,Panoptic Segmentation,COCO panoptic - Panoptic Segmentation benchmarking,2019-01,Panoptic-FPN-ResNet-101,43.0,100.0,43,100,43,0.66,PQ
18,1,Panoptic Segmentation,Indian Driving Dataset - Panoptic Segmentation benchmarking,2019-01,Panoptic FPN,46.7,96.29,46.7,100,48.5,0.71,PQ
19,1,Panoptic Segmentation,Indian Driving Dataset - Panoptic Segmentation benchmarking,2019-01,UPSNet,47.1,97.11,0.4,0.2222,48.5,0.72,PQ
20,1,Panoptic Segmentation,Indian Driving Dataset - Panoptic Segmentation benchmarking,2019-05,Seamless,48.5,100.0,1.4,0.7778,48.5,0.74,PQ
0,1,3D Human Pose Estimation,Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking,2017-04,Baseline model,83.6,87.54,83.6,100,95.5,0.88,PCK3D\\ \\(CA\\)
1,1,3D Human Pose Estimation,Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking,2017-05,SIM-G-F,95.5,100.0,11.9,1.0,95.5,1.0,PCK3D\\ \\(CA\\)
0,1,3D Human Pose Estimation,Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking,2017-04,Baseline model,81.3,86.58,81.3,100,93.9,0.87,PCK3D\\ \\(CS\\)
1,1,3D Human Pose Estimation,Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking,2017-05,SIM-G-F,93.9,100.0,12.6,1.0,93.9,1.0,PCK3D\\ \\(CS\\)
0,1,3D Human Pose Estimation,Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking,2017-04,Baseline model,99.4,100.0,99.4,100,99.4,1.0,MPJPE\\ \\(CS\\)
0,1,3D Human Pose Estimation,Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking,2017-04,Baseline model,89.2,100.0,89.2,100,89.2,1.0,MPJPE\\ \\(CA\\)
0,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,58.1,100.0,58.1,100,58.1,1.0,NDCG\\ \\(x\\ 100\\)
0,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,58.8,84.85,58.8,100,69.3,0.85,MRR\\ \\(x\\ 100\\)
1,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),61.5,88.74,2.7,0.2571,69.3,0.89,MRR\\ \\(x\\ 100\\)
2,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,63.2,91.2,1.7,0.1619,69.3,0.91,MRR\\ \\(x\\ 100\\)
3,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,64.22,92.67,1.02,0.0971,69.3,0.93,MRR\\ \\(x\\ 100\\)
4,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),69.3,100.0,5.08,0.4838,69.3,1.0,MRR\\ \\(x\\ 100\\)
0,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,-4.4,140.13,-4.4,100,-3.14,1.4,Mean
1,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),-4.4,140.13,0.0,0.0,-3.14,1.4,Mean
2,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,-4.3,136.94,0.1,0.0794,-3.14,1.37,Mean
3,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,-4.2,133.76,0.1,0.0794,-3.14,1.34,Mean
4,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),-3.14,100.0,1.06,0.8413,-3.14,1.0,Mean
0,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2017-04,InteractNet,145.0,28.32,145,100,512,0.28,Time\\ Per\\ Frame\\ \\(ms\\)
1,1,Human-Object Interaction Detection,HICO-DET - Human-Object Interaction Detection benchmarking,2018-11,Interactiveness (CVPR\'19),512.0,100.0,367,1.0,512,1.0,Time\\ Per\\ Frame\\ \\(ms\\)
0,1,Skeleton Based Action Recognition,PKU-MMD - Skeleton Based Action Recognition benchmarking,2017-04,Li et al. [[Li et al.2017b]],93.7,99.26,93.7,100,94.4,0.99,mAP\\-at\\-0\\.50\\ \\(CV\\)
1,1,Skeleton Based Action Recognition,PKU-MMD - Skeleton Based Action Recognition benchmarking,2018-04,HCN,94.2,99.79,0.5,0.7143,94.4,1.0,mAP\\-at\\-0\\.50\\ \\(CV\\)
2,1,Skeleton Based Action Recognition,PKU-MMD - Skeleton Based Action Recognition benchmarking,2019-09,RF-Action,94.4,100.0,0.2,0.2857,94.4,1.0,mAP\\-at\\-0\\.50\\ \\(CV\\)
0,1,Skeleton Based Action Recognition,PKU-MMD - Skeleton Based Action Recognition benchmarking,2017-04,Li et al. [[Li et al.2017b]],90.4,97.31,90.4,100,92.9,0.97,mAP\\-at\\-0\\.50\\ \\(CS\\)
1,1,Skeleton Based Action Recognition,PKU-MMD - Skeleton Based Action Recognition benchmarking,2018-04,HCN,92.6,99.68,2.2,0.88,92.9,1.0,mAP\\-at\\-0\\.50\\ \\(CS\\)
2,1,Skeleton Based Action Recognition,PKU-MMD - Skeleton Based Action Recognition benchmarking,2019-09,RF-Action,92.9,100.0,0.3,0.12,92.9,1.0,mAP\\-at\\-0\\.50\\ \\(CS\\)
0,1,Defocus Estimation,CUHK - Blur Detection Dataset - Defocus Estimation benchmarking,2017-04,DHDE,83.73,100.0,83.73,100,83.73,1.0,Blur\\ Segmentation\\ Accuracy
0,1,3D Human Pose Estimation,MPI-INF-3DHP - 3D Human Pose Estimation benchmarking,2017-05,VNect (Augm.),78.1,82.12,78.1,100,95.1,0.82,3DPCK
1,1,3D Human Pose Estimation,MPI-INF-3DHP - 3D Human Pose Estimation benchmarking,2018-06,"MargiPose (Procrustes alignment, PA, multi-crop)",95.1,100.0,17.0,1.0,95.1,1.0,3DPCK
2,1,3D Multi-Person Pose Estimation,MuPoTS-3D - 3D Multi-Person Pose Estimation benchmarking,2019-07,SelecSLS,75.8,100.0,75.8,100,75.8,0.8,3DPCK
0,1,3D Human Pose Estimation,MPI-INF-3DHP - 3D Human Pose Estimation benchmarking,2017-05,VNect (Augm.),-119.2,198.34,-119.2,100,-60.1,1.98,MJPE
1,1,3D Human Pose Estimation,MPI-INF-3DHP - 3D Human Pose Estimation benchmarking,2018-06,"MargiPose (Procrustes alignment, PA, multi-crop)",-60.1,100.0,59.1,1.0,-60.1,1.0,MJPE
0,1,Action Classification,Toyota Smarthome dataset - Action Classification benchmarking,2017-05,I3D,53.4,99.63,53.4,100,53.6,1.0,CS
1,1,Action Classification,Toyota Smarthome dataset - Action Classification benchmarking,2017-11,I3D + Non Local,53.6,100.0,0.2,1.0,53.6,1.0,CS
0,1,Action Classification,Toyota Smarthome dataset - Action Classification benchmarking,2017-05,I3D,34.9,100.0,34.9,100,34.9,1.0,CV1
0,1,Action Classification,Toyota Smarthome dataset - Action Classification benchmarking,2017-05,I3D,45.1,100.0,45.1,100,45.1,1.0,CV2
0,1,Action Recognition,AVA v2.1 - Action Recognition benchmarking,2017-05,S3D-G w/ ResNet RPN (Kinetics-400 pretraining(,22.0,77.74,22.0,100,28.3,0.78,mAP\\ \\(Val\\)
1,1,Action Recognition,AVA v2.1 - Action Recognition benchmarking,2018-12,I3D Tx HighRes,27.6,97.53,5.6,0.8889,28.3,0.98,mAP\\ \\(Val\\)
2,1,Action Recognition,AVA v2.1 - Action Recognition benchmarking,2018-12,"SlowFast++ (Kinetics-600 pretraining, NL)",28.3,100.0,0.7,0.1111,28.3,1.0,mAP\\ \\(Val\\)
0,1,Temporal Action Localization,UCF101-24 - Temporal Action Localization benchmarking,2017-05,Faster-RCNN + two-stream I3D conv,59.9,100.0,59.9,100,59.9,0.76,Video\\-mAP\\ 0\\.5
1,1,Temporal Action Localization,J-HMDB-21 - Temporal Action Localization benchmarking,2017-05,Faster-RCNN + two-stream I3D conv,78.6,100.0,78.6,100,78.6,1.0,Video\\-mAP\\ 0\\.5
0,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2017-05,PG2,3.5,13.41,3.5,100,26.1,0.13,AMT
1,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2017-12,DPIG,7.1,27.2,3.6,0.1593,26.1,0.27,AMT
2,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2017-12,PoseGAN,9.3,35.63,2.2,0.0973,26.1,0.36,AMT
3,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,26.1,100.0,16.8,0.7434,26.1,1.0,AMT
4,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-05,PG2,2.8,12.39,2.8,100,22.6,0.11,AMT
5,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-12,DPIG,6.9,30.53,4.1,0.2071,22.6,0.26,AMT
6,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2017-12,PoseGAN,8.6,38.05,1.7,0.0859,22.6,0.33,AMT
7,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,22.6,100.0,14.0,0.7071,22.6,0.87,AMT
0,1,Edge Detection,SBD - Edge Detection benchmarking,2017-05,CASENet,71.4,100.0,71.4,100,71.4,1.0,Maximum\\ F\\-measure
1,1,Edge Detection,Cityscapes test - Edge Detection benchmarking,2017-05,CASENet,71.3,100.0,71.3,100,71.3,1.0,Maximum\\ F\\-measure
0,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,48.5,55.3,48.5,100,87.7,0.49,Rank\\-10
1,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,68.0,77.54,19.5,0.4974,87.7,0.68,Rank\\-10
2,1,Unsupervised Person Re-Identification,DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),87.7,100.0,19.7,0.5026,87.7,0.88,Rank\\-10
3,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-05,PUL,66.7,68.9,66.7,100,96.8,0.67,Rank\\-10
4,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN+LMP,82.4,85.12,15.7,0.5216,96.8,0.83,Rank\\-10
5,1,Unsupervised Person Re-Identification,Market-1501 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),96.8,100.0,14.4,0.4784,96.8,0.97,Rank\\-10
6,1,Unsupervised Person Re-Identification,MSMT17->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2017-11,SPGAN,68.0,100.0,68,100,68,0.68,Rank\\-10
7,1,Person Re-Identification,CUHK03 - Person Re-Identification benchmarking,2017-11,AlignedReID (RK),99.8,100.0,99.8,100,99.8,1.0,Rank\\-10
8,1,Unsupervised Person Re-Identification,Market-1501->MSMT17 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),45.7,100.0,45.7,100,45.7,0.46,Rank\\-10
9,1,Unsupervised Person Re-Identification,DukeMTMC-reID->MSMT17 - Unsupervised Person Re-Identification benchmarking,2018-11,Self-Similarity Grouping (one shot),61.8,100.0,61.8,100,61.8,0.62,Rank\\-10
10,1,Person Re-Identification,Market-1501 - Person Re-Identification benchmarking,2018-12,"st-ReID(RE, RK)",99.1,100.0,99.1,100,99.1,0.99,Rank\\-10
11,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2019-03,UTAL,66.4,69.38,66.4,100,95.7,0.67,Rank\\-10
12,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2019-08,TKP,95.7,100.0,29.3,1.0,95.7,0.96,Rank\\-10
13,1,Unsupervised Person Re-Identification,DukeMTMC-reID->Market-1501 - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,91.6,100.0,91.6,100,91.6,0.92,Rank\\-10
14,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,80.4,90.54,80.4,100,88.8,0.81,Rank\\-10
15,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,88.8,100.0,8.4,1.0,88.8,0.89,Rank\\-10
16,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2019-06,Dispersion based Clustering,70.1,73.79,70.1,100,95.0,0.7,Rank\\-10
17,1,Person Re-Identification,DukeMTMC-reID - Person Re-Identification benchmarking,2019-10,P2-Net (triplet loss),95.0,100.0,24.9,1.0,95.0,0.95,Rank\\-10
18,1,Person Re-Identification,iLIDS-VID - Person Re-Identification benchmarking,2019-08,TKP,86.9,100.0,86.9,100,86.9,0.87,Rank\\-10
0,1,Face Alignment,300W - Face Alignment benchmarking,2017-06,DAN-Menpo + inter-ocular normalization,50.84,66.67,50.84,100,76.26,0.67,AUC0\\.08\\ private
1,1,Face Alignment,300W - Face Alignment benchmarking,2018-03,SIR-LAN,58.11,76.2,7.27,0.286,76.26,0.76,AUC0\\.08\\ private
2,1,Face Alignment,300W - Face Alignment benchmarking,2018-05,LAB + Oracle + Inter-ocular Normalisation,76.26,100.0,18.15,0.714,76.26,1.0,AUC0\\.08\\ private
0,1,Face Alignment,300W - Face Alignment benchmarking,2017-06,DAN-Menpo + inter-ocular normalization,3.44,61.1,3.44,100,5.63,0.61,Fullset\\ \\(public\\)
1,1,Face Alignment,300W - Face Alignment benchmarking,2018-03,SIR-LAN,5.04,89.52,1.6,0.7306,5.63,0.9,Fullset\\ \\(public\\)
2,1,Face Alignment,300W - Face Alignment benchmarking,2018-04,3DDFA,5.63,100.0,0.59,0.2694,5.63,1.0,Fullset\\ \\(public\\)
0,1,Face Alignment,300W - Face Alignment benchmarking,2017-06,DAN-Menpo + inter-ocular normalization,1.83,100.0,1.83,100,1.83,1.0,Failure\\ private
0,1,Face Alignment,300W - Face Alignment benchmarking,2017-06,DAN-Menpo + inter-ocular normalization,3.97,100.0,3.97,100,3.97,1.0,Mean\\ Error\\ Rate\\ private
0,1,Action Recognition,Something-Something V2 - Action Recognition benchmarking,2017-06,model3D_1 with left-right augmentation and fps jitter,80.46,88.03,80.46,100,91.4,0.81,Top\\-5\\ Accuracy
1,1,Action Recognition,Something-Something V2 - Action Recognition benchmarking,2017-11,2-Stream TRN,83.06,90.88,2.6,0.2377,91.4,0.84,Top\\-5\\ Accuracy
2,1,Action Recognition,Something-Something V2 - Action Recognition benchmarking,2018-11,TSM (RGB + Flow),91.3,99.89,8.24,0.7532,91.4,0.92,Top\\-5\\ Accuracy
3,1,Action Recognition,Something-Something V2 - Action Recognition benchmarking,2019-08,TRG (Inception-V3),91.4,100.0,0.1,0.0091,91.4,0.92,Top\\-5\\ Accuracy
4,1,Action Classification,Kinetics-600 - Action Classification benchmarking,2018-12,SlowFast 16x8 (ResNet-101),95.1,98.86,95.1,100,96.2,0.96,Top\\-5\\ Accuracy
5,1,Action Classification,Kinetics-600 - Action Classification benchmarking,2019-06,LGD-3D Two-stream,96.2,100.0,1.1,1.0,96.2,0.97,Top\\-5\\ Accuracy
6,1,Action Recognition,EgoGesture - Action Recognition benchmarking,2020-04,TSM+W3,99.2,100.0,99.2,100,99.2,1.0,Top\\-5\\ Accuracy
0,1,RGB Salient Object Detection,SOC - RGB Salient Object Detection benchmarking,2017-07,NLDF,0.837,96.88,0.837,100,0.864,0.93,mean\\ E\\-Measure
1,1,RGB Salient Object Detection,SOC - RGB Salient Object Detection benchmarking,2019-06,BASNet,0.864,100.0,0.027,1.0,0.864,0.96,mean\\ E\\-Measure
2,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2017-08,PiCANet,0.853,95.2,0.853,100,0.896,0.95,mean\\ E\\-Measure
3,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2018-06,DGRL,0.887,99.0,0.034,0.7907,0.896,0.99,mean\\ E\\-Measure
4,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2019-06,BASNet,0.896,100.0,0.009,0.2093,0.896,1.0,mean\\ E\\-Measure
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,0.189,98.44,0.189,100,0.192,0.98,MAE\\ \\(Valence\\)
1,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2018-06,CMN,0.192,100.0,0.003,1.0,0.192,1.0,MAE\\ \\(Valence\\)
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,0.213,100.0,0.213,100,0.213,1.0,MAE\\ \\(Arousal\\)
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,0.19,97.44,0.19,100,0.195,0.97,MAE\\ \\(Expectancy\\)
1,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2018-06,CMN,0.195,100.0,0.005,1.0,0.195,1.0,MAE\\ \\(Expectancy\\)
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,8.67,99.2,8.67,100,8.74,0.99,MAE\\ \\(Power\\)
1,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2018-06,CMN,8.74,100.0,0.07,1.0,8.74,1.0,MAE\\ \\(Power\\)
0,1,3D Human Pose Estimation,Surreal - 3D Human Pose Estimation benchmarking,2017-12,self-supervised mocap,-64.4,173.58,-64.4,100,-37.1,2.62,MPJPE
1,1,3D Human Pose Estimation,Surreal - 3D Human Pose Estimation benchmarking,2018-04,BodyNet,-49.1,132.35,15.3,0.5604,-37.1,2.0,MPJPE
2,1,3D Human Pose Estimation,Surreal - 3D Human Pose Estimation benchmarking,2020-04,Cross Dataset Generalization,-37.1,100.0,12.0,0.4396,-37.1,1.51,MPJPE
3,1,3D Human Pose Estimation,3DPW - 3D Human Pose Estimation benchmarking,2017-12,HMR,-130.0,139.04,-130.0,100,-93.5,5.28,MPJPE
4,1,3D Human Pose Estimation,3DPW - 3D Human Pose Estimation benchmarking,2019-09,SPIN (SMPL oPtimization IN the loop),-96.9,103.64,33.1,0.9068,-93.5,3.94,MPJPE
5,1,3D Human Pose Estimation,3DPW - 3D Human Pose Estimation benchmarking,2019-12,VIBE,-93.5,100.0,3.4,0.0932,-93.5,3.8,MPJPE
6,1,3D Human Pose Estimation,CHALL H80K - 3D Human Pose Estimation benchmarking,2018-09,ResNet,-55.3,100.0,-55.3,100,-55.3,2.25,MPJPE
7,1,3D Human Pose Estimation,3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking,2019-07,RootNet,-84.28,100.0,-84.28,100,-84.28,3.43,MPJPE
8,1,3D Absolute Human Pose Estimation,Total Capture - 3D Absolute Human Pose Estimation benchmarking,2020-03,GeoFuse,-24.6,100.0,-24.6,100,-24.6,1.0,MPJPE
0,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,56.19,87.55,56.19,100,64.18,0.88,Weighted\\-F1
1,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-10,ICON,58.54,91.21,2.35,0.2941,64.18,0.91,Weighted\\-F1
2,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,62.75,97.77,4.21,0.5269,64.18,0.98,Weighted\\-F1
3,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,64.18,100.0,1.43,0.179,64.18,1.0,Weighted\\-F1
4,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,56.44,97.01,56.44,100,58.18,0.88,Weighted\\-F1
5,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,57.03,98.02,0.59,0.3391,58.18,0.89,Weighted\\-F1
6,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,58.1,99.86,1.07,0.6149,58.18,0.91,Weighted\\-F1
7,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2019-09,KET,58.18,100.0,0.08,0.046,58.18,0.91,Weighted\\-F1
0,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2017-07,bc-LSTM ,0.741,96.86,0.741,100,0.765,0.97,UA
1,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (T+V),0.759,99.22,0.018,0.75,0.765,0.99,UA
2,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (A+T+V),0.765,100.0,0.006,0.25,0.765,1.0,UA
3,1,Speech Emotion Recognition,IEMOCAP - Speech Emotion Recognition benchmarking,2018-02,CNN+LSTM,0.65,92.72,0.65,100,0.701,0.85,UA
4,1,Speech Emotion Recognition,IEMOCAP - Speech Emotion Recognition benchmarking,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression),0.701,100.0,0.051,1.0,0.701,0.92,UA
0,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,54.84,86.46,54.84,100,63.43,0.86,Macro\\-F1
1,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-10,ICON,56.52,89.11,1.68,0.1956,63.43,0.89,Macro\\-F1
2,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,60.66,95.63,4.14,0.482,63.43,0.96,Macro\\-F1
3,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,63.43,100.0,2.77,0.3225,63.43,1.0,Macro\\-F1
0,1,Image Cropping,AVA - Image Cropping benchmarking,2017-07,Crop,1.0,100.0,1,100,1,1.0,Bounding\\ Box\\ AP
0,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2017-07,Lin et al.,64.4,95.98,64.4,100,67.1,0.96,AUC\\ \\(val\\)
1,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2018-06,BSN,66.17,98.61,1.77,0.6556,67.1,0.99,AUC\\ \\(val\\)
2,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2018-11,MGG,66.43,99.0,0.26,0.0963,67.1,0.99,AUC\\ \\(val\\)
3,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2019-07,BMN,67.1,100.0,0.67,0.2481,67.1,1.0,AUC\\ \\(val\\)
0,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2017-07,Lin et al.,64.8,97.8,64.8,100,66.26,0.98,AUC\\ \\(test\\)
1,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2018-06,BSN,66.26,100.0,1.46,1.0,66.26,1.0,AUC\\ \\(test\\)
0,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2017-07,Lin et al.,73.01,97.33,73.01,100,75.01,0.97,AR@100
1,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2018-06,BSN,74.16,98.87,1.15,0.575,75.01,0.99,AR@100
2,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2018-11,MGG,74.54,99.37,0.38,0.19,75.01,0.99,AR@100
3,1,Temporal Action Proposal Generation,ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking,2019-07,BMN,75.01,100.0,0.47,0.235,75.01,1.0,AR@100
4,1,Temporal Action Proposal Generation,THUMOS' 14 - Temporal Action Proposal Generation benchmarking,2018-06,BSN + Soft-NMS,46.06,100.0,46.06,100,46.06,0.61,AR@100
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,78.71,84.54,78.71,100,93.1,0.85,Consistency
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",89.59,96.23,10.88,0.7561,93.1,0.96,Consistency
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",93.1,100.0,3.51,0.2439,93.1,1.0,Consistency
3,1,Classification Consistency,ImageNet - Classification Consistency benchmarking,2019-04,ResNet50,91.31,100.0,91.31,100,91.31,0.98,Consistency
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,84.57,99.25,84.57,100,85.21,0.99,Plausibility
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",85.21,100.0,0.64,1.0,85.21,1.0,Plausibility
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,96.18,99.81,96.18,100,96.36,1.0,Validity
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",96.35,99.99,0.17,0.9444,96.36,1.0,Validity
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",96.36,100.0,0.01,0.0556,96.36,1.0,Validity
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,5.98,93.15,5.98,100,6.42,0.93,Distribution
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",6.42,100.0,0.44,1.0,6.42,1.0,Distribution
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,66.64,83.52,66.64,100,79.79,0.84,Binary
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",77.16,96.7,10.52,0.8,79.79,0.97,Binary
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",79.79,100.0,2.63,0.2,79.79,1.0,Binary
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,34.83,73.11,34.83,100,47.64,0.73,Open
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",45.47,95.45,10.64,0.8306,47.64,0.95,Open
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",47.64,100.0,2.17,0.1694,47.64,1.0,Open
0,1,Crowd Counting,ShanghaiTech A - Crowd Counting benchmarking,2017-07,Cascaded-MTL,-152.4,100.0,-152.4,100,-152.4,28.75,MSE
1,1,Video Prediction,Human3.6M - Video Prediction benchmarking,2017-12,PredRNN,-484.1,112.61,-484.1,100,-429.9,91.34,MSE
2,1,Video Prediction,Human3.6M - Video Prediction benchmarking,2018-11,MIM,-429.9,100.0,54.2,1.0,-429.9,81.11,MSE
3,1,Gesture-to-Gesture Translation,NTU Hand Digit - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,-105.7286,100.0,-105.7286,100,-105.7286,19.95,MSE
4,1,Gesture-to-Gesture Translation,Senz3D - Gesture-to-Gesture Translation benchmarking,2018-08,GestureGAN,-169.9219,100.0,-169.9219,100,-169.9219,32.06,MSE
5,1,Horizon Line Estimation,KITTI Horizon - Horizon Line Estimation benchmarking,2019-07,"ConvLSTM (Huber Loss, naive residual path)",-6.731,100.0,-6.731,100,-6.731,1.27,MSE
6,1,Image Matting,Composition-1K - Image Matting benchmarking,2019-08,IndexNet-Matting,-13.0,245.28,-13.0,100,-5.3,2.45,MSE
7,1,Image Matting,Composition-1K - Image Matting benchmarking,2020-03,FBA Matting,-5.3,100.0,7.7,1.0,-5.3,1.0,MSE
0,1,Audio Super-Resolution,Piano - Audio Super-Resolution benchmarking,2017-08,U-Net,3.4,100.0,3.4,100,3.4,1.0,Log\\-Spectral\\ Distance
1,1,Audio Super-Resolution,VCTK Multi-Speaker - Audio Super-Resolution benchmarking,2017-08,U-Net,3.1,100.0,3.1,100,3.1,0.91,Log\\-Spectral\\ Distance
0,1,Video Frame Interpolation,Middlebury - Video Frame Interpolation benchmarking,2017-08,SepConv-L1,5.61,100.0,5.61,100,5.61,1.0,Interpolation\\ Error
0,1,Face Verification,Oulu-CASIA NIR-VIS - Face Verification benchmarking,2017-08,W-CNN He et al. (2018),54.6,58.77,54.6,100,92.9,0.55,TAR\\ at\\ FAR=0\\.001
1,1,Face Verification,Oulu-CASIA NIR-VIS - Face Verification benchmarking,2018-09,DVR Wu et al. (2019),84.9,91.39,30.3,0.7911,92.9,0.85,TAR\\ at\\ FAR=0\\.001
2,1,Face Verification,Oulu-CASIA NIR-VIS - Face Verification benchmarking,2019-03,LightCNN-29 + DVG,92.9,100.0,8.0,0.2089,92.9,0.93,TAR\\ at\\ FAR=0\\.001
3,1,Face Verification,CASIA NIR-VIS 2.0 - Face Verification benchmarking,2017-08,W-CNN He et al. (2018),98.4,98.6,98.4,100,99.8,0.99,TAR\\ at\\ FAR=0\\.001
4,1,Face Verification,CASIA NIR-VIS 2.0 - Face Verification benchmarking,2018-09,DVR Wu et al. (2019),99.6,99.8,1.2,0.8571,99.8,1.0,TAR\\ at\\ FAR=0\\.001
5,1,Face Verification,CASIA NIR-VIS 2.0 - Face Verification benchmarking,2019-03,LightCNN-29 + DVG,99.8,100.0,0.2,0.1429,99.8,1.0,TAR\\ at\\ FAR=0\\.001
6,1,Face Verification,BUAA-VisNir - Face Verification benchmarking,2017-08,W-CNN He et al. (2018),91.9,94.45,91.9,100,97.3,0.92,TAR\\ at\\ FAR=0\\.001
7,1,Face Verification,BUAA-VisNir - Face Verification benchmarking,2018-09,DVR Wu et al. (2019),96.9,99.59,5.0,0.9259,97.3,0.97,TAR\\ at\\ FAR=0\\.001
8,1,Face Verification,BUAA-VisNir - Face Verification benchmarking,2019-03,LightCNN-29 + DVG,97.3,100.0,0.4,0.0741,97.3,0.97,TAR\\ at\\ FAR=0\\.001
9,1,Face Verification,IJB-C - Face Verification benchmarking,2017-10,VGGFace2_ft,92.7,97.08,92.7,100,95.49,0.93,TAR\\ at\\ FAR=0\\.001
10,1,Face Verification,IJB-C - Face Verification benchmarking,2019-04,PFEfuse + match,95.49,100.0,2.79,1.0,95.49,0.96,TAR\\ at\\ FAR=0\\.001
11,1,Face Verification,IJB-B - Face Verification benchmarking,2017-10,VGGFace2_ft,90.8,100.0,90.8,100,90.8,0.91,TAR\\ at\\ FAR=0\\.001
12,1,Face Verification,IJB-A - Face Verification benchmarking,2017-10,VGGFace2_ft,92.1,96.69,92.1,100,95.25,0.92,TAR\\ at\\ FAR=0\\.001
13,1,Face Verification,IJB-A - Face Verification benchmarking,2019-04,PFEfuse + match,95.25,100.0,3.15,1.0,95.25,0.95,TAR\\ at\\ FAR=0\\.001
0,1,Action Recognition,ActionNet-VE - Action Recognition benchmarking,2017-08,Baseline,90.27,100.0,90.27,100,90.27,1.0,F\\-measure\\ \\(%\\)
1,1,Point Cloud Super Resolution,SHREC15 - Point Cloud Super Resolution benchmarking,2018-01,PU-NET,56.4,100.0,56.4,100,56.4,0.62,F\\-measure\\ \\(%\\)
0,1,Facial Expression Recognition,AffectNet - Facial Expression Recognition benchmarking,2017-08,Up-Sampling,47.0,78.99,47.0,100,59.5,0.79,Accuracy\\ \\(8\\ emotion\\)
1,1,Facial Expression Recognition,AffectNet - Facial Expression Recognition benchmarking,2017-08,Weighted-Loss,58.0,97.48,11.0,0.88,59.5,0.97,Accuracy\\ \\(8\\ emotion\\)
2,1,Facial Expression Recognition,AffectNet - Facial Expression Recognition benchmarking,2019-05,RAN (ResNet-18+),59.5,100.0,1.5,0.12,59.5,1.0,Accuracy\\ \\(8\\ emotion\\)
0,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2017-08,PiCANet,0.757,91.98,0.757,100,0.823,0.92,mean\\ F\\-Measure
1,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2018-06,DGRL,0.79,95.99,0.033,0.5,0.823,0.96,mean\\ F\\-Measure
2,1,RGB Salient Object Detection,DUTS-TE - RGB Salient Object Detection benchmarking,2019-06,BASNet,0.823,100.0,0.033,0.5,0.823,1.0,mean\\ F\\-Measure
0,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2017-08,SparseConvs,10.0,14.29,10,100,70,0.14,Runtime\\ \\[ms\\]
1,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-08,Spade-sD,40.0,57.14,30,0.5,70,0.57,Runtime\\ \\[ms\\]
2,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2018-08,Spade-RGBsD,70.0,100.0,30,0.5,70,1.0,Runtime\\ \\[ms\\]
0,1,Facial Landmark Detection,300W - Facial Landmark Detection benchmarking,2017-08,FPN,0.1043,100.0,0.1043,100,0.1043,0.02,Mean\\ Error\\ Rate
1,1,Face Alignment,COFW - Face Alignment benchmarking,2018-02,PCD-CNNCVPR 18,5.77,100.0,5.77,100,5.77,0.86,Mean\\ Error\\ Rate
2,1,Face Alignment,IBUG - Face Alignment benchmarking,2018-12,DenseU-Net + Dual Transformer,6.73,100.0,6.73,100,6.73,1.0,Mean\\ Error\\ Rate
0,1,Abnormal Event Detection In Video,UBI-Fights - Abnormal Event Detection In Video benchmarking,2017-08,Adversarial Generator,0.011,100.0,0.011,100,0.011,0.07,Decidability
1,1,Semi-supervised Anomaly Detection,UBI-Fights - Semi-supervised Anomaly Detection benchmarking,2017-08,Adversarial Generator,0.147,100.0,0.147,100,0.147,1.0,Decidability
0,1,Video Summarization,SumMe - Video Summarization benchmarking,2017-08,M-AVS,44.4,89.32,44.4,100,49.71,0.72,F1\\-score\\ \\(Canonical\\)
1,1,Video Summarization,SumMe - Video Summarization benchmarking,2018-12,VASNet,49.71,100.0,5.31,1.0,49.71,0.81,F1\\-score\\ \\(Canonical\\)
2,1,Video Summarization,TvSum - Video Summarization benchmarking,2017-08,M-AVS,61.0,99.32,61.0,100,61.42,0.99,F1\\-score\\ \\(Canonical\\)
3,1,Video Summarization,TvSum - Video Summarization benchmarking,2018-12,VASNet,61.42,100.0,0.42,1.0,61.42,1.0,F1\\-score\\ \\(Canonical\\)
4,1,Supervised Video Summarization,SumMe - Supervised Video Summarization benchmarking,2017-12,DR-DSN,42.1,86.63,42.1,100,48.6,0.69,F1\\-score\\ \\(Canonical\\)
5,1,Supervised Video Summarization,SumMe - Supervised Video Summarization benchmarking,2018-11,CSNet,48.6,100.0,6.5,1.0,48.6,0.79,F1\\-score\\ \\(Canonical\\)
6,1,Supervised Video Summarization,TvSum - Supervised Video Summarization benchmarking,2017-12,DR-DSN,58.1,99.32,58.1,100,58.5,0.95,F1\\-score\\ \\(Canonical\\)
7,1,Supervised Video Summarization,TvSum - Supervised Video Summarization benchmarking,2018-11,CSNet,58.5,100.0,0.4,1.0,58.5,0.95,F1\\-score\\ \\(Canonical\\)
0,1,Video Summarization,SumMe - Video Summarization benchmarking,2017-08,M-AVS,46.1,90.23,46.1,100,51.09,0.74,F1\\-score\\ \\(Augmented\\)
1,1,Video Summarization,SumMe - Video Summarization benchmarking,2018-12,VASNet,51.09,100.0,4.99,1.0,51.09,0.82,F1\\-score\\ \\(Augmented\\)
2,1,Video Summarization,TvSum - Video Summarization benchmarking,2017-08,M-AVS,61.8,99.09,61.8,100,62.37,0.99,F1\\-score\\ \\(Augmented\\)
3,1,Video Summarization,TvSum - Video Summarization benchmarking,2018-12,VASNet,62.37,100.0,0.57,1.0,62.37,1.0,F1\\-score\\ \\(Augmented\\)
4,1,Supervised Video Summarization,SumMe - Supervised Video Summarization benchmarking,2017-12,DR-DSN,43.9,90.14,43.9,100,48.7,0.7,F1\\-score\\ \\(Augmented\\)
5,1,Supervised Video Summarization,SumMe - Supervised Video Summarization benchmarking,2018-11,CSNet,48.7,100.0,4.8,1.0,48.7,0.78,F1\\-score\\ \\(Augmented\\)
6,1,Supervised Video Summarization,TvSum - Supervised Video Summarization benchmarking,2017-12,DR-DSN,59.8,100.0,59.8,100,59.8,0.96,F1\\-score\\ \\(Augmented\\)
0,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2017-09,FTSN + MNMS,82.0,98.91,82.0,100,82.9,0.86,H\\-Mean
1,1,Scene Text Detection,MSRA-TD500 - Scene Text Detection benchmarking,2019-04,CRAFT,82.9,100.0,0.9,1.0,82.9,0.87,H\\-Mean
2,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-01,SLPR,74.8,89.58,74.8,100,83.5,0.79,H\\-Mean
3,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2019-04,CRAFT,83.5,100.0,8.7,1.0,83.5,0.88,H\\-Mean
4,1,Scene Text Detection,ICDAR 2017 MLT - Scene Text Detection benchmarking,2019-04,CRAFT,73.9,100.0,73.9,100,73.9,0.78,H\\-Mean
5,1,Scene Text Detection,ICDAR 2015 - Scene Text Detection benchmarking,2019-04,CRAFT,86.9,100.0,86.9,100,86.9,0.91,H\\-Mean
6,1,Scene Text Detection,ICDAR 2013 - Scene Text Detection benchmarking,2019-04,CRAFT,95.2,100.0,95.2,100,95.2,1.0,H\\-Mean
7,1,Scene Text Detection,Total-Text - Scene Text Detection benchmarking,2019-04,CRAFT,83.6,100.0,83.6,100,83.6,0.88,H\\-Mean
8,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,16.1,100.0,16.1,100,16.1,0.17,H\\-Mean
0,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-09,DGM+IDE+,96.4,96.4,96.4,100,100.0,0.96,Rank\\-20
1,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-09,DGM+MLAPG+,99.0,99.0,2.6,0.7222,100.0,0.99,Rank\\-20
2,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2017-10,SMP*,99.4,99.4,0.4,0.1111,100.0,0.99,Rank\\-20
3,1,Person Re-Identification,PRID2011 - Person Re-Identification benchmarking,2018-06,Snippet (Supervised),100.0,100.0,0.6,0.1667,100.0,1.0,Rank\\-20
4,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2018-09,TAUDL,57.2,74.77,57.2,100,76.5,0.57,Rank\\-20
5,1,Person Re-Identification,DukeTracklet - Person Re-Identification benchmarking,2019-03,UTAL,76.5,100.0,19.3,1.0,76.5,0.76,Rank\\-20
6,1,Person Re-Identification,iLIDS-VID - Person Re-Identification benchmarking,2019-03,UTAL,83.8,89.63,83.8,100,93.5,0.84,Rank\\-20
7,1,Person Re-Identification,iLIDS-VID - Person Re-Identification benchmarking,2019-08,TKP,93.5,100.0,9.7,1.0,93.5,0.94,Rank\\-20
8,1,Person Re-Identification,MARS - Person Re-Identification benchmarking,2019-03,UTAL,77.8,100.0,77.8,100,77.8,0.78,Rank\\-20
9,1,Unsupervised Person Re-Identification,DukeMTMC-reID->Market-1501 - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,94.5,100.0,94.5,100,94.5,0.94,Rank\\-20
10,1,Unsupervised Person Re-Identification,Market-1501->DukeMTMC-reID - Unsupervised Person Re-Identification benchmarking,2019-04,ECN,84.2,100.0,84.2,100,84.2,0.84,Rank\\-20
0,1,Sign Language Recognition,RWTH-PHOENIX-Weather 2014 - Sign Language Recognition benchmarking,2017-10,SubUNets,40.7,100.0,40.7,100,40.7,1.0,Word\\ Error\\ Rate\\ \\(WER\\)
0,1,Head Pose Estimation,BIWI - Head Pose Estimation benchmarking,2017-10,Multi-Loss ResNet50,4.895,100.0,4.895,100,4.895,1.0,MAE\\ \\(trained\\ with\\ BIWI\\ data\\)
0,1,Face Verification,IJB-A - Face Verification benchmarking,2017-10,VGGFace2_ft,0.99,100.0,0.99,100,0.99,1.0,TAR\\ at\\ FAR=0\\.1
0,1,Face Sketch Synthesis,CUHK - Face Sketch Synthesis benchmarking,2017-10,PS2-MAN,73.61,99.16,73.61,100,74.23,0.99,FSIM
1,1,Face Sketch Synthesis,CUHK - Face Sketch Synthesis benchmarking,2018-12,Residual net + Pseudo Sketch Feature Loss + LSGAN,74.23,100.0,0.62,1.0,74.23,1.0,FSIM
2,1,Face Sketch Synthesis,CUFSF - Face Sketch Synthesis benchmarking,2018-12,Residual net + Pseudo Sketch Feature Loss + LSGAN,71.59,100.0,71.59,100,71.59,0.96,FSIM
3,1,Face Sketch Synthesis,CUFS - Face Sketch Synthesis benchmarking,2018-12,Residual net + Pseudo Sketch Feature Loss + LSGAN,72.56,100.0,72.56,100,72.56,0.98,FSIM
0,1,Skeleton Based Action Recognition,J-HMBD Early Action - Skeleton Based Action Recognition benchmarking,2017-10,GAT,58.1,95.87,58.1,100,60.6,0.48,10%
1,1,Skeleton Based Action Recognition,J-HMBD Early Action - Skeleton Based Action Recognition benchmarking,2018-02,DR^2N,60.6,100.0,2.5,1.0,60.6,0.5,10%
2,1,Face Anonymization,2019_test set - Face Anonymization benchmarking,2019-09,sm,122.0,100.0,122,100,122,1.0,10%
0,1,6D Pose Estimation,YCB-Video - 6D Pose Estimation benchmarking,2017-11,PoseCNN+ICP,93.0,96.77,93.0,100,96.1,0.97,ADDS\\ AUC
1,1,6D Pose Estimation,YCB-Video - 6D Pose Estimation benchmarking,2019-01,DenseFusion,93.1,96.88,0.1,0.0323,96.1,0.97,ADDS\\ AUC
2,1,6D Pose Estimation,YCB-Video - 6D Pose Estimation benchmarking,2019-11,PVN3D,96.1,100.0,3.0,0.9677,96.1,1.0,ADDS\\ AUC
0,1,6D Pose Estimation using RGB,YCB-Video - 6D Pose Estimation using RGB benchmarking,2017-11,PoseCNN,75.9,100.0,75.9,100,75.9,0.79,Mean\\ ADD\\-S
1,1,6D Pose Estimation using RGBD,YCB-Video - 6D Pose Estimation using RGBD benchmarking,2017-11,ALL PoseCNN+ICP,93.0,97.38,93.0,100,95.5,0.97,Mean\\ ADD\\-S
2,1,6D Pose Estimation using RGBD,YCB-Video - 6D Pose Estimation using RGBD benchmarking,2019-11,PVN3D,95.5,100.0,2.5,1.0,95.5,1.0,Mean\\ ADD\\-S
0,1,Synthetic-to-Real Translation,GTAV-to-Cityscapes Labels - Synthetic-to-Real Translation benchmarking,2017-11,CyCADA pixel+feat,72.4,100.0,72.4,100,72.4,0.84,fwIOU
1,1,Image-to-Image Translation,SYNTHIA Fall-to-Winter - Image-to-Image Translation benchmarking,2017-11,CyCADA,85.7,100.0,85.7,100,85.7,1.0,fwIOU
0,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,63.0,68.11,63.0,100,92.5,0.65,rank\\-10
1,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,83.2,89.95,20.2,0.6847,92.5,0.86,rank\\-10
2,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,92.5,100.0,9.3,0.3153,92.5,0.95,rank\\-10
3,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,76.8,79.26,76.8,100,96.9,0.79,rank\\-10
4,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,92.4,95.36,15.6,0.7761,96.9,0.95,rank\\-10
5,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,96.9,100.0,4.5,0.2239,96.9,1.0,rank\\-10
6,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2017-11,PTGAN,24.4,35.47,24.4,100,68.8,0.25,rank\\-10
7,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,49.6,72.09,25.2,0.5676,68.8,0.51,rank\\-10
8,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,68.8,100.0,19.2,0.4324,68.8,0.71,rank\\-10
9,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2017-11,PTGAN,27.4,39.26,27.4,100,69.8,0.28,rank\\-10
10,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,51.2,73.35,23.8,0.5613,69.8,0.53,rank\\-10
11,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,69.8,100.0,18.6,0.4387,69.8,0.72,rank\\-10
0,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,51.5,58.72,51.5,100,87.7,0.59,rank\\-1
1,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,80.0,91.22,28.5,0.7873,87.7,0.91,rank\\-1
2,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,87.7,100.0,7.7,0.2127,87.7,1.0,rank\\-1
3,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,41.1,52.69,41.1,100,78.0,0.47,rank\\-1
4,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,73.0,93.59,31.9,0.8645,78.0,0.83,rank\\-1
5,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,78.0,100.0,5.0,0.1355,78.0,0.89,rank\\-1
6,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2017-11,PTGAN,10.2,20.73,10.2,100,49.2,0.12,rank\\-1
7,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,31.6,64.23,21.4,0.5487,49.2,0.36,rank\\-1
8,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,49.2,100.0,17.6,0.4513,49.2,0.56,rank\\-1
9,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2017-11,PTGAN,11.8,23.55,11.8,100,50.1,0.13,rank\\-1
10,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,32.2,64.27,20.4,0.5326,50.1,0.37,rank\\-1
11,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,50.1,100.0,17.9,0.4674,50.1,0.57,rank\\-1
0,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,70.1,73.87,70.1,100,94.9,0.74,rank\\-5
1,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,90.0,94.84,19.9,0.8024,94.9,0.95,rank\\-5
2,1,Unsupervised Domain Adaptation,Duke to Market - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,94.9,100.0,4.9,0.1976,94.9,1.0,rank\\-5
3,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2017-11,SPGAN,56.6,63.74,56.6,100,88.8,0.6,rank\\-5
4,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2018-11,SSG,80.6,90.77,24.0,0.7453,88.8,0.85,rank\\-5
5,1,Unsupervised Domain Adaptation,Market to Duke - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,88.8,100.0,8.2,0.2547,88.8,0.94,rank\\-5
6,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2019-04,ECN,41.5,64.95,41.5,100,63.9,0.44,rank\\-5
7,1,Unsupervised Domain Adaptation,Duke to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,63.9,100.0,22.4,1.0,63.9,0.67,rank\\-5
8,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2019-04,ECN,36.3,57.53,36.3,100,63.1,0.38,rank\\-5
9,1,Unsupervised Domain Adaptation,Market to MSMT - Unsupervised Domain Adaptation benchmarking,2020-01,MMT,63.1,100.0,26.8,1.0,63.1,0.66,rank\\-5
0,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-11,RepLoss,7.6,76.0,7.6,100,10.0,0.76,Bare\\ MR\\^\\-2
1,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2018-07,TLL+MRF,9.2,92.0,1.6,0.6667,10.0,0.92,Bare\\ MR\\^\\-2
2,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2018-07,TLL,10.0,100.0,0.8,0.3333,10.0,1.0,Bare\\ MR\\^\\-2
0,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-11,RepLoss,56.9,100.0,56.9,100,56.9,1.0,Heavy\\ MR\\^\\-2
0,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2017-11,RepLoss,16.8,97.67,16.8,100,17.2,0.98,Partial\\ MR\\^\\-2
1,1,Pedestrian Detection,CityPersons - Pedestrian Detection benchmarking,2018-07,TLL,17.2,100.0,0.4,1.0,17.2,1.0,Partial\\ MR\\^\\-2
0,1,Action Recognition,Jester - Action Recognition benchmarking,2017-11,MultiScale TRN,95.31,98.56,95.31,100,96.7,0.99,Val
1,1,Action Recognition,Jester - Action Recognition benchmarking,2018-07,MFNet,96.68,99.98,1.37,0.9856,96.7,1.0,Val
2,1,Action Recognition,Jester - Action Recognition benchmarking,2019-05,"CPNet Res34, 5 CP",96.7,100.0,0.02,0.0144,96.7,1.0,Val
0,1,3D Semantic Instance Segmentation,ScanNetV2 - 3D Semantic Instance Segmentation benchmarking,2017-11,SGPN,14.3,23.4,14.3,100,61.1,0.15,mAP\\-at\\-0\\.50
1,1,3D Semantic Instance Segmentation,ScanNetV2 - 3D Semantic Instance Segmentation benchmarking,2018-12,3D-SIS,38.2,62.52,23.9,0.5107,61.1,0.4,mAP\\-at\\-0\\.50
2,1,3D Semantic Instance Segmentation,ScanNetV2 - 3D Semantic Instance Segmentation benchmarking,2020-03,3D-MPA,61.1,100.0,22.9,0.4893,61.1,0.64,mAP\\-at\\-0\\.50
3,1,Traffic Sign Recognition,DFG traffic-sign dataset - Traffic Sign Recognition benchmarking,2019-04,Mask R-CNN (ResNet50),93.0,97.38,93.0,100,95.5,0.97,mAP\\-at\\-0\\.50
4,1,Traffic Sign Recognition,DFG traffic-sign dataset - Traffic Sign Recognition benchmarking,2019-04,Mask R-CNN with adaptations for traffic sings  (ResNet50),95.2,99.69,2.2,0.88,95.5,1.0,mAP\\-at\\-0\\.50
5,1,Traffic Sign Recognition,DFG traffic-sign dataset - Traffic Sign Recognition benchmarking,2019-04,Mask R-CNN with adaptations for traffic sings and augmentations (ResNet50),95.5,100.0,0.3,0.12,95.5,1.0,mAP\\-at\\-0\\.50
0,1,6D Pose Estimation using RGBD,Tejani - 6D Pose Estimation using RGBD benchmarking,2017-11,SSD-6D,0.988,100.0,0.988,100,0.988,1.0,IoU\\-2D
0,1,6D Pose Estimation using RGBD,Tejani - 6D Pose Estimation using RGBD benchmarking,2017-11,SSD-6D,0.963,100.0,0.963,100,0.963,1.0,IoU\\-3D
0,1,6D Pose Estimation using RGBD,Tejani - 6D Pose Estimation using RGBD benchmarking,2017-11,SSD-6D,0.724,100.0,0.724,100,0.724,1.0,VSS\\-2D
0,1,6D Pose Estimation using RGBD,Tejani - 6D Pose Estimation using RGBD benchmarking,2017-11,SSD-6D,0.854,100.0,0.854,100,0.854,1.0,VSS\\-3D
0,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,-11.9,100.0,-11.9,100,-11.9,1.0,Real
1,1,DeepFake Detection,FaceForensics - DeepFake Detection benchmarking,2019-01,XceptionNet,-52.4,100.0,-52.4,100,-52.4,4.4,Real
0,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2017-11,AttnGAN,25.88,77.39,25.88,100,33.44,0.77,SOA\\-C
1,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-04,DM-GAN,33.44,100.0,7.56,1.0,33.44,1.0,SOA\\-C
0,1,Fundus to Angiography Generation,Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking,2017-11,pix2pixHD,0.00258,100.0,0.00258,100,0.00258,0.0,Kernel\\ Inception\\ Distance
1,1,Image-to-Image Translation,cat2dog - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,7.07,100.0,7.07,100,7.07,0.61,Kernel\\ Inception\\ Distance
2,1,Image-to-Image Translation,photo2portrait - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,1.79,100.0,1.79,100,1.79,0.15,Kernel\\ Inception\\ Distance
3,1,Image-to-Image Translation,anime-to-selfie - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,11.52,100.0,11.52,100,11.52,0.99,Kernel\\ Inception\\ Distance
4,1,Image-to-Image Translation,selfie-to-anime - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,11.61,100.0,11.61,100,11.61,1.0,Kernel\\ Inception\\ Distance
5,1,Image-to-Image Translation,zebra2horse - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,7.47,100.0,7.47,100,7.47,0.64,Kernel\\ Inception\\ Distance
6,1,Image-to-Image Translation,horse2zebra - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,7.06,100.0,7.06,100,7.06,0.61,Kernel\\ Inception\\ Distance
7,1,Image-to-Image Translation,dog2cat - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,8.15,100.0,8.15,100,8.15,0.7,Kernel\\ Inception\\ Distance
8,1,Image-to-Image Translation,portrait2photo - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,1.69,100.0,1.69,100,1.69,0.15,Kernel\\ Inception\\ Distance
9,1,Image-to-Image Translation,vangogh2photo - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,5.61,100.0,5.61,100,5.61,0.48,Kernel\\ Inception\\ Distance
10,1,Image-to-Image Translation,photo2vangogh - Image-to-Image Translation benchmarking,2019-07,U-GAT-IT,4.28,100.0,4.28,100,4.28,0.37,Kernel\\ Inception\\ Distance
0,1,Shadow Detection,SBU - Shadow Detection benchmarking,2017-12,A+DNet,5.37,65.97,5.37,100,8.14,0.66,BER
1,1,Shadow Detection,SBU - Shadow Detection benchmarking,2017-12,ST-CGAN,8.14,100.0,2.77,1.0,8.14,1.0,BER
0,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2017-12,STPN,27.0,76.49,27.0,100,35.3,0.76,mAP@0\\.1:0\\.7
1,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2019-05,MAAN,31.6,89.52,4.6,0.5542,35.3,0.9,mAP@0\\.1:0\\.7
2,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2019-06,CMCS,32.4,91.78,0.8,0.0964,35.3,0.92,mAP@0\\.1:0\\.7
3,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2019-11,BaS-Net,35.3,100.0,2.9,0.3494,35.3,1.0,mAP@0\\.1:0\\.7
0,1,3D Human Pose Estimation,3DPW - 3D Human Pose Estimation benchmarking,2017-12,HMR,37.4,100.0,37.4,100,37.4,1.0,acceleration\\ error
0,1,3D Human Pose Estimation,3DPW - 3D Human Pose Estimation benchmarking,2017-12,HMR,81.3,100.0,81.3,100,81.3,1.0,PA\\-MPJPE
0,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2017-12,Deformable GAN,30.07,100.0,30.07,100,30.07,1.0,Retrieval\\ Top10\\ Recall
0,1,Multivariate Time Series Imputation,PEMS-SF - Multivariate Time Series Imputation benchmarking,2018-01,MaskGAN,6.02,39.5,6.02,100,15.24,0.4,L2\\ Loss\\ \\(10\\^\\-4\\)
1,1,Multivariate Time Series Imputation,PEMS-SF - Multivariate Time Series Imputation benchmarking,2018-12,GRUI,15.24,100.0,9.22,1.0,15.24,1.0,L2\\ Loss\\ \\(10\\^\\-4\\)
0,1,Panoptic Segmentation,Cityscapes val - Panoptic Segmentation benchmarking,2018-01,MRCNN + PSPNet (ResNet-101),66.4,100.0,66.4,100,66.4,1.0,PQst
1,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2018-09,JSIS-Net (ResNet-50),23.4,63.76,23.4,100,36.7,0.35,PQst
2,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2018-12,AUNet (ResNeXt-152-FPN),32.5,88.56,9.1,0.6842,36.7,0.49,PQst
3,1,Panoptic Segmentation,COCO test-dev - Panoptic Segmentation benchmarking,2019-01,UPSNet (ResNet-101-FPN),36.7,100.0,4.2,0.3158,36.7,0.55,PQst
0,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,SoundNet,18.0,31.21,18.0,100,57.67,0.31,Top\\-5\\ \\(%\\)
1,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,TSN-Flow,34.65,60.08,16.65,0.4197,57.67,0.6,Top\\-5\\ \\(%\\)
2,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,TRN-Multiscale,53.87,93.41,19.22,0.4845,57.67,0.93,Top\\-5\\ \\(%\\)
3,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,Ensemble (SVM),57.67,100.0,3.8,0.0958,57.67,1.0,Top\\-5\\ \\(%\\)
0,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,SoundNet,7.6,24.39,7.6,100,31.16,0.09,Top\\-1\\ \\(%\\)
1,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,TSN-Flow,15.71,50.42,8.11,0.3442,31.16,0.18,Top\\-1\\ \\(%\\)
2,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,TRN-Multiscale,28.27,90.73,12.56,0.5331,31.16,0.32,Top\\-1\\ \\(%\\)
3,1,Multimodal Activity Recognition,Moments in Time Dataset - Multimodal Activity Recognition benchmarking,2018-01,Ensemble (SVM),31.16,100.0,2.89,0.1227,31.16,0.36,Top\\-1\\ \\(%\\)
4,1,Unsupervised Person Re-Identification,DukeMTMC-reID->Market-1501 - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,87.7,100.0,87.7,100,87.7,1.0,Top\\-1\\ \\(%\\)
5,1,Unsupervised Person Re-Identification,Market-1501->MSMT17 - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,49.2,100.0,49.2,100,49.2,0.56,Top\\-1\\ \\(%\\)
6,1,Unsupervised Person Re-Identification,DukeMTMC-reID->MSMT17 - Unsupervised Person Re-Identification benchmarking,2020-01,MMT-ResNet50,50.0,100.0,50,100,50,0.57,Top\\-1\\ \\(%\\)
0,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-01,MaskGAN,0.793,69.5,0.793,100,1.141,0.7,Path\\ Length
1,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-12,GRUI,1.141,100.0,0.348,1.0,1.141,1.0,Path\\ Length
0,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-01,MaskGAN,9.622,64.36,9.622,100,14.95,0.64,Step\\ Change\\ \\(10\\^−3\\)
1,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-12,GRUI,14.95,100.0,5.328,1.0,14.95,1.0,Step\\ Change\\ \\(10\\^−3\\)
0,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-01,MaskGAN,0.68,98.55,0.68,100,0.69,0.99,Path\\ Difference
1,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-12,GRUI,0.69,100.0,0.01,1.0,0.69,1.0,Path\\ Difference
0,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-01,MaskGAN,0.427,100.0,0.427,100,0.427,1.0,Player\\ Distance
0,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-01,MaskGAN,4.592,97.64,4.592,100,4.703,0.98,OOB\\ Rate\\ \\(10\\^−3\\)
1,1,Multivariate Time Series Imputation,Basketball Players Movement - Multivariate Time Series Imputation benchmarking,2018-12,GRUI,4.703,100.0,0.111,1.0,4.703,1.0,OOB\\ Rate\\ \\(10\\^−3\\)
0,1,Image Inpainting,Places2 val - Image Inpainting benchmarking,2018-01,ContextAttention,8.6,100.0,8.6,100,8.6,1.0,rect\\ mask\\ l1\\ error
0,1,Image Inpainting,Places2 val - Image Inpainting benchmarking,2018-01,ContextAttention,2.1,100.0,2.1,100,2.1,1.0,rect\\ mask\\ l2\\ err
0,1,Image Inpainting,Places2 val - Image Inpainting benchmarking,2018-01,ContextAttention,17.2,100.0,17.2,100,17.2,1.0,free\\-form\\ mask\\ l1\\ err
0,1,Image Inpainting,Places2 val - Image Inpainting benchmarking,2018-01,ContextAttention,4.7,100.0,4.7,100,4.7,1.0,free\\-form\\ mask\\ l2\\ err
0,1,3D Facial Expression Recognition,2017_test set - 3D Facial Expression Recognition benchmarking,2018-02,aan,2.0,100.0,2,100,2,0.02,14\\ gestures\\ accuracy
1,1,Visual Question Answering,100 sleep nights of 8 caregivers - Visual Question Answering benchmarking,2018-10,TallyQA,10.0,100.0,10,100,10,0.11,14\\ gestures\\ accuracy
2,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-01,MFA-Net,91.3,96.51,91.3,100,94.6,0.97,14\\ gestures\\ accuracy
3,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-01,STA-Res-TCN,93.6,98.94,2.3,0.697,94.6,0.99,14\\ gestures\\ accuracy
4,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-07,DD-Net,94.6,100.0,1.0,0.303,94.6,1.0,14\\ gestures\\ accuracy
5,1,Hand Gesture Recognition,SHREC 2017 - Hand Gesture Recognition benchmarking,2019-07,DG-STA,94.4,100.0,94.4,100,94.4,1.0,14\\ gestures\\ accuracy
6,1,Hand Gesture Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Hand Gesture Recognition benchmarking,2019-07,DD-Net,94.6,100.0,94.6,100,94.6,1.0,14\\ gestures\\ accuracy
0,1,Video Instance Segmentation,YouTube-VIS validation - Video Instance Segmentation benchmarking,2018-02,OSMN,28.6,100.0,28.6,100,28.6,1.0,AR1
0,1,Video Instance Segmentation,YouTube-VIS validation - Video Instance Segmentation benchmarking,2018-02,OSMN,33.1,100.0,33.1,100,33.1,1.0,AR10
0,1,Facial Beauty Prediction,ECCV HotOrNot - Facial Beauty Prediction benchmarking,2018-03,CNN features + Bayesian ridge regression,0.468,100.0,0.468,100,0.468,1.0,Pearson\\ Correlation
0,1,One-Shot Segmentation,Cluttered Omniglot - One-Shot Segmentation benchmarking,2018-03,MaskNet,65.6,100.0,65.6,100,65.6,1.0,IoU\\ \\[32\\ distractors\\]
0,1,One-Shot Segmentation,Cluttered Omniglot - One-Shot Segmentation benchmarking,2018-03,MaskNet,95.8,98.66,95.8,100,97.1,0.99,IoU\\ \\[4\\ distractors\\]
1,1,One-Shot Segmentation,Cluttered Omniglot - One-Shot Segmentation benchmarking,2018-03,Siamese-U-Net,97.1,100.0,1.3,1.0,97.1,1.0,IoU\\ \\[4\\ distractors\\]
0,1,One-Shot Segmentation,Cluttered Omniglot - One-Shot Segmentation benchmarking,2018-03,MaskNet,43.7,100.0,43.7,100,43.7,1.0,IoU\\ \\[256\\ distractors\\]
0,1,6D Pose Estimation using RGBD,YCB-Video - 6D Pose Estimation using RGBD benchmarking,2018-03,PoseCNN + DeepIM,92.4,100.0,92.4,100,92.4,1.0,Mean\\ ADI
1,1,6D Pose Estimation using RGB,YCB-Video - 6D Pose Estimation using RGB benchmarking,2018-03,PoseCNN + DeepIM,84.2,100.0,84.2,100,84.2,0.91,Mean\\ ADI
0,1,Video Captioning,YouCook2 - Video Captioning benchmarking,2018-04,Zhou,0.38,69.09,0.38,100,0.55,0.0,CIDEr
1,1,Video Captioning,YouCook2 - Video Captioning benchmarking,2019-04,VideoBERT + S3D,0.55,100.0,0.17,1.0,0.55,0.0,CIDEr
2,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",77.6,100.0,77.6,100,77.6,0.66,CIDEr
3,1,Image Captioning,Flickr30k Captions test - Image Captioning benchmarking,2019-09,Unified VLP,67.4,100.0,67.4,100,67.4,0.58,CIDEr
4,1,Image Captioning,COCO Captions test - Image Captioning benchmarking,2019-09,Unified VLP,116.9,100.0,116.9,100,116.9,1.0,CIDEr
5,1,Video Captioning,VATEX - Video Captioning benchmarking,2020-02,ORG-TRL,49.7,100.0,49.7,100,49.7,0.43,CIDEr
6,1,Video Captioning,MSR-VTT - Video Captioning benchmarking,2020-02,ORG-TRL,50.9,100.0,50.9,100,50.9,0.44,CIDEr
7,1,Video Captioning,MSVD - Video Captioning benchmarking,2020-02,ORG-TRL,95.2,100.0,95.2,100,95.2,0.81,CIDEr
0,1,Video Captioning,YouCook2 - Video Captioning benchmarking,2018-04,Zhou,27.44,95.28,27.44,100,28.8,0.37,ROUGE\\-L
1,1,Video Captioning,YouCook2 - Video Captioning benchmarking,2019-04,VideoBERT + S3D,28.8,100.0,1.36,1.0,28.8,0.39,ROUGE\\-L
2,1,Video Captioning,MSVD - Video Captioning benchmarking,2020-02,ORG-TRL,73.9,100.0,73.9,100,73.9,1.0,ROUGE\\-L
3,1,Video Captioning,VATEX - Video Captioning benchmarking,2020-02,ORG-TRL,48.9,100.0,48.9,100,48.9,0.66,ROUGE\\-L
4,1,Video Captioning,MSR-VTT - Video Captioning benchmarking,2020-02,ORG-TRL,62.1,100.0,62.1,100,62.1,0.84,ROUGE\\-L
0,1,Video Captioning,YouCook2 - Video Captioning benchmarking,2018-04,Zhou,4.38,100.0,4.38,100,4.38,0.08,BLEU\\-4
1,1,Sign Language Translation,RWTH-PHOENIX-Weather 2014 T - Sign Language Translation benchmarking,2018-06,Sign2Gloss2Text,19.26,100.0,19.26,100,19.26,0.35,BLEU\\-4
2,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",24.9,100.0,24.9,100,24.9,0.46,BLEU\\-4
3,1,Image Captioning,Flickr30k Captions test - Image Captioning benchmarking,2019-09,Unified VLP,30.1,100.0,30.1,100,30.1,0.55,BLEU\\-4
4,1,Image Captioning,COCO Captions test - Image Captioning benchmarking,2019-09,Unified VLP,36.5,100.0,36.5,100,36.5,0.67,BLEU\\-4
5,1,Video Captioning,VATEX - Video Captioning benchmarking,2020-02,ORG-TRL,32.1,100.0,32.1,100,32.1,0.59,BLEU\\-4
6,1,Video Captioning,MSR-VTT - Video Captioning benchmarking,2020-02,ORG-TRL,43.6,100.0,43.6,100,43.6,0.8,BLEU\\-4
7,1,Video Captioning,MSVD - Video Captioning benchmarking,2020-02,ORG-TRL,54.3,100.0,54.3,100,54.3,1.0,BLEU\\-4
8,1,Dense Video Captioning,ActivityNet Captions - Dense Video Captioning benchmarking,2020-03,MDVC,1.07,100.0,1.07,100,1.07,0.02,BLEU\\-4
0,1,Video Captioning,YouCook2 - Video Captioning benchmarking,2018-04,Zhou,7.53,99.21,7.53,100,7.59,0.22,BLEU\\-3
1,1,Video Captioning,YouCook2 - Video Captioning benchmarking,2019-04,VideoBERT + S3D,7.59,100.0,0.06,1.0,7.59,0.23,BLEU\\-3
2,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",33.6,100.0,33.6,100,33.6,1.0,BLEU\\-3
3,1,Dense Video Captioning,ActivityNet Captions - Dense Video Captioning benchmarking,2020-03,MDVC,2.6,100.0,2.6,100,2.6,0.08,BLEU\\-3
0,1,Motion Segmentation,KT3DMoSeg - Motion Segmentation benchmarking,2018-04,MultiViewClustering,7.92,100.0,7.92,100,7.92,1.0,Error
1,1,Image Classification,Kuzushiji-MNIST - Image Classification benchmarking,2019-01,VGG8B(2x) + LocalLearning + CO,0.99,100.0,0.99,100,0.99,0.12,Error
0,1,3D Shape Reconstruction,Pix3D - 3D Shape Reconstruction benchmarking,2018-04,MarrNet extension (w/ Pose),-0.119,100.0,-0.119,100,-0.119,1.0,CD
0,1,3D Shape Reconstruction,Pix3D - 3D Shape Reconstruction benchmarking,2018-04,MarrNet extension (w/ Pose),-0.118,100.0,-0.118,100,-0.118,1.0,EMD
0,1,RF-based Pose Estimation,RF-MMD - RF-based Pose Estimation benchmarking,2018-04,HCN,78.5,90.75,78.5,100,86.5,0.35,"mAP\\ \\(at\\-0\\.1,\\ Through\\-wall\\)"
1,1,RF-based Pose Estimation,RF-MMD - RF-based Pose Estimation benchmarking,2019-09,RF-Action,86.5,100.0,8.0,1.0,86.5,0.39,"mAP\\ \\(at\\-0\\.1,\\ Through\\-wall\\)"
2,1,Low-Light Image Enhancement,3DMatch Benchmark - Low-Light Image Enhancement benchmarking,2019-08,nnh,224.0,100.0,224,100,224,1.0,"mAP\\ \\(at\\-0\\.1,\\ Through\\-wall\\)"
0,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2018-04,TAL-Net,18.3,52.62,18.3,100,34.78,0.53,mAP\\ IOU\\-at\\-0\\.75
1,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2018-06,BSN,29.96,86.14,11.66,0.7075,34.78,0.86,mAP\\ IOU\\-at\\-0\\.75
2,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2019-07,BMN,34.78,100.0,4.82,0.2925,34.78,1.0,mAP\\ IOU\\-at\\-0\\.75
0,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2018-04,TAL-Net,1.3,14.41,1.3,100,9.02,0.14,mAP\\ IOU\\-at\\-0\\.95
1,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2018-06,BSN,8.02,88.91,6.72,0.8705,9.02,0.89,mAP\\ IOU\\-at\\-0\\.95
2,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2019-07,BMN,8.29,91.91,0.27,0.035,9.02,0.92,mAP\\ IOU\\-at\\-0\\.95
3,1,Temporal Action Localization,ActivityNet-1.3 - Temporal Action Localization benchmarking,2019-11,G-TAD,9.02,100.0,0.73,0.0946,9.02,1.0,mAP\\ IOU\\-at\\-0\\.95
0,1,Disguised Face Verification,Disguised Faces in the Wild - Disguised Face Verification benchmarking,2018-04,DisguiseNet,23.25,100.0,23.25,100,23.25,1.0,GAR\\ at\\-0\\.1%\\ FAR
0,1,Disguised Face Verification,Disguised Faces in the Wild - Disguised Face Verification benchmarking,2018-04,DisguiseNet,60.89,100.0,60.89,100,60.89,1.0,GAR\\ at\\-1%\\ FAR
0,1,Disguised Face Verification,Disguised Faces in the Wild - Disguised Face Verification benchmarking,2018-04,DisguiseNet,98.99,100.0,98.99,100,98.99,1.0,GAR\\ at\\-10%\\ FAR
0,1,Facial Expression Recognition,AffectNet - Facial Expression Recognition benchmarking,2018-04,CNNs and BOVW + local SVM,59.58,96.85,59.58,100,61.52,0.97,Accuracy\\ \\(7\\ emotion\\)
1,1,Facial Expression Recognition,AffectNet - Facial Expression Recognition benchmarking,2019-02,Facial Motion Prior Network,61.52,100.0,1.94,1.0,61.52,1.0,Accuracy\\ \\(7\\ emotion\\)
0,1,Pulmonary Artery–Vein Classification,LUMC - Pulmonary Artery–Vein Classification benchmarking,2018-05,CNN3D,0.693,93.9,0.693,100,0.738,0.89,Accuracy\\ \\(median\\)
1,1,Pulmonary Artery–Vein Classification,LUMC - Pulmonary Artery–Vein Classification benchmarking,2019-09,CNN-GCNt,0.738,100.0,0.045,1.0,0.738,0.95,Accuracy\\ \\(median\\)
2,1,Pulmonary Artery–Vein Classification,SunYs - Pulmonary Artery–Vein Classification benchmarking,2018-05,CNN3D,0.727,93.44,0.727,100,0.778,0.93,Accuracy\\ \\(median\\)
3,1,Pulmonary Artery–Vein Classification,SunYs - Pulmonary Artery–Vein Classification benchmarking,2019-09,CNN-GCN,0.764,98.2,0.037,0.7255,0.778,0.98,Accuracy\\ \\(median\\)
4,1,Pulmonary Artery–Vein Classification,SunYs - Pulmonary Artery–Vein Classification benchmarking,2019-09,CNN-GCNt,0.778,100.0,0.014,0.2745,0.778,1.0,Accuracy\\ \\(median\\)
0,1,Gaze Estimation,UT Multi-view - Gaze Estimation benchmarking,2018-10,RT-GENE 4 model ensemble,5.1,100.0,5.1,100,5.1,0.66,Angular\\ Error
1,1,Gaze Estimation,MPII Gaze - Gaze Estimation benchmarking,2018-10,RT-GENE 2 model ensemble,4.6,95.83,4.6,100,4.8,0.6,Angular\\ Error
2,1,Gaze Estimation,MPII Gaze - Gaze Estimation benchmarking,2018-10,RT-GENE single model,4.8,100.0,0.2,1.0,4.8,0.62,Angular\\ Error
3,1,Gaze Estimation,RT-GENE - Gaze Estimation benchmarking,2018-10,RT-GENE 4 model ensemble,7.7,100.0,7.7,100,7.7,1.0,Angular\\ Error
0,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2018-06,TDRN,74.1,85.96,74.1,100,86.2,0.86,Edit
1,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2019-03,MS-TCN,81.4,94.43,7.3,0.6033,86.2,0.94,Edit
2,1,Action Segmentation,GTEA - Action Segmentation benchmarking,2020-03,SSTDA,86.2,100.0,4.8,0.3967,86.2,1.0,Edit
3,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2019-03,MS-TCN (IDT),61.4,83.31,61.4,100,73.7,0.71,Edit
4,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2019-03,MS-TCN (I3D),61.7,83.72,0.3,0.0244,73.7,0.72,Edit
5,1,Action Segmentation,Breakfast - Action Segmentation benchmarking,2020-03,SSTDA,73.7,100.0,12.0,0.9756,73.7,0.85,Edit
6,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2019-03,MS-TCN,67.9,89.58,67.9,100,75.8,0.79,Edit
7,1,Action Segmentation,50 Salads - Action Segmentation benchmarking,2020-03,SSTDA,75.8,100.0,7.9,1.0,75.8,0.88,Edit
0,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2018-06,InstDisc (ResNet-50),46.5,98.73,46.5,100,47.1,0.99,"Top\\ 1\\ Accuracy\\ \\(kNN,\\ k=20\\)"
1,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-11,MoCo (ResNet-50),47.1,100.0,0.6,1.0,47.1,1.0,"Top\\ 1\\ Accuracy\\ \\(kNN,\\ k=20\\)"
0,1,Monocular Depth Estimation,Make3D - Monocular Depth Estimation benchmarking,2018-06,Monodepth2,3.589,100.0,3.589,100,3.589,1.0,Sq\\ Rel
0,1,Temporal Action Proposal Generation,THUMOS' 14 - Temporal Action Proposal Generation benchmarking,2018-06,BSN + Soft-NMS,64.52,100.0,64.52,100,64.52,1.0,AR@1000
0,1,Temporal Action Proposal Generation,THUMOS' 14 - Temporal Action Proposal Generation benchmarking,2018-06,BSN + Soft-NMS,53.21,100.0,53.21,100,53.21,1.0,AR@200
0,1,Temporal Action Proposal Generation,THUMOS' 14 - Temporal Action Proposal Generation benchmarking,2018-06,BSN + Soft-NMS,37.46,100.0,37.46,100,37.46,1.0,AR@50
0,1,Temporal Action Proposal Generation,THUMOS' 14 - Temporal Action Proposal Generation benchmarking,2018-06,BSN + Soft-NMS,60.64,100.0,60.64,100,60.64,1.0,AR@500
0,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,213.8,100.0,213.8,100,213.8,1.0,text\\-to\\-video\\ Mean\\ Rank
1,1,Video Retrieval,MSVD - Video Retrieval benchmarking,2019-07,Collaborative Experts,23.1,100.0,23.1,100,23.1,0.11,text\\-to\\-video\\ Mean\\ Rank
2,1,Video Retrieval,DiDeMo - Video Retrieval benchmarking,2019-07,Collaborative Experts,43.7,100.0,43.7,100,43.7,0.2,text\\-to\\-video\\ Mean\\ Rank
3,1,Video Retrieval,ActivityNet - Video Retrieval benchmarking,2019-07,Collaborative Experts,23.1,100.0,23.1,100,23.1,0.11,text\\-to\\-video\\ Mean\\ Rank
4,1,Video Retrieval,MSR-VTT-1kA - Video Retrieval benchmarking,2019-07,Collaborative Experts,28.2,100.0,28.2,100,28.2,0.13,text\\-to\\-video\\ Mean\\ Rank
0,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,134.0,100.0,134.0,100,134.0,1.0,video\\-to\\-text\\ Mean\\ Rank
0,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,16.0,100.0,16.0,100,16.0,1.0,video\\-to\\-text\\ Median\\ Rank
0,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,12.5,80.13,12.5,100,15.6,0.8,video\\-to\\-text\\ R\\-at\\-1
1,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2019-07,Collaborative Experts,15.6,100.0,3.1,1.0,15.6,1.0,video\\-to\\-text\\ R\\-at\\-1
0,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2018-06,JEMC,42.2,76.45,42.2,100,55.2,0.76,video\\-to\\-text\\ R\\-at\\-10
1,1,Video Retrieval,MSR-VTT - Video Retrieval benchmarking,2019-07,Collaborative Experts,55.2,100.0,13.0,1.0,55.2,1.0,video\\-to\\-text\\ R\\-at\\-10
0,1,Multivariate Time Series Imputation,PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking,2018-06,Latent ODE (RNN enc.),3.907,65.89,3.907,100,5.93,0.66,mse\\ \\(10\\^\\-3\\)
1,1,Multivariate Time Series Imputation,PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking,2018-06,RNN-VAE,5.93,100.0,2.023,1.0,5.93,1.0,mse\\ \\(10\\^\\-3\\)
0,1,Object Localization,Plant - Object Localization benchmarking,2018-06,Hausdorff Loss,88.6,100.0,88.6,100,88.6,1.0,F\\-Score
0,1,License Plate Recognition,Chinese License Plates - License Plate Recognition benchmarking,2018-06,LPRNet basic,-0.34,100.0,-0.34,100,-0.34,1.0,GFLOPs
0,1,Stroke Classification from CT data,CT Lesion Stroke Dataset - Stroke Classification from CT data benchmarking,2018-07,"PSO+CNN (Cifar-10, 50/50, Original)",93.46,94.54,93.46,100,98.86,0.95,Average\\ Class\\ Accuracy
1,1,Stroke Classification from CT data,CT Lesion Stroke Dataset - Stroke Classification from CT data benchmarking,2018-07,"PSO+CNN (Cifar-10, 75/25, Cranium Segmented)",98.86,100.0,5.4,1.0,98.86,1.0,Average\\ Class\\ Accuracy
0,1,Domain Adaptation,Office-Caltech-10 - Domain Adaptation benchmarking,2018-07,MEDA,92.8,100.0,92.8,100,92.8,1.0,Accuracy\\ \\(%\\)
1,1,Partial Domain Adaptation,Office-Home - Partial Domain Adaptation benchmarking,2018-11,SAFN,71.8,100.0,71.8,100,71.8,0.77,Accuracy\\ \\(%\\)
0,1,Point Cloud Completion,ShapeNet - Point Cloud Completion benchmarking,2018-08,PCN,9.636,100.0,9.636,100,9.636,0.53,Chamfer\\ Distance
1,1,Point Cloud Completion,Completion3D - Point Cloud Completion benchmarking,2018-08,PCN,18.22,100.0,18.22,100,18.22,1.0,Chamfer\\ Distance
0,1,Point Cloud Completion,ShapeNet - Point Cloud Completion benchmarking,2018-08,PCN,0.695,100.0,0.695,100,0.695,1.0,F\\-Score@1%
0,1,Multi-Frame Super-Resolution,PROBA-V - Multi-Frame Super-Resolution benchmarking,2018-08,3DWDSR,0.9462525077016232,99.73,0.9462525077016232,100,0.948844910272775,1.0,Normalized\\ cPSNR
1,1,Multi-Frame Super-Resolution,PROBA-V - Multi-Frame Super-Resolution benchmarking,2019-07,DeepSUM,0.948844910272775,100.0,0.0026,1.0029,0.948844910272775,1.0,Normalized\\ cPSNR
0,1,RF-based Pose Estimation,RF-MMD - RF-based Pose Estimation benchmarking,2018-09,Aryokee,78.3,86.9,78.3,100,90.1,0.87,"mAP\\ \\(at\\-0\\.1,\\ Visible\\)"
1,1,RF-based Pose Estimation,RF-MMD - RF-based Pose Estimation benchmarking,2019-09,RF-Action,90.1,100.0,11.8,1.0,90.1,1.0,"mAP\\ \\(at\\-0\\.1,\\ Visible\\)"
0,1,Age-Invariant Face Recognition,MORPH Album2 - Age-Invariant Face Recognition benchmarking,2018-09,AIM,99.13,99.48,99.13,100,99.65,0.99,Rank\\-1\\ Recognition\\ Rate
1,1,Age-Invariant Face Recognition,MORPH Album2 - Age-Invariant Face Recognition benchmarking,2018-09,AIM + CAFR,99.65,100.0,0.52,1.0,99.65,1.0,Rank\\-1\\ Recognition\\ Rate
0,1,Optical Flow Estimation,KITTI 2015 - Optical Flow Estimation benchmarking,2018-09,PWC-Net + ft - axXiv,7.72,91.69,7.72,100,8.42,0.92,Fl\\-all
1,1,Optical Flow Estimation,KITTI 2015 - Optical Flow Estimation benchmarking,2019-04,SelFlow,8.42,100.0,0.7,1.0,8.42,1.0,Fl\\-all
0,1,Synthetic-to-Real Translation,SYNTHIA-to-Cityscapes - Synthetic-to-Real Translation benchmarking,2018-09,CLAN,47.8,88.85,47.8,100,53.8,0.89,MIoU\\ \\(13\\ classes\\)
1,1,Synthetic-to-Real Translation,SYNTHIA-to-Cityscapes - Synthetic-to-Real Translation benchmarking,2019-10,CAG-UDA,52.6,97.77,4.8,0.8,53.8,0.98,MIoU\\ \\(13\\ classes\\)
2,1,Synthetic-to-Real Translation,SYNTHIA-to-Cityscapes - Synthetic-to-Real Translation benchmarking,2019-12,MRNet(ResNet-101),53.8,100.0,1.2,0.2,53.8,1.0,MIoU\\ \\(13\\ classes\\)
0,1,Stereo Depth Estimation,KITTI2015 - Stereo Depth Estimation benchmarking,2018-10,AnyNet,6.2,100.0,6.2,100,6.2,1.0,three\\ pixel\\ error
1,1,Stereo Depth Estimation,KITTI2012 - Stereo Depth Estimation benchmarking,2018-10,AnyNet,6.1,100.0,6.1,100,6.1,0.98,three\\ pixel\\ error
0,1,Visual Object Tracking,GOT-10k - Visual Object Tracking benchmarking,2018-11,ATOM,63.4,100.0,63.4,100,63.4,1.0,Success\\ Rate\\ 0\\.5
0,1,Visual Object Tracking,GOT-10k - Visual Object Tracking benchmarking,2018-11,ATOM,55.6,100.0,55.6,100,55.6,1.0,Average\\ Overlap
0,1,Human-Object Interaction Detection,V-COCO - Human-Object Interaction Detection benchmarking,2018-11,Interactiveness,513.0,100.0,513,100,513,1.0,Time\\ Per\\ Frame\\(ms\\)
0,1,Scene Text Detection,SCUT-CTW1500 - Scene Text Detection benchmarking,2018-11,PAN,65.2,100.0,65.2,100,65.2,1.0,TIoU
0,1,Vision-Language Navigation,Room2Room - Vision-Language Navigation benchmarking,2018-11,RCM + SIL,0.59,96.72,0.59,100,0.61,0.75,spl
1,1,Vision-Language Navigation,Room2Room - Vision-Language Navigation benchmarking,2019-04,R2R+EnvDrop,0.61,100.0,0.02,1.0,0.61,0.77,spl
2,1,PointGoal Navigation,Gibson PointGoal Navigation - PointGoal Navigation benchmarking,2019-04,Depth PPO,0.79,100.0,0.79,100,0.79,1.0,spl
0,1,Egocentric Activity Recognition,EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking,2018-11,LSTA,16.63,64.96,16.63,100,25.6,0.65,Actions\\ Top\\-1\\ \\(S2\\)
1,1,Egocentric Activity Recognition,EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking,2018-12,LFB Max,21.2,82.81,4.57,0.5095,25.6,0.83,Actions\\ Top\\-1\\ \\(S2\\)
2,1,Egocentric Activity Recognition,EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking,2019-05,R(2+1)D-152-SE (ig),25.6,100.0,4.4,0.4905,25.6,1.0,Actions\\ Top\\-1\\ \\(S2\\)
0,1,Phrase Grounding,Visual Genome - Phrase Grounding benchmarking,2018-11,VG_ELMo_PNASNet,55.16,100.0,55.16,100,55.16,0.8,Pointing\\ Game\\ Accuracy
1,1,Phrase Grounding,Flickr30k - Phrase Grounding benchmarking,2018-11,COCO_ELMo_PNASNet,69.19,100.0,69.19,100,69.19,1.0,Pointing\\ Game\\ Accuracy
2,1,Phrase Grounding,ReferIt - Phrase Grounding benchmarking,2018-11,VG_BiLSTM_VGG,62.76,100.0,62.76,100,62.76,0.91,Pointing\\ Game\\ Accuracy
0,1,Domain Generalization,ImageNet-C - Domain Generalization benchmarking,2018-11,Stylized ImageNet (ResNet-50),69.3,90.35,69.3,100,76.7,0.9,mean\\ Corruption\\ Error\\ \\(mCE\\)
1,1,Domain Generalization,ImageNet-C - Domain Generalization benchmarking,2019-03,ResNet-50,76.7,100.0,7.4,1.0,76.7,1.0,mean\\ Corruption\\ Error\\ \\(mCE\\)
0,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2018-12,Joint-candidate SPPE +,75.5,86.38,75.5,100,87.4,0.86,AP\\ Easy
1,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet(HR-Net-48),87.4,100.0,11.9,1.0,87.4,1.0,AP\\ Easy
0,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2018-12,Joint-candidate SPPE +,66.3,91.32,66.3,100,72.6,0.91,AP\\ Medium
1,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2019-07,OccNet,66.6,91.74,0.3,0.0476,72.6,0.92,AP\\ Medium
2,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet(HR-Net-48),72.6,100.0,6.0,0.9524,72.6,1.0,AP\\ Medium
0,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2018-12,Joint-candidate SPPE +,57.4,75.73,57.4,100,75.8,0.76,AP\\ Hard
1,1,Multi-Person Pose Estimation,CrowdPose - Multi-Person Pose Estimation benchmarking,2019-08,HigherHRNet(HR-Net-48),75.8,100.0,18.4,1.0,75.8,1.0,AP\\ Hard
0,1,Action Recognition,AVA v2.1 - Action Recognition benchmarking,2018-12,I3D Tx HighRes,39.6,100.0,39.6,100,39.6,1.0,GFlops
0,1,Action Recognition,AVA v2.1 - Action Recognition benchmarking,2018-12,I3D Tx HighRes,19.3,100.0,19.3,100,19.3,1.0,Params\\ \\(M\\)
0,1,Out-of-Distribution Detection,CIFAR-10 vs CIFAR-100 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 (MSP Baseline),87.9,94.21,87.9,100,93.3,0.9,AUROC
1,1,Out-of-Distribution Detection,CIFAR-10 vs CIFAR-100 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 + OE,93.3,100.0,5.4,1.0,93.3,0.95,AUROC
2,1,Out-of-Distribution Detection,CIFAR-10 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 + OE,97.8,100.0,97.8,100,97.8,1.0,AUROC
0,1,Out-of-Distribution Detection,CIFAR-100 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 (MSP Baseline),-62.66,162.75,-62.66,100,-38.5,6.6,FPR95
1,1,Out-of-Distribution Detection,CIFAR-100 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 + OE,-38.5,100.0,24.16,1.0,-38.5,4.05,FPR95
2,1,Out-of-Distribution Detection,CIFAR-10 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 + OE,-9.5,100.0,-9.5,100,-9.5,1.0,FPR95
0,1,Out-of-Distribution Detection,CIFAR-10 vs CIFAR-100 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 (MSP Baseline),55.8,73.23,55.8,100,76.2,0.73,AUPR
1,1,Out-of-Distribution Detection,CIFAR-10 vs CIFAR-100 - Out-of-Distribution Detection benchmarking,2018-12,WRN 40-2 + OE,76.2,100.0,20.4,1.0,76.2,1.0,AUPR
0,1,Egocentric Activity Recognition,EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking,2018-12,LFB Max,32.7,93.97,32.7,100,34.8,0.94,Actions\\ Top\\-1\\ \\(S1\\)
1,1,Egocentric Activity Recognition,EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking,2019-08,TBN,34.8,100.0,2.1,1.0,34.8,1.0,Actions\\ Top\\-1\\ \\(S1\\)
0,1,Image Generation,LSUN Bedroom - Image Generation benchmarking,2018-12,StyleGAN,2.65,100.0,2.65,100,2.65,1.0,FID\\-50k
0,1,6D Pose Estimation using RGB,YCB-Video - 6D Pose Estimation using RGB benchmarking,2018-12,PVNet,73.4,100.0,73.4,100,73.4,1.0,Mean\\ AUC
0,1,6D Pose Estimation using RGBD,REAL275 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),26.7,100.0,26.7,100,26.7,0.43,"mAP\\ 10,\\ 10cm"
1,1,6D Pose Estimation using RGBD,CAMERA25 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),62.2,100.0,62.2,100,62.2,1.0,"mAP\\ 10,\\ 10cm"
0,1,6D Pose Estimation using RGBD,CAMERA25 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),61.7,100.0,61.7,100,61.7,1.0,"mAP\\ 10,\\ 5cm"
1,1,6D Pose Estimation using RGBD,REAL275 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),26.7,100.0,26.7,100,26.7,0.43,"mAP\\ 10,\\ 5cm"
0,1,6D Pose Estimation using RGBD,CAMERA25 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),91.4,100.0,91.4,100,91.4,1.0,mAP\\ 3DIou\\-at\\-25
1,1,6D Pose Estimation using RGBD,REAL275 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),84.9,100.0,84.9,100,84.9,0.93,mAP\\ 3DIou\\-at\\-25
0,1,6D Pose Estimation using RGBD,REAL275 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),80.5,100.0,80.5,100,80.5,0.94,mAP\\ 3DIou\\-at\\-50
1,1,6D Pose Estimation using RGBD,CAMERA25 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),85.3,100.0,85.3,100,85.3,1.0,mAP\\ 3DIou\\-at\\-50
0,1,6D Pose Estimation using RGBD,REAL275 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),9.5,100.0,9.5,100,9.5,0.24,"mAP\\ 5,\\ 5cm"
1,1,6D Pose Estimation using RGBD,CAMERA25 - 6D Pose Estimation using RGBD benchmarking,2019-01,NOCS (128 bins),38.8,100.0,38.8,100,38.8,1.0,"mAP\\ 5,\\ 5cm"
0,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-01,MFA-Net,86.6,94.23,86.6,100,91.9,0.94,28\\ gestures\\ accuracy
1,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-01,STA-Res-TCN,90.7,98.69,4.1,0.7736,91.9,0.99,28\\ gestures\\ accuracy
2,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-07,DD-Net,91.9,100.0,1.2,0.2264,91.9,1.0,28\\ gestures\\ accuracy
3,1,Hand Gesture Recognition,SHREC 2017 - Hand Gesture Recognition benchmarking,2019-07,DG-STA,90.7,100.0,90.7,100,90.7,0.99,28\\ gestures\\ accuracy
0,1,Semantic Segmentation,ADE20K val - Semantic Segmentation benchmarking,2019-01,Auto-DeepLab-L,81.72,100.0,81.72,100,81.72,1.0,Pixel\\ Accuracy
0,1,DeepFake Detection,FaceForensics - DeepFake Detection benchmarking,2019-01,XceptionNet,86.86,100.0,86.86,100,86.86,1.0,FSF
0,1,DeepFake Detection,FaceForensics - DeepFake Detection benchmarking,2019-01,XceptionNet,70.1,100.0,70.1,100,70.1,1.0,Total\\ Accuracy
0,1,DeepFake Detection,FaceForensics - DeepFake Detection benchmarking,2019-01,XceptionNet,96.36,100.0,96.36,100,96.36,1.0,DF
0,1,DeepFake Detection,FaceForensics - DeepFake Detection benchmarking,2019-01,XceptionNet,90.29,100.0,90.29,100,90.29,1.0,FS
0,1,DeepFake Detection,FaceForensics - DeepFake Detection benchmarking,2019-01,XceptionNet,80.67,100.0,80.67,100,80.67,1.0,NT
0,1,6D Pose Estimation using RGB,T-LESS - 6D Pose Estimation using RGB benchmarking,2019-02,Augmented Autoencoder,36.8,100.0,36.8,100,36.8,0.51,Mean\\ Recall
1,1,6D Pose Estimation using RGBD,T-LESS - 6D Pose Estimation using RGBD benchmarking,2019-02,Augmented Autoencoder,72.76,100.0,72.76,100,72.76,1.0,Mean\\ Recall
0,1,Trajectory Prediction,ActEV - Trajectory Prediction benchmarking,2019-02,Next,17.99,100.0,17.99,100,17.99,1.0,ADE\\-8/12
1,1,Trajectory Prediction,ETH/UCY - Trajectory Prediction benchmarking,2019-02,Next,0.46,100.0,0.46,100,0.46,0.03,ADE\\-8/12
2,1,Trajectory Forecasting,ActEV - Trajectory Forecasting benchmarking,2019-02,Next,17.99,100.0,17.99,100,17.99,1.0,ADE\\-8/12
3,1,Trajectory Prediction,Hotel BIWI Walking Pedestrians dataset - Trajectory Prediction benchmarking,2019-04,Social Ways,0.39,100.0,0.39,100,0.39,0.02,ADE\\-8/12
0,1,Trajectory Prediction,ActEV - Trajectory Prediction benchmarking,2019-02,Next,37.24,100.0,37.24,100,37.24,1.0,FDE\\-8/12
0,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2019-02,FusionNet (RGB_guide&certainty),0.93,77.5,0.93,100,1.2,0.02,iMAE
1,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2019-05,VOICED,1.2,100.0,0.27,1.0,1.2,0.02,iMAE
2,1,Depth Completion,VOID - Depth Completion benchmarking,2019-05,VOICED,48.92,100.0,48.92,100,48.92,1.0,iMAE
0,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2019-02,FusionNet (RGB_guide&certainty),2.19,61.52,2.19,100,3.56,0.02,iRMSE
1,1,Depth Completion,KITTI Depth Completion - Depth Completion benchmarking,2019-05,VOICED,3.56,100.0,1.37,1.0,3.56,0.03,iRMSE
2,1,Depth Completion,VOID - Depth Completion benchmarking,2019-05,VOICED,104.02,100.0,104.02,100,104.02,1.0,iRMSE
0,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2019-02,ASIS (PN++),-47.5,109.45,-47.5,100,-43.4,1.09,mRec
1,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2019-03,PartNet,-43.4,100.0,4.1,1.0,-43.4,1.0,mRec
0,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2019-02,ASIS (PN++),63.6,87.36,63.6,100,72.8,0.87,mPrec
1,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2019-06,3D-BoNet,65.6,90.11,2.0,0.2174,72.8,0.9,mPrec
2,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2019-12,JSNet,66.9,91.9,1.3,0.1413,72.8,0.92,mPrec
3,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2020-03,OccuSeg,72.8,100.0,5.9,0.6413,72.8,1.0,mPrec
0,1,4D Spatio Temporal Semantic Segmentation,4 - 4D Spatio Temporal Semantic Segmentation benchmarking,2019-03,4,7.0,100.0,7,100,7,1.0,4
0,1,Medical Image Segmentation,ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking,2019-03,CE-Net,0.9878,100.0,0.9878,100,0.9878,1.0,VInfo
0,1,Medical Image Segmentation,ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking,2019-03,CE-Net,0.9743,100.0,0.9743,100,0.9743,1.0,VRand
0,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2019-03,PI-REC,-0.112,100.0,-0.112,100,-0.112,1.38,MMD
1,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2019-03,PI-REC,-0.081,100.0,-0.081,100,-0.081,1.0,MMD
0,1,Image Reconstruction,Edge-to-Shoes - Image Reconstruction benchmarking,2019-03,PI-REC,62.3,100.0,62.3,100,62.3,1.0,HP
1,1,Image Reconstruction,Edge-to-Handbags - Image Reconstruction benchmarking,2019-03,PI-REC,57.1,100.0,57.1,100,57.1,0.92,HP
0,1,3D Object Detection,nuScenes - 3D Object Detection benchmarking,2019-03,PointPillars,0.442,98.44,0.442,100,0.449,0.98,NDS
1,1,3D Object Detection,nuScenes - 3D Object Detection benchmarking,2019-03,PointPillars (KITTI),0.448,99.78,0.006,0.8571,0.449,1.0,NDS
2,1,3D Object Detection,nuScenes - 3D Object Detection benchmarking,2019-03,PointPillars (ImageNet),0.449,100.0,0.001,0.1429,0.449,1.0,NDS
0,1,Emotion Recognition in Conversation,EC - Emotion Recognition in Conversation benchmarking,2019-03,HRLCE + BERT,0.7709,99.28,0.7709,100,0.7765,0.01,Micro\\-F1
1,1,Emotion Recognition in Conversation,EC - Emotion Recognition in Conversation benchmarking,2019-04,NELEC,0.7765,100.0,0.0056,1.0,0.7765,0.01,Micro\\-F1
2,1,Emotion Recognition in Conversation,DailyDialog - Emotion Recognition in Conversation benchmarking,2019-09,KET,53.37,100.0,53.37,100,53.37,1.0,Micro\\-F1
0,1,Video Frame Interpolation,X4K1000FPS - Video Frame Interpolation benchmarking,2019-04,DAIN_f,-3.47,100.0,-3.47,100,-3.47,1.0,tOF
0,1,Action Classification,YouCook2 - Action Classification benchmarking,2019-04,VideoBERT (cross modal),33.7,100.0,33.7,100,33.7,1.0,Object\\ Top\\ 5\\ Accuracy
0,1,Action Classification,YouCook2 - Action Classification benchmarking,2019-04,VideoBERT (cross modal),13.1,100.0,13.1,100,13.1,1.0,Object\\ Top\\-1\\ Accuracy
0,1,Action Classification,YouCook2 - Action Classification benchmarking,2019-04,VideoBERT (cross modal),3.2,100.0,3.2,100,3.2,1.0,Verb\\ Top\\-1\\ Accuracy
0,1,Action Classification,YouCook2 - Action Classification benchmarking,2019-04,VideoBERT (cross modal),43.3,100.0,43.3,100,43.3,1.0,Verb\\ Top\\-5\\ Accuracy
0,1,LIDAR Semantic Segmentation,Paris-Lille-3D - LIDAR Semantic Segmentation benchmarking,2019-04,ConvPoint_Keras,0.72,94.86,0.72,100,0.759,0.95,mIOU
1,1,LIDAR Semantic Segmentation,Paris-Lille-3D - LIDAR Semantic Segmentation benchmarking,2019-04,ConvPoint,0.759,100.0,0.039,1.0,0.759,1.0,mIOU
0,1,Pose Transfer,Market-1501 - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,0.94,100.0,0.94,100,0.94,0.98,PCKh
1,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,0.96,100.0,0.96,100,0.96,1.0,PCKh
0,1,Pose Transfer,Market-1501 - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,0.811,100.0,0.811,100,0.811,1.0,mask\\-SSIM
0,1,Pose Transfer,Market-1501 - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,0.74,100.0,0.74,100,0.74,0.76,DS
1,1,Pose Transfer,Deep-Fashion - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,0.976,100.0,0.976,100,0.976,1.0,DS
0,1,Pose Transfer,Market-1501 - Pose Transfer benchmarking,2019-04,Progressive Pose Attention,3.773,100.0,3.773,100,3.773,1.0,mask\\-IS
0,1,Camouflaged Object Segmentation,COD - Camouflaged Object Segmentation benchmarking,2019-04,CPD,77.0,100.0,77.0,100,77.0,1.0,E\\-Measure
1,1,Camouflaged Object Segmentation,CAMO - Camouflaged Object Segmentation benchmarking,2019-06,BASNet,66.1,100.0,66.1,100,66.1,0.86,E\\-Measure
0,1,Camouflaged Object Segmentation,COD - Camouflaged Object Segmentation benchmarking,2019-04,CPD,50.8,100.0,50.8,100,50.8,1.0,Weighted\\ F\\-Measure
1,1,Camouflaged Object Segmentation,CAMO - Camouflaged Object Segmentation benchmarking,2019-06,BASNet,41.3,100.0,41.3,100,41.3,0.81,Weighted\\ F\\-Measure
0,1,Lung Nodule Segmentation,NIH - Lung Nodule Segmentation benchmarking,2019-04,U-Net+R+A4,0.262,100.0,0.262,100,0.262,1.0,AVD
0,1,Lung Nodule Segmentation,NIH - Lung Nodule Segmentation benchmarking,2019-04,U-Net+R+A4,0.985,100.0,0.985,100,0.985,0.01,VS
1,1,Medical Image Segmentation,HSVM - Medical Image Segmentation benchmarking,2019-06,MS-Dual-Guided,94.45,100.0,94.45,100,94.45,1.0,VS
2,1,Medical Image Segmentation,CHAOS MRI Dataset - Medical Image Segmentation benchmarking,2019-06,MS-Dual-Guided,93.85,100.0,93.85,100,93.85,0.99,VS
3,1,Brain Tumor Segmentation,BRATS 2018 - Brain Tumor Segmentation benchmarking,2019-06,MS-Dual-Guided,93.08,100.0,93.08,100,93.08,0.99,VS
0,1,Trajectory Prediction,Stanford Drone - Trajectory Prediction benchmarking,2019-04,Social-Ways,1.16,100.0,1.16,100,1.16,1.0,FDE\\ \\(in\\ world\\ coordinates\\)
0,1,Trajectory Prediction,Stanford Drone - Trajectory Prediction benchmarking,2019-04,Social-Ways,0.62,100.0,0.62,100,0.62,1.0,ADE\\ \\(in\\ world\\ coordinates\\)
0,1,3D Object Detection,nuScenes-F - 3D Object Detection benchmarking,2019-05,RRPN + R101 - F,41.2,100.0,41.2,100,41.2,1.0,ARm
1,1,3D Object Detection,nuScenes-FB - 3D Object Detection benchmarking,2019-05,RRPN + R101 - FB,39.1,100.0,39.1,100,39.1,0.95,ARm
0,1,3D Object Detection,nuScenes-FB - 3D Object Detection benchmarking,2019-05,RRPN + R101 - FB,21.1,100.0,21.1,100,21.1,1.0,ARs
1,1,3D Object Detection,nuScenes-F - 3D Object Detection benchmarking,2019-05,RRPN + R101 - F,4.0,100.0,4,100,4,0.19,ARs
0,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",-49.0,100.0,-49,100,-49,1.0,ROUGE
0,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",64.2,100.0,64.2,100,64.2,1.0,BLEU\\-1
0,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",46.3,100.0,46.3,100,46.3,1.0,BLEU\\-2
0,1,Unsupervised Domain Adaptation,PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking,2019-05,CycleGAN,16.5,96.49,16.5,100,17.1,0.96,AP\\-at\\-0\\.7
1,1,Unsupervised Domain Adaptation,PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking,2019-11,PointDAN,17.1,100.0,0.6,1.0,17.1,1.0,AP\\-at\\-0\\.7
0,1,Homography Estimation,COCO 2014 - Homography Estimation benchmarking,2019-05,PFNet,0.92,100.0,0.92,100,0.92,1.0,MACE
0,1,Video Prediction,CMU Mocap-1 - Video Prediction benchmarking,2019-05,ODE2VAE,93.07,100.0,93.07,100,93.07,1.0,Test\\ Error
1,1,Video Prediction,CMU Mocap-2 - Video Prediction benchmarking,2019-05,ODE2VAE-KL,8.09,80.42,8.09,100,10.06,0.09,Test\\ Error
2,1,Video Prediction,CMU Mocap-2 - Video Prediction benchmarking,2019-05,ODE2VAE,10.06,100.0,1.97,1.0,10.06,0.11,Test\\ Error
0,1,Image Inpainting,Paris StreetView - Image Inpainting benchmarking,2019-05,Coherent Semantic Attention for Image Inpainting,23.1,100.0,23.1,100,23.1,1.0,40\\-50%\\ Mask\\ PSNR
0,1,Image Inpainting,Paris StreetView - Image Inpainting benchmarking,2019-05,Coherent Semantic Attention for Image Inpainting,32.67,100.0,32.67,100,32.67,1.0,10\\-20%\\ Mask\\ PSNR
0,1,Image Inpainting,Paris StreetView - Image Inpainting benchmarking,2019-05,Coherent Semantic Attention for Image Inpainting,30.32,100.0,30.32,100,30.32,1.0,20\\-30%\\ Mask\\ PSNR
0,1,Image Inpainting,Paris StreetView - Image Inpainting benchmarking,2019-05,Coherent Semantic Attention for Image Inpainting,24.85,100.0,24.85,100,24.85,1.0,30\\-40%\\ Mask\\ PSNR
0,1,Salient Object Detection,DUTS-TE - Salient Object Detection benchmarking,2019-06,BASNET (ResNet-34),0.803,100.0,0.803,100,0.803,0.89,Fwβ
1,1,Salient Object Detection,ECSSD - Salient Object Detection benchmarking,2019-06,BASNet (ResNet-34),0.904,100.0,0.904,100,0.904,1.0,Fwβ
0,1,Salient Object Detection,ECSSD - Salient Object Detection benchmarking,2019-06,BASNet (ResNet-34),0.916,100.0,0.916,100,0.916,1.0,Sm
1,1,Salient Object Detection,DUTS-TE - Salient Object Detection benchmarking,2019-06,BASNET (ResNet-34),0.853,100.0,0.853,100,0.853,0.93,Sm
0,1,Salient Object Detection,DUTS-TE - Salient Object Detection benchmarking,2019-06,BASNET (ResNet-34),0.758,100.0,0.758,100,0.758,0.92,relaxFbβ
1,1,Salient Object Detection,ECSSD - Salient Object Detection benchmarking,2019-06,BASNet (ResNet-34),0.826,100.0,0.826,100,0.826,1.0,relaxFbβ
0,1,Salient Object Detection,ECSSD - Salient Object Detection benchmarking,2019-06,BASNet (ResNet-34),0.942,100.0,0.942,100,0.942,1.0,\\{max\\}Fβ
1,1,Salient Object Detection,DUTS-TE - Salient Object Detection benchmarking,2019-06,BASNET (ResNet-34),0.86,100.0,0.86,100,0.86,0.91,\\{max\\}Fβ
0,1,Salient Object Detection,ECSSD - Salient Object Detection benchmarking,2019-06,BASNet (ResNet-34),348.5,100.0,348.5,100,348.5,1.0,Model\\ Size\\ \\(MB\\)
0,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2019-06,Waterz (3D U-Net) + Refinement,0.647,80.17,0.647,100,0.807,0.8,Total\\ Variation\\ of\\ Information
1,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2019-06,Waterz (3D U-Net),0.807,100.0,0.16,1.0,0.807,1.0,Total\\ Variation\\ of\\ Information
0,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2019-06,Waterz (3D U-Net) + Refinement,0.438,76.71,0.438,100,0.571,0.77,VI\\ Split
1,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2019-06,Waterz (3D U-Net),0.571,100.0,0.133,1.0,0.571,1.0,VI\\ Split
0,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2019-06,Waterz (3D U-Net) + Refinement,0.209,88.56,0.209,100,0.236,0.89,VI\\ Merge
1,1,Electron Microscopy Image Segmentation,SNEMI3D - Electron Microscopy Image Segmentation benchmarking,2019-06,Waterz (3D U-Net),0.236,100.0,0.027,1.0,0.236,1.0,VI\\ Merge
0,1,Self-Supervised Image Classification,ImageNet - Self-Supervised Image Classification benchmarking,2019-06,AMDIM (large),626000000.0,100.0,626000000,100,626000000,1.0,Number\\ of\\ Params
0,1,3D Shape Modeling,Pix3D S1 - 3D Shape Modeling benchmarking,2019-06,Mesh R-CNN,51.1,100.0,51.1,100,51.1,1.0,mesh\\ AP
1,1,3D Shape Modeling,Pix3D S2 - 3D Shape Modeling benchmarking,2019-06,Mesh R-CNN,28.8,100.0,28.8,100,28.8,0.56,mesh\\ AP
0,1,Zero-Shot Learning,CUB-200 - 0-Shot Learning - Zero-Shot Learning benchmarking,2019-06,zsl_ADA,70.9,100.0,70.9,100,70.9,1.0,Average\\ Per\\-Class\\ Accuracy
0,1,Medical Image Segmentation,HSVM - Medical Image Segmentation benchmarking,2019-06,MS-Dual-Guided,1.19,100.0,1.19,100,1.19,0.02,MSD
1,1,Medical Image Segmentation,CHAOS MRI Dataset - Medical Image Segmentation benchmarking,2019-06,MS-Dual-Guided,66.0,100.0,66,100,66,1.0,MSD
2,1,Brain Tumor Segmentation,BRATS 2018 - Brain Tumor Segmentation benchmarking,2019-06,MS-Dual-Guided,0.9,100.0,0.9,100,0.9,0.01,MSD
0,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2019-06,FixResNeXt-101 32x48d,829000000.0,89.33,829000000,100,928000000,0.89,Params
1,1,Image Classification,ImageNet ReaL - Image Classification benchmarking,2019-12,BiT-L,928000000.0,100.0,99000000,1.0,928000000,1.0,Params
0,1,Point Cloud Generation,ShapeNet Airplane - Point Cloud Generation benchmarking,2019-06,PointFlow,0.217,100.0,0.217,100,0.217,0.09,MMD\\-CD
1,1,Point Cloud Generation,ShapeNet Car - Point Cloud Generation benchmarking,2019-06,PointFlow,0.91,100.0,0.91,100,0.91,0.38,MMD\\-CD
2,1,Point Cloud Generation,ShapeNet Chair - Point Cloud Generation benchmarking,2019-06,PointFlow,2.42,100.0,2.42,100,2.42,1.0,MMD\\-CD
0,1,Point Cloud Generation,ShapeNet Airplane - Point Cloud Generation benchmarking,2019-06,PointFlow,75.68,100.0,75.68,100,75.68,1.0,1\\-NNA\\-CD
1,1,Point Cloud Generation,ShapeNet Car - Point Cloud Generation benchmarking,2019-06,PointFlow,60.65,100.0,60.65,100,60.65,0.8,1\\-NNA\\-CD
2,1,Point Cloud Generation,ShapeNet Chair - Point Cloud Generation benchmarking,2019-06,PointFlow,60.88,100.0,60.88,100,60.88,0.8,1\\-NNA\\-CD
0,1,Video Generation,BAIR Robot Pushing - Video Generation benchmarking,2019-07,DVD-GAN-FP,109.8,100.0,109.8,100,109.8,1.0,FVD\\ score
0,1,Robust Object Detection,PASCAL VOC 2007 - Robust Object Detection benchmarking,2019-07,Faster R-CNN with Stylized Training Data,56.2,100.0,56.2,100,56.2,1.0,mPC\\ \\[AP50\\]
0,1,Robust Object Detection,Cityscapes test - Robust Object Detection benchmarking,2019-07,Faster R-CNN,12.2,70.93,12.2,100,17.2,0.6,mPC\\ \\[AP\\]
1,1,Robust Object Detection,Cityscapes test - Robust Object Detection benchmarking,2019-07,Faster R-CNN with Stylized Training Data,17.2,100.0,5.0,1.0,17.2,0.84,mPC\\ \\[AP\\]
2,1,Robust Object Detection,COCO - Robust Object Detection benchmarking,2019-07,Faster R-CNN,18.2,89.22,18.2,100,20.4,0.89,mPC\\ \\[AP\\]
3,1,Robust Object Detection,COCO - Robust Object Detection benchmarking,2019-07,Faster R-CNN with Stylized Training Data,20.4,100.0,2.2,1.0,20.4,1.0,mPC\\ \\[AP\\]
0,1,Robust Object Detection,PASCAL VOC 2007 - Robust Object Detection benchmarking,2019-07,Faster R-CNN with Stylized Training Data,69.9,100.0,69.9,100,69.9,1.0,rPC\\ \\[%\\]
1,1,Robust Object Detection,Cityscapes test - Robust Object Detection benchmarking,2019-07,Faster R-CNN,33.4,70.46,33.4,100,47.4,0.48,rPC\\ \\[%\\]
2,1,Robust Object Detection,Cityscapes test - Robust Object Detection benchmarking,2019-07,Faster R-CNN with Stylized Training Data,47.4,100.0,14.0,1.0,47.4,0.68,rPC\\ \\[%\\]
3,1,Robust Object Detection,COCO - Robust Object Detection benchmarking,2019-07,Faster R-CNN,50.2,85.23,50.2,100,58.9,0.72,rPC\\ \\[%\\]
4,1,Robust Object Detection,COCO - Robust Object Detection benchmarking,2019-07,Faster R-CNN with Stylized Training Data,58.9,100.0,8.7,1.0,58.9,0.84,rPC\\ \\[%\\]
0,1,Horizon Line Estimation,KITTI Horizon - Horizon Line Estimation benchmarking,2019-07,"ConvLSTM (Huber Loss, naive residual path)",4.984,100.0,4.984,100,4.984,1.0,ATV
0,1,Skeleton Based Action Recognition,SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking,2019-07,DD-Net,1820000.0,100.0,1820000,100,1820000,1.0,No\\.\\ parameters
0,1,3D Human Pose Estimation,3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking,2019-07,RootNet,-21.25,100.0,-21.25,100,-21.25,1.0,MPJAE
0,1,3D Absolute Human Pose Estimation,Human3.6M - 3D Absolute Human Pose Estimation benchmarking,2019-07,RootNet,-120.0,100.0,-120,100,-120,1.0,MRPE
0,1,Video Retrieval,DiDeMo - Video Retrieval benchmarking,2019-07,Collaborative Experts,82.7,100.0,82.7,100,82.7,0.9,text\\-to\\-video\\ R\\-at\\-50
1,1,Video Retrieval,ActivityNet - Video Retrieval benchmarking,2019-07,Collaborative Experts,91.4,100.0,91.4,100,91.4,1.0,text\\-to\\-video\\ R\\-at\\-50
2,1,Video Retrieval,MSVD - Video Retrieval benchmarking,2019-07,Collaborative Experts,89.0,100.0,89,100,89,0.97,text\\-to\\-video\\ R\\-at\\-50
0,1,Image Matting,Composition-1K - Image Matting benchmarking,2019-08,IndexNet-Matting,-45.8,172.83,-45.8,100,-26.5,1.73,SAD
1,1,Image Matting,Composition-1K - Image Matting benchmarking,2020-03,FBA Matting,-26.5,100.0,19.3,1.0,-26.5,1.0,SAD
2,1,Semantic Image Matting,Semantic Image Matting Dataset - Semantic Image Matting benchmarking,2019-08,IndexNet,-51.29,100.0,-51.29,100,-51.29,1.94,SAD
0,1,Image Matting,Composition-1K - Image Matting benchmarking,2019-08,IndexNet-Matting,-25.9,244.34,-25.9,100,-10.6,2.44,Grad
1,1,Image Matting,Composition-1K - Image Matting benchmarking,2020-03,FBA Matting,-10.6,100.0,15.3,1.0,-10.6,1.0,Grad
2,1,Semantic Image Matting,Semantic Image Matting Dataset - Semantic Image Matting benchmarking,2019-08,IndexNet,-34.19,100.0,-34.19,100,-34.19,3.23,Grad
0,1,Semantic Image Matting,Semantic Image Matting Dataset - Semantic Image Matting benchmarking,2019-08,IndexNet,14.0,100.0,14,100,14,1.0,MSE\\(10\\^3\\)
0,1,Image Matting,Composition-1K - Image Matting benchmarking,2019-08,IndexNet-Matting,-43.7,200.46,-43.7,100,-21.8,2.0,Conn
1,1,Image Matting,Composition-1K - Image Matting benchmarking,2020-03,FBA Matting,-21.8,100.0,21.9,1.0,-21.8,1.0,Conn
2,1,Semantic Image Matting,Semantic Image Matting Dataset - Semantic Image Matting benchmarking,2019-08,IndexNet,-48.77,100.0,-48.77,100,-48.77,2.24,Conn
0,1,Visual Reasoning,NLVR - Visual Reasoning benchmarking,2019-08,VisualBERT,67.4,100.0,67.4,100,67.4,1.0,Accuracy\\ \\(Dev\\)
0,1,Visual Reasoning,NLVR - Visual Reasoning benchmarking,2019-08,VisualBERT,67.0,100.0,67,100,67,1.0,Accuracy\\ \\(Test\\-P\\)
0,1,Visual Reasoning,NLVR - Visual Reasoning benchmarking,2019-08,VisualBERT,67.3,100.0,67.3,100,67.3,1.0,Accuracy\\ \\(Test\\-U\\)
0,1,Layout-to-Image Generation,COCO-Stuff 128x128 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,20.03,59.86,20.03,100,33.46,0.6,SceneFID
1,1,Layout-to-Image Generation,COCO-Stuff 128x128 - Layout-to-Image Generation benchmarking,2019-09,SOARISG,33.46,100.0,13.43,1.0,33.46,1.0,SceneFID
2,1,Layout-to-Image Generation,Visual Genome 128x128 - Layout-to-Image Generation benchmarking,2019-08,LostGAN,13.17,100.0,13.17,100,13.17,0.39,SceneFID
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",24.76,100.0,24.76,100,24.76,1.0,number
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",82.26,100.0,82.26,100,82.26,1.0,unanswerable
0,1,6D Pose Estimation using RGB,T-LESS - 6D Pose Estimation using RGB benchmarking,2019-08,Pix2Pose without ICP,29.5,100.0,29.5,100,29.5,1.0,Recall\\ \\(VSD\\)
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",74.0,100.0,74,100,74,1.0,yes/no
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",39.0,100.0,39,100,39,1.0,other
0,1,Image Retrieval,DeepFashion - Image Retrieval benchmarking,2019-08,RCCapsNet,84.6,100.0,84.6,100,84.6,1.0,Recall\\-at\\-20
0,1,Hand Pose Estimation,K2HPD - Hand Pose Estimation benchmarking,2019-08,A2J,76.3,100.0,76.3,100,76.3,1.0,PDJ@5mm
0,1,Lesion Segmentation,ISIC 2018 - Lesion Segmentation benchmarking,2019-08,BCDU-Net (d=3),0.851,100.0,0.851,100,0.851,1.0,F1\\-Score
0,1,Edge Detection,BIPED - Edge Detection benchmarking,2019-09,DexiNed (WACV\'2020),0.859,100.0,0.859,100,0.859,1.0,ODS
1,1,Edge Detection,CID - Edge Detection benchmarking,2019-09,DexiNed (WACV\'2020),0.65,100.0,0.65,100,0.65,0.76,ODS
0,1,Multimodal Activity Recognition,Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking,2019-09,ST-GCN,52.9,100.0,52.9,100,52.9,1.0,Train\\ F\\-measure
0,1,Image Super-Resolution,USR-248 - 4x upscaling - Image Super-Resolution benchmarking,2019-09,SRDRM-GAN,2.48,100.0,2.48,100,2.48,1.0,UIQM
0,1,Heterogeneous Face Recognition,Disguised Faces in the Wild - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,88.97,100.0,88.97,100,88.97,1.0,GAR\\ at\\-1%\\ FAR\\ Obfuscation
0,1,Heterogeneous Face Recognition,Disguised Faces in the Wild - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,89.3,100.0,89.3,100,89.3,1.0,GAR\\ at\\-1%\\ FAR\\ Overall
0,1,Heterogeneous Face Recognition,Disguised Faces in the Wild - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,75.38,100.0,75.38,100,75.38,1.0,GAR\\ at\\-0\\.1%\\ FAR\\ Impersonation
0,1,Heterogeneous Face Recognition,Disguised Faces in the Wild - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,72.13,100.0,72.13,100,72.13,1.0,GAR\\ at\\-0\\.1%\\ FAR\\ Obfuscation
0,1,Heterogeneous Face Recognition,Disguised Faces in the Wild - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,72.72,100.0,72.72,100,72.72,1.0,GAR\\ at\\-0\\.1%\\ FAR\\ Overall
0,1,Heterogeneous Face Recognition,Disguised Faces in the Wild - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,95.73,100.0,95.73,100,95.73,1.0,GAR\\ at\\-1%\\ FAR\\ Impersonation
0,1,Heterogeneous Face Recognition,CMU-MPIE - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,92.4,100.0,92.4,100,92.4,1.0,16x16\\ Accuracy
0,1,Heterogeneous Face Recognition,CMU-MPIE - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,92.6,100.0,92.6,100,92.6,1.0,24x24\\ Accuracy
0,1,Heterogeneous Face Recognition,CMU-MPIE - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,92.8,100.0,92.8,100,92.8,1.0,32x32\\ Accuracy
0,1,Heterogeneous Face Recognition,CMU-MPIE - Heterogeneous Face Recognition benchmarking,2019-09,A-LINK,92.9,100.0,92.9,100,92.9,1.0,48x48\\ Accuracy
0,1,Image Captioning,Flickr30k Captions test - Image Captioning benchmarking,2019-09,Unified VLP,17.0,100.0,17,100,17,0.8,SPICE
1,1,Image Captioning,COCO Captions test - Image Captioning benchmarking,2019-09,Unified VLP,21.2,100.0,21.2,100,21.2,1.0,SPICE
0,1,Emotion Recognition in Conversation,EmoryNLP - Emotion Recognition in Conversation benchmarking,2019-09,KET,34.39,100.0,34.39,100,34.39,1.0,Weighted\\ Macro\\-F1
0,1,Multiple Object Forecasting,Citywalks - Multiple Object Forecasting benchmarking,2019-09,STED,26.7,100.0,26.7,100,26.7,1.0,ADE
0,1,Multiple Object Forecasting,Citywalks - Multiple Object Forecasting benchmarking,2019-09,STED,54.3,100.0,54.3,100,54.3,1.0,AIOU
0,1,3D Human Pose Estimation,3DPW - 3D Human Pose Estimation benchmarking,2019-09,SPIN (SMPL oPtimization IN the loop),116.4,100.0,116.4,100,116.4,1.0,MPVPE
0,1,Gesture Recognition,GesturePod - Gesture Recognition benchmarking,2019-10,GesturePod,92.0,100.0,92,100,92,1.0,Real\\ World\\ Accuracy
0,1,6D Pose Estimation,NOCS-REAL275 - 6D Pose Estimation benchmarking,2019-10,6-PACK,94.2,100.0,94.2,100,94.2,1.0,IOU25
0,1,6D Pose Estimation,NOCS-REAL275 - 6D Pose Estimation benchmarking,2019-10,6-PACK,33.3,100.0,33.3,100,33.3,1.0,5°5\\ cm
0,1,6D Pose Estimation,NOCS-REAL275 - 6D Pose Estimation benchmarking,2019-10,6-PACK,16.0,100.0,16,100,16,1.0,Rerr
0,1,6D Pose Estimation,NOCS-REAL275 - 6D Pose Estimation benchmarking,2019-10,6-PACK,3.5,100.0,3.5,100,3.5,1.0,Terr
0,1,Synthetic-to-Real Translation,SYNTHIA-to-Cityscapes - Synthetic-to-Real Translation benchmarking,2019-10,CAG-UDA,44.5,95.7,44.5,100,46.5,0.96,MIoU\\ \\(16\\ classes\\)
1,1,Synthetic-to-Real Translation,SYNTHIA-to-Cityscapes - Synthetic-to-Real Translation benchmarking,2019-12,MRNet(ResNet-101),46.5,100.0,2.0,1.0,46.5,1.0,MIoU\\ \\(16\\ classes\\)
0,1,Weakly Supervised Action Localization,THUMOS 2014 - Weakly Supervised Action Localization benchmarking,2019-11,BaS-Net,43.6,100.0,43.6,100,43.6,1.0,mAP@0\\.1:0\\.5
0,1,Camera Localization,Aachen Day-Night benchmark - Camera Localization benchmarking,2019-12,"R2D2 WASF N8 (full scale, 10K kpts)",45.9,100.0,45.9,100,45.9,1.0,"Acc\\-at\\-0\\.5m,\\ 2°"
0,1,Camera Localization,Aachen Day-Night benchmark - Camera Localization benchmarking,2019-12,"R2D2 WASF N8 (full scale, 10K kpts)",66.3,100.0,66.3,100,66.3,1.0,"Acc\\-at\\-1m,\\ 5°"
0,1,Camera Localization,Aachen Day-Night benchmark - Camera Localization benchmarking,2019-12,"R2D2 WASF N8 (full scale, 10K kpts)",88.8,100.0,88.8,100,88.8,1.0,"Acc\\-at\\-5m,\\ 10°"
0,1,Semantic Segmentation,Cityscapes test - Semantic Segmentation benchmarking,2019-12,LightSeg-DarkNet19,88.29,100.0,88.29,100,88.29,1.0,Category\\ mIoU
0,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2019-12,JSNet,54.1,100.0,54.1,100,54.1,1.0,mCov
0,1,3D Instance Segmentation,S3DIS - 3D Instance Segmentation benchmarking,2019-12,JSNet,58.0,100.0,58,100,58,1.0,mWCov
0,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2019-12,LGGAN,2.55,100.0,2.55,100,2.55,1.0,KL
0,1,Cross-View Image-to-Image Translation,cvusa - Cross-View Image-to-Image Translation benchmarking,2019-12,LGGAN,19.744,100.0,19.744,100,19.744,1.0,SD
0,1,Fine-Grained Image Classification,SOP - Fine-Grained Image Classification benchmarking,2020-01,Assemble-ResNet-FGVC-50,85.9,100.0,85.9,100,85.9,1.0,Recall\\-at\\-1
0,1,Video Question Answering,SUTD-TrafficQA - Video Question Answering benchmarking,2020-02,HCRN,36.49,100.0,36.49,100,36.49,1.0,1/4
0,1,Video Question Answering,SUTD-TrafficQA - Video Question Answering benchmarking,2020-02,HCRN,63.79,100.0,63.79,100,63.79,1.0,1/2
0,1,Scene Graph Generation,Visual Genome - Scene Graph Generation benchmarking,2020-02,Causal-TDE,6.9,100.0,6.9,100,6.9,1.0,mean\\ Recall\\ @20
0,1,Image Denoising,"ultracold fermions Technion system, pixelfly - Image Denoising benchmarking",2020-03,absDL,0.0711,100.0,0.0711,100,0.0711,1.0,ODRMSE
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,3.0,100.0,3,100,3,1.0,Test\\ AUC\\ top\\ 1
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,7.6,100.0,7.6,100,7.6,1.0,Test\\ AUC\\ top\\ 2
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,12.3,100.0,12.3,100,12.3,1.0,Test\\ AUC\\ top\\ 3
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,4.3,100.0,4.3,100,4.3,1.0,Val\\ AUC\\ top\\ 1
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,9.8,100.0,9.8,100,9.8,1.0,Val\\ AUC\\ top\\ 2
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,14.8,100.0,14.8,100,14.8,1.0,Val\\ AUC\\ top\\ 3
0,1,Compositional Zero-Shot Learning,MIT-States - Compositional Zero-Shot Learning benchmarking,2020-04,SymNet,28.2,100.0,28.2,100,28.2,0.42,Top\\-2\\ accuracy\\ %
1,1,Compositional Zero-Shot Learning,UT-Zappos - Compositional Zero-Shot Learning benchmarking,2020-04,SymNet,67.8,100.0,67.8,100,67.8,1.0,Top\\-2\\ accuracy\\ %
0,1,Compositional Zero-Shot Learning,UT-Zappos - Compositional Zero-Shot Learning benchmarking,2020-04,SymNet,76.0,100.0,76,100,76,1.0,Top\\-3\\ accuracy\\ %
1,1,Compositional Zero-Shot Learning,MIT-States - Compositional Zero-Shot Learning benchmarking,2020-04,SymNet,33.8,100.0,33.8,100,33.8,0.44,Top\\-3\\ accuracy\\ %
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,24.4,100.0,24.4,100,24.4,1.0,Seen\\ accuracy
0,1,Compositional Zero-Shot Learning,"MIT-States, generalized split - Compositional Zero-Shot Learning benchmarking",2020-04,SymNet,25.2,100.0,25.2,100,25.2,1.0,Unseen\\ accuracy
0,1,3D Human Pose Estimation,Surreal - 3D Human Pose Estimation benchmarking,2020-04,Cross Dataset Generalization,97.3,100.0,97.3,100,97.3,1.0,PCK3D
