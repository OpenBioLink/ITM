,ds_count,task,ds,date,model_label,value,percent_of_max_sota,gain,ratio,max_sota,percent_of_max_metric,merge
0,1,Citation Intent Classification,ACL-ARC - Citation Intent Classification benchmarking,2013-06,SVM,41.0,75.09,41.0,100,54.6,0.42,F1
1,1,Citation Intent Classification,ACL-ARC - Citation Intent Classification benchmarking,2016-06,BiLSTM-Attention,51.8,94.87,10.8,0.7941,54.6,0.53,F1
2,1,Citation Intent Classification,ACL-ARC - Citation Intent Classification benchmarking,2018-01,Feature-rich Random Forest,53.0,97.07,1.2,0.0882,54.6,0.54,F1
3,1,Citation Intent Classification,ACL-ARC - Citation Intent Classification benchmarking,2018-02,BiLSTM-Attention + ELMo,54.6,100.0,1.6,0.1176,54.6,0.55,F1
4,1,Paraphrase Identification,MSRP - Paraphrase Identification benchmarking,2013-10,NMF factorization-unigrams-TFKLD,81.48,94.79,81.48,100,85.96,0.83,F1
5,1,Paraphrase Identification,MSRP - Paraphrase Identification benchmarking,2013-10,"FEAT2, TFKLD, SVM, Fine-grained features",85.96,100.0,4.48,1.0,85.96,0.87,F1
6,1,Semantic Textual Similarity,MRPC - Semantic Textual Similarity benchmarking,2013-10,TF-KLD,85.9,100.0,85.9,100,85.9,0.87,F1
7,1,Question Answering,WebQuestions - Question Answering benchmarking,2014-04,Weakly Supervised Embeddings,29.7,70.38,29.7,100,42.2,0.3,F1
8,1,Question Answering,WebQuestions - Question Answering benchmarking,2014-06,Subgraph embeddings,39.2,92.89,9.5,0.76,42.2,0.4,F1
9,1,Question Answering,WebQuestions - Question Answering benchmarking,2015-06,Memory Networks (ensemble),42.2,100.0,3.0,0.24,42.2,0.43,F1
10,1,Named Entity Recognition,SciERC - Named Entity Recognition benchmarking,2014-09,SciBERT (Base Vocab),65.12,99.42,65.12,100,65.5,0.66,F1
11,1,Named Entity Recognition,SciERC - Named Entity Recognition benchmarking,2018-10,BERT Base,65.24,99.6,0.12,0.3158,65.5,0.66,F1
12,1,Named Entity Recognition,SciERC - Named Entity Recognition benchmarking,2019-03,SciBERT (SciVocab),65.5,100.0,0.26,0.6842,65.5,0.67,F1
13,1,Word Sense Disambiguation,SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking,2015-05,IMS + adapted CW,66.2,86.51,66.2,100,76.52,0.67,F1
14,1,Word Sense Disambiguation,SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking,2016-06,BiLSTM with GloVe,66.9,87.43,0.7,0.0678,76.52,0.68,F1
15,1,Word Sense Disambiguation,SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking,2019-09,kNN-BERT,76.52,100.0,9.62,0.9322,76.52,0.78,F1
16,1,Word Sense Disambiguation,SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking,2015-05,IMS + adapted CW,73.4,91.61,73.4,100,80.12,0.75,F1
17,1,Word Sense Disambiguation,SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking,2019-09,kNN-BERT,80.12,100.0,6.72,1.0,80.12,0.81,F1
18,1,Question Answering,SimpleQuestions - Question Answering benchmarking,2015-06,Memory Networks (ensemble),63.9,100.0,63.9,100,63.9,0.65,F1
19,1,Chinese Word Segmentation,MSRA - Chinese Word Segmentation benchmarking,2015-09,Pre-trained+bigram+ LSTM+CRF,97.4,100.0,97.4,100,97.4,0.99,F1
20,1,Sentence Compression,Google Dataset - Sentence Compression benchmarking,2015-09,LSTM,0.82,96.36,0.82,100,0.851,0.01,F1
21,1,Sentence Compression,Google Dataset - Sentence Compression benchmarking,2018-07,BiRNN + LM Evaluator,0.851,100.0,0.031,1.0,0.851,0.01,F1
22,1,Word Sense Disambiguation,SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking,2016-03,LSTM (T:SemCor),67.0,85.13,67.0,100,78.7,0.68,F1
23,1,Word Sense Disambiguation,SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking,2016-03,"LSTMLP (T:SemCor, U:OMSTI)",67.9,86.28,0.9,0.0769,78.7,0.69,F1
24,1,Word Sense Disambiguation,SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking,2016-03,"LSTMLP (T:SemCor, U:1K)",69.5,88.31,1.6,0.1368,78.7,0.71,F1
25,1,Word Sense Disambiguation,SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",72.63,92.29,3.13,0.2675,78.7,0.74,F1
26,1,Word Sense Disambiguation,SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",78.7,100.0,6.07,0.5188,78.7,0.8,F1
27,1,Word Sense Disambiguation,SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking,2016-03,LSTM (T:OMSTI),81.1,89.71,81.1,100,90.4,0.82,F1
28,1,Word Sense Disambiguation,SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking,2016-03,"LSTMLP (T:SemCor, U:OMSTI)",84.3,93.25,3.2,0.3441,90.4,0.86,F1
29,1,Word Sense Disambiguation,SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",86.02,95.15,1.72,0.1849,90.4,0.87,F1
30,1,Word Sense Disambiguation,SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",90.4,100.0,4.38,0.471,90.4,0.92,F1
31,1,Word Sense Disambiguation,SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking,2016-03,LSTM (T:SemCor),64.2,87.47,64.2,100,73.4,0.65,F1
32,1,Word Sense Disambiguation,SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",66.81,91.02,2.61,0.2837,73.4,0.68,F1
33,1,Word Sense Disambiguation,SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",73.4,100.0,6.59,0.7163,73.4,0.75,F1
34,1,Word Sense Disambiguation,SensEval 2 - Word Sense Disambiguation benchmarking,2016-03,LSTM (T:SemCor),73.6,92.35,73.6,100,79.7,0.75,F1
35,1,Word Sense Disambiguation,SensEval 2 - Word Sense Disambiguation benchmarking,2016-03,"LSTMLP (T:SemCor, U:1K)",73.8,92.6,0.2,0.0328,79.7,0.75,F1
36,1,Word Sense Disambiguation,SensEval 2 - Word Sense Disambiguation benchmarking,2016-03,"LSTMLP (T:OMSTI, U:1K)",74.4,93.35,0.6,0.0984,79.7,0.76,F1
37,1,Word Sense Disambiguation,SensEval 2 - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",75.15,94.29,0.75,0.123,79.7,0.76,F1
38,1,Word Sense Disambiguation,SensEval 2 - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",79.7,100.0,4.55,0.7459,79.7,0.81,F1
39,1,Word Sense Disambiguation,SensEval 3 Task 1 - Word Sense Disambiguation benchmarking,2016-03,"LSTMLP (T:SemCor, U:OMSTI)",71.1,91.39,71.1,100,77.8,0.72,F1
40,1,Word Sense Disambiguation,SensEval 3 Task 1 - Word Sense Disambiguation benchmarking,2016-03,"LSTMLP (T:SemCor, U:1K)",71.8,92.29,0.7,0.1045,77.8,0.73,F1
41,1,Word Sense Disambiguation,SensEval 3 Task 1 - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",77.8,100.0,6.0,0.8955,77.8,0.79,F1
42,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2016-04,Global,64.21,80.67,64.21,100,79.6,0.65,F1
43,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2016-06,NN Cluster Ranker,65.29,82.02,1.08,0.0702,79.6,0.66,F1
44,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2016-09,Reward Rescaling,65.73,82.58,0.44,0.0286,79.6,0.67,F1
45,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2017-07,e2e-coref,67.2,84.42,1.47,0.0955,79.6,0.68,F1
46,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2018-02,e2e-coref + ELMo,70.4,88.44,3.2,0.2079,79.6,0.72,F1
47,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2018-04,c2f-coref,73.0,91.71,2.6,0.1689,79.6,0.74,F1
48,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2019-07,BERT + EE,76.61,96.24,3.61,0.2346,79.6,0.78,F1
49,1,Coreference Resolution,OntoNotes - Coreference Resolution benchmarking,2019-07,SpanBERT,79.6,100.0,2.99,0.1943,79.6,0.81,F1
50,1,Relation Extraction,SemEval-2010 Task 8 - Relation Extraction benchmarking,2016-08,Att-Pooling-CNN,88.0,96.7,88.0,100,91.0,0.89,F1
51,1,Relation Extraction,SemEval-2010 Task 8 - Relation Extraction benchmarking,2019-02,Entity-Aware BERT,89.0,97.8,1.0,0.3333,91.0,0.9,F1
52,1,Relation Extraction,SemEval-2010 Task 8 - Relation Extraction benchmarking,2019-05,R-BERT,89.25,98.08,0.25,0.0833,91.0,0.91,F1
53,1,Relation Extraction,SemEval-2010 Task 8 - Relation Extraction benchmarking,2019-06,BERTEM+MTB,89.5,98.35,0.25,0.0833,91.0,0.91,F1
54,1,Relation Extraction,SemEval-2010 Task 8 - Relation Extraction benchmarking,2019-11,EPGNN,90.2,99.12,0.7,0.2333,91.0,0.92,F1
55,1,Relation Extraction,SemEval-2010 Task 8 - Relation Extraction benchmarking,2020-04,REDN,91.0,100.0,0.8,0.2667,91.0,0.92,F1
56,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-08,Match-LSTM with Ans-Ptr (Sentence),67.748,71.25,67.748,100,95.08,0.69,F1
57,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-08,Match-LSTM with Ans-Ptr (Boundary) (ensemble),77.022,81.01,9.274,0.3393,95.08,0.78,F1
58,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-09,ReasoNet (single model),79.364,83.47,2.342,0.0857,95.08,0.81,F1
59,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-09,ReasoNet (ensemble),82.552,86.82,3.188,0.1166,95.08,0.84,F1
60,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2017-05,Reinforced Mnemonic Reader (ensemble model),88.533,93.11,5.981,0.2188,95.08,0.9,F1
61,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2018-10,BERT (ensemble),93.16,97.98,4.627,0.1693,95.08,0.95,F1
62,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2019-06,XLNet (single model),95.08,100.0,1.92,0.0702,95.08,0.97,F1
63,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2016-08,Match-LSTM with Bi-Ans-Ptr (Boundary+Search+b) ,64.7,68.03,64.7,100,95.1,0.66,F1
64,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2016-10,DCR,71.2,74.87,6.5,0.2138,95.1,0.72,F1
65,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2016-11,RASOR,74.9,78.76,3.7,0.1217,95.1,0.76,F1
66,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2016-11,BIDAF (single),77.3,81.28,2.4,0.0789,95.1,0.79,F1
67,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-03,SEDT-LSTM,77.42,81.41,0.12,0.0039,95.1,0.79,F1
68,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-03,FastQAExt (beam-size 5),78.5,82.54,1.08,0.0355,95.1,0.8,F1
69,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-03,DrQA (Document Reader only),78.8,82.86,0.3,0.0099,95.1,0.8,F1
70,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-04,Ruminating Reader,79.5,83.6,0.7,0.023,95.1,0.81,F1
71,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-05,R.M-Reader (single),86.3,90.75,6.8,0.2237,95.1,0.88,F1
72,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2018-10,BERT large,90.9,95.58,4.6,0.1513,95.1,0.92,F1
73,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2018-10,BERT large (+TriviaQA),91.1,95.79,0.2,0.0066,95.1,0.93,F1
74,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2019-06,XLNet (single model),95.1,100.0,4.0,0.1316,95.1,0.97,F1
75,1,Question Answering,NewsQA - Question Answering benchmarking,2017-03,FastQAExt,56.1,76.22,56.1,100,73.6,0.57,F1
76,1,Question Answering,NewsQA - Question Answering benchmarking,2018-01,AMANDA,63.7,86.55,7.6,0.4343,73.6,0.65,F1
77,1,Question Answering,NewsQA - Question Answering benchmarking,2018-11,DecaProp,66.3,90.08,2.6,0.1486,73.6,0.67,F1
78,1,Question Answering,NewsQA - Question Answering benchmarking,2019-07,SpanBERT,73.6,100.0,7.3,0.4171,73.6,0.75,F1
79,1,Question Answering,TriviaQA - Question Answering benchmarking,2017-05,Mnemonic Reader,52.85,63.22,52.85,100,83.6,0.54,F1
80,1,Question Answering,TriviaQA - Question Answering benchmarking,2017-06,Reading Twice for NLU,56.73,67.86,3.88,0.1262,83.6,0.58,F1
81,1,Question Answering,TriviaQA - Question Answering benchmarking,2017-10,S-Norm,71.32,85.31,14.59,0.4745,83.6,0.72,F1
82,1,Question Answering,TriviaQA - Question Answering benchmarking,2018-10,MemoReader,73.26,87.63,1.94,0.0631,83.6,0.74,F1
83,1,Question Answering,TriviaQA - Question Answering benchmarking,2019-07,SpanBERT,83.6,100.0,10.34,0.3363,83.6,0.85,F1
84,1,Relation Extraction,WebNLG - Relation Extraction benchmarking,2017-06,NovelTagging,28.3,34.06,28.3,100,83.1,0.29,F1
85,1,Relation Extraction,WebNLG - Relation Extraction benchmarking,2018-07,CopyRE MultiDecoder,37.1,44.65,8.8,0.1606,83.1,0.38,F1
86,1,Relation Extraction,WebNLG - Relation Extraction benchmarking,2019-09,ETL-Span,83.1,100.0,46.0,0.8394,83.1,0.84,F1
87,1,Relation Extraction,NYT-single - Relation Extraction benchmarking,2017-06,NovelTagging,49.5,83.9,49.5,100,59.0,0.5,F1
88,1,Relation Extraction,NYT-single - Relation Extraction benchmarking,2019-07,PA-LSTM,53.8,91.19,4.3,0.4526,59.0,0.55,F1
89,1,Relation Extraction,NYT-single - Relation Extraction benchmarking,2019-09,ETL-Span,59.0,100.0,5.2,0.5474,59.0,0.6,F1
90,1,Relation Extraction,NYT - Relation Extraction benchmarking,2017-06,NovelTagging,42.0,53.85,42.0,100,78.0,0.43,F1
91,1,Relation Extraction,NYT - Relation Extraction benchmarking,2018-07,CopyRE MultiDecoder,58.7,75.26,16.7,0.4639,78.0,0.6,F1
92,1,Relation Extraction,NYT - Relation Extraction benchmarking,2019-09,ETL-Span,78.0,100.0,19.3,0.5361,78.0,0.79,F1
93,1,Predicate Detection,CoNLL 2005 - Predicate Detection benchmarking,2017-07,DeepSRL,96.4,97.97,96.4,100,98.4,0.98,F1
94,1,Predicate Detection,CoNLL 2005 - Predicate Detection benchmarking,2018-04,LISA,98.4,100.0,2.0,1.0,98.4,1.0,F1
95,1,Semantic Role Labeling,OntoNotes - Semantic Role Labeling benchmarking,2017-07,He et al.,81.7,93.91,81.7,100,87.0,0.83,F1
96,1,Semantic Role Labeling,OntoNotes - Semantic Role Labeling benchmarking,2017-12,Tan et al.,82.7,95.06,1.0,0.1887,87.0,0.84,F1
97,1,Semantic Role Labeling,OntoNotes - Semantic Role Labeling benchmarking,2018-02,"He et al., 2017 + ELMo",84.6,97.24,1.9,0.3585,87.0,0.86,F1
98,1,Semantic Role Labeling,OntoNotes - Semantic Role Labeling benchmarking,2018-05,"He et al.,",85.5,98.28,0.9,0.1698,87.0,0.87,F1
99,1,Semantic Role Labeling,OntoNotes - Semantic Role Labeling benchmarking,2018-10,BiLSTM-Span (Ensemble),87.0,100.0,1.5,0.283,87.0,0.88,F1
100,1,Question Answering,COMPLEXQUESTIONS - Question Answering benchmarking,2017-07,WebQA,53.6,100.0,53.6,100,53.6,0.54,F1
101,1,Sarcasm Detection,SCv1 - Sarcasm Detection benchmarking,2017-08,DeepMoji,0.69,100.0,0.69,100,0.69,0.01,F1
102,1,Relation Extraction,Re-TACRED - Relation Extraction benchmarking,2017-09,PA-LSTM,79.4,93.08,79.4,100,85.3,0.81,F1
103,1,Relation Extraction,Re-TACRED - Relation Extraction benchmarking,2018-09,C-GCN,80.3,94.14,0.9,0.1525,85.3,0.82,F1
104,1,Relation Extraction,Re-TACRED - Relation Extraction benchmarking,2019-07,SpanBERT,85.3,100.0,5.0,0.8475,85.3,0.87,F1
105,1,Named Entity Recognition,Long-tail emerging entities - Named Entity Recognition benchmarking,2017-09,SpinningBytes,40.78,81.24,40.78,100,50.2,0.41,F1
106,1,Named Entity Recognition,Long-tail emerging entities - Named Entity Recognition benchmarking,2018-08,Flair embeddings,50.2,100.0,9.42,1.0,50.2,0.51,F1
107,1,Relation Extraction,TACRED - Relation Extraction benchmarking,2017-09,PA-LSTM,65.1,91.05,65.1,100,71.5,0.66,F1
108,1,Relation Extraction,TACRED - Relation Extraction benchmarking,2018-09,C-GCN + PA-LSTM,68.2,95.38,3.1,0.4844,71.5,0.69,F1
109,1,Relation Extraction,TACRED - Relation Extraction benchmarking,2019-05,R-BERT,69.4,97.06,1.2,0.1875,71.5,0.71,F1
110,1,Relation Extraction,TACRED - Relation Extraction benchmarking,2019-06,BERTEM+MTB,71.5,100.0,2.1,0.3281,71.5,0.73,F1
111,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2017-11,FusionNet++ (ensemble),72.484,78.6,72.484,100,92.215,0.74,F1
112,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2017-12,SAN (ensemble model),73.704,79.93,1.22,0.0618,92.215,0.75,F1
113,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2018-08,Reinforced Mnemonic Reader + Answer Verifier (single model),74.295,80.57,0.591,0.03,92.215,0.76,F1
114,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2018-10,BERT (single model),83.061,90.07,8.766,0.4443,92.215,0.84,F1
115,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2019-06,XLNet (single model),90.689,98.35,7.628,0.3866,92.215,0.92,F1
116,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2019-08,XLNet + SG-Net Verifier (ensemble),90.702,98.36,0.013,0.0007,92.215,0.92,F1
117,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2019-09,ALBERT (ensemble model),92.215,100.0,1.513,0.0767,92.215,0.94,F1
118,1,Sentence Classification,SciCite - Sentence Classification benchmarking,2018-01,Feature-Rich Random Forest,79.6,93.76,79.6,100,84.9,0.81,F1
119,1,Sentence Classification,SciCite - Sentence Classification benchmarking,2018-10,BERT,84.4,99.41,4.8,0.9057,84.9,0.86,F1
120,1,Sentence Classification,SciCite - Sentence Classification benchmarking,2019-03,SciBERT,84.9,100.0,0.5,0.0943,84.9,0.86,F1
121,1,Sentence Classification,ACL-ARC - Sentence Classification benchmarking,2018-01,Random Forest,53.0,74.67,53.0,100,70.98,0.54,F1
122,1,Sentence Classification,ACL-ARC - Sentence Classification benchmarking,2019-03,SciBERT,70.98,100.0,17.98,1.0,70.98,0.72,F1
123,1,Entity Linking,WebQSP-WD - Entity Linking benchmarking,2018-04,VCG,0.73,100.0,0.73,100,0.73,0.01,F1
124,1,Predicate Detection,CoNLL 2012 - Predicate Detection benchmarking,2018-04,LISA,97.2,100.0,97.2,100,97.2,0.99,F1
125,1,Semantic Role Labeling,CoNLL 2005 - Semantic Role Labeling benchmarking,2018-04,LISA,86.04,97.22,86.04,100,88.5,0.87,F1
126,1,Semantic Role Labeling,CoNLL 2005 - Semantic Role Labeling benchmarking,2018-10,BiLSTM-Span,87.6,98.98,1.56,0.6341,88.5,0.89,F1
127,1,Semantic Role Labeling,CoNLL 2005 - Semantic Role Labeling benchmarking,2018-10,"BiLSTM-Span (Ensemble, predicates given)",88.5,100.0,0.9,0.3659,88.5,0.9,F1
128,1,Chinese Named Entity Recognition,OntoNotes 4 - Chinese Named Entity Recognition benchmarking,2018-05,Lattice,73.88,100.0,73.88,100,73.88,0.75,F1
129,1,Chinese Named Entity Recognition,Weibo NER - Chinese Named Entity Recognition benchmarking,2018-05,Lattice,58.79,100.0,58.79,100,58.79,0.6,F1
130,1,Chinese Named Entity Recognition,MSRA - Chinese Named Entity Recognition benchmarking,2018-05,Lattice,93.18,97.83,93.18,100,95.25,0.95,F1
131,1,Chinese Named Entity Recognition,MSRA - Chinese Named Entity Recognition benchmarking,2019-04,ERNIE,93.8,98.48,0.62,0.2995,95.25,0.95,F1
132,1,Chinese Named Entity Recognition,MSRA - Chinese Named Entity Recognition benchmarking,2019-07,ERNIE 2.0 Large,95.0,99.74,1.2,0.5797,95.25,0.97,F1
133,1,Chinese Named Entity Recognition,MSRA - Chinese Named Entity Recognition benchmarking,2019-11,ZEN (Init with Chinese BERT),95.25,100.0,0.25,0.1208,95.25,0.97,F1
134,1,Chinese Named Entity Recognition,Resume NER - Chinese Named Entity Recognition benchmarking,2018-05,Lattice,94.46,99.43,94.46,100,95.0,0.96,F1
135,1,Chinese Named Entity Recognition,Resume NER - Chinese Named Entity Recognition benchmarking,2019-11,TENER,95.0,100.0,0.54,1.0,95.0,0.97,F1
136,1,Word Sense Disambiguation,SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking,2018-05,GAS (Linear),71.6,86.68,71.6,100,82.6,0.73,F1
137,1,Word Sense Disambiguation,SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking,2018-05,GASext (Concatenation),72.6,87.89,1.0,0.0909,82.6,0.74,F1
138,1,Word Sense Disambiguation,SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",74.46,90.15,1.86,0.1691,82.6,0.76,F1
139,1,Word Sense Disambiguation,SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",82.6,100.0,8.14,0.74,82.6,0.84,F1
140,1,Named Entity Recognition,CoNLL 2000 - Named Entity Recognition benchmarking,2018-05,SWEM-CRF,90.34,100.0,90.34,100,90.34,0.92,F1
141,1,Named Entity Recognition,ACE 2005 - Named Entity Recognition benchmarking,2018-06,Neural layered model,72.2,85.62,72.2,100,84.33,0.73,F1
142,1,Named Entity Recognition,ACE 2005 - Named Entity Recognition benchmarking,2018-10,Neural segmental hypergraphs,74.5,88.34,2.3,0.1896,84.33,0.76,F1
143,1,Named Entity Recognition,ACE 2005 - Named Entity Recognition benchmarking,2019-06,Anchor-Region Networks,74.9,88.82,0.4,0.033,84.33,0.76,F1
144,1,Named Entity Recognition,ACE 2005 - Named Entity Recognition benchmarking,2019-06,MGNER,78.2,92.73,3.3,0.2721,84.33,0.79,F1
145,1,Named Entity Recognition,ACE 2005 - Named Entity Recognition benchmarking,2019-06,Merge and Label,82.4,97.71,4.2,0.3462,84.33,0.84,F1
146,1,Named Entity Recognition,ACE 2005 - Named Entity Recognition benchmarking,2019-08,seq2seq+BERT+Flair,84.33,100.0,1.93,0.1591,84.33,0.86,F1
147,1,Named Entity Recognition,GENIA - Named Entity Recognition benchmarking,2018-06,Neural layered model,74.7,95.39,74.7,100,78.31,0.76,F1
148,1,Named Entity Recognition,GENIA - Named Entity Recognition benchmarking,2018-10,Neural segmental hypergraphs,75.1,95.9,0.4,0.1108,78.31,0.76,F1
149,1,Named Entity Recognition,GENIA - Named Entity Recognition benchmarking,2019-08,seq2seq+BERT+Flair,78.31,100.0,3.21,0.8892,78.31,0.8,F1
150,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (T+V),0.756,98.44,0.756,100,0.768,0.01,F1
151,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (A+T+V),0.768,100.0,0.012,1.0,0.768,0.01,F1
152,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-07,Denoising QA,64.5,76.06,64.5,100,84.8,0.66,F1
153,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2019-07,SpanBERT,84.8,100.0,20.3,1.0,84.8,0.86,F1
154,1,Dependency Parsing,GENIA - LAS - Dependency Parsing benchmarking,2018-08,BiLSTM-CRF,91.92,100.0,91.92,100,91.92,0.93,F1
155,1,Dependency Parsing,GENIA - UAS - Dependency Parsing benchmarking,2018-08,BiLSTM-CRF,92.84,100.0,92.84,100,92.84,0.94,F1
156,1,Question Answering,SQuAD2.0 dev - Question Answering benchmarking,2018-08,RMR + ELMo (Model-III),74.8,82.56,74.8,100,90.6,0.76,F1
157,1,Question Answering,SQuAD2.0 dev - Question Answering benchmarking,2018-10,BERT large,81.9,90.4,7.1,0.4494,90.6,0.83,F1
158,1,Question Answering,SQuAD2.0 dev - Question Answering benchmarking,2019-06,XLNet (single model),90.6,100.0,8.7,0.5506,90.6,0.92,F1
159,1,Sentence Classification,PubMed 20k RCT - Sentence Classification benchmarking,2018-08,Hierarchical Neural Networks,92.6,100.0,92.6,100,92.6,0.94,F1
160,1,Named Entity Recognition,BC5CDR - Named Entity Recognition benchmarking,2018-09,CollaboNet,87.12,96.88,87.12,100,89.93,0.89,F1
161,1,Named Entity Recognition,BC5CDR - Named Entity Recognition benchmarking,2019-03,SciBERT (SciVocab),88.94,98.9,1.82,0.6477,89.93,0.9,F1
162,1,Named Entity Recognition,BC5CDR - Named Entity Recognition benchmarking,2019-08,BioFLAIR,89.42,99.43,0.48,0.1708,89.93,0.91,F1
163,1,Named Entity Recognition,BC5CDR - Named Entity Recognition benchmarking,2019-11,NER+PA+RL (PubMed),89.93,100.0,0.51,0.1815,89.93,0.91,F1
164,1,Chinese Named Entity Recognition,SighanNER - Chinese Named Entity Recognition benchmarking,2018-10,BiLSTM+CRF+adversarial+self-attention,90.64,100.0,90.64,100,90.64,0.92,F1
165,1,Nested Mention Recognition,ACE 2004 - Nested Mention Recognition benchmarking,2018-10,Neural transition-based model,73.1,100.0,73.1,100,73.1,0.74,F1
166,1,Nested Named Entity Recognition,ACE 2005 - Nested Named Entity Recognition benchmarking,2018-10,neural transition-based model,73.0,96.18,73.0,100,75.9,0.74,F1
167,1,Nested Named Entity Recognition,ACE 2005 - Nested Named Entity Recognition benchmarking,2019-06,Anchor-Region Networks,75.9,100.0,2.9,1.0,75.9,0.77,F1
168,1,Named Entity Recognition,ACE 2004 - Named Entity Recognition benchmarking,2018-10,Neural transition-based model,73.3,86.85,73.3,100,84.4,0.74,F1
169,1,Named Entity Recognition,ACE 2004 - Named Entity Recognition benchmarking,2019-06,MGNER,79.5,94.19,6.2,0.5586,84.4,0.81,F1
170,1,Named Entity Recognition,ACE 2004 - Named Entity Recognition benchmarking,2019-08,seq2seq+BERT+Flair,84.4,100.0,4.9,0.4414,84.4,0.86,F1
171,1,Question Answering,QuAC - Question Answering benchmarking,2018-10,FlowQA (single model),64.1,100.0,64.1,100,64.1,0.65,F1
172,1,Named Entity Recognition,NCBI-disease - Named Entity Recognition benchmarking,2018-10,BERT Base,86.37,96.28,86.37,100,89.71,0.88,F1
173,1,Named Entity Recognition,NCBI-disease - Named Entity Recognition benchmarking,2019-01,BioBERT,89.71,100.0,3.34,1.0,89.71,0.91,F1
174,1,Common Sense Reasoning,ReCoRD - Common Sense Reasoning benchmarking,2018-10,BERT-Base (single model),56.065,100.0,56.065,100,56.065,0.57,F1
175,1,Slot Filling,ATIS - Slot Filling benchmarking,2018-12,Capsule-NLU,0.952,99.37,0.952,100,0.958,0.01,F1
176,1,Slot Filling,ATIS - Slot Filling benchmarking,2019-06,SF-ID,0.958,100.0,0.006,1.0,0.958,0.01,F1
177,1,Relation Extraction,ChemProt - Relation Extraction benchmarking,2019-01,BioBERT,76.46,91.42,76.46,100,83.64,0.78,F1
178,1,Relation Extraction,ChemProt - Relation Extraction benchmarking,2019-03,SciBert (Finetune),83.64,100.0,7.18,1.0,83.64,0.85,F1
179,1,Named Entity Recognition,JNLPBA - Named Entity Recognition benchmarking,2019-01,BioBERT,77.59,100.0,77.59,100,77.59,0.79,F1
180,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2019-01,MT-DNN,72.4,97.57,72.4,100,74.2,0.74,F1
181,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2019-06,XLNet-Large (ensemble),74.2,100.0,1.8,1.0,74.2,0.75,F1
182,1,Document Classification,Reuters-21578 - Document Classification benchmarking,2019-02,VLAWE,89.3,99.33,89.3,100,89.9,0.91,F1
183,1,Document Classification,Reuters-21578 - Document Classification benchmarking,2020-02,MAGNET,89.9,100.0,0.6,1.0,89.9,0.91,F1
184,1,Relation Extraction,JNLPBA - Relation Extraction benchmarking,2019-03,SciBERT (SciVocab),76.09,100.0,76.09,100,76.09,0.77,F1
185,1,Sentence Classification,ScienceCite - Sentence Classification benchmarking,2019-03,SciBERT (Base Vocab),84.43,99.34,84.43,100,84.99,0.86,F1
186,1,Sentence Classification,ScienceCite - Sentence Classification benchmarking,2019-03,SciBERT (SciVocab),84.99,100.0,0.56,1.0,84.99,0.86,F1
187,1,Relation Extraction,SciERC - Relation Extraction benchmarking,2019-03,SciBERT (Base Vocab),74.42,99.71,74.42,100,74.64,0.76,F1
188,1,Relation Extraction,SciERC - Relation Extraction benchmarking,2019-03,SciBERT (SciVocab),74.64,100.0,0.22,1.0,74.64,0.76,F1
189,1,Sentence Classification,Paper Field - Sentence Classification benchmarking,2019-03,SciBERT (SciVocab),65.71,100.0,65.71,100,65.71,0.67,F1
190,1,Citation Intent Classification,SciCite - Citation Intent Classification benchmarking,2019-03,SciBERT,84.99,100.0,84.99,100,84.99,0.86,F1
191,1,Named Entity Recognition,WetLab - Named Entity Recognition benchmarking,2019-04,BiLSTM-CRF with ELMo,79.62,100.0,79.62,100,79.62,0.81,F1
192,1,Relation Extraction,WLPC - Relation Extraction benchmarking,2019-04,DyGIE,64.1,100.0,64.1,100,64.1,0.65,F1
193,1,Named Entity Recognition,WLPC - Named Entity Recognition benchmarking,2019-04,DyGIE,79.5,100.0,79.5,100,79.5,0.81,F1
194,1,Speech Emotion Recognition,IEMOCAP - Speech Emotion Recognition benchmarking,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression),0.718,100.0,0.718,100,0.718,0.01,F1
195,1,Document Classification,AAPD - Document Classification benchmarking,2019-04,KD-LSTMreg,72.9,100.0,72.9,100,72.9,0.74,F1
196,1,Chinese Named Entity Recognition,MSRA Dev - Chinese Named Entity Recognition benchmarking,2019-04,ERNIE,95.0,98.65,95.0,100,96.3,0.97,F1
197,1,Chinese Named Entity Recognition,MSRA Dev - Chinese Named Entity Recognition benchmarking,2019-07,ERNIE 2.0 Base,95.2,98.86,0.2,0.1538,96.3,0.97,F1
198,1,Chinese Named Entity Recognition,MSRA Dev - Chinese Named Entity Recognition benchmarking,2019-07,ERNIE 2.0 Large,96.3,100.0,1.1,0.8462,96.3,0.98,F1
199,1,Relation Extraction,FewRel - Relation Extraction benchmarking,2019-05,ERNIE,88.32,100.0,88.32,100,88.32,0.9,F1
200,1,Entity Typing,Open Entity - Entity Typing benchmarking,2019-05,ERNIE,75.56,100.0,75.56,100,75.56,0.77,F1
201,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-06,DocRED-LSTM,50.12,90.14,50.12,100,55.6,0.51,F1
202,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-06,DocRED-Context-Aware,50.64,91.08,0.52,0.0949,55.6,0.51,F1
203,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-06,DocRED-BiLSTM,51.06,91.83,0.42,0.0766,55.6,0.52,F1
204,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-09,Two-Step+BERT-base,53.92,96.98,2.86,0.5219,55.6,0.55,F1
205,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2020-03,HIN-BERT-base,55.6,100.0,1.68,0.3066,55.6,0.57,F1
206,1,Sentiment Analysis,ChnSentiCorp - Sentiment Analysis benchmarking,2019-06,RoBERTa-wwm-ext-large,95.8,100.0,95.8,100,95.8,0.97,F1
207,1,Intent Detection,ATIS - Intent Detection benchmarking,2019-06,SF-ID (BLSTM) network,95.8,100.0,95.8,100,95.8,0.97,F1
208,1,Question Answering,NaturalQA - Question Answering benchmarking,2019-07,SpanBERT,82.5,100.0,82.5,100,82.5,0.84,F1
209,1,Named Entity Recognition,Species-800 - Named Entity Recognition benchmarking,2019-08,BioFLAIR,82.44,100.0,82.44,100,82.44,0.84,F1
210,1,Named Entity Recognition,LINNAEUS - Named Entity Recognition benchmarking,2019-08,BioFLAIR,87.02,100.0,87.02,100,87.02,0.88,F1
211,1,Named Entity Recognition,Code-Switching English-Spanish NER - Named Entity Recognition benchmarking,2019-09,HME (word + BPE + char),69.17,100.0,69.17,100,69.17,0.7,F1
212,1,Named Entity Recognition,ontontoes chinese v5 - Named Entity Recognition benchmarking,2019-09,DGLSTM-CRF,79.92,100.0,79.92,100,79.92,0.81,F1
213,1,Chinese Word Segmentation,MSR - Chinese Word Segmentation benchmarking,2019-11,ZEN (Init with Chinese BERT),98.35,100.0,98.35,100,98.35,1.0,F1
214,1,Relation Extraction,NYT29 - Relation Extraction benchmarking,2019-11,WDec,71.6,100.0,71.6,100,71.6,0.73,F1
215,1,Relation Extraction,NYT24 - Relation Extraction benchmarking,2019-11,WDec,84.4,100.0,84.4,100,84.4,0.86,F1
216,1,Intent Detection,ASOS.com user intent - Intent Detection benchmarking,2019-12,linear-Ngrams,0.865,97.52,0.865,100,0.887,0.01,F1
217,1,Intent Detection,ASOS.com user intent - Intent Detection benchmarking,2019-12,plain-LSTM,0.887,100.0,0.022,1.0,0.887,0.01,F1
218,1,Negation Scope Resolution,SFU Review Corpus - Negation Scope Resolution benchmarking,2020-01,XLNet,91.25,100.0,91.25,100,91.25,0.93,F1
219,1,Negation Scope Resolution,BioScope : Abstracts - Negation Scope Resolution benchmarking,2020-01,XLNet,95.74,100.0,95.74,100,95.74,0.97,F1
220,1,Negation Scope Resolution,_sem 2012 Shared Task: Sherlock Dataset - Negation Scope Resolution benchmarking,2020-01,RoBERTa,91.59,100.0,91.59,100,91.59,0.93,F1
221,1,Speculation Scope Resolution,BioScope : Abstracts - Speculation Scope Resolution benchmarking,2020-01,XLNet,97.87,100.0,97.87,100,97.87,0.99,F1
222,1,Speculation Scope Resolution,SFU Review Corpus - Speculation Scope Resolution benchmarking,2020-01,XLNet,91.0,100.0,91,100,91,0.92,F1
223,1,Negation Scope Resolution,BioScope : Full Papers - Negation Scope Resolution benchmarking,2020-01,XLNet,94.4,100.0,94.4,100,94.4,0.96,F1
224,1,Speculation Scope Resolution,BioScope : Full Papers - Speculation Scope Resolution benchmarking,2020-01,XLNet,96.91,100.0,96.91,100,96.91,0.98,F1
225,1,Multi-Label Text Classification,AAPD - Multi-Label Text Classification benchmarking,2020-02,MAGNET,69.6,100.0,69.6,100,69.6,0.71,F1
226,1,Named Entity Recognition,SoSciSoCi - Named Entity Recognition benchmarking,2020-03,Bi-LSTM-CRF (SSC->GSC),0.82,100.0,0.82,100,0.82,0.01,F1
0,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2013-10,RNTN,85.4,87.95,85.4,100,97.1,0.86,Accuracy
1,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2014-08,CNN-MC [kim:13],88.1,90.73,2.7,0.2308,97.1,0.88,Accuracy
2,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2015-06,DMN [ankit16],88.6,91.25,0.5,0.0427,97.1,0.89,Accuracy
3,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2016-07,Neural Semantic Encoder,89.7,92.38,1.1,0.094,97.1,0.9,Accuracy
4,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2017-04,bmLSTM,91.8,94.54,2.1,0.1795,97.1,0.92,Accuracy
5,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2017-12,Block-sparse LSTM,93.2,95.98,1.4,0.1197,97.1,0.94,Accuracy
6,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2018-10,Bidirectional Encoder Representations from Transformers ,94.9,97.73,1.7,0.1453,97.1,0.95,Accuracy
7,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2019-01,MT-DNN,95.6,98.46,0.7,0.0598,97.1,0.96,Accuracy
8,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2019-06,XLNet-Large (ensemble),96.8,99.69,1.2,0.1026,97.1,0.97,Accuracy
9,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2019-06,XLNet (single model),97.0,99.9,0.2,0.0171,97.1,0.97,Accuracy
10,1,Sentiment Analysis,SST-2 Binary classification - Sentiment Analysis benchmarking,2019-09,ALBERT,97.1,100.0,0.1,0.0085,97.1,0.97,Accuracy
11,1,Semantic Textual Similarity,MRPC - Semantic Textual Similarity benchmarking,2013-10,TF-KLD,80.4,86.08,80.4,100,93.4,0.81,Accuracy
12,1,Semantic Textual Similarity,MRPC - Semantic Textual Similarity benchmarking,2019-05,ERNIE,88.2,94.43,7.8,0.6,93.4,0.89,Accuracy
13,1,Semantic Textual Similarity,MRPC - Semantic Textual Similarity benchmarking,2019-06,XLNet (single model),90.8,97.22,2.6,0.2,93.4,0.91,Accuracy
14,1,Semantic Textual Similarity,MRPC - Semantic Textual Similarity benchmarking,2019-07,SpanBERT,90.9,97.32,0.1,0.0077,93.4,0.91,Accuracy
15,1,Semantic Textual Similarity,MRPC - Semantic Textual Similarity benchmarking,2019-07,RoBERTa,92.3,98.82,1.4,0.1077,93.4,0.93,Accuracy
16,1,Semantic Textual Similarity,MRPC - Semantic Textual Similarity benchmarking,2019-09,ALBERT,93.4,100.0,1.1,0.0846,93.4,0.94,Accuracy
17,1,Sentiment Analysis,SST-5 Fine-grained classification - Sentiment Analysis benchmarking,2013-10,MV-RNN,44.4,79.0,44.4,100,56.2,0.45,Accuracy
18,1,Sentiment Analysis,SST-5 Fine-grained classification - Sentiment Analysis benchmarking,2013-10,RNTN,45.7,81.32,1.3,0.1102,56.2,0.46,Accuracy
19,1,Sentiment Analysis,SST-5 Fine-grained classification - Sentiment Analysis benchmarking,2014-06,Epic,49.6,88.26,3.9,0.3305,56.2,0.5,Accuracy
20,1,Sentiment Analysis,SST-5 Fine-grained classification - Sentiment Analysis benchmarking,2015-02,Constituency Tree-LSTM,51.0,90.75,1.4,0.1186,56.2,0.51,Accuracy
21,1,Sentiment Analysis,SST-5 Fine-grained classification - Sentiment Analysis benchmarking,2017-08,BCN+Char+CoVe,53.7,95.55,2.7,0.2288,56.2,0.54,Accuracy
22,1,Sentiment Analysis,SST-5 Fine-grained classification - Sentiment Analysis benchmarking,2018-02,BCN+ELMo,54.7,97.33,1.0,0.0847,56.2,0.55,Accuracy
23,1,Sentiment Analysis,SST-5 Fine-grained classification - Sentiment Analysis benchmarking,2018-05,BCN+Suffix BiLSTM-Tied+CoVe,56.2,100.0,1.5,0.1271,56.2,0.56,Accuracy
24,1,Paraphrase Identification,MSRP - Paraphrase Identification benchmarking,2013-10,NMF factorization-unigrams-TFKLD,72.75,90.47,72.75,100,80.41,0.73,Accuracy
25,1,Paraphrase Identification,MSRP - Paraphrase Identification benchmarking,2013-10,"FEAT2, TFKLD, SVM, Fine-grained features",80.41,100.0,7.66,1.0,80.41,0.81,Accuracy
26,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 English-to-German - Cross-Lingual Document Classification benchmarking,2013-12,biCVM+,86.2,92.99,86.2,100,92.7,0.87,Accuracy
27,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 English-to-German - Cross-Lingual Document Classification benchmarking,2014-04,Bi+,88.1,95.04,1.9,0.2923,92.7,0.88,Accuracy
28,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 English-to-German - Cross-Lingual Document Classification benchmarking,2014-12,Biinclusion (Euro500kReuters),92.7,100.0,4.6,0.7077,92.7,0.93,Accuracy
29,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 German-to-English - Cross-Lingual Document Classification benchmarking,2013-12,biCVM+,76.9,91.11,76.9,100,84.4,0.77,Accuracy
30,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 German-to-English - Cross-Lingual Document Classification benchmarking,2014-04,Bi+,79.2,93.84,2.3,0.3067,84.4,0.8,Accuracy
31,1,Cross-Lingual Document Classification,Reuters RCV1/RCV2 German-to-English - Cross-Lingual Document Classification benchmarking,2014-12,Biinclusion (Euro500kReuters),84.4,100.0,5.2,0.6933,84.4,0.85,Accuracy
32,1,Document Classification,Cora - Document Classification benchmarking,2014-03,DeepWalk,67.2,80.48,67.2,100,83.5,0.67,Accuracy
33,1,Document Classification,Cora - Document Classification benchmarking,2016-03,Planetoid*,75.7,90.66,8.5,0.5215,83.5,0.76,Accuracy
34,1,Document Classification,Cora - Document Classification benchmarking,2016-09,Graph-CNN,81.5,97.6,5.8,0.3558,83.5,0.82,Accuracy
35,1,Document Classification,Cora - Document Classification benchmarking,2016-11,MoNet,81.7,97.84,0.2,0.0123,83.5,0.82,Accuracy
36,1,Document Classification,Cora - Document Classification benchmarking,2017-10,GAT,83.0,99.4,1.3,0.0798,83.5,0.83,Accuracy
37,1,Document Classification,Cora - Document Classification benchmarking,2018-08,LGCN,83.3,99.76,0.3,0.0184,83.5,0.84,Accuracy
38,1,Document Classification,Cora - Document Classification benchmarking,2019-04,ACNet,83.5,100.0,0.2,0.0123,83.5,0.84,Accuracy
39,1,Question Answering,Reverb - Question Answering benchmarking,2014-04,Weakly Supervised Embeddings,73.0,100.0,73,100,73,0.73,Accuracy
40,1,Document Classification,Reuters De-En - Document Classification benchmarking,2014-10,BilBOWA,75.0,100.0,75,100,75,0.75,Accuracy
41,1,Document Classification,Reuters En-De - Document Classification benchmarking,2014-10,BilBOWA,86.5,100.0,86.5,100,86.5,0.87,Accuracy
42,1,Semantic Parsing,ATIS - Semantic Parsing benchmarking,2014-11,"ZH15 (Zhao and Huang, 2015)",84.2,97.68,84.2,100,86.2,0.85,Accuracy
43,1,Semantic Parsing,ATIS - Semantic Parsing benchmarking,2017-04,"ASN (Rabinovich et al., 2017)",85.3,98.96,1.1,0.55,86.2,0.86,Accuracy
44,1,Semantic Parsing,ATIS - Semantic Parsing benchmarking,2018-10,Tranx,86.2,100.0,0.9,0.45,86.2,0.87,Accuracy
45,1,Sentiment Analysis,IMDb - Sentiment Analysis benchmarking,2014-12,seq2-bown-CNN,92.33,94.79,92.33,100,97.4,0.93,Accuracy
46,1,Sentiment Analysis,IMDb - Sentiment Analysis benchmarking,2016-02,oh-LSTM,94.1,96.61,1.77,0.3491,97.4,0.94,Accuracy
47,1,Sentiment Analysis,IMDb - Sentiment Analysis benchmarking,2017-12,Block-sparse LSTM,94.99,97.53,0.89,0.1755,97.4,0.95,Accuracy
48,1,Sentiment Analysis,IMDb - Sentiment Analysis benchmarking,2018-01,ULMFiT,95.4,97.95,0.41,0.0809,97.4,0.96,Accuracy
49,1,Sentiment Analysis,IMDb - Sentiment Analysis benchmarking,2019-05,BERT_large+ITPT,95.79,98.35,0.39,0.0769,97.4,0.96,Accuracy
50,1,Sentiment Analysis,IMDb - Sentiment Analysis benchmarking,2019-06,XLNet,96.21,98.78,0.42,0.0828,97.4,0.97,Accuracy
51,1,Sentiment Analysis,IMDb - Sentiment Analysis benchmarking,2019-07,NB-weighted-BON + dv-cosine,97.4,100.0,1.19,0.2347,97.4,0.98,Accuracy
52,1,Subjectivity Analysis,SUBJ - Subjectivity Analysis benchmarking,2015-04,AdaSent,95.5,100.0,95.5,100,95.5,0.96,Accuracy
53,1,Part-Of-Speech Tagging,Penn Treebank - Part-Of-Speech Tagging benchmarking,2015-08,Bi-LSTM,97.36,99.39,97.36,100,97.96,0.98,Accuracy
54,1,Part-Of-Speech Tagging,Penn Treebank - Part-Of-Speech Tagging benchmarking,2015-08,Char Bi-LSTM,97.78,99.82,0.42,0.7,97.96,0.98,Accuracy
55,1,Part-Of-Speech Tagging,Penn Treebank - Part-Of-Speech Tagging benchmarking,2018-05,Meta BiLSTM,97.96,100.0,0.18,0.3,97.96,0.98,Accuracy
56,1,Machine Translation,20NEWS - Machine Translation benchmarking,2015-08,12,1.0,100.0,1,100,1,0.01,Accuracy
57,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2015-11,SAN (VGG),58.9,91.18,58.9,100,64.6,0.59,Accuracy
58,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2016-03,DMN+,60.4,93.5,1.5,0.2632,64.6,0.61,Accuracy
59,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2016-05,HieCoAtt (ResNet),62.1,96.13,1.7,0.2982,64.6,0.62,Accuracy
60,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2016-06,RAU (ResNet),63.2,97.83,1.1,0.193,64.6,0.63,Accuracy
61,1,Visual Question Answering,VQA v1 test-std - Visual Question Answering benchmarking,2017-04,SAAA (ResNet),64.6,100.0,1.4,0.2456,64.6,0.65,Accuracy
62,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2015-11,NMN+LSTM+FT,58.6,90.85,58.6,100,64.5,0.59,Accuracy
63,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-03,DMN+,60.3,93.49,1.7,0.2881,64.5,0.61,Accuracy
64,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-05,HieCoAtt (ResNet),61.8,95.81,1.5,0.2542,64.5,0.62,Accuracy
65,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-06,MCB (ResNet),64.2,99.53,2.4,0.4068,64.5,0.64,Accuracy
66,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2016-11,DAN (ResNet),64.3,99.69,0.1,0.0169,64.5,0.65,Accuracy
67,1,Visual Question Answering,VQA v1 test-dev - Visual Question Answering benchmarking,2017-04,SAAA (ResNet),64.5,100.0,0.2,0.0339,64.5,0.65,Accuracy
68,1,Text Classification,RCV1 - Text Classification benchmarking,2016-02,oh-CNN + two LSTM tv-embed.,92.85,100.0,92.85,100,92.85,0.93,Accuracy
69,1,Dialog Act Classification,Switchboard corpus - Dialog Act Classification benchmarking,2016-03,CNN[[Lee and Dernoncourt2016]],73.1,89.91,73.1,100,81.3,0.73,Accuracy
70,1,Dialog Act Classification,Switchboard corpus - Dialog Act Classification benchmarking,2017-09,Bi-LSTM-CRF,79.2,97.42,6.1,0.7439,81.3,0.8,Accuracy
71,1,Dialog Act Classification,Switchboard corpus - Dialog Act Classification benchmarking,2017-11,CRF-ASN,81.3,100.0,2.1,0.2561,81.3,0.82,Accuracy
72,1,Code Generation,Django - Code Generation benchmarking,2016-03,"lpn (Ling et al., 2016)",62.3,84.53,62.3,100,73.7,0.63,Accuracy
73,1,Code Generation,Django - Code Generation benchmarking,2018-10,Tranx,73.7,100.0,11.4,1.0,73.7,0.74,Accuracy
74,1,Question Answering,MCTest-500 - Question Answering benchmarking,2016-03,"syntax, frame, coreference, and word embedding features",69.94,98.51,69.94,100,71.0,0.7,Accuracy
75,1,Question Answering,MCTest-500 - Question Answering benchmarking,2016-03,Parallel-Hierarchical,71.0,100.0,1.06,1.0,71.0,0.71,Accuracy
76,1,Question Answering,MCTest-160 - Question Answering benchmarking,2016-03,"syntax, frame, coreference, and word embedding features",75.27,100.0,75.27,100,75.27,0.76,Accuracy
77,1,Question Answering,Story Cloze Test - Question Answering benchmarking,2016-06,Memory chains and semantic supervision,78.7,90.98,78.7,100,86.5,0.79,Accuracy
78,1,Question Answering,Story Cloze Test - Question Answering benchmarking,2018-06,Finetuned Transformer LM,86.5,100.0,7.8,1.0,86.5,0.87,Accuracy
79,1,Phrase Grounding,ReferIt - Phrase Grounding benchmarking,2016-06,MCB,28.91,100.0,28.91,100,28.91,0.29,Accuracy
80,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2016-06,MCB,48.69,100.0,48.69,100,48.69,0.49,Accuracy
81,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2016-06,MCB,64.7,90.12,64.7,100,71.79,0.65,Accuracy
82,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2017-04,"N2NMN (ResNet-152, policy search)",64.9,90.4,0.2,0.0282,71.79,0.65,Accuracy
83,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2017-05,MUTAN,67.42,93.91,2.52,0.3554,71.79,0.68,Accuracy
84,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2017-08,"Image features from bottom-up attention (adaptive K, ensemble)",69.87,97.33,2.45,0.3456,71.79,0.7,Accuracy
85,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2018-05,BAN+Glove+Counter,70.04,97.56,0.17,0.024,71.79,0.7,Accuracy
86,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-06,MCANed-6,70.63,98.38,0.59,0.0832,71.79,0.71,Accuracy
87,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-08,VisualBERT,70.8,98.62,0.17,0.024,71.79,0.71,Accuracy
88,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-08,VL-BERTBASE,71.16,99.12,0.36,0.0508,71.79,0.71,Accuracy
89,1,Visual Question Answering,VQA v2 test-dev - Visual Question Answering benchmarking,2019-08,VL-BERTLARGE,71.79,100.0,0.63,0.0889,71.79,0.72,Accuracy
90,1,Sentiment Analysis,Sogou News - Sentiment Analysis benchmarking,2016-07,"fastText, h=10, bigram",96.8,100.0,96.8,100,96.8,0.97,Accuracy
91,1,Sentiment Analysis,Amazon Review Full - Sentiment Analysis benchmarking,2016-07,FastText,60.2,92.35,60.2,100,65.19,0.6,Accuracy
92,1,Sentiment Analysis,Amazon Review Full - Sentiment Analysis benchmarking,2017-07,DPCNN,65.19,100.0,4.99,1.0,65.19,0.65,Accuracy
93,1,Text Classification,Yahoo! Answers - Text Classification benchmarking,2016-07,FastText,72.3,93.15,72.3,100,77.62,0.73,Accuracy
94,1,Text Classification,Yahoo! Answers - Text Classification benchmarking,2018-05,SWEM-concat,73.53,94.73,1.23,0.2312,77.62,0.74,Accuracy
95,1,Text Classification,Yahoo! Answers - Text Classification benchmarking,2018-07,DRNN,76.26,98.25,2.73,0.5132,77.62,0.77,Accuracy
96,1,Text Classification,Yahoo! Answers - Text Classification benchmarking,2019-05,BERT-ITPT-FiT,77.62,100.0,1.36,0.2556,77.62,0.78,Accuracy
97,1,Sentiment Analysis,Amazon Review Polarity - Sentiment Analysis benchmarking,2016-07,FastText,94.6,97.85,94.6,100,96.68,0.95,Accuracy
98,1,Sentiment Analysis,Amazon Review Polarity - Sentiment Analysis benchmarking,2017-07,DPCNN,96.68,100.0,2.08,1.0,96.68,0.97,Accuracy
99,1,Sentiment Analysis,MR - Sentiment Analysis benchmarking,2017-02,GRU-RNN-WORD2VEC,78.26,90.16,78.26,100,86.8,0.79,Accuracy
100,1,Sentiment Analysis,MR - Sentiment Analysis benchmarking,2018-02,RNN-Capsule,83.8,96.54,5.54,0.6487,86.8,0.84,Accuracy
101,1,Sentiment Analysis,MR - Sentiment Analysis benchmarking,2018-05,byte mLSTM7,86.8,100.0,3.0,0.3513,86.8,0.87,Accuracy
102,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2017-02,BiMPM,88.17,97.64,88.17,100,90.3,0.89,Accuracy
103,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2017-04,pt-DecAtt,88.4,97.9,0.23,0.108,90.3,0.89,Accuracy
104,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2017-09,DIIN,89.06,98.63,0.66,0.3099,90.3,0.89,Accuracy
105,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2018-07,MwAN ,89.12,98.69,0.06,0.0282,90.3,0.89,Accuracy
106,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2019-01,MT-DNN,89.6,99.22,0.48,0.2254,90.3,0.9,Accuracy
107,1,Paraphrase Identification,Quora Question Pairs - Paraphrase Identification benchmarking,2019-06,XLNet-Large (ensemble),90.3,100.0,0.7,0.3286,90.3,0.91,Accuracy
108,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2017-04,ST-VQA,0.313,86.7,0.313,100,0.361,0.0,Accuracy
109,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2018-03,Co-Mem,0.317,87.81,0.004,0.0833,0.361,0.0,Accuracy
110,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2019-04,HMEMA,0.337,93.35,0.02,0.4167,0.361,0.0,Accuracy
111,1,Visual Question Answering,MSVD-QA - Visual Question Answering benchmarking,2020-02,HCRN,0.361,100.0,0.024,0.5,0.361,0.0,Accuracy
112,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2017-04,ST-VQA,0.309,86.8,0.309,100,0.356,0.0,Accuracy
113,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2018-03,Co-Mem,0.32,89.89,0.011,0.234,0.356,0.0,Accuracy
114,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2019-04,HMEMA,0.33,92.7,0.01,0.2128,0.356,0.0,Accuracy
115,1,Visual Question Answering,MSRVTT-QA - Visual Question Answering benchmarking,2020-02,HCRN,0.356,100.0,0.026,0.5532,0.356,0.0,Accuracy
116,1,Stance Detection,RumourEval - Stance Detection benchmarking,2017-04,Kochkina et al. 2017,0.784,100.0,0.784,100,0.784,0.01,Accuracy
117,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-French - Cross-Lingual Natural Language Inference benchmarking,2017-05,X-CBOW,60.3,89.07,60.3,100,67.7,0.61,Accuracy
118,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-French - Cross-Lingual Natural Language Inference benchmarking,2017-05,X-BiLSTM,67.7,100.0,7.4,1.0,67.7,0.68,Accuracy
119,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-Spanish - Cross-Lingual Natural Language Inference benchmarking,2017-05,X-CBOW,60.7,81.7,60.7,100,74.3,0.61,Accuracy
120,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-Spanish - Cross-Lingual Natural Language Inference benchmarking,2017-05,X-BiLSTM,68.7,92.46,8.0,0.5882,74.3,0.69,Accuracy
121,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-Spanish - Cross-Lingual Natural Language Inference benchmarking,2018-10,BERT,74.3,100.0,5.6,0.4118,74.3,0.75,Accuracy
122,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-German - Cross-Lingual Natural Language Inference benchmarking,2017-05,X-CBOW,61.0,86.52,61.0,100,70.5,0.61,Accuracy
123,1,Cross-Lingual Natural Language Inference,XNLI Zero-Shot English-to-German - Cross-Lingual Natural Language Inference benchmarking,2018-10,BERT,70.5,100.0,9.5,1.0,70.5,0.71,Accuracy
124,1,Multimodal Sentiment Analysis,MOSI - Multimodal Sentiment Analysis benchmarking,2017-07,bc-LSTM,80.3,97.56,80.3,100,82.31,0.81,Accuracy
125,1,Multimodal Sentiment Analysis,MOSI - Multimodal Sentiment Analysis benchmarking,2018-10,MMMU-BA,82.31,100.0,2.01,1.0,82.31,0.83,Accuracy
126,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,56.32,86.31,56.32,100,65.25,0.57,Accuracy
127,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-06,CMN,56.56,86.68,0.24,0.0269,65.25,0.57,Accuracy
128,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-10,ICON,59.09,90.56,2.53,0.2833,65.25,0.59,Accuracy
129,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,63.4,97.16,4.31,0.4826,65.25,0.64,Accuracy
130,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,65.25,100.0,1.85,0.2072,65.25,0.66,Accuracy
131,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,57.5,96.57,57.5,100,59.54,0.58,Accuracy
132,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,59.54,100.0,2.04,1.0,59.54,0.6,Accuracy
133,1,Text Classification,Ohsumed - Text Classification benchmarking,2017-07,CNN+Lowercased,36.2,52.85,36.2,100,68.5,0.36,Accuracy
134,1,Text Classification,Ohsumed - Text Classification benchmarking,2018-09,Text GCN,68.36,99.8,32.16,0.9957,68.5,0.69,Accuracy
135,1,Text Classification,Ohsumed - Text Classification benchmarking,2019-02,SGC,68.5,100.0,0.14,0.0043,68.5,0.69,Accuracy
136,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,49.74,79.32,49.74,100,62.71,0.5,Accuracy
137,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",60.33,96.2,10.59,0.8165,62.71,0.61,Accuracy
138,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",62.71,100.0,2.38,0.1835,62.71,0.63,Accuracy
139,1,Document Classification,WOS-11967 - Document Classification benchmarking,2017-09,HDLTex,86.07,100.0,86.07,100,86.07,0.86,Accuracy
140,1,Document Classification,WOS-46985 - Document Classification benchmarking,2017-09,HDLTex,76.58,100.0,76.58,100,76.58,0.77,Accuracy
141,1,Document Classification,WOS-5736 - Document Classification benchmarking,2017-09,HDLTex,90.93,100.0,90.93,100,90.93,0.91,Accuracy
142,1,Lexical Normalization,LexNorm - Lexical Normalization benchmarking,2017-10,MoNoise,87.63,100.0,87.63,100,87.63,0.88,Accuracy
143,1,Sentiment Analysis,CR - Sentiment Analysis benchmarking,2017-12,Block-sparse LSTM,92.2,100.0,92.2,100,92.2,0.93,Accuracy
144,1,Natural Language Inference,SciTail - Natural Language Inference benchmarking,2017-12,CAFE,83.3,88.52,83.3,100,94.1,0.84,Accuracy
145,1,Natural Language Inference,SciTail - Natural Language Inference benchmarking,2018-06,Finetuned Transformer LM,88.3,93.84,5.0,0.463,94.1,0.89,Accuracy
146,1,Natural Language Inference,SciTail - Natural Language Inference benchmarking,2018-10,BERT,92.0,97.77,3.7,0.3426,94.1,0.92,Accuracy
147,1,Natural Language Inference,SciTail - Natural Language Inference benchmarking,2019-01,MT-DNN,94.1,100.0,2.1,0.1944,94.1,0.94,Accuracy
148,1,Sentiment Analysis,MPQA - Sentiment Analysis benchmarking,2018-03,USE_T+DAN (w2v w.e.) ,88.14,98.12,88.14,100,89.83,0.89,Accuracy
149,1,Sentiment Analysis,MPQA - Sentiment Analysis benchmarking,2018-05,byte mLSTM7,88.8,98.85,0.66,0.3905,89.83,0.89,Accuracy
150,1,Sentiment Analysis,MPQA - Sentiment Analysis benchmarking,2019-05,STM+TSED+PT+2L,89.83,100.0,1.03,0.6095,89.83,0.9,Accuracy
151,1,Text Classification,20NEWS - Text Classification benchmarking,2018-05,RMDL (15 RDLs),87.91,99.33,87.91,100,88.5,0.88,Accuracy
152,1,Text Classification,20NEWS - Text Classification benchmarking,2019-02,SGCN,88.5,100.0,0.59,1.0,88.5,0.89,Accuracy
153,1,Semantic Parsing,Geo - Semantic Parsing benchmarking,2018-05,coarse2fine,88.2,100.0,88.2,100,88.2,0.89,Accuracy
154,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian - Cross-Lingual Document Classification benchmarking,2018-05,MultiCCA + CNN,60.8,67.78,60.8,100,89.7,0.61,Accuracy
155,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian - Cross-Lingual Document Classification benchmarking,2018-05,BiLSTM (UN),61.42,68.47,0.62,0.0215,89.7,0.62,Accuracy
156,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian - Cross-Lingual Document Classification benchmarking,2018-12,Massively Multilingual Sentence Embeddings,67.78,75.56,6.36,0.2201,89.7,0.68,Accuracy
157,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Russian - Cross-Lingual Document Classification benchmarking,2019-09,XLMft UDA,89.7,100.0,21.92,0.7585,89.7,0.9,Accuracy
158,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Chinese - Cross-Lingual Document Classification benchmarking,2018-05,MultiCCA + CNN,74.73,80.08,74.73,100,93.32,0.75,Accuracy
159,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Chinese - Cross-Lingual Document Classification benchmarking,2019-09,XLMft UDA,93.32,100.0,18.59,1.0,93.32,0.94,Accuracy
160,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Italian - Cross-Lingual Document Classification benchmarking,2018-05,BiLSTM (Europarl),60.73,87.47,60.73,100,69.43,0.61,Accuracy
161,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Italian - Cross-Lingual Document Classification benchmarking,2018-05,MultiCCA + CNN,69.38,99.93,8.65,0.9943,69.43,0.7,Accuracy
162,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Italian - Cross-Lingual Document Classification benchmarking,2018-12,Massively Multilingual Sentence Embeddings,69.43,100.0,0.05,0.0057,69.43,0.7,Accuracy
163,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German - Cross-Lingual Document Classification benchmarking,2018-05,MultiCCA + CNN,81.2,83.75,81.2,100,96.95,0.82,Accuracy
164,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German - Cross-Lingual Document Classification benchmarking,2018-12,Massively Multilingual Sentence Embeddings,84.78,87.45,3.58,0.2273,96.95,0.85,Accuracy
165,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-German - Cross-Lingual Document Classification benchmarking,2019-09,XLMft UDA,96.95,100.0,12.17,0.7727,96.95,0.97,Accuracy
166,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French - Cross-Lingual Document Classification benchmarking,2018-05,BiLSTM (Europarl),72.83,75.83,72.83,100,96.05,0.73,Accuracy
167,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French - Cross-Lingual Document Classification benchmarking,2018-05,BiLSTM (UN),74.52,77.58,1.69,0.0728,96.05,0.75,Accuracy
168,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French - Cross-Lingual Document Classification benchmarking,2018-12,Massively Multilingual Sentence Embeddings,77.95,81.16,3.43,0.1477,96.05,0.78,Accuracy
169,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-French - Cross-Lingual Document Classification benchmarking,2019-09,XLMft UDA,96.05,100.0,18.1,0.7795,96.05,0.96,Accuracy
170,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish - Cross-Lingual Document Classification benchmarking,2018-05,BiLSTM (UN),69.5,71.8,69.5,100,96.8,0.7,Accuracy
171,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish - Cross-Lingual Document Classification benchmarking,2018-05,MultiCCA + CNN,72.5,74.9,3.0,0.1099,96.8,0.73,Accuracy
172,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish - Cross-Lingual Document Classification benchmarking,2018-12,Massively Multilingual Sentence Embeddings,77.33,79.89,4.83,0.1769,96.8,0.78,Accuracy
173,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Spanish - Cross-Lingual Document Classification benchmarking,2019-09,XLMft UDA,96.8,100.0,19.47,0.7132,96.8,0.97,Accuracy
174,1,Cross-Lingual Document Classification,MLDoc Zero-Shot German-to-French - Cross-Lingual Document Classification benchmarking,2018-05,BiLSTM (Europarl),75.45,100.0,75.45,100,75.45,0.76,Accuracy
175,1,Cross-Lingual Document Classification,MLDoc Zero-Shot English-to-Japanese - Cross-Lingual Document Classification benchmarking,2018-05,MultiCCA + CNN,67.63,100.0,67.63,100,67.63,0.68,Accuracy
176,1,Question Answering,Quora Question Pairs - Question Answering benchmarking,2018-05,SWEM-concat,83.03,89.96,83.03,100,92.3,0.83,Accuracy
177,1,Question Answering,Quora Question Pairs - Question Answering benchmarking,2019-06,XLNet (single model),92.3,100.0,9.27,1.0,92.3,0.93,Accuracy
178,1,Memex Question Answering,MemexQA - Memex Question Answering benchmarking,2018-06,FVTA,0.357,100.0,0.357,100,0.357,0.0,Accuracy
179,1,Entity Typing,Freebase FIGER - Entity Typing benchmarking,2018-06,TextEnt-full,37.4,100.0,37.4,100,37.4,0.38,Accuracy
180,1,Text Classification,R8 - Text Classification benchmarking,2018-06,TextEnt-full,96.7,98.77,96.7,100,97.9,0.97,Accuracy
181,1,Text Classification,R8 - Text Classification benchmarking,2018-09,Text GCN,97.07,99.15,0.37,0.3083,97.9,0.97,Accuracy
182,1,Text Classification,R8 - Text Classification benchmarking,2019-02,SGCN,97.2,99.28,0.13,0.1083,97.9,0.98,Accuracy
183,1,Text Classification,R8 - Text Classification benchmarking,2019-06,GraphStar,97.4,99.49,0.2,0.1667,97.9,0.98,Accuracy
184,1,Text Classification,R8 - Text Classification benchmarking,2019-09,NABoE-full,97.9,100.0,0.5,0.4167,97.9,0.98,Accuracy
185,1,Natural Language Inference,V-SNLI - Natural Language Inference benchmarking,2018-06,BiMPM,86.41,99.33,86.41,100,86.99,0.87,Accuracy
186,1,Natural Language Inference,V-SNLI - Natural Language Inference benchmarking,2018-06,V-BiMPM,86.99,100.0,0.58,1.0,86.99,0.87,Accuracy
187,1,Multimodal Sentiment Analysis,CMU-MOSEI - Multimodal Sentiment Analysis benchmarking,2018-07,Graph-MFN,76.9,100.0,76.9,100,76.9,0.77,Accuracy
188,1,Language Modelling,LAMBADA - Language Modelling benchmarking,2018-07,Universal Transformer (w/ dynamic halting),56.25,100.0,56.25,100,56.25,0.56,Accuracy
189,1,Visual Question Answering,CLEVR - Visual Question Answering benchmarking,2018-08,CNN + LSTM + RN + HAN,98.8,100.0,98.8,100,98.8,0.99,Accuracy
190,1,Query Wellformedness,Query Wellformedness - Query Wellformedness benchmarking,2018-08,"word-1, 2 POS-1, 2, 3",70.7,100.0,70.7,100,70.7,0.71,Accuracy
191,1,Natural Language Inference,XNLI French - Natural Language Inference benchmarking,2018-09,BiLSTM-max,68.3,85.16,68.3,100,80.2,0.69,Accuracy
192,1,Natural Language Inference,XNLI French - Natural Language Inference benchmarking,2019-01,XLM (MLM+TLM),80.2,100.0,11.9,1.0,80.2,0.81,Accuracy
193,1,Text Classification,R52 - Text Classification benchmarking,2018-09,Text GCN,93.56,98.48,93.56,100,95.0,0.94,Accuracy
194,1,Text Classification,R52 - Text Classification benchmarking,2019-02,SGC,94.0,98.95,0.44,0.3056,95.0,0.94,Accuracy
195,1,Text Classification,R52 - Text Classification benchmarking,2019-06,GraphStar,95.0,100.0,1.0,0.6944,95.0,0.95,Accuracy
196,1,Semantic Parsing,spider - Semantic Parsing benchmarking,2018-09,Exact Set Matching,19.7,100.0,19.7,100,19.7,0.2,Accuracy
197,1,Natural Language Understanding,PDP60 - Natural Language Understanding benchmarking,2018-10,BERTLARGE,78.3,100.0,78.3,100,78.3,0.79,Accuracy
198,1,Natural Language Understanding,WNLI - Natural Language Understanding benchmarking,2018-10,BERTLARGE,65.1,100.0,65.1,100,65.1,0.65,Accuracy
199,1,Text Classification,Sogou News - Text Classification benchmarking,2018-10,CCCapsNet,97.25,99.16,97.25,100,98.07,0.98,Accuracy
200,1,Text Classification,Sogou News - Text Classification benchmarking,2019-05,BERT-ITPT-FiT,98.07,100.0,0.82,1.0,98.07,0.98,Accuracy
201,1,Visual Question Answering,HowmanyQA - Visual Question Answering benchmarking,2018-10,RCN (Ours),60.3,100.0,60.3,100,60.3,0.61,Accuracy
202,1,Visual Question Answering,TallyQA - Visual Question Answering benchmarking,2018-10,RCN (Ours),71.8,100.0,71.8,100,71.8,0.72,Accuracy
203,1,Common Sense Reasoning,CommonsenseQA - Common Sense Reasoning benchmarking,2018-11,BERT-LARGE,55.9,73.07,55.9,100,76.5,0.56,Accuracy
204,1,Common Sense Reasoning,CommonsenseQA - Common Sense Reasoning benchmarking,2019-06,CAGE-reasoning,64.7,84.58,8.8,0.4272,76.5,0.65,Accuracy
205,1,Common Sense Reasoning,CommonsenseQA - Common Sense Reasoning benchmarking,2019-07,RoBERTa Liu et al. (2019),72.1,94.25,7.4,0.3592,76.5,0.72,Accuracy
206,1,Common Sense Reasoning,CommonsenseQA - Common Sense Reasoning benchmarking,2019-09,Albert Lan et al. (2020) (ensemble),76.5,100.0,4.4,0.2136,76.5,0.77,Accuracy
207,1,Natural Language Inference,Quora Question Pairs - Natural Language Inference benchmarking,2018-12,aESIM,88.01,100.0,88.01,100,88.01,0.88,Accuracy
208,1,Hate Speech Detection,Automatic Misogynistic Identification - Hate Speech Detection benchmarking,2018-12,Logistic Regression,0.704,100.0,0.704,100,0.704,0.01,Accuracy
209,1,Intent Detection,ATIS - Intent Detection benchmarking,2018-12,Capsule-NLU,95.0,97.18,95.0,100,97.76,0.95,Accuracy
210,1,Intent Detection,ATIS - Intent Detection benchmarking,2019-06,SF-ID (BLSTM) network,97.76,100.0,2.76,1.0,97.76,0.98,Accuracy
211,1,Text Classification,Yelp-5 - Text Classification benchmarking,2019-01,HAHNN (CNN),73.28,100.0,73.28,100,73.28,0.74,Accuracy
212,1,Linguistic Acceptability Assessment,CoLA - Linguistic Acceptability Assessment benchmarking,2019-01,MT-DNN,68.4,98.99,68.4,100,69.1,0.69,Accuracy
213,1,Linguistic Acceptability Assessment,CoLA - Linguistic Acceptability Assessment benchmarking,2019-06,XLNet (single model),69.0,99.86,0.6,0.8571,69.1,0.69,Accuracy
214,1,Linguistic Acceptability Assessment,CoLA - Linguistic Acceptability Assessment benchmarking,2019-09,ALBERT,69.1,100.0,0.1,0.1429,69.1,0.69,Accuracy
215,1,Sentiment Analysis,Twitter - Sentiment Analysis benchmarking,2019-02,AEN-BERT,74.71,100.0,74.71,100,74.71,0.75,Accuracy
216,1,Visual Question Answering,GQA test-std - Visual Question Answering benchmarking,2019-02,CNN+LSTM,46.55,73.69,46.55,100,63.17,0.47,Accuracy
217,1,Visual Question Answering,GQA test-std - Visual Question Answering benchmarking,2019-02,MAC,54.06,85.58,7.51,0.4519,63.17,0.54,Accuracy
218,1,Visual Question Answering,GQA test-std - Visual Question Answering benchmarking,2019-07,NSM,63.17,100.0,9.11,0.5481,63.17,0.63,Accuracy
219,1,Visual Question Answering,TDIUC - Visual Question Answering benchmarking,2019-02,Accuracy,88.2,100.0,88.2,100,88.2,0.89,Accuracy
220,1,Document Classification,Recipe - Document Classification benchmarking,2019-04,ApproxRepSet,59.06,100.0,59.06,100,59.06,0.59,Accuracy
221,1,Document Classification,Amazon - Document Classification benchmarking,2019-04,ApproxRepSet,94.31,100.0,94.31,100,94.31,0.95,Accuracy
222,1,Document Classification,Classic - Document Classification benchmarking,2019-04,ApproxRepSet,96.24,100.0,96.24,100,96.24,0.97,Accuracy
223,1,Document Classification,BBCSport - Document Classification benchmarking,2019-04,ApproxRepSet,95.73,96.12,95.73,100,99.59,0.96,Accuracy
224,1,Document Classification,BBCSport - Document Classification benchmarking,2019-08,MPAD-path,99.59,100.0,3.86,1.0,99.59,1.0,Accuracy
225,1,Document Classification,Twitter - Document Classification benchmarking,2019-04,ApproxRepSet,72.6,100.0,72.6,100,72.6,0.73,Accuracy
226,1,Document Classification,Reuters-21578 - Document Classification benchmarking,2019-04,ApproxRepSet,97.17,99.72,97.17,100,97.44,0.98,Accuracy
227,1,Document Classification,Reuters-21578 - Document Classification benchmarking,2019-08,MPAD-path,97.44,100.0,0.27,1.0,97.44,0.98,Accuracy
228,1,Question Answering,CODAH - Question Answering benchmarking,2019-04,BERT Large,69.6,100.0,69.6,100,69.6,0.7,Accuracy
229,1,Document Classification,Yelp-14 - Document Classification benchmarking,2019-04,KD-LSTMreg,69.4,100.0,69.4,100,69.4,0.7,Accuracy
230,1,Natural Language Inference,XNLI Chinese Dev - Natural Language Inference benchmarking,2019-04,ERNIE,79.9,96.73,79.9,100,82.6,0.8,Accuracy
231,1,Natural Language Inference,XNLI Chinese Dev - Natural Language Inference benchmarking,2019-07,ERNIE 2.0 Base,81.2,98.31,1.3,0.4815,82.6,0.82,Accuracy
232,1,Natural Language Inference,XNLI Chinese Dev - Natural Language Inference benchmarking,2019-07,ERNIE 2.0 Large,82.6,100.0,1.4,0.5185,82.6,0.83,Accuracy
233,1,Natural Language Inference,XNLI Chinese - Natural Language Inference benchmarking,2019-04,ERNIE,78.4,96.79,78.4,100,81.0,0.79,Accuracy
234,1,Natural Language Inference,XNLI Chinese - Natural Language Inference benchmarking,2019-07,ERNIE 2.0 Base,79.7,98.4,1.3,0.5,81.0,0.8,Accuracy
235,1,Natural Language Inference,XNLI Chinese - Natural Language Inference benchmarking,2019-07,ERNIE 2.0 Large,81.0,100.0,1.3,0.5,81.0,0.81,Accuracy
236,1,Text Classification,Yelp-2 - Text Classification benchmarking,2019-05,BERT-ITPT-FiT,98.08,99.44,98.08,100,98.63,0.98,Accuracy
237,1,Text Classification,Yelp-2 - Text Classification benchmarking,2019-06,XLNet,98.63,100.0,0.55,1.0,98.63,0.99,Accuracy
238,1,Natural Language Inference,QNLI - Natural Language Inference benchmarking,2019-05,ERNIE,91.3,92.04,91.3,100,99.2,0.92,Accuracy
239,1,Natural Language Inference,QNLI - Natural Language Inference benchmarking,2019-06,XLNet (single model),94.9,95.67,3.6,0.4557,99.2,0.95,Accuracy
240,1,Natural Language Inference,QNLI - Natural Language Inference benchmarking,2019-07,RoBERTa,98.9,99.7,4.0,0.5063,99.2,0.99,Accuracy
241,1,Natural Language Inference,QNLI - Natural Language Inference benchmarking,2019-09,ALBERT,99.2,100.0,0.3,0.038,99.2,1.0,Accuracy
242,1,Entity Linking,FIGER - Entity Linking benchmarking,2019-05,ERNIE,57.19,100.0,57.19,100,57.19,0.57,Accuracy
243,1,Natural Language Inference,RTE - Natural Language Inference benchmarking,2019-05,ERNIE,68.8,77.13,68.8,100,89.2,0.69,Accuracy
244,1,Natural Language Inference,RTE - Natural Language Inference benchmarking,2019-06,XLNet (single model),85.9,96.3,17.1,0.8382,89.2,0.86,Accuracy
245,1,Natural Language Inference,RTE - Natural Language Inference benchmarking,2019-07,RoBERTa,88.2,98.88,2.3,0.1127,89.2,0.89,Accuracy
246,1,Natural Language Inference,RTE - Natural Language Inference benchmarking,2019-09,ALBERT,89.2,100.0,1.0,0.049,89.2,0.9,Accuracy
247,1,Document Classification,IMDb-M - Document Classification benchmarking,2019-06,LSTM-reg (single model),52.8,100.0,52.8,100,52.8,0.53,Accuracy
248,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-06,XLNet,85.4,93.95,85.4,100,90.9,0.86,Accuracy
249,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-09,Megatron-BERT,89.5,98.46,4.1,0.7455,90.9,0.9,Accuracy
250,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-09,Megatron-BERT (ensemble),90.9,100.0,1.4,0.2545,90.9,0.91,Accuracy
251,1,Natural Language Inference,WNLI - Natural Language Inference benchmarking,2019-06,XLNet,92.5,100.0,92.5,100,92.5,0.93,Accuracy
252,1,Visual Question Answering,GQA test-dev - Visual Question Answering benchmarking,2019-07,NSM,62.95,100.0,62.95,100,62.95,0.63,Accuracy
253,1,Prosody Prediction,Helsinki Prosody Corpus - Prosody Prediction benchmarking,2019-08,CRF (MarMoT),81.8,98.32,81.8,100,83.2,0.82,Accuracy
254,1,Prosody Prediction,Helsinki Prosody Corpus - Prosody Prediction benchmarking,2019-08,BERT,83.2,100.0,1.4,1.0,83.2,0.84,Accuracy
255,1,Visual Reasoning,NLVR2 Dev - Visual Reasoning benchmarking,2019-08,VisualBERT,66.7,89.05,66.7,100,74.9,0.67,Accuracy
256,1,Visual Reasoning,NLVR2 Dev - Visual Reasoning benchmarking,2019-08,LXMERT (Pre-train + scratch),74.9,100.0,8.2,1.0,74.9,0.75,Accuracy
257,1,Document Classification,MPQA - Document Classification benchmarking,2019-08,MPAD-path,89.81,100.0,89.81,100,89.81,0.9,Accuracy
258,1,Visual Reasoning,NLVR2 Test - Visual Reasoning benchmarking,2019-08,LXMERT,76.2,100.0,76.2,100,76.2,0.77,Accuracy
259,1,Sentiment Analysis,Financial PhraseBank - Sentiment Analysis benchmarking,2019-08,FinBERT,86.0,100.0,86,100,86,0.86,Accuracy
260,1,Semantic Parsing,WikiSQL - Semantic Parsing benchmarking,2019-10,NL2SQL-BERT,89.0,100.0,89,100,89,0.89,Accuracy
261,1,Entity Linking,CoNLL-Aida - Entity Linking benchmarking,2020-01,RELIC + CoNLL-Aida tuning,94.9,100.0,94.9,100,94.9,0.95,Accuracy
262,1,Entity Linking,TAC-KBP 2010 - Entity Linking benchmarking,2020-01,RELIC + CoNLL-Aida tuning,89.8,100.0,89.8,100,89.8,0.9,Accuracy
263,1,Handwritten Digit Recognition,MNIST - Handwritten Digit Recognition benchmarking,2020-04,CNN,96.95,100.0,96.95,100,96.95,0.97,Accuracy
0,1,Language Modelling,One Billion Word - Language Modelling benchmarking,2013-12,RNN-1024 + 9 Gram,51.3,96.98,51.3,100,52.9,0.92,PPL
1,1,Language Modelling,One Billion Word - Language Modelling benchmarking,2014-12,Sparse Non-Negative,52.9,100.0,1.6,1.0,52.9,0.94,PPL
2,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2017-09,C2F + ALTERNATE,23.6,72.06,23.6,100,32.75,0.42,PPL
3,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2018-08,Bottom-Up Sum,32.75,100.0,9.15,1.0,32.75,0.58,PPL
4,1,Language Modelling,PTB - Language Modelling benchmarking,2019-11,I-DARTS,56.0,100.0,56,100,56,1.0,PPL
0,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2014-04,DCNN [[Blunsom et al.2014]],86.8,94.45,86.8,100,91.9,0.94,%\\ Test\\ Accuracy
1,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2014-08,CNN-MC [[Kim2014]],88.1,95.87,1.3,0.2549,91.9,0.96,%\\ Test\\ Accuracy
2,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2016-09,600D ESIM + 300D Syntactic TreeLSTM,88.6,96.41,0.5,0.098,91.9,0.96,%\\ Test\\ Accuracy
3,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2017-02,BiMPM Ensemble,88.8,96.63,0.2,0.0392,91.9,0.97,%\\ Test\\ Accuracy
4,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2017-09,"448D Densely Interactive Inference Network (DIIN, code) Ensemble",88.9,96.74,0.1,0.0196,91.9,0.97,%\\ Test\\ Accuracy
5,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2017-11,KIM Ensemble,89.1,96.95,0.2,0.0392,91.9,0.97,%\\ Test\\ Accuracy
6,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2017-12,300D CAFE Ensemble,89.3,97.17,0.2,0.0392,91.9,0.97,%\\ Test\\ Accuracy
7,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble,90.1,98.04,0.8,0.1569,91.9,0.98,%\\ Test\\ Accuracy
8,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2018-09,SJRC (BERT-Large +SRL),91.3,99.35,1.2,0.2353,91.9,0.99,%\\ Test\\ Accuracy
9,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2019-01,MT-DNN,91.6,99.67,0.3,0.0588,91.9,1.0,%\\ Test\\ Accuracy
10,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2019-09,SemBERT,91.9,100.0,0.3,0.0588,91.9,1.0,%\\ Test\\ Accuracy
0,1,Question Answering,QASent - Question Answering benchmarking,2014-05,Paragraph vector (lexical overlap + dist output),0.7514,92.57,0.7514,100,0.8117,0.01,MRR
1,1,Question Answering,QASent - Question Answering benchmarking,2014-12,Bigram-CNN (lexical overlap + dist output),0.7846,96.66,0.0332,0.5506,0.8117,0.01,MRR
2,1,Question Answering,QASent - Question Answering benchmarking,2015-11,Attentive LSTM,0.8117,100.0,0.0271,0.4494,0.8117,0.01,MRR
3,1,Question Answering,WikiQA - Question Answering benchmarking,2014-05,Paragraph vector (lexical overlap + dist output),0.6058,77.27,0.6058,100,0.784,0.01,MRR
4,1,Question Answering,WikiQA - Question Answering benchmarking,2014-12,Bigram-CNN,0.6281,80.11,0.0223,0.1251,0.784,0.01,MRR
5,1,Question Answering,WikiQA - Question Answering benchmarking,2014-12,Bigram-CNN (lexical overlap + dist output),0.6652,84.85,0.0371,0.2082,0.784,0.01,MRR
6,1,Question Answering,WikiQA - Question Answering benchmarking,2015-11,LSTM (lexical overlap + dist output),0.6988,89.13,0.0336,0.1886,0.784,0.01,MRR
7,1,Question Answering,WikiQA - Question Answering benchmarking,2015-11,Attentive LSTM,0.7069,90.17,0.0081,0.0455,0.784,0.01,MRR
8,1,Question Answering,WikiQA - Question Answering benchmarking,2016-02,LDC,0.7226,92.17,0.0157,0.0881,0.784,0.01,MRR
9,1,Question Answering,WikiQA - Question Answering benchmarking,2016-06,PWIM,0.7234,92.27,0.0008,0.0045,0.784,0.01,MRR
10,1,Question Answering,WikiQA - Question Answering benchmarking,2016-06,Key-Value Memory Network,0.7265,92.67,0.0031,0.0174,0.784,0.01,MRR
11,1,Question Answering,WikiQA - Question Answering benchmarking,2017-07,HyperQA,0.727,92.73,0.0005,0.0028,0.784,0.01,MRR
12,1,Question Answering,WikiQA - Question Answering benchmarking,2019-05,Comp-Clip + LM + LC,0.784,100.0,0.057,0.3199,0.784,0.01,MRR
13,1,Question Answering,TrecQA - Question Answering benchmarking,2014-12,CNN,0.785,84.59,0.785,100,0.928,0.01,MRR
14,1,Question Answering,TrecQA - Question Answering benchmarking,2016-06,PWIN,0.8219,88.57,0.0369,0.258,0.928,0.01,MRR
15,1,Question Answering,TrecQA - Question Answering benchmarking,2017-07,HyperQA,0.825,88.9,0.0031,0.0217,0.928,0.01,MRR
16,1,Question Answering,TrecQA - Question Answering benchmarking,2019-05,Comp-Clip + LM + LC,0.928,100.0,0.103,0.7203,0.928,0.01,MRR
17,1,Question Answering,YahooCQA - Question Answering benchmarking,2016-02,AP-CNN,0.726,90.64,0.726,100,0.801,0.01,MRR
18,1,Question Answering,YahooCQA - Question Answering benchmarking,2016-02,AP-BiLSTM,0.731,91.26,0.005,0.0667,0.801,0.01,MRR
19,1,Question Answering,YahooCQA - Question Answering benchmarking,2017-07,HyperQA,0.801,100.0,0.07,0.9333,0.801,0.01,MRR
20,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,57.88,83.98,57.88,100,68.92,0.59,MRR
21,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,63.98,92.83,6.1,0.5525,68.92,0.65,MRR
22,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),64.1,93.01,0.12,0.0109,68.92,0.65,MRR
23,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,DAN,66.38,96.31,2.28,0.2065,68.92,0.67,MRR
24,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),68.92,100.0,2.54,0.2301,68.92,0.7,MRR
25,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2016-11,vTE,41.07,75.16,41.07,100,54.64,0.42,MRR
26,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2018-06,CRIM,54.64,100.0,13.57,1.0,54.64,0.55,MRR
27,1,Hypernym Discovery,General - Hypernym Discovery benchmarking,2016-11,vTE,23.83,66.01,23.83,100,36.1,0.24,MRR
28,1,Hypernym Discovery,General - Hypernym Discovery benchmarking,2018-06,CRIM,36.1,100.0,12.27,1.0,36.1,0.37,MRR
29,1,Hypernym Discovery,Music domain - Hypernym Discovery benchmarking,2016-11,vTE,39.36,64.6,39.36,100,60.93,0.4,MRR
30,1,Hypernym Discovery,Music domain - Hypernym Discovery benchmarking,2018-06,CRIM,60.93,100.0,21.57,1.0,60.93,0.62,MRR
31,1,Passage Re-Ranking,MS MARCO - Passage Re-Ranking benchmarking,2019-01,BERT + Small Training,0.359,97.55,0.359,100,0.368,0.0,MRR
32,1,Passage Re-Ranking,MS MARCO - Passage Re-Ranking benchmarking,2019-04,BERT + Doc2query,0.368,100.0,0.009,1.0,0.368,0.0,MRR
33,1,Type prediction,Py150 - Type prediction benchmarking,2020-03,DFSud,98.7,100.0,98.7,100,98.7,1.0,MRR
34,1,Value prediction,Py150 - Value prediction benchmarking,2020-03,DFSud,73.6,100.0,73.6,100,73.6,0.75,MRR
0,1,Text Classification,IMDb - Text Classification benchmarking,2014-05,Paragraph Vectors Le & Mikolov (2014),92.58,95.64,92.58,100,96.8,0.96,Accuracy\\ \\(2\\ classes\\)
1,1,Text Classification,IMDb - Text Classification benchmarking,2019-01,HAHNN (CNN),95.17,98.32,2.59,0.6137,96.8,0.98,Accuracy\\ \\(2\\ classes\\)
2,1,Text Classification,IMDb - Text Classification benchmarking,2019-05,BERT-ITPT-FiT,95.63,98.79,0.46,0.109,96.8,0.99,Accuracy\\ \\(2\\ classes\\)
3,1,Text Classification,IMDb - Text Classification benchmarking,2019-06,XLNet,96.8,100.0,1.17,0.2773,96.8,1.0,Accuracy\\ \\(2\\ classes\\)
0,1,Question Answering,WikiQA - Question Answering benchmarking,2014-05,Paragraph vector (lexical overlap + dist output),0.5976,78.22,0.5976,100,0.764,0.01,MAP
1,1,Question Answering,WikiQA - Question Answering benchmarking,2014-12,Bigram-CNN,0.619,81.02,0.0214,0.1286,0.764,0.02,MAP
2,1,Question Answering,WikiQA - Question Answering benchmarking,2014-12,Bigram-CNN (lexical overlap + dist output),0.652,85.34,0.033,0.1983,0.764,0.02,MAP
3,1,Question Answering,WikiQA - Question Answering benchmarking,2015-11,LSTM (lexical overlap + dist output),0.682,89.27,0.03,0.1803,0.764,0.02,MAP
4,1,Question Answering,WikiQA - Question Answering benchmarking,2015-11,Attentive LSTM,0.6886,90.13,0.0066,0.0397,0.764,0.02,MAP
5,1,Question Answering,WikiQA - Question Answering benchmarking,2016-02,LDC,0.7058,92.38,0.0172,0.1034,0.764,0.02,MAP
6,1,Question Answering,WikiQA - Question Answering benchmarking,2016-06,PWIM,0.709,92.8,0.0032,0.0192,0.764,0.02,MAP
7,1,Question Answering,WikiQA - Question Answering benchmarking,2017-07,HyperQA,0.712,93.19,0.003,0.018,0.764,0.02,MAP
8,1,Question Answering,WikiQA - Question Answering benchmarking,2019-05,Comp-Clip + LM + LC,0.764,100.0,0.052,0.3125,0.764,0.02,MAP
9,1,Question Answering,QASent - Question Answering benchmarking,2014-05,Paragraph vector (lexical overlap + dist output),0.6762,92.14,0.6762,100,0.7339,0.02,MAP
10,1,Question Answering,QASent - Question Answering benchmarking,2014-12,Bigram-CNN (lexical overlap + dist output),0.7113,96.92,0.0351,0.6083,0.7339,0.02,MAP
11,1,Question Answering,QASent - Question Answering benchmarking,2015-11,Attentive LSTM,0.7339,100.0,0.0226,0.3917,0.7339,0.02,MAP
12,1,Question Answering,TrecQA - Question Answering benchmarking,2014-12,CNN,0.711,81.91,0.711,100,0.868,0.02,MAP
13,1,Question Answering,TrecQA - Question Answering benchmarking,2016-06,PWIN,0.7588,87.42,0.0478,0.3045,0.868,0.02,MAP
14,1,Question Answering,TrecQA - Question Answering benchmarking,2017-07,HyperQA,0.77,88.71,0.0112,0.0713,0.868,0.02,MAP
15,1,Question Answering,TrecQA - Question Answering benchmarking,2019-05,Comp-Clip + LM + LC,0.868,100.0,0.098,0.6242,0.868,0.02,MAP
16,1,Question Answering,SemEvalCQA - Question Answering benchmarking,2015-03,ARC-II,0.78,98.11,0.78,100,0.795,0.02,MAP
17,1,Question Answering,SemEvalCQA - Question Answering benchmarking,2016-06,Kelp,0.792,99.62,0.012,0.8,0.795,0.02,MAP
18,1,Question Answering,SemEvalCQA - Question Answering benchmarking,2017-07,HyperQA,0.795,100.0,0.003,0.2,0.795,0.02,MAP
19,1,Hypernym Discovery,Music domain - Hypernym Discovery benchmarking,2016-11,vTE,12.99,31.71,12.99,100,40.97,0.32,MAP
20,1,Hypernym Discovery,Music domain - Hypernym Discovery benchmarking,2018-06,CRIM,40.97,100.0,27.98,1.0,40.97,1.0,MAP
21,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2016-11,vTE,18.84,55.33,18.84,100,34.05,0.46,MAP
22,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2018-06,300-sparsans,20.75,60.94,1.91,0.1256,34.05,0.51,MAP
23,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2018-06,CRIM,34.05,100.0,13.3,0.8744,34.05,0.83,MAP
24,1,Hypernym Discovery,General - Hypernym Discovery benchmarking,2016-11,vTE,10.6,53.59,10.6,100,19.78,0.26,MAP
25,1,Hypernym Discovery,General - Hypernym Discovery benchmarking,2018-06,CRIM,19.78,100.0,9.18,1.0,19.78,0.48,MAP
26,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2017-04,FNRM-Rank_Embed,0.2811,85.75,0.2811,100,0.3278,0.01,MAP
27,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2017-04,FNRM-RankProb_Embed,0.2837,86.55,0.0026,0.0557,0.3278,0.01,MAP
28,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2018-10,SNRM-PRF,0.2971,90.63,0.0134,0.2869,0.3278,0.01,MAP
29,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2018-12,Anserini BM25+RM3,0.302,92.13,0.0049,0.1049,0.3278,0.01,MAP
30,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2019-03,BERT FT(Microblog),0.3278,100.0,0.0258,0.5525,0.3278,0.01,MAP
0,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2014-06,Joint w/ Global,45.3,91.7,45.3,100,49.4,0.66,RE\\+\\ Micro\\ F1
1,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2016-01,SPTree,48.4,97.98,3.1,0.7561,49.4,0.7,RE\\+\\ Micro\\ F1
2,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2019-05,Multi-turn QA,49.4,100.0,1.0,0.2439,49.4,0.72,RE\\+\\ Micro\\ F1
3,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2014-06,Joint w/ Global,49.5,82.23,49.5,100,60.2,0.72,RE\\+\\ Micro\\ F1
4,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2016-01,SPTree,55.6,92.36,6.1,0.5701,60.2,0.81,RE\\+\\ Micro\\ F1
5,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2017-09,Global,57.5,95.51,1.9,0.1776,60.2,0.83,RE\\+\\ Micro\\ F1
6,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2018-10,MRT,59.6,99.0,2.1,0.1963,60.2,0.87,RE\\+\\ Micro\\ F1
7,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2019-05,Multi-turn QA,60.2,100.0,0.6,0.0561,60.2,0.87,RE\\+\\ Micro\\ F1
8,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2014-10,Table Representation,61.0,88.53,61.0,100,68.9,0.89,RE\\+\\ Micro\\ F1
9,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2017-09,Global,67.8,98.4,6.8,0.8608,68.9,0.98,RE\\+\\ Micro\\ F1
10,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2019-05,Multi-turn QA,68.9,100.0,1.1,0.1392,68.9,1.0,RE\\+\\ Micro\\ F1
0,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2014-06,Joint w/ Global,48.3,80.9,48.3,100,59.7,0.76,RE\\ Micro\\ F1
1,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2017-07,Attention,49.3,82.58,1.0,0.0877,59.7,0.78,RE\\ Micro\\ F1
2,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2019-04,DYGIE,59.7,100.0,10.4,0.9123,59.7,0.94,RE\\ Micro\\ F1
3,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2014-06,Joint w/ Global,52.1,82.18,52.1,100,63.4,0.82,RE\\ Micro\\ F1
4,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2017-07,Attention,55.9,88.17,3.8,0.3363,63.4,0.88,RE\\ Micro\\ F1
5,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2019-04,DYGIE,63.2,99.68,7.3,0.646,63.4,1.0,RE\\ Micro\\ F1
6,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2019-09,DYGIE++,63.4,100.0,0.2,0.0177,63.4,1.0,RE\\ Micro\\ F1
0,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2014-06,Joint w/ Global,79.7,91.19,79.7,100,87.4,0.9,NER\\ Micro\\ F1
1,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2016-01,SPTree,81.8,93.59,2.1,0.2727,87.4,0.92,NER\\ Micro\\ F1
2,1,Relation Extraction,ACE 2004 - Relation Extraction benchmarking,2019-04,DYGIE,87.4,100.0,5.6,0.7273,87.4,0.99,NER\\ Micro\\ F1
3,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2014-06,Joint w/ Global,80.8,91.2,80.8,100,88.6,0.91,NER\\ Micro\\ F1
4,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2016-01,SPTree,83.4,94.13,2.6,0.3333,88.6,0.94,NER\\ Micro\\ F1
5,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2017-09,Global,83.6,94.36,0.2,0.0256,88.6,0.94,NER\\ Micro\\ F1
6,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2019-04,DYGIE,88.4,99.77,4.8,0.6154,88.6,1.0,NER\\ Micro\\ F1
7,1,Relation Extraction,ACE 2005 - Relation Extraction benchmarking,2019-09,DYGIE++,88.6,100.0,0.2,0.0256,88.6,1.0,NER\\ Micro\\ F1
8,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2014-10,Table Representation,80.7,91.91,80.7,100,87.8,0.91,NER\\ Micro\\ F1
9,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2017-09,Global,85.6,97.49,4.9,0.6901,87.8,0.97,NER\\ Micro\\ F1
10,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2019-05,Multi-turn QA,87.8,100.0,2.2,0.3099,87.8,0.99,NER\\ Micro\\ F1
0,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2014-06,CSLM + RNN + WP,34.54,75.75,34.54,100,45.6,0.76,BLEU\\ score
1,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2014-09,RNN-search50*,36.2,79.39,1.66,0.1501,45.6,0.79,BLEU\\ score
2,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2014-09,SMT+LSTM5,36.5,80.04,0.3,0.0271,45.6,0.8,BLEU\\ score
3,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2014-10,LSTM6 + PosUnk,37.5,82.24,1.0,0.0904,45.6,0.82,BLEU\\ score
4,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2016-06,Deep-Att + PosUnk,39.2,85.96,1.7,0.1537,45.6,0.86,BLEU\\ score
5,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2016-09,GNMT+RL,39.9,87.5,0.7,0.0633,45.6,0.87,BLEU\\ score
6,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2017-01,MoE,40.56,88.95,0.66,0.0597,45.6,0.89,BLEU\\ score
7,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2017-05,ConvS2S (ensemble),41.3,90.57,0.74,0.0669,45.6,0.91,BLEU\\ score
8,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2017-11,Weighted Transformer (large),41.4,90.79,0.1,0.009,45.6,0.91,BLEU\\ score
9,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2018-03,Transformer (big) + Relative Position Representations,41.5,91.01,0.1,0.009,45.6,0.91,BLEU\\ score
10,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2018-06,Transformer Big,43.2,94.74,1.7,0.1537,45.6,0.95,BLEU\\ score
11,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2018-08,Noisy back-translation,45.6,100.0,2.4,0.217,45.6,1.0,BLEU\\ score
12,1,Machine Translation,IWSLT2015 German-English - Machine Translation benchmarking,2014-09,Bi-GRU (MLE+SLE),28.53,81.1,28.53,100,35.18,0.63,BLEU\\ score
13,1,Machine Translation,IWSLT2015 German-English - Machine Translation benchmarking,2016-07,RNNsearch,29.98,85.22,1.45,0.218,35.18,0.66,BLEU\\ score
14,1,Machine Translation,IWSLT2015 German-English - Machine Translation benchmarking,2016-11,Conv-LSTM (deep+pos),30.4,86.41,0.42,0.0632,35.18,0.67,BLEU\\ score
15,1,Machine Translation,IWSLT2015 German-English - Machine Translation benchmarking,2017-05,ConvS2S,32.31,91.84,1.91,0.2872,35.18,0.71,BLEU\\ score
16,1,Machine Translation,IWSLT2015 German-English - Machine Translation benchmarking,2017-11,ConvS2S+Risk,32.93,93.6,0.62,0.0932,35.18,0.72,BLEU\\ score
17,1,Machine Translation,IWSLT2015 German-English - Machine Translation benchmarking,2018-08,Pervasive Attention,34.18,97.16,1.25,0.188,35.18,0.75,BLEU\\ score
18,1,Machine Translation,IWSLT2015 German-English - Machine Translation benchmarking,2019-06,Transformer Base + adversarial MLE,35.18,100.0,1.0,0.1504,35.18,0.77,BLEU\\ score
19,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2015-08,RNN Enc-Dec Att,20.9,59.71,20.9,100,35.0,0.46,BLEU\\ score
20,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2016-09,GNMT+RL,26.3,75.14,5.4,0.383,35.0,0.58,BLEU\\ score
21,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2017-05,ConvS2S (ensemble),26.4,75.43,0.1,0.0071,35.0,0.58,BLEU\\ score
22,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2017-06,Transformer Big,28.4,81.14,2.0,0.1418,35.0,0.62,BLEU\\ score
23,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2017-11,Weighted Transformer (large),28.9,82.57,0.5,0.0355,35.0,0.63,BLEU\\ score
24,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2018-03,Transformer (big) + Relative Position Representations,29.2,83.43,0.3,0.0213,35.0,0.64,BLEU\\ score
25,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2018-06,Transformer Big,29.3,83.71,0.1,0.0071,35.0,0.64,BLEU\\ score
26,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2018-08,Noisy back-translation,35.0,100.0,5.7,0.4043,35.0,0.77,BLEU\\ score
27,1,Machine Translation,WMT2015 English-German - Machine Translation benchmarking,2015-08,BPE word segmentation,22.8,86.69,22.8,100,26.3,0.5,BLEU\\ score
28,1,Machine Translation,WMT2015 English-German - Machine Translation benchmarking,2016-03,Enc-Dec Att (char),23.5,89.35,0.7,0.2,26.3,0.52,BLEU\\ score
29,1,Machine Translation,WMT2015 English-German - Machine Translation benchmarking,2016-10,ByteNet,26.3,100.0,2.8,0.8,26.3,0.58,BLEU\\ score
30,1,Machine Translation,WMT2015 English-Russian - Machine Translation benchmarking,2015-08,C2-50k Segmentation,20.9,100.0,20.9,100,20.9,0.46,BLEU\\ score
31,1,Machine Translation,WMT2016 English-Russian - Machine Translation benchmarking,2016-06,Attentional encoder-decoder + BPE,26.0,100.0,26.0,100,26.0,0.57,BLEU\\ score
32,1,Machine Translation,WMT2016 English-Czech - Machine Translation benchmarking,2016-06,Attentional encoder-decoder + BPE,25.8,100.0,25.8,100,25.8,0.57,BLEU\\ score
33,1,Machine Translation,WMT2016 Czech-English - Machine Translation benchmarking,2016-06,Attentional encoder-decoder + BPE,31.4,100.0,31.4,100,31.4,0.69,BLEU\\ score
34,1,Machine Translation,WMT2016 English-German - Machine Translation benchmarking,2016-06,Linguistic Input Features,28.4,69.81,28.4,100,40.68,0.62,BLEU\\ score
35,1,Machine Translation,WMT2016 English-German - Machine Translation benchmarking,2016-06,Attentional encoder-decoder + BPE,34.2,84.07,5.8,0.4723,40.68,0.75,BLEU\\ score
36,1,Machine Translation,WMT2016 English-German - Machine Translation benchmarking,2019-05,MADL,40.68,100.0,6.48,0.5277,40.68,0.89,BLEU\\ score
37,1,Machine Translation,WMT2016 English-Romanian - Machine Translation benchmarking,2016-06,BiGRU,28.1,86.86,28.1,100,32.35,0.62,BLEU\\ score
38,1,Machine Translation,WMT2016 English-Romanian - Machine Translation benchmarking,2016-08,GRU BPE90k,28.9,89.34,0.8,0.1882,32.35,0.63,BLEU\\ score
39,1,Machine Translation,WMT2016 English-Romanian - Machine Translation benchmarking,2017-05,ConvS2S BPE40k,29.9,92.43,1.0,0.2353,32.35,0.66,BLEU\\ score
40,1,Machine Translation,WMT2016 English-Romanian - Machine Translation benchmarking,2019-09,FlowSeq-large (IWD n = 15),31.08,96.07,1.18,0.2776,32.35,0.68,BLEU\\ score
41,1,Machine Translation,WMT2016 English-Romanian - Machine Translation benchmarking,2019-09,FlowSeq-large (NPD n=15),31.97,98.83,0.89,0.2094,32.35,0.7,BLEU\\ score
42,1,Machine Translation,WMT2016 English-Romanian - Machine Translation benchmarking,2019-09,FlowSeq-large (NPD n = 30),32.35,100.0,0.38,0.0894,32.35,0.71,BLEU\\ score
43,1,Machine Translation,WMT2016 Romanian-English - Machine Translation benchmarking,2016-06,Attentional encoder-decoder + BPE,33.3,94.33,33.3,100,35.3,0.73,BLEU\\ score
44,1,Machine Translation,WMT2016 Romanian-English - Machine Translation benchmarking,2019-01,MLM pretraining,35.3,100.0,2.0,1.0,35.3,0.77,BLEU\\ score
45,1,Machine Translation,WMT2016 Russian-English - Machine Translation benchmarking,2016-06,Attentional encoder-decoder + BPE,28.0,100.0,28,100,28,0.61,BLEU\\ score
46,1,Machine Translation,WMT2016 German-English - Machine Translation benchmarking,2016-06,Linguistic Input Features,32.9,85.23,32.9,100,38.6,0.72,BLEU\\ score
47,1,Machine Translation,WMT2016 German-English - Machine Translation benchmarking,2016-06,Attentional encoder-decoder + BPE,38.6,100.0,5.7,1.0,38.6,0.85,BLEU\\ score
48,1,Machine Translation,IWSLT2015 Thai-English - Machine Translation benchmarking,2016-06,Seq-KD + Seq-Inter + Word-KD,14.2,100.0,14.2,100,14.2,0.31,BLEU\\ score
49,1,Machine Translation,IWSLT2015 English-German - Machine Translation benchmarking,2016-07,RNNsearch,25.04,88.7,25.04,100,28.23,0.55,BLEU\\ score
50,1,Machine Translation,IWSLT2015 English-German - Machine Translation benchmarking,2017-05,ConvS2S,26.73,94.69,1.69,0.5298,28.23,0.59,BLEU\\ score
51,1,Machine Translation,IWSLT2015 English-German - Machine Translation benchmarking,2017-06,Transformer,28.23,100.0,1.5,0.4702,28.23,0.62,BLEU\\ score
52,1,Machine Translation,IWSLT2014 German-English - Machine Translation benchmarking,2016-07,Actor-Critic [Bahdanau2017],28.53,79.92,28.53,100,35.7,0.63,BLEU\\ score
53,1,Machine Translation,IWSLT2014 German-English - Machine Translation benchmarking,2017-06,Transformer,34.44,96.47,5.91,0.8243,35.7,0.76,BLEU\\ score
54,1,Machine Translation,IWSLT2014 German-English - Machine Translation benchmarking,2019-01,DynamicConv,35.2,98.6,0.76,0.106,35.7,0.77,BLEU\\ score
55,1,Machine Translation,IWSLT2014 German-English - Machine Translation benchmarking,2019-05,Local Joint Self-attention,35.7,100.0,0.5,0.0697,35.7,0.78,BLEU\\ score
56,1,Machine Translation,WMT2014 German-English - Machine Translation benchmarking,2017-11,NAT +FT + NPD,23.2,82.01,23.2,100,28.29,0.51,BLEU\\ score
57,1,Machine Translation,WMT2014 German-English - Machine Translation benchmarking,2018-02,Denoising autoencoders (non-autoregressive),25.43,89.89,2.23,0.4381,28.29,0.56,BLEU\\ score
58,1,Machine Translation,WMT2014 German-English - Machine Translation benchmarking,2019-09,FlowSeq-large (NPD n = 30),28.29,100.0,2.86,0.5619,28.29,0.62,BLEU\\ score
59,1,Machine Translation,WMT 2017 English-Chinese - Machine Translation benchmarking,2018-03,Hassan et al. (2018),24.2,99.18,24.2,100,24.4,0.53,BLEU\\ score
60,1,Machine Translation,WMT 2017 English-Chinese - Machine Translation benchmarking,2019-01,DynamicConv,24.4,100.0,0.2,1.0,24.4,0.54,BLEU\\ score
61,1,Machine Translation,WMT2014 French-English - Machine Translation benchmarking,2018-09,SMT + iterative backtranslation (unsupervised),25.87,100.0,25.87,100,25.87,0.57,BLEU\\ score
62,1,Machine Translation,WMT2014 English-Czech - Machine Translation benchmarking,2019-01,Evolved Transformer Base,27.6,97.87,27.6,100,28.2,0.61,BLEU\\ score
63,1,Machine Translation,WMT2014 English-Czech - Machine Translation benchmarking,2019-01,Evolved Transformer Big,28.2,100.0,0.6,1.0,28.2,0.62,BLEU\\ score
64,1,Machine Translation,WMT2019 English-German - Machine Translation benchmarking,2019-07,Facebook FAIR (ensemble),43.1,100.0,43.1,100,43.1,0.95,BLEU\\ score
0,1,Dialog Generation,Persona-Chat - Dialog Generation benchmarking,2014-09,Seq2Seq + Attention,16.18,81.84,16.18,100,19.77,0.21,Avg\\ F1
1,1,Dialog Generation,Persona-Chat - Dialog Generation benchmarking,2020-04,P^2 Bot,19.77,100.0,3.59,1.0,19.77,0.26,Avg\\ F1
2,1,Coreference Resolution,CoNLL 2012 - Coreference Resolution benchmarking,2017-07,e2e-coref (ensemble),68.8,89.47,68.8,100,76.9,0.89,Avg\\ F1
3,1,Coreference Resolution,CoNLL 2012 - Coreference Resolution benchmarking,2017-07,e2e-coref + ELMo,70.4,91.55,1.6,0.1975,76.9,0.92,Avg\\ F1
4,1,Coreference Resolution,CoNLL 2012 - Coreference Resolution benchmarking,2018-04,c2f-coref + ELMo,73.0,94.93,2.6,0.321,76.9,0.95,Avg\\ F1
5,1,Coreference Resolution,CoNLL 2012 - Coreference Resolution benchmarking,2019-07,EE + BERT-large,76.61,99.62,3.61,0.4457,76.9,1.0,Avg\\ F1
6,1,Coreference Resolution,CoNLL 2012 - Coreference Resolution benchmarking,2019-08,c2f-coref + BERT-large,76.9,100.0,0.29,0.0358,76.9,1.0,Avg\\ F1
7,1,Aspect Term Extraction and Sentiment Classification,SemEval - Aspect Term Extraction and Sentiment Classification benchmarking,2019-06,IMN-BERT,64.23,100.0,64.23,100,64.23,0.84,Avg\\ F1
0,1,Topic modeling,20 Newsgroups - Topic modeling benchmarking,2015-11,NVDM,836.0,100.0,836,100,836,1.0,Test\\ perplexity
1,1,Language Modelling,WikiText-2 - Language Modelling benchmarking,2016-11,Inan et al. (2016) - Variational LSTM (tied) (h=650),87.7,88.32,87.7,100,99.3,0.1,Test\\ perplexity
2,1,Language Modelling,WikiText-2 - Language Modelling benchmarking,2016-12,Grave et al. (2016) - LSTM,99.3,100.0,11.6,1.0,99.3,0.12,Test\\ perplexity
3,1,Language Modelling,WikiText-103 - Language Modelling benchmarking,2016-12,"Neural cache model (size = 2,000)",40.8,83.78,40.8,100,48.7,0.05,Test\\ perplexity
4,1,Language Modelling,WikiText-103 - Language Modelling benchmarking,2016-12,Neural cache model (size = 100),44.8,91.99,4.0,0.5063,48.7,0.05,Test\\ perplexity
5,1,Language Modelling,WikiText-103 - Language Modelling benchmarking,2016-12,LSTM,48.7,100.0,3.9,0.4937,48.7,0.06,Test\\ perplexity
0,1,Language Modelling,WikiText-2 - Language Modelling benchmarking,2016-11,Inan et al. (2016) - Variational LSTM (tied) (h=650),92.3,100.0,92.3,100,92.3,1.0,Validation\\ perplexity
1,1,Language Modelling,WikiText-103 - Language Modelling benchmarking,2018-03,4 layer QRNN,32.0,88.89,32.0,100,36.0,0.35,Validation\\ perplexity
2,1,Language Modelling,WikiText-103 - Language Modelling benchmarking,2018-03,LSTM,36.0,100.0,4.0,1.0,36.0,0.39,Validation\\ perplexity
3,1,Language Modelling,One Billion Word - Language Modelling benchmarking,2018-09,Adaptive Input Very Large,22.92,96.18,22.92,100,23.83,0.25,Validation\\ perplexity
4,1,Language Modelling,One Billion Word - Language Modelling benchmarking,2018-09,Adaptive Input Large,23.83,100.0,0.91,1.0,23.83,0.26,Validation\\ perplexity
0,1,Question Answering,MS MARCO - Question Answering benchmarking,2016-11,BiDaF Baseline,10.64,19.47,10.64,100,54.64,0.17,BLEU\\-1
1,1,Question Answering,MS MARCO - Question Answering benchmarking,2018-05,VNET,54.37,99.51,43.73,0.9939,54.64,0.85,BLEU\\-1
2,1,Question Answering,MS MARCO - Question Answering benchmarking,2018-11,Deep Cascade QA,54.64,100.0,0.27,0.0061,54.64,0.85,BLEU\\-1
3,1,Question Answering,NarrativeQA - Question Answering benchmarking,2016-11,BiDAF,33.45,61.82,33.45,100,54.11,0.52,BLEU\\-1
4,1,Question Answering,NarrativeQA - Question Answering benchmarking,2018-09,MHPGM + NOIC,43.63,80.63,10.18,0.4927,54.11,0.68,BLEU\\-1
5,1,Question Answering,NarrativeQA - Question Answering benchmarking,2018-11,DecaProp,44.35,81.96,0.72,0.0348,54.11,0.69,BLEU\\-1
6,1,Question Answering,NarrativeQA - Question Answering benchmarking,2019-01,Masque (NarrativeQA only),48.7,90.0,4.35,0.2106,54.11,0.76,BLEU\\-1
7,1,Question Answering,NarrativeQA - Question Answering benchmarking,2019-01,Masque (NarrativeQA + MS MARCO),54.11,100.0,5.41,0.2619,54.11,0.84,BLEU\\-1
8,1,Paraphrase Generation,quora - Paraphrase Generation benchmarking,2017-09,VAE-SVG-eq,22.9,50.11,22.9,100,45.7,0.36,BLEU\\-1
9,1,Paraphrase Generation,quora - Paraphrase Generation benchmarking,2018-06,EDD-LG,45.7,100.0,22.8,1.0,45.7,0.71,BLEU\\-1
10,1,Question Generation,Visual Question Generation - Question Generation benchmarking,2018-08,MDN,36.0,100.0,36,100,36,0.56,BLEU\\-1
11,1,Text Generation,DailyDialog - Text Generation benchmarking,2018-08,AEM+Attention,14.17,100.0,14.17,100,14.17,0.22,BLEU\\-1
12,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",64.2,100.0,64.2,100,64.2,1.0,BLEU\\-1
0,1,Constituency Parsing,Penn Treebank - Constituency Parsing benchmarking,2014-12,Semi-supervised LSTM,92.1,96.34,92.1,100,95.6,0.95,F1\\ score
1,1,Constituency Parsing,Penn Treebank - Constituency Parsing benchmarking,2016-11,Semi-supervised LSTM-LM,93.8,98.12,1.7,0.4857,95.6,0.97,F1\\ score
2,1,Constituency Parsing,Penn Treebank - Constituency Parsing benchmarking,2017-07,Model combination,94.66,99.02,0.86,0.2457,95.6,0.98,F1\\ score
3,1,Constituency Parsing,Penn Treebank - Constituency Parsing benchmarking,2018-05,Self-attentive encoder + ELMo,95.13,99.51,0.47,0.1343,95.6,0.98,F1\\ score
4,1,Constituency Parsing,Penn Treebank - Constituency Parsing benchmarking,2019-03,CNN Large + fine-tune,95.6,100.0,0.47,0.1343,95.6,0.99,F1\\ score
5,1,Cross-Lingual Bitext Mining,BUCC French-to-English - Cross-Lingual Bitext Mining benchmarking,2015-11,Monolingual training data,75.8,80.72,75.8,100,93.91,0.78,F1\\ score
6,1,Cross-Lingual Bitext Mining,BUCC French-to-English - Cross-Lingual Bitext Mining benchmarking,2018-11,Multilingual Sentence Embeddings,92.89,98.91,17.09,0.9437,93.91,0.96,F1\\ score
7,1,Cross-Lingual Bitext Mining,BUCC French-to-English - Cross-Lingual Bitext Mining benchmarking,2018-12,Massively Multilingual Sentence Embeddings,93.91,100.0,1.02,0.0563,93.91,0.97,F1\\ score
8,1,Cross-Lingual Bitext Mining,BUCC German-to-English - Cross-Lingual Bitext Mining benchmarking,2015-11,Monolingual training data,76.9,79.95,76.9,100,96.19,0.8,F1\\ score
9,1,Cross-Lingual Bitext Mining,BUCC German-to-English - Cross-Lingual Bitext Mining benchmarking,2018-11,Multilingual Sentence Embeddings,95.58,99.37,18.68,0.9684,96.19,0.99,F1\\ score
10,1,Cross-Lingual Bitext Mining,BUCC German-to-English - Cross-Lingual Bitext Mining benchmarking,2018-12,Massively Multilingual Sentence Embeddings,96.19,100.0,0.61,0.0316,96.19,0.99,F1\\ score
11,1,Chunking,Penn Treebank - Chunking benchmarking,2016-08,Low supervision,95.57,98.81,95.57,100,96.72,0.99,F1\\ score
12,1,Chunking,Penn Treebank - Chunking benchmarking,2016-11,JMT,95.77,99.02,0.2,0.1739,96.72,0.99,F1\\ score
13,1,Chunking,Penn Treebank - Chunking benchmarking,2018-08,Flair embeddings,96.72,100.0,0.95,0.8261,96.72,1.0,F1\\ score
14,1,Temporal Information Extraction,TimeBank - Temporal Information Extraction benchmarking,2016-12,Catena,0.511,100.0,0.511,100,0.511,0.01,F1\\ score
15,1,Extract Aspect,SemEval 2015 Task 12 - Extract Aspect benchmarking,2017-10,Syntactic Grammar Model,0.63,90.0,0.63,100,0.7,0.01,F1\\ score
16,1,Extract Aspect,SemEval 2015 Task 12 - Extract Aspect benchmarking,2017-10,EliXa,0.7,100.0,0.07,1.0,0.7,0.01,F1\\ score
17,1,Extract aspect-polarity tuple,SemEval 2015 Task 12 - Extract aspect-polarity tuple benchmarking,2017-10,Syntactic Grammer Model,0.51,100.0,0.51,100,0.51,0.01,F1\\ score
18,1,Cross-Lingual Bitext Mining,BUCC Chinese-to-English - Cross-Lingual Bitext Mining benchmarking,2018-12,Massively Multilingual Sentence Embeddings,92.27,100.0,92.27,100,92.27,0.95,F1\\ score
19,1,Cross-Lingual Bitext Mining,BUCC Russian-to-English - Cross-Lingual Bitext Mining benchmarking,2018-12,Massively Multilingual Sentence Embeddings,93.3,100.0,93.3,100,93.3,0.96,F1\\ score
20,1,Sentiment Analysis,Financial PhraseBank - Sentiment Analysis benchmarking,2019-08,FinBERT,84.0,100.0,84,100,84,0.87,F1\\ score
21,1,Low Resource Named Entity Recognition,Conll 2003 Spanish - Low Resource Named Entity Recognition benchmarking,2019-11,Zero-Resource Cross-lingual Transfer From CoNLL-2003 English dataset.,75.93,100.0,75.93,100,75.93,0.79,F1\\ score
22,1,Low Resource Named Entity Recognition,CONLL 2003 Dutch - Low Resource Named Entity Recognition benchmarking,2019-11,Zero-Resource Transfer From CoNLL-2003 English dataset.,74.61,100.0,74.61,100,74.61,0.77,F1\\ score
23,1,Low Resource Named Entity Recognition,CONLL 2003 German - Low Resource Named Entity Recognition benchmarking,2019-11,Zero-Resource Transfer From CoNLL-2003 English dataset.,65.24,100.0,65.24,100,65.24,0.67,F1\\ score
24,1,Question Similarity,Q2Q Arabic Benchmark - Question Similarity benchmarking,2019-12,Tha3aroon,0.94848,98.88,0.94848,100,0.95924,0.01,F1\\ score
25,1,Question Similarity,Q2Q Arabic Benchmark - Question Similarity benchmarking,2020-04,Ensemble multilingual BERT model,0.95924,100.0,0.0108,1.0037,0.95924,0.01,F1\\ score
0,1,Semantic Similarity Estimation,SICK - Semantic Similarity Estimation benchmarking,2015-02,"Dependency Tree-LSTM (Tai et al., 2015)",0.8676,100.0,0.8676,100,0.8676,0.94,Pearson\\ Correlation
1,1,Semantic Textual Similarity,STS Benchmark - Semantic Textual Similarity benchmarking,2018-03,USE_T,0.782,84.54,0.782,100,0.925,0.85,Pearson\\ Correlation
2,1,Semantic Textual Similarity,STS Benchmark - Semantic Textual Similarity benchmarking,2019-05,ERNIE,0.832,89.95,0.05,0.3497,0.925,0.9,Pearson\\ Correlation
3,1,Semantic Textual Similarity,STS Benchmark - Semantic Textual Similarity benchmarking,2019-06,XLNet (single model),0.925,100.0,0.093,0.6503,0.925,1.0,Pearson\\ Correlation
0,1,Semantic Similarity Estimation,SICK - Semantic Similarity Estimation benchmarking,2015-02,"Dependency Tree-LSTM (Tai et al., 2015)",0.8083,100.0,0.8083,100,0.8083,1.0,Spearman\\ Correlation
0,1,Semantic Similarity Estimation,SICK - Semantic Similarity Estimation benchmarking,2015-02,"Dependency Tree-LSTM (Tai et al., 2015)",-0.2532,100.0,-0.2532,100,-0.2532,3.62,MSE
1,1,Sentiment Analysis,FiQA - Sentiment Analysis benchmarking,2019-08,FinBERT,-0.07,100.0,-0.07,100,-0.07,1.0,MSE
0,1,Question Answering,SemEvalCQA - Question Answering benchmarking,2015-03,ARC-II,0.753,93.08,0.753,100,0.809,0.01,P\\-at\\-1
1,1,Question Answering,SemEvalCQA - Question Answering benchmarking,2016-02,AP-CNN,0.755,93.33,0.002,0.0357,0.809,0.01,P\\-at\\-1
2,1,Question Answering,SemEvalCQA - Question Answering benchmarking,2017-07,HyperQA,0.809,100.0,0.054,0.9643,0.809,0.01,P\\-at\\-1
3,1,Question Answering,YahooCQA - Question Answering benchmarking,2016-02,AP-CNN,0.56,81.99,0.56,100,0.683,0.01,P\\-at\\-1
4,1,Question Answering,YahooCQA - Question Answering benchmarking,2016-02,AP-BiLSTM,0.568,83.16,0.008,0.065,0.683,0.01,P\\-at\\-1
5,1,Question Answering,YahooCQA - Question Answering benchmarking,2017-07,HyperQA,0.683,100.0,0.115,0.935,0.683,0.01,P\\-at\\-1
6,1,Question Answering,AI2 Kaggle Dataset - Question Answering benchmarking,2017-08,Our Approach w/o IR,50.54,93.59,50.54,100,54.0,0.52,P\\-at\\-1
7,1,Question Answering,AI2 Kaggle Dataset - Question Answering benchmarking,2017-08,IR++,50.7,93.89,0.16,0.0462,54.0,0.52,P\\-at\\-1
8,1,Question Answering,AI2 Kaggle Dataset - Question Answering benchmarking,2017-08,OUR APPROACH,54.0,100.0,3.3,0.9538,54.0,0.56,P\\-at\\-1
9,1,Word Alignment,fr-en - Word Alignment benchmarking,2017-10,Adv - Refine - CSLS,82.1,100.0,82.1,100,82.1,0.85,P\\-at\\-1
10,1,Word Alignment,en-es - Word Alignment benchmarking,2017-10,Adv - Refine - CSLS,81.7,100.0,81.7,100,81.7,0.84,P\\-at\\-1
11,1,Word Alignment,en-fr - Word Alignment benchmarking,2017-10,Adv - Refine - CSLS,82.3,100.0,82.3,100,82.3,0.85,P\\-at\\-1
12,1,Word Alignment,es-en - Word Alignment benchmarking,2017-10,Adv - Refine - CSLS,83.3,100.0,83.3,100,83.3,0.86,P\\-at\\-1
13,1,Entity Typing,Freebase FIGER - Entity Typing benchmarking,2018-06,TextEnt-full,93.2,100.0,93.2,100,93.2,0.96,P\\-at\\-1
14,1,Multi-Label Text Classification,AAPD - Multi-Label Text Classification benchmarking,2019-05,LAHA,84.48,100.0,84.48,100,84.48,0.87,P\\-at\\-1
15,1,Multi-Label Text Classification,Amazon-12K - Multi-Label Text Classification benchmarking,2019-05,LAHA,94.87,100.0,94.87,100,94.87,0.98,P\\-at\\-1
16,1,Multi-Label Text Classification,Kan-Shan Cup - Multi-Label Text Classification benchmarking,2019-05,LAHA,54.38,100.0,54.38,100,54.38,0.56,P\\-at\\-1
17,1,Multi-Label Text Classification,Wiki-30K - Multi-Label Text Classification benchmarking,2019-05,LAHA,84.18,100.0,84.18,100,84.18,0.87,P\\-at\\-1
18,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-05,LAHA,74.95,93.45,74.95,100,80.2,0.77,P\\-at\\-1
19,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,NLP-Cap,80.2,100.0,5.25,1.0,80.2,0.83,P\\-at\\-1
20,1,Text Classification,RCV1 - Text Classification benchmarking,2019-06,NLP-Cap,97.05,100.0,97.05,100,97.05,1.0,P\\-at\\-1
0,1,Question Answering,bAbi - Question Answering benchmarking,2015-03,End-To-End Memory Networks,7.5,26.13,7.5,100,28.7,0.26,Mean\\ Error\\ Rate
1,1,Question Answering,bAbi - Question Answering benchmarking,2016-10,LSTM,28.7,100.0,21.2,1.0,28.7,1.0,Mean\\ Error\\ Rate
0,1,Question Answering,bAbi - Question Answering benchmarking,2015-03,End-To-End Memory Networks,93.4,93.68,93.4,100,99.7,0.94,Accuracy\\ \\(trained\\ on\\ 10k\\)
1,1,Question Answering,bAbi - Question Answering benchmarking,2016-06,QRN,99.7,100.0,6.3,1.0,99.7,1.0,Accuracy\\ \\(trained\\ on\\ 10k\\)
0,1,Question Answering,bAbi - Question Answering benchmarking,2015-03,End-To-End Memory Networks,86.1,95.56,86.1,100,90.1,0.96,Accuracy\\ \\(trained\\ on\\ 1k\\)
1,1,Question Answering,bAbi - Question Answering benchmarking,2016-06,QRN,90.1,100.0,4.0,1.0,90.1,1.0,Accuracy\\ \\(trained\\ on\\ 1k\\)
0,1,Text Classification,TREC-6 - Text Classification benchmarking,2015-04,TBCNN,4.0,41.67,4.0,100,9.6,0.09,Error
1,1,Text Classification,TREC-6 - Text Classification benchmarking,2015-11,C-LSTM,5.4,56.25,1.4,0.25,9.6,0.12,Error
2,1,Text Classification,TREC-6 - Text Classification benchmarking,2017-02,GRU-RNN-GLOVE,7.0,72.92,1.6,0.2857,9.6,0.15,Error
3,1,Text Classification,TREC-6 - Text Classification benchmarking,2018-03,Capsule-B,7.2,75.0,0.2,0.0357,9.6,0.15,Error
4,1,Text Classification,TREC-6 - Text Classification benchmarking,2018-05,byte mLSTM7,9.6,100.0,2.4,0.4286,9.6,0.21,Error
5,1,Sentiment Analysis,Yelp Binary classification - Sentiment Analysis benchmarking,2015-09,Char-level CNN,4.88,100.0,4.88,100,4.88,0.1,Error
6,1,Text Classification,AG News - Text Classification benchmarking,2015-09,Char-level CNN,9.51,67.93,9.51,100,14.0,0.2,Error
7,1,Text Classification,AG News - Text Classification benchmarking,2018-08,ToWE-SG,14.0,100.0,4.49,1.0,14.0,0.3,Error
8,1,Sentiment Analysis,Yelp Fine-grained classification - Sentiment Analysis benchmarking,2015-09,Char-level CNN,37.95,81.09,37.95,100,46.8,0.81,Error
9,1,Sentiment Analysis,Yelp Fine-grained classification - Sentiment Analysis benchmarking,2019-01,SVDCNN,46.8,100.0,8.85,1.0,46.8,1.0,Error
10,1,Text Classification,DBpedia - Text Classification benchmarking,2015-09,Char-level CNN,1.55,100.0,1.55,100,1.55,0.03,Error
11,1,Text Classification,TREC-50 - Text Classification benchmarking,2016-12,Rules,2.8,100.0,2.8,100,2.8,0.06,Error
12,1,Text Classification,Amazon-2 - Text Classification benchmarking,2019-06,XLNet,2.11,54.1,2.11,100,3.9,0.05,Error
13,1,Text Classification,Amazon-2 - Text Classification benchmarking,2019-09,ULMFiT (Small data),3.9,100.0,1.79,1.0,3.9,0.08,Error
14,1,Text Classification,Amazon-5 - Text Classification benchmarking,2019-06,XLNet,31.67,88.22,31.67,100,35.9,0.68,Error
15,1,Text Classification,Amazon-5 - Text Classification benchmarking,2019-09,ULMFiT (Small data),35.9,100.0,4.23,1.0,35.9,0.77,Error
0,1,Visual Question Answering,Visual7W - Visual Question Answering benchmarking,2016-06,MCB+Att.,62.2,85.76,62.2,100,72.53,0.86,Percentage\\ correct
1,1,Visual Question Answering,Visual7W - Visual Question Answering benchmarking,2016-11,CMN,72.53,100.0,10.33,1.0,72.53,1.0,Percentage\\ correct
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-05,DANN,-76.26,100.0,-76.26,100,-76.26,1.0,Average
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-05,DANN,75.4,93.09,75.4,100,81.0,0.93,DVD
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-11,VFAE,76.57,94.53,1.17,0.2089,81.0,0.95,DVD
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2018-04,Multi-task tri-training,78.14,96.47,1.57,0.2804,81.0,0.96,DVD
3,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2018-10,Distributional Correspondence Indexing,81.0,100.0,2.86,0.5107,81.0,1.0,DVD
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-05,DANN,71.43,87.75,71.43,100,81.4,0.88,Books
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-11,VFAE,73.4,90.17,1.97,0.1976,81.4,0.9,Books
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2018-04,Multi-task tri-training,74.86,91.97,1.46,0.1464,81.4,0.92,Books
3,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2018-10,Distributional Correspondence Indexing,81.4,100.0,6.54,0.656,81.4,1.0,Books
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-05,DANN,77.67,95.36,77.67,100,81.45,0.95,Electronics
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-11,VFAE,80.53,98.87,2.86,0.7566,81.45,0.99,Electronics
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2018-04,Multi-task tri-training,81.45,100.0,0.92,0.2434,81.45,1.0,Electronics
0,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-05,DANN,80.53,93.75,80.53,100,85.9,0.94,Kitchen
1,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2015-11,VFAE,82.93,96.54,2.4,0.4469,85.9,0.97,Kitchen
2,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2017-02,Asymmetric tri-training,83.97,97.75,1.04,0.1937,85.9,0.98,Kitchen
3,1,Sentiment Analysis,Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking,2018-10,Distributional Correspondence Indexing,85.9,100.0,1.93,0.3594,85.9,1.0,Kitchen
0,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2015-06,Impatient Reader,63.8,81.17,63.8,100,78.6,0.81,CNN
1,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2015-06,MemNNs (ensemble),69.4,88.3,5.6,0.3784,78.6,0.88,CNN
2,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2016-03,AS Reader (ensemble model),75.4,95.93,6.0,0.4054,78.6,0.96,CNN
3,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2016-06,GA Reader,77.9,99.11,2.5,0.1689,78.6,0.99,CNN
4,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2017-03,GA+MAGE (32),78.6,100.0,0.7,0.0473,78.6,1.0,CNN
0,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2015-06,Impatient Reader,68.0,84.05,68.0,100,80.9,0.84,Daily\\ Mail
1,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2015-06,Attentive Reader,69.0,85.29,1.0,0.0775,80.9,0.85,Daily\\ Mail
2,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2016-03,AS Reader (ensemble model),77.7,96.04,8.7,0.6744,80.9,0.96,Daily\\ Mail
3,1,Question Answering,CNN / Daily Mail - Question Answering benchmarking,2016-06,GA Reader,80.9,100.0,3.2,0.2481,80.9,1.0,Daily\\ Mail
0,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2015-06,Weiss et al.,92.06,96.15,92.06,100,95.75,0.96,LAS
1,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2016-03,Andor et al.,92.79,96.91,0.73,0.1978,95.75,0.97,LAS
2,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2016-11,Deep Biaffine + RoBERTa,95.75,100.0,2.96,0.8022,95.75,1.0,LAS
3,1,Dependency Parsing,CoNLL-2009 - Dependency Parsing benchmarking,2016-11,Biaffine Parser,85.38,100.0,85.38,100,85.38,0.89,LAS
0,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2015-06,Weiss et al.,94.01,96.63,94.01,100,97.29,0.97,UAS
1,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2016-03,Andor et al.,94.61,97.25,0.6,0.1829,97.29,0.97,UAS
2,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2016-11,Deep Biaffine + RoBERTa,97.29,100.0,2.68,0.8171,97.29,1.0,UAS
3,1,Dependency Parsing,CoNLL-2009 - Dependency Parsing benchmarking,2016-11,Biaffine Parser,88.9,100.0,88.9,100,88.9,0.91,UAS
4,1,Dependency Grammar Induction,WSJ10 - Dependency Grammar Induction benchmarking,2019-07,D-NDMV,75.6,100.0,75.6,100,75.6,0.78,UAS
0,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2015-06,Weiss et al.,97.3,99.32,97.3,100,97.97,0.99,POS
1,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2016-03,BIST transition-based parser,97.44,99.46,0.14,0.209,97.97,0.99,POS
2,1,Dependency Parsing,Penn Treebank - Dependency Parsing benchmarking,2018-07,jPTDP,97.97,100.0,0.53,0.791,97.97,1.0,POS
0,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2015-08,Unlexicalized features,49.4,49.55,49.4,100,99.7,0.5,%\\ Train\\ Accuracy
1,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2015-08,100D LSTM encoders,84.8,85.06,35.4,0.7038,99.7,0.85,%\\ Train\\ Accuracy
2,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2015-08,+ Unigram and bigram features,99.7,100.0,14.9,0.2962,99.7,1.0,%\\ Train\\ Accuracy
0,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2015-08,100D LSTM encoders,220000.0,0.06,220000,100,339000000,0.0,Parameters
1,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2015-09,100D LSTMs w/ word-by-word attention,250000.0,0.07,30000,0.0001,339000000,0.0,Parameters
2,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2015-11,1024D GRU encoders w/ unsupervised \'skip-thoughts\' pre-training,15000000.0,4.42,14750000,0.0435,339000000,0.04,Parameters
3,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2017-05,4096D BiLSTM with max-pooling,40000000.0,11.8,25000000,0.0738,339000000,0.12,Parameters
4,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2017-11,KIM Ensemble,43000000.0,12.68,3000000,0.0089,339000000,0.13,Parameters
5,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2018-02,450D DR-BiLSTM Ensemble,45000000.0,13.27,2000000,0.0059,339000000,0.13,Parameters
6,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2018-05,Densely-Connected Recurrent and Co-Attentive Network Ensemble,53300000.0,15.72,8300000,0.0245,339000000,0.16,Parameters
7,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2018-06,Fine-Tuned LM-Pretrained Transformer,85000000.0,25.07,31700000,0.0936,339000000,0.25,Parameters
8,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2018-09,SJRC (BERT-Large +SRL),308000000.0,90.86,223000000,0.6582,339000000,0.91,Parameters
9,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2019-01,MT-DNN,330000000.0,97.35,22000000,0.0649,339000000,0.97,Parameters
10,1,Natural Language Inference,SNLI - Natural Language Inference benchmarking,2019-09,SemBERT,339000000.0,100.0,9000000,0.0266,339000000,1.0,Parameters
0,1,Relationship extraction using distant supervision,New York Times Corpus - Relationship extraction using distant supervision benchmarking,2015-09,PCNN,61.3,86.46,61.3,100,70.9,0.83,P\\-at\\-10%
1,1,Relationship extraction using distant supervision,New York Times Corpus - Relationship extraction using distant supervision benchmarking,2018-04,BGWA,70.9,100.0,9.6,1.0,70.9,0.96,P\\-at\\-10%
2,1,Relation Extraction,NYT Corpus - Relation Extraction benchmarking,2016-08,PCNN+ATT,69.4,94.29,69.4,100,73.6,0.94,P\\-at\\-10%
3,1,Relation Extraction,NYT Corpus - Relation Extraction benchmarking,2018-12,RESIDE,73.6,100.0,4.2,1.0,73.6,1.0,P\\-at\\-10%
0,1,Relationship extraction using distant supervision,New York Times Corpus - Relationship extraction using distant supervision benchmarking,2015-09,PCNN,46.5,88.74,46.5,100,52.4,0.78,P\\-at\\-30%
1,1,Relationship extraction using distant supervision,New York Times Corpus - Relationship extraction using distant supervision benchmarking,2018-04,BGWA,52.4,100.0,5.9,1.0,52.4,0.88,P\\-at\\-30%
2,1,Relation Extraction,NYT Corpus - Relation Extraction benchmarking,2016-08,PCNN+ATT,51.8,87.06,51.8,100,59.5,0.87,P\\-at\\-30%
3,1,Relation Extraction,NYT Corpus - Relation Extraction benchmarking,2018-12,RESIDE,59.5,100.0,7.7,1.0,59.5,1.0,P\\-at\\-30%
0,1,Sentence Compression,Google Dataset - Sentence Compression benchmarking,2015-09,LSTM,0.38,88.37,0.38,100,0.43,0.88,CR
1,1,Sentence Compression,Google Dataset - Sentence Compression benchmarking,2017-07,BiLSTM,0.43,100.0,0.05,1.0,0.43,1.0,CR
0,1,Text Summarization,GigaWord - Text Summarization benchmarking,2015-09,Abs,30.88,79.38,30.88,100,38.9,0.58,ROUGE\\-1
1,1,Text Summarization,GigaWord - Text Summarization benchmarking,2015-09,Abs+,31.0,79.69,0.12,0.015,38.9,0.58,ROUGE\\-1
2,1,Text Summarization,GigaWord - Text Summarization benchmarking,2016-02,words-lvt5k-1sent,36.4,93.57,5.4,0.6733,38.9,0.69,ROUGE\\-1
3,1,Text Summarization,GigaWord - Text Summarization benchmarking,2017-06,Transformer,37.57,96.58,1.17,0.1459,38.9,0.71,ROUGE\\-1
4,1,Text Summarization,GigaWord - Text Summarization benchmarking,2019-05,MASS,38.73,99.56,1.16,0.1446,38.9,0.73,ROUGE\\-1
5,1,Text Summarization,GigaWord - Text Summarization benchmarking,2019-05,UniLM,38.9,100.0,0.17,0.0212,38.9,0.73,ROUGE\\-1
6,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2015-09,Abs+,28.18,85.78,28.18,100,32.85,0.53,ROUGE\\-1
7,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2016-02,words-lvt5k-1sent,28.61,87.09,0.43,0.0921,32.85,0.54,ROUGE\\-1
8,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2016-06,RAS-Elman,28.97,88.19,0.36,0.0771,32.85,0.55,ROUGE\\-1
9,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2018-08,Seq2seq + selective + MTL + ERAM,29.33,89.28,0.36,0.0771,32.85,0.55,ROUGE\\-1
10,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble,32.85,100.0,3.52,0.7537,32.85,0.62,ROUGE\\-1
11,1,Extractive Text Summarization,DUC 2004 Task 1 - Extractive Text Summarization benchmarking,2015-09,Abs,26.55,100.0,26.55,100,26.55,0.5,ROUGE\\-1
12,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2016-02,LEAD-3,40.42,91.53,40.42,100,44.16,0.76,ROUGE\\-1
13,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2018-08,Bottom-Up Summarization,41.22,93.34,0.8,0.2139,44.16,0.78,ROUGE\\-1
14,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2019-05,UniLM,43.08,97.55,1.86,0.4973,44.16,0.81,ROUGE\\-1
15,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2019-10,BART,44.16,100.0,1.08,0.2888,44.16,0.83,ROUGE\\-1
16,1,Text Summarization,Pubmed - Text Summarization benchmarking,2017-04,Pntr-Gen-Seq2Seq,35.86,100.0,35.86,100,35.86,0.68,ROUGE\\-1
17,1,Text Summarization,arXiv - Text Summarization benchmarking,2017-04,Pntr-Gen-Seq2Seq,32.06,100.0,32.06,100,32.06,0.6,ROUGE\\-1
18,1,Extractive Text Summarization,CNN / Daily Mail - Extractive Text Summarization benchmarking,2017-04,Lead-3 baseline,40.34,93.27,40.34,100,43.25,0.76,ROUGE\\-1
19,1,Extractive Text Summarization,CNN / Daily Mail - Extractive Text Summarization benchmarking,2019-03,BERTSUM,43.25,100.0,2.91,1.0,43.25,0.81,ROUGE\\-1
20,1,Query-Based Extractive Summarization,Debatepedia - Query-Based Extractive Summarization benchmarking,2017-04,SD2,41.26,77.72,41.26,100,53.09,0.78,ROUGE\\-1
21,1,Query-Based Extractive Summarization,Debatepedia - Query-Based Extractive Summarization benchmarking,2018-01,RSA Word Count,53.09,100.0,11.83,1.0,53.09,1.0,ROUGE\\-1
22,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2017-05,"ML + Intra-Attention (Paulus et al., 2017)",38.3,87.34,38.3,100,43.85,0.72,ROUGE\\-1
23,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2017-05,"ML + RL (Paulus et al., 2017)",39.87,90.92,1.57,0.2829,43.85,0.75,ROUGE\\-1
24,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2018-08,Bottom-Up Sum,41.22,94.0,1.35,0.2432,43.85,0.78,ROUGE\\-1
25,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2019-03,BERTSUM+Transformer,43.25,98.63,2.03,0.3658,43.85,0.81,ROUGE\\-1
26,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2019-08,BertSumExt,43.85,100.0,0.6,0.1081,43.85,0.83,ROUGE\\-1
27,1,Multi-Document Summarization,Multi-News - Multi-Document Summarization benchmarking,2018-08,CopyTransformer,43.57,100.0,43.57,100,43.57,0.82,ROUGE\\-1
28,1,Reader-Aware Summarization,RASG - Reader-Aware Summarization benchmarking,2018-12,RASG,30.33,100.0,30.33,100,30.33,0.57,ROUGE\\-1
29,1,Timeline Summarization,MTS - Timeline Summarization benchmarking,2019-08,MTS,39.78,100.0,39.78,100,39.78,0.75,ROUGE\\-1
30,1,Text Summarization,X-Sum - Text Summarization benchmarking,2019-08,BertSumExtAbs,38.81,85.98,38.81,100,45.14,0.73,ROUGE\\-1
31,1,Text Summarization,X-Sum - Text Summarization benchmarking,2019-10,BART,45.14,100.0,6.33,1.0,45.14,0.85,ROUGE\\-1
0,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2015-09,Abs+,8.49,72.07,8.49,100,11.78,0.38,ROUGE\\-2
1,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2016-02,words-lvt5k-1sent,9.42,79.97,0.93,0.2827,11.78,0.42,ROUGE\\-2
2,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2018-08,Seq2seq + selective + MTL + ERAM,10.24,86.93,0.82,0.2492,11.78,0.46,ROUGE\\-2
3,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble,11.78,100.0,1.54,0.4681,11.78,0.53,ROUGE\\-2
4,1,Extractive Text Summarization,DUC 2004 Task 1 - Extractive Text Summarization benchmarking,2015-09,Abs,7.06,100.0,7.06,100,7.06,0.32,ROUGE\\-2
5,1,Text Summarization,GigaWord - Text Summarization benchmarking,2016-02,words-lvt5k-1sent,17.7,88.28,17.7,100,20.05,0.79,ROUGE\\-2
6,1,Text Summarization,GigaWord - Text Summarization benchmarking,2017-06,Transformer,18.9,94.26,1.2,0.5106,20.05,0.85,ROUGE\\-2
7,1,Text Summarization,GigaWord - Text Summarization benchmarking,2018-07,Re^3 Sum,19.03,94.91,0.13,0.0553,20.05,0.85,ROUGE\\-2
8,1,Text Summarization,GigaWord - Text Summarization benchmarking,2019-05,MASS,19.71,98.3,0.68,0.2894,20.05,0.89,ROUGE\\-2
9,1,Text Summarization,GigaWord - Text Summarization benchmarking,2019-05,UniLM,20.05,100.0,0.34,0.1447,20.05,0.9,ROUGE\\-2
10,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2016-02,LEAD-3,17.62,82.8,17.62,100,21.28,0.79,ROUGE\\-2
11,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2018-08,Bottom-Up Summarization,18.68,87.78,1.06,0.2896,21.28,0.84,ROUGE\\-2
12,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2019-05,UniLM,20.43,96.01,1.75,0.4781,21.28,0.92,ROUGE\\-2
13,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2019-10,BART,21.28,100.0,0.85,0.2322,21.28,0.96,ROUGE\\-2
14,1,Extractive Text Summarization,CNN / Daily Mail - Extractive Text Summarization benchmarking,2017-04,Lead-3 baseline,17.7,87.45,17.7,100,20.24,0.79,ROUGE\\-2
15,1,Extractive Text Summarization,CNN / Daily Mail - Extractive Text Summarization benchmarking,2019-03,BERTSUM,20.24,100.0,2.54,1.0,20.24,0.91,ROUGE\\-2
16,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2017-05,"ML + Intra-Attention (Paulus et al., 2017)",14.81,72.49,14.81,100,20.43,0.67,ROUGE\\-2
17,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2017-05,"ML + RL (Paulus et al., 2017)",15.82,77.44,1.01,0.1797,20.43,0.71,ROUGE\\-2
18,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2018-08,Bottom-Up Sum,18.68,91.43,2.86,0.5089,20.43,0.84,ROUGE\\-2
19,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2019-03,BERTSUM+Transformer,20.24,99.07,1.56,0.2776,20.43,0.91,ROUGE\\-2
20,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2019-05,UniLM (Abstractive Summarization),20.43,100.0,0.19,0.0338,20.43,0.92,ROUGE\\-2
21,1,Multi-Document Summarization,Multi-News - Multi-Document Summarization benchmarking,2018-08,CopyTransformer,14.03,98.87,14.03,100,14.19,0.63,ROUGE\\-2
22,1,Multi-Document Summarization,Multi-News - Multi-Document Summarization benchmarking,2018-08,PG-BRNN,14.19,100.0,0.16,1.0,14.19,0.64,ROUGE\\-2
23,1,Text Summarization,X-Sum - Text Summarization benchmarking,2019-08,BertSumExtAbs,16.5,74.09,16.5,100,22.27,0.74,ROUGE\\-2
24,1,Text Summarization,X-Sum - Text Summarization benchmarking,2019-10,BART,22.27,100.0,5.77,1.0,22.27,1.0,ROUGE\\-2
0,1,Extractive Text Summarization,DUC 2004 Task 1 - Extractive Text Summarization benchmarking,2015-09,Abs,22.05,100.0,22.05,100,22.05,0.31,ROUGE\\-L
1,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2015-09,ABS,22.05,77.31,22.05,100,28.52,0.31,ROUGE\\-L
2,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2015-09,Abs+,23.81,83.49,1.76,0.272,28.52,0.34,ROUGE\\-L
3,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2016-02,words-lvt5k-1sent,25.24,88.5,1.43,0.221,28.52,0.36,ROUGE\\-L
4,1,Text Summarization,DUC 2004 Task 1 - Text Summarization benchmarking,2019-04,Transformer+LRPE+PE+Re-ranking+Ensemble,28.52,100.0,3.28,0.507,28.52,0.4,ROUGE\\-L
5,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2016-02,LEAD-3,36.67,89.66,36.67,100,40.9,0.52,ROUGE\\-L
6,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2018-08,Bottom-Up Summarization,38.34,93.74,1.67,0.3948,40.9,0.54,ROUGE\\-L
7,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2019-05,UniLM,40.34,98.63,2.0,0.4728,40.9,0.57,ROUGE\\-L
8,1,Abstractive Text Summarization,CNN / Daily Mail - Abstractive Text Summarization benchmarking,2019-10,BART,40.9,100.0,0.56,0.1324,40.9,0.58,ROUGE\\-L
9,1,Text Summarization,GigaWord - Text Summarization benchmarking,2016-02,words-lvt5k-1sent,33.71,93.64,33.71,100,36.0,0.48,ROUGE\\-L
10,1,Text Summarization,GigaWord - Text Summarization benchmarking,2017-06,Transformer,34.69,96.36,0.98,0.4279,36.0,0.49,ROUGE\\-L
11,1,Text Summarization,GigaWord - Text Summarization benchmarking,2019-05,MASS,35.96,99.89,1.27,0.5546,36.0,0.51,ROUGE\\-L
12,1,Text Summarization,GigaWord - Text Summarization benchmarking,2019-05,UniLM,36.0,100.0,0.04,0.0175,36.0,0.51,ROUGE\\-L
13,1,Extractive Text Summarization,CNN / Daily Mail - Extractive Text Summarization benchmarking,2017-04,Lead-3 baseline,36.57,92.28,36.57,100,39.63,0.52,ROUGE\\-L
14,1,Extractive Text Summarization,CNN / Daily Mail - Extractive Text Summarization benchmarking,2019-03,BERTSUM,39.63,100.0,3.06,1.0,39.63,0.56,ROUGE\\-L
15,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2017-05,"ML + Intra-Attention (Paulus et al., 2017)",35.49,87.98,35.49,100,40.34,0.5,ROUGE\\-L
16,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2017-05,"ML + RL (Paulus et al., 2017)",36.9,91.47,1.41,0.2907,40.34,0.52,ROUGE\\-L
17,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2018-08,Bottom-Up Sum,38.34,95.04,1.44,0.2969,40.34,0.54,ROUGE\\-L
18,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2019-03,BERTSUM+Transformer,39.63,98.24,1.29,0.266,40.34,0.56,ROUGE\\-L
19,1,Document Summarization,CNN / Daily Mail - Document Summarization benchmarking,2019-05,UniLM (Abstractive Summarization),40.34,100.0,0.71,0.1464,40.34,0.57,ROUGE\\-L
20,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2017-12,Gong,66.45,93.82,66.45,100,70.83,0.94,ROUGE\\-L
21,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2018-03,Zhang,70.83,100.0,4.38,1.0,70.83,1.0,ROUGE\\-L
22,1,Paper generation,ACL Title and Abstract Dataset - Paper generation benchmarking,2018-05,Writing-editing Network,20.3,100.0,20.3,100,20.3,0.29,ROUGE\\-L
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2015-12,TD-LSTM,71.88,83.35,71.88,100,86.24,0.83,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
1,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2016-05,MemNet,76.58,88.8,4.7,0.3273,86.24,0.89,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
2,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2017-09,RAM,77.36,89.7,0.78,0.0543,86.24,0.9,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
3,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-02,LCR-Rot,78.29,90.78,0.93,0.0648,86.24,0.91,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
4,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-04,SA-LSTM-P,78.35,90.85,0.06,0.0042,86.24,0.91,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
5,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-05,TNet-LF,78.4,90.91,0.05,0.0035,86.24,0.91,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
6,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-10,HAPN,79.75,92.47,1.35,0.094,86.24,0.92,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
7,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-02,BERT-SPC,81.73,94.77,1.98,0.1379,86.24,0.95,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
8,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-06,SDGCN-BERT,82.46,95.62,0.73,0.0508,86.24,0.96,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
9,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-08,BERT-ADA,84.06,97.47,1.6,0.1114,86.24,0.97,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
10,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-12,LCF-ATEPC,86.24,100.0,2.18,0.1518,86.24,1.0,Mean\\ Acc\\ \\(Restaurant\\ \\+\\ Laptop\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2015-12,TD-LSTM,75.63,83.87,75.63,100,90.18,0.84,Restaurant\\ \\(Acc\\)
1,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2016-05,MemNet,80.95,89.76,5.32,0.3656,90.18,0.9,Restaurant\\ \\(Acc\\)
2,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-02,LCR-Rot,81.34,90.2,0.39,0.0268,90.18,0.9,Restaurant\\ \\(Acc\\)
3,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-04,SA-LSTM-P,81.6,90.49,0.26,0.0179,90.18,0.9,Restaurant\\ \\(Acc\\)
4,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-10,HAPN,82.23,91.18,0.63,0.0433,90.18,0.91,Restaurant\\ \\(Acc\\)
5,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-02,BERT-SPC,84.46,93.66,2.23,0.1533,90.18,0.94,Restaurant\\ \\(Acc\\)
6,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-04,BERT-PT,84.95,94.2,0.49,0.0337,90.18,0.94,Restaurant\\ \\(Acc\\)
7,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-08,BERT-ADA,87.89,97.46,2.94,0.2021,90.18,0.97,Restaurant\\ \\(Acc\\)
8,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-12,LCF-ATEPC,90.18,100.0,2.29,0.1574,90.18,1.0,Restaurant\\ \\(Acc\\)
9,1,Aspect-Based Sentiment Analysis,SemEval-2016 Task 5 Subtask 1 - Aspect-Based Sentiment Analysis benchmarking,2019-03,HAABSA,88.0,100.0,88,100,88,0.98,Restaurant\\ \\(Acc\\)
10,1,Aspect-Based Sentiment Analysis,SemEval 2015 Task 12 - Aspect-Based Sentiment Analysis benchmarking,2019-03,HAABSA,80.6,98.65,80.6,100,81.7,0.89,Restaurant\\ \\(Acc\\)
11,1,Aspect-Based Sentiment Analysis,SemEval 2015 Task 12 - Aspect-Based Sentiment Analysis benchmarking,2020-04,HAABSA++,81.7,100.0,1.1,1.0,81.7,0.91,Restaurant\\ \\(Acc\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2015-12,TD-LSTM,68.13,82.79,68.13,100,82.29,0.83,Laptop\\ \\(Acc\\)
1,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2016-05,MemNet,72.21,87.75,4.08,0.2881,82.29,0.88,Laptop\\ \\(Acc\\)
2,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2017-09,RAM,74.49,90.52,2.28,0.161,82.29,0.91,Laptop\\ \\(Acc\\)
3,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-02,LCR-Rot,75.24,91.43,0.75,0.053,82.29,0.91,Laptop\\ \\(Acc\\)
4,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-05,TNet-LF,76.01,92.37,0.77,0.0544,82.29,0.92,Laptop\\ \\(Acc\\)
5,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2018-10,HAPN,77.27,93.9,1.26,0.089,82.29,0.94,Laptop\\ \\(Acc\\)
6,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-02,BERT-SPC,78.99,95.99,1.72,0.1215,82.29,0.96,Laptop\\ \\(Acc\\)
7,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-02,AEN-BERT,79.93,97.13,0.94,0.0664,82.29,0.97,Laptop\\ \\(Acc\\)
8,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-06,SDGCN-BERT,81.35,98.86,1.42,0.1003,82.29,0.99,Laptop\\ \\(Acc\\)
9,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 2 - Aspect-Based Sentiment Analysis benchmarking,2019-12,LCF-ATEPC,82.29,100.0,0.94,0.0664,82.29,1.0,Laptop\\ \\(Acc\\)
0,1,Machine Translation,IWSLT2015 English-Vietnamese - Machine Translation benchmarking,2015-12,LSTM+Attention+Ensemble,26.4,79.35,26.4,100,33.27,0.38,BLEU
1,1,Machine Translation,IWSLT2015 English-Vietnamese - Machine Translation benchmarking,2018-06,DeconvDec,28.47,85.57,2.07,0.3013,33.27,0.42,BLEU
2,1,Machine Translation,IWSLT2015 English-Vietnamese - Machine Translation benchmarking,2018-08,Self-Adaptive Control of Temperature,29.12,87.53,0.65,0.0946,33.27,0.42,BLEU
3,1,Machine Translation,IWSLT2015 English-Vietnamese - Machine Translation benchmarking,2018-09,CVT,29.6,88.97,0.48,0.0699,33.27,0.43,BLEU
4,1,Machine Translation,IWSLT2015 English-Vietnamese - Machine Translation benchmarking,2019-10,Transformer+BPE+FixNorm+ScaleNorm,32.8,98.59,3.2,0.4658,33.27,0.48,BLEU
5,1,Machine Translation,IWSLT2015 English-Vietnamese - Machine Translation benchmarking,2019-10,Transformer+BPE-dropout,33.27,100.0,0.47,0.0684,33.27,0.48,BLEU
6,1,Table-to-Text Generation,WikiBio - Table-to-Text Generation benchmarking,2016-03,Table NLM,34.7,77.3,34.7,100,44.89,0.51,BLEU
7,1,Table-to-Text Generation,WikiBio - Table-to-Text Generation benchmarking,2017-11,Field-gating Seq2seq + dual attention,44.89,100.0,10.19,1.0,44.89,0.65,BLEU
8,1,Data-to-Text Generation,RotoWire - Data-to-Text Generation benchmarking,2017-07,Encoder-decoder + conditional copy,14.19,81.09,14.19,100,17.5,0.21,BLEU
9,1,Data-to-Text Generation,RotoWire - Data-to-Text Generation benchmarking,2018-09,Neural Content Planning + conditional copy,16.5,94.29,2.31,0.6979,17.5,0.24,BLEU
10,1,Data-to-Text Generation,RotoWire - Data-to-Text Generation benchmarking,2019-12,Hierarchical transformer encoder + conditional copy,17.5,100.0,1.0,0.3021,17.5,0.26,BLEU
11,1,Machine Translation,WMT 2017 Latvian-English - Machine Translation benchmarking,2017-09,mLSTM with factored data,20.8,85.35,20.8,100,24.37,0.3,BLEU
12,1,Machine Translation,WMT 2017 Latvian-English - Machine Translation benchmarking,2018-10,Transformer trained on highly filtered data,24.37,100.0,3.57,1.0,24.37,0.36,BLEU
13,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2017-12,Gong,64.22,93.62,64.22,100,68.6,0.94,BLEU
14,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2018-03,Zhang,65.45,95.41,1.23,0.2808,68.6,0.95,BLEU
15,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2018-04,Sys1-Primary,65.61,95.64,0.16,0.0365,68.6,0.96,BLEU
16,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2018-05,Slug,66.19,96.49,0.58,0.1324,68.6,0.96,BLEU
17,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2019-04,S_1^R,68.6,100.0,2.41,0.5502,68.6,1.0,BLEU
18,1,Unsupervised Machine Translation,WMT2016 English-German - Unsupervised Machine Translation benchmarking,2018-04,PBSMT + NMT,20.2,71.38,20.2,100,28.3,0.29,BLEU
19,1,Unsupervised Machine Translation,WMT2016 English-German - Unsupervised Machine Translation benchmarking,2019-01,SMT as posterior regularization,21.7,76.68,1.5,0.1852,28.3,0.32,BLEU
20,1,Unsupervised Machine Translation,WMT2016 English-German - Unsupervised Machine Translation benchmarking,2019-01,MLM pretraining for encoder and decoder,26.4,93.29,4.7,0.5802,28.3,0.38,BLEU
21,1,Unsupervised Machine Translation,WMT2016 English-German - Unsupervised Machine Translation benchmarking,2019-02,SMT + NMT (tuning and joint refinement),26.9,95.05,0.5,0.0617,28.3,0.39,BLEU
22,1,Unsupervised Machine Translation,WMT2016 English-German - Unsupervised Machine Translation benchmarking,2019-05,MASS (6-layer Transformer),28.3,100.0,1.4,0.1728,28.3,0.41,BLEU
23,1,Unsupervised Machine Translation,WMT2014 English-French - Unsupervised Machine Translation benchmarking,2018-04,PBSMT + NMT,27.6,73.6,27.6,100,37.5,0.4,BLEU
24,1,Unsupervised Machine Translation,WMT2014 English-French - Unsupervised Machine Translation benchmarking,2019-01,SMT as posterior regularization,29.5,78.67,1.9,0.1919,37.5,0.43,BLEU
25,1,Unsupervised Machine Translation,WMT2014 English-French - Unsupervised Machine Translation benchmarking,2019-01,MLM pretraining for encoder and decoder,33.4,89.07,3.9,0.3939,37.5,0.49,BLEU
26,1,Unsupervised Machine Translation,WMT2014 English-French - Unsupervised Machine Translation benchmarking,2019-02,SMT + NMT (tuning and joint refinement),36.2,96.53,2.8,0.2828,37.5,0.53,BLEU
27,1,Unsupervised Machine Translation,WMT2014 English-French - Unsupervised Machine Translation benchmarking,2019-05,MASS (6-layer Transformer),37.5,100.0,1.3,0.1313,37.5,0.55,BLEU
28,1,Unsupervised Machine Translation,WMT2014 French-English - Unsupervised Machine Translation benchmarking,2018-04,PBSMT + NMT,27.7,79.37,27.7,100,34.9,0.4,BLEU
29,1,Unsupervised Machine Translation,WMT2014 French-English - Unsupervised Machine Translation benchmarking,2019-01,SMT as posterior regularization,28.9,82.81,1.2,0.1667,34.9,0.42,BLEU
30,1,Unsupervised Machine Translation,WMT2014 French-English - Unsupervised Machine Translation benchmarking,2019-01,MLM pretraining for encoder and decoder,33.3,95.42,4.4,0.6111,34.9,0.49,BLEU
31,1,Unsupervised Machine Translation,WMT2014 French-English - Unsupervised Machine Translation benchmarking,2019-02,SMT + NMT (tuning and joint refinement),33.5,95.99,0.2,0.0278,34.9,0.49,BLEU
32,1,Unsupervised Machine Translation,WMT2014 French-English - Unsupervised Machine Translation benchmarking,2019-05,MASS (6-layer Transformer),34.9,100.0,1.4,0.1944,34.9,0.51,BLEU
33,1,Unsupervised Machine Translation,WMT2016 German-English - Unsupervised Machine Translation benchmarking,2018-04,PBSMT,25.2,71.59,25.2,100,35.2,0.37,BLEU
34,1,Unsupervised Machine Translation,WMT2016 German-English - Unsupervised Machine Translation benchmarking,2018-10,Synthetic bilingual data init,26.7,75.85,1.5,0.15,35.2,0.39,BLEU
35,1,Unsupervised Machine Translation,WMT2016 German-English - Unsupervised Machine Translation benchmarking,2019-01,MLM pretraining for encoder and decoder,34.3,97.44,7.6,0.76,35.2,0.5,BLEU
36,1,Unsupervised Machine Translation,WMT2016 German-English - Unsupervised Machine Translation benchmarking,2019-02,SMT + NMT (tuning and joint refinement),34.4,97.73,0.1,0.01,35.2,0.5,BLEU
37,1,Unsupervised Machine Translation,WMT2016 German-English - Unsupervised Machine Translation benchmarking,2019-05,MASS (6-layer Transformer),35.2,100.0,0.8,0.08,35.2,0.51,BLEU
38,1,Machine Translation,ACCURAT balanced test corpus for under resourced languages Estonian-Russian - Machine Translation benchmarking,2018-05,Multilingual Transformer,19.18,100.0,19.18,100,19.18,0.28,BLEU
39,1,Machine Translation,ACCURAT balanced test corpus for under resourced languages Russian-Estonian - Machine Translation benchmarking,2018-05,Multilingual Transformer,18.03,100.0,18.03,100,18.03,0.26,BLEU
40,1,Graph-to-Sequence,LDC2015E86: - Graph-to-Sequence benchmarking,2018-05,GRN,33.6,100.0,33.6,100,33.6,0.49,BLEU
41,1,Text Generation,LDC2016E25 - Text Generation benchmarking,2018-05,Graph2Seq,22.0,100.0,22,100,22,0.32,BLEU
42,1,KB-to-Language Generation,Wikipedia Person and Animal Dataset - KB-to-Language Generation benchmarking,2018-09,KB-to-Language Generation Model,23.2,100.0,23.2,100,23.2,0.34,BLEU
43,1,Table-to-Text Generation,Wikipedia Person and Animal Dataset - Table-to-Text Generation benchmarking,2018-09,KB-to-Language Generation Model,23.2,100.0,23.2,100,23.2,0.34,BLEU
44,1,Machine Translation,WMT 2018 English-Estonian - Machine Translation benchmarking,2018-10,Multi-pass backtranslated adapted transformer,24.1,100.0,24.1,100,24.1,0.35,BLEU
45,1,Machine Translation,WMT 2018 Estonian-English - Machine Translation benchmarking,2018-10,Multi-pass backtranslated adapted transformer,29.0,100.0,29,100,29,0.42,BLEU
46,1,Code Generation,CoNaLa-Ext - Code Generation benchmarking,2018-10,TranX,18.85,100.0,18.85,100,18.85,0.27,BLEU
47,1,Code Generation,CoNaLa - Code Generation benchmarking,2018-10,TranX,24.3,100.0,24.3,100,24.3,0.35,BLEU
48,1,Machine Translation,WMT 2018 English-Finnish - Machine Translation benchmarking,2018-10,Transformer trained on highly filtered data,17.4,100.0,17.4,100,17.4,0.25,BLEU
49,1,Machine Translation,WMT 2017 English-Latvian - Machine Translation benchmarking,2018-10,Transformer trained on highly filtered data,22.89,100.0,22.89,100,22.89,0.33,BLEU
50,1,Machine Translation,WMT 2018 Finnish-English - Machine Translation benchmarking,2018-10,Transformer trained on highly filtered data,24.0,90.57,24.0,100,26.5,0.35,BLEU
51,1,Machine Translation,WMT 2018 Finnish-English - Machine Translation benchmarking,2019-06,CT+B/S construction,26.5,100.0,2.5,1.0,26.5,0.39,BLEU
52,1,Data-to-Text Generation,WebNLG - Data-to-Text Generation benchmarking,2018-10,GCN EC,0.559,100.0,0.559,100,0.559,0.01,BLEU
53,1,Data-to-Text Generation,SR11Deep - Data-to-Text Generation benchmarking,2018-10,GCN + feat,0.666,100.0,0.666,100,0.666,0.01,BLEU
54,1,Unsupervised Machine Translation,WMT2014 German-English - Unsupervised Machine Translation benchmarking,2019-01,SMT as posterior regularization,20.4,75.56,20.4,100,27.0,0.3,BLEU
55,1,Unsupervised Machine Translation,WMT2014 German-English - Unsupervised Machine Translation benchmarking,2019-02,SMT + NMT (tuning and joint refinement),27.0,100.0,6.6,1.0,27.0,0.39,BLEU
56,1,Unsupervised Machine Translation,WMT2014 English-German - Unsupervised Machine Translation benchmarking,2019-01,SMT as posterior regularization,17.0,75.56,17.0,100,22.5,0.25,BLEU
57,1,Unsupervised Machine Translation,WMT2014 English-German - Unsupervised Machine Translation benchmarking,2019-02,SMT + NMT (tuning and joint refinement),22.5,100.0,5.5,1.0,22.5,0.33,BLEU
58,1,Unsupervised Machine Translation,WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking,2019-01,MLM pretraining for encoder and decoder,31.8,96.07,31.8,100,33.1,0.46,BLEU
59,1,Unsupervised Machine Translation,WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking,2019-05,MASS (6-layer Transformer),33.1,100.0,1.3,1.0,33.1,0.48,BLEU
60,1,Question Answering,JD Product Question Answer - Question Answering benchmarking,2019-01,PAAG,2.0189,100.0,2.0189,100,2.0189,0.03,BLEU
61,1,Machine Translation,WMT2017 Finnish-English - Machine Translation benchmarking,2019-06,CT+B/S construction,35.5,100.0,35.5,100,35.5,0.52,BLEU
62,1,Machine Translation,WMT2016 Finnish-English - Machine Translation benchmarking,2019-06,CT+B/S construction,32.4,100.0,32.4,100,32.4,0.47,BLEU
63,1,Machine Translation,WMT2019 Finnish-English - Machine Translation benchmarking,2019-06,CT+B/S construction,34.1,100.0,34.1,100,34.1,0.5,BLEU
64,1,Data-to-Text Generation,WebNLG Full - Data-to-Text Generation benchmarking,2019-08,Transformer (Pipeline),51.68,100.0,51.68,100,51.68,0.75,BLEU
65,1,Data-to-Text Generation,ViGGO - Data-to-Text Generation benchmarking,2019-10,Bo3,52.1,100.0,52.1,100,52.1,0.76,BLEU
66,1,Data-to-Text Generation,Cleaned E2E NLG Challenge - Data-to-Text Generation benchmarking,2019-11,TGen,40.73,100.0,40.73,100,40.73,0.59,BLEU
67,1,Machine Translation,IWSLT2015 Chinese-English - Machine Translation benchmarking,2019-11,BP-Transformer,19.84,100.0,19.84,100,19.84,0.29,BLEU
0,1,Entity Disambiguation,TAC2010 - Entity Disambiguation benchmarking,2016-01,Wikipedia2Vec,85.2,97.15,85.2,100,87.7,0.97,Micro\\ Precision
1,1,Entity Disambiguation,TAC2010 - Entity Disambiguation benchmarking,2017-05,NTEE,87.7,100.0,2.5,1.0,87.7,1.0,Micro\\ Precision
0,1,Entity Disambiguation,AIDA-CoNLL - Entity Disambiguation benchmarking,2016-01,Wikipedia2Vec-GBRT,93.1,98.0,93.1,100,95.0,0.98,In\\-KB\\ Accuracy
1,1,Entity Disambiguation,AIDA-CoNLL - Entity Disambiguation benchmarking,2017-05,NTEE,94.7,99.68,1.6,0.8421,95.0,1.0,In\\-KB\\ Accuracy
2,1,Entity Disambiguation,AIDA-CoNLL - Entity Disambiguation benchmarking,2019-09,confidence-order,95.0,100.0,0.3,0.1579,95.0,1.0,In\\-KB\\ Accuracy
0,1,Language Modelling,Text8 - Language Modelling benchmarking,2016-02,"td-LSTM (Zhang et al., 2016)",1.63,100.0,1.63,100,1.63,1.0,Bit\\ per\\ Character\\ \\(BPC\\)
1,1,Language Modelling,enwik8 - Language Modelling benchmarking,2016-07,Recurrent Highway Networks,1.27,94.78,1.27,100,1.34,0.78,Bit\\ per\\ Character\\ \\(BPC\\)
2,1,Language Modelling,enwik8 - Language Modelling benchmarking,2016-09,LN HM-LSTM,1.32,98.51,0.05,0.7143,1.34,0.81,Bit\\ per\\ Character\\ \\(BPC\\)
3,1,Language Modelling,enwik8 - Language Modelling benchmarking,2016-09,Hypernetworks,1.34,100.0,0.02,0.2857,1.34,0.82,Bit\\ per\\ Character\\ \\(BPC\\)
4,1,Language Modelling,Hutter Prize - Language Modelling benchmarking,2016-07,RHN - depth 5 [zilly2016recurrent],1.31,100.0,1.31,100,1.31,0.8,Bit\\ per\\ Character\\ \\(BPC\\)
0,1,Question Answering,Children's Book Test - Question Answering benchmarking,2016-03,AS reader (greedy),67.5,72.35,67.5,100,93.3,0.72,Accuracy\\-CN
1,1,Question Answering,Children's Book Test - Question Answering benchmarking,2016-03,AS reader (avg),68.9,73.85,1.4,0.0543,93.3,0.74,Accuracy\\-CN
2,1,Question Answering,Children's Book Test - Question Answering benchmarking,2016-06,GA + feature + fix L(w),70.7,75.78,1.8,0.0698,93.3,0.76,Accuracy\\-CN
3,1,Question Answering,Children's Book Test - Question Answering benchmarking,2016-06,NSE,71.9,77.06,1.2,0.0465,93.3,0.77,Accuracy\\-CN
4,1,Question Answering,Children's Book Test - Question Answering benchmarking,2019-02,GPT-2,93.3,100.0,21.4,0.8295,93.3,1.0,Accuracy\\-CN
0,1,Question Answering,Children's Book Test - Question Answering benchmarking,2016-03,AS reader (greedy),71.0,79.73,71.0,100,89.05,0.8,Accuracy\\-NE
1,1,Question Answering,Children's Book Test - Question Answering benchmarking,2016-06,GA + feature + fix L(w),74.9,84.11,3.9,0.2161,89.05,0.84,Accuracy\\-NE
2,1,Question Answering,Children's Book Test - Question Answering benchmarking,2019-02,GPT-2,89.05,100.0,14.15,0.7839,89.05,1.0,Accuracy\\-NE
0,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2016-03,ASR,41.3,66.4,41.3,100,62.2,0.66,Unigram\\ Acc
1,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-01,AMANDA,46.8,75.24,5.5,0.2632,62.2,0.75,Unigram\\ Acc
2,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-10,Bi-Attention + DCU-LSTM,49.4,79.42,2.6,0.1244,62.2,0.79,Unigram\\ Acc
3,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-11,DecaProp,62.2,100.0,12.8,0.6124,62.2,1.0,Unigram\\ Acc
0,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2016-03,ASR,22.8,32.2,22.8,100,70.8,0.32,N\\-gram\\ F1
1,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-01,AMANDA,56.6,79.94,33.8,0.7042,70.8,0.8,N\\-gram\\ F1
2,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-10,Bi-Attention + DCU-LSTM,59.5,84.04,2.9,0.0604,70.8,0.84,N\\-gram\\ F1
3,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-11,DecaProp,70.8,100.0,11.3,0.2354,70.8,1.0,N\\-gram\\ F1
0,1,Table-to-Text Generation,WikiBio - Table-to-Text Generation benchmarking,2016-03,Table NLM,-25.8,100.0,-25.8,100,-25.8,1.1,ROUGE
1,1,Table-to-Text Generation,Wikipedia Person and Animal Dataset - Table-to-Text Generation benchmarking,2018-09,KB-to-Language Generation Model,-23.4,100.0,-23.4,100,-23.4,1.0,ROUGE
2,1,KB-to-Language Generation,Wikipedia Person and Animal Dataset - KB-to-Language Generation benchmarking,2018-09,KB-to-Language Generation Model,-42.0,100.0,-42,100,-42,1.79,ROUGE
3,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",-49.0,100.0,-49,100,-49,2.09,ROUGE
0,1,Part-Of-Speech Tagging,UD - Part-Of-Speech Tagging benchmarking,2016-04,Bi-LSTM,96.4,99.5,96.4,100,96.88,1.0,Avg\\ accuracy
1,1,Part-Of-Speech Tagging,UD - Part-Of-Speech Tagging benchmarking,2017-11,Adversarial Bi-LSTM,96.65,99.76,0.25,0.5208,96.88,1.0,Avg\\ accuracy
2,1,Part-Of-Speech Tagging,UD - Part-Of-Speech Tagging benchmarking,2019-08,BiLSTM-LAN,96.88,100.0,0.23,0.4792,96.88,1.0,Avg\\ accuracy
0,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,43.51,78.88,43.51,100,55.16,0.61,R\\-at\\-1
1,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-09,AMEM,48.53,87.98,5.02,0.4309,55.16,0.68,R\\-at\\-1
2,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,50.29,91.17,1.76,0.1511,55.16,0.71,R\\-at\\-1
3,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),50.92,92.31,0.63,0.0541,55.16,0.71,R\\-at\\-1
4,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,HACAN,54.76,99.27,3.84,0.3296,55.16,0.77,R\\-at\\-1
5,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),55.16,100.0,0.4,0.0343,55.16,0.77,R\\-at\\-1
6,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,44.15,79.34,44.15,100,55.65,0.62,R\\-at\\-1
7,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),47.55,85.44,3.4,0.2957,55.65,0.67,R\\-at\\-1
8,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,49.63,89.18,2.08,0.1809,55.65,0.7,R\\-at\\-1
9,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,50.88,91.43,1.25,0.1087,55.65,0.71,R\\-at\\-1
10,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),55.65,100.0,4.77,0.4148,55.65,0.78,R\\-at\\-1
11,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2018-05,BAN (Bottom-Up detector),69.69,97.7,69.69,100,71.33,0.98,R\\-at\\-1
12,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2019-08,VisualBERT,71.33,100.0,1.64,1.0,71.33,1.0,R\\-at\\-1
13,1,Conversational Response Selection,Advising Corpus - Conversational Response Selection benchmarking,2019-01,CtxDec & -Rev,31.0,100.0,31,100,31,0.43,R\\-at\\-1
14,1,Phrase Grounding,Flickr30k Entities Dev - Phrase Grounding benchmarking,2019-08,VisualBERT,70.4,100.0,70.4,100,70.4,0.99,R\\-at\\-1
0,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,83.96,90.33,83.96,100,92.95,0.89,R\\-at\\-10
1,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-09,AMEM,87.43,94.06,3.47,0.386,92.95,0.93,R\\-at\\-10
2,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,88.81,95.55,1.38,0.1535,92.95,0.94,R\\-at\\-10
3,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,HACAN,90.68,97.56,1.87,0.208,92.95,0.96,R\\-at\\-10
4,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),92.95,100.0,2.27,0.2525,92.95,0.99,R\\-at\\-10
5,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,86.88,92.38,86.88,100,94.05,0.92,R\\-at\\-10
6,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),88.8,94.42,1.92,0.2678,94.05,0.94,R\\-at\\-10
7,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,89.35,95.0,0.55,0.0767,94.05,0.95,R\\-at\\-10
8,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,89.45,95.11,0.1,0.0139,94.05,0.95,R\\-at\\-10
9,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),94.05,100.0,4.6,0.6416,94.05,1.0,R\\-at\\-10
10,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2018-05,BAN (Bottom-Up detector),86.35,99.82,86.35,100,86.51,0.92,R\\-at\\-10
11,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2019-08,VisualBERT,86.51,100.0,0.16,1.0,86.51,0.92,R\\-at\\-10
12,1,Conversational Response Selection,Advising Corpus - Conversational Response Selection benchmarking,2019-01,CtxDec & -Rev,78.8,100.0,78.8,100,78.8,0.84,R\\-at\\-10
13,1,Phrase Grounding,Flickr30k Entities Dev - Phrase Grounding benchmarking,2019-08,VisualBERT,86.31,100.0,86.31,100,86.31,0.92,R\\-at\\-10
0,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,74.49,86.36,74.49,100,86.26,0.86,R\\-at\\-5
1,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-09,AMEM,78.66,91.19,4.17,0.3543,86.26,0.91,R\\-at\\-5
2,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2017-11,CoAtt,80.71,93.57,2.05,0.1742,86.26,0.93,R\\-at\\-5
3,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-02,HACAN,83.03,96.26,2.32,0.1971,86.26,0.96,R\\-at\\-5
4,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2019-04,9xFGA (VGG),86.26,100.0,3.23,0.2744,86.26,0.99,R\\-at\\-5
5,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,76.88,88.64,76.88,100,86.73,0.89,R\\-at\\-5
6,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),78.1,90.05,1.22,0.1239,86.73,0.9,R\\-at\\-5
7,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,79.75,91.95,1.65,0.1675,86.73,0.92,R\\-at\\-5
8,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,80.63,92.97,0.88,0.0893,86.73,0.93,R\\-at\\-5
9,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),86.73,100.0,6.1,0.6193,86.73,1.0,R\\-at\\-5
10,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2018-05,BAN (Bottom-Up detector),84.22,99.11,84.22,100,84.98,0.97,R\\-at\\-5
11,1,Phrase Grounding,Flickr30k Entities Test - Phrase Grounding benchmarking,2019-08,VisualBERT,84.98,100.0,0.76,1.0,84.98,0.98,R\\-at\\-5
12,1,Phrase Grounding,Flickr30k Entities Dev - Phrase Grounding benchmarking,2019-08,VisualBERT,84.49,100.0,84.49,100,84.49,0.97,R\\-at\\-5
0,1,Visual Dialog,VisDial v0.9 val - Visual Dialog benchmarking,2016-05,HieCoAtt-QI,5.84,100.0,5.84,100,5.84,1.0,Mean\\ Rank
0,1,Grammatical Error Correction,CoNLL-2014 Shared Task - Grammatical Error Correction benchmarking,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),71.57,100.0,71.57,100,71.57,0.81,Precision
1,1,Relation Extraction,FewRel - Relation Extraction benchmarking,2019-05,ERNIE,88.49,100.0,88.49,100,88.49,1.0,Precision
2,1,Entity Typing,Open Entity - Entity Typing benchmarking,2019-05,ERNIE,78.42,100.0,78.42,100,78.42,0.89,Precision
3,1,Text Classification,20NEWS - Text Classification benchmarking,2019-11,SCDV-MS,86.2,100.0,86.2,100,86.2,0.97,Precision
4,1,Named Entity Recognition,SoSciSoCi - Named Entity Recognition benchmarking,2020-03,Bi-LSTM-CRF (SSC->GSC),0.83,100.0,0.83,100,0.83,0.01,Precision
0,1,Grammatical Error Correction,CoNLL-2014 Shared Task - Grammatical Error Correction benchmarking,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),38.65,100.0,38.65,100,38.65,0.44,Recall
1,1,Relation Extraction,FewRel - Relation Extraction benchmarking,2019-05,ERNIE,88.44,100.0,88.44,100,88.44,1.0,Recall
2,1,Entity Typing,Open Entity - Entity Typing benchmarking,2019-05,ERNIE,72.9,100.0,72.9,100,72.9,0.82,Recall
3,1,Text Classification,20NEWS - Text Classification benchmarking,2019-11,SCDV-MS,86.18,100.0,86.18,100,86.18,0.97,Recall
4,1,Named Entity Recognition,SoSciSoCi - Named Entity Recognition benchmarking,2020-03,Bi-LSTM-CRF (SSC->GSC),0.82,100.0,0.82,100,0.82,0.01,Recall
0,1,Open-Domain Question Answering,Quasar - Open-Domain Question Answering benchmarking,2016-06,GA,26.4,62.41,26.4,100,42.3,0.62,EM\\ \\(Quasar\\-T\\)
1,1,Open-Domain Question Answering,Quasar - Open-Domain Question Answering benchmarking,2017-11,Evidence Aggregation via R^3 Re-Ranking,42.3,100.0,15.9,1.0,42.3,1.0,EM\\ \\(Quasar\\-T\\)
0,1,Open-Domain Question Answering,Quasar - Open-Domain Question Answering benchmarking,2016-06,GA,26.4,53.23,26.4,100,49.6,0.53,F1\\ \\(Quasar\\-T\\)
1,1,Open-Domain Question Answering,Quasar - Open-Domain Question Answering benchmarking,2016-11,BiDAF,28.5,57.46,2.1,0.0905,49.6,0.57,F1\\ \\(Quasar\\-T\\)
2,1,Open-Domain Question Answering,Quasar - Open-Domain Question Answering benchmarking,2017-11,Evidence Aggregation via R^3 Re-Ranking,49.6,100.0,21.1,0.9095,49.6,1.0,F1\\ \\(Quasar\\-T\\)
0,1,Dialog State Tracking,Wizard-of-Oz - Dialog State Tracking benchmarking,2016-06,Neural belief tracker,84.4,94.94,84.4,100,88.9,0.95,Joint
1,1,Dialog State Tracking,Wizard-of-Oz - Dialog State Tracking benchmarking,2018-05,Zhong et al.,88.1,99.1,3.7,0.8222,88.9,0.99,Joint
2,1,Dialog State Tracking,Wizard-of-Oz - Dialog State Tracking benchmarking,2018-10,StateNet,88.9,100.0,0.8,0.1778,88.9,1.0,Joint
3,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2016-06,Neural belief tracker,73.4,97.22,73.4,100,75.5,0.83,Joint
4,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2018-05,Zhong et al.,74.5,98.68,1.1,0.5238,75.5,0.84,Joint
5,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2018-10,StateNet,75.5,100.0,1.0,0.4762,75.5,0.85,Joint
0,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2016-06,Neural belief tracker,90.0,100.0,90,100,90,1.0,Area
0,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2016-06,Neural belief tracker,84.0,100.0,84,100,84,1.0,Food
0,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2016-06,Neural belief tracker,94.0,100.0,94,100,94,1.0,Price
0,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2016-06,Neural belief tracker,96.5,98.97,96.5,100,97.5,0.99,Request
1,1,Dialog State Tracking,Second dialogue state tracking challenge - Dialog State Tracking benchmarking,2018-05,Zhong et al.,97.5,100.0,1.0,1.0,97.5,1.0,Request
2,1,Dialog State Tracking,Wizard-of-Oz - Dialog State Tracking benchmarking,2016-06,Neural belief tracker,96.5,99.38,96.5,100,97.1,0.99,Request
3,1,Dialog State Tracking,Wizard-of-Oz - Dialog State Tracking benchmarking,2018-05,Zhong et al.,97.1,100.0,0.6,1.0,97.1,1.0,Request
0,1,Grammatical Error Detection,CoNLL-2014 A1 - Grammatical Error Detection benchmarking,2016-07,Bi-LSTM (unrestricted data),34.3,95.01,34.3,100,36.1,0.56,F0\\.5
1,1,Grammatical Error Detection,CoNLL-2014 A1 - Grammatical Error Detection benchmarking,2017-07,Bi-LSTM + POS (unrestricted data),36.1,100.0,1.8,1.0,36.1,0.59,F0\\.5
2,1,Grammatical Error Detection,FCE - Grammatical Error Detection benchmarking,2016-07,Bi-LSTM,41.1,78.93,41.1,100,52.07,0.67,F0\\.5
3,1,Grammatical Error Detection,FCE - Grammatical Error Detection benchmarking,2016-11,Bi-LSTM + charattn,41.88,80.43,0.78,0.0711,52.07,0.68,F0\\.5
4,1,Grammatical Error Detection,FCE - Grammatical Error Detection benchmarking,2017-04,Bi-LSTM + LMcost,48.48,93.11,6.6,0.6016,52.07,0.79,F0\\.5
5,1,Grammatical Error Detection,FCE - Grammatical Error Detection benchmarking,2017-07,Ann+PAT+MT,49.11,94.32,0.63,0.0574,52.07,0.8,F0\\.5
6,1,Grammatical Error Detection,FCE - Grammatical Error Detection benchmarking,2018-11,BiLSTM-JOINT,52.07,100.0,2.96,0.2698,52.07,0.85,F0\\.5
7,1,Grammatical Error Detection,CoNLL-2014 A2 - Grammatical Error Detection benchmarking,2016-07,Bi-LSTM (trained on FCE),23.9,52.99,23.9,100,45.1,0.39,F0\\.5
8,1,Grammatical Error Detection,CoNLL-2014 A2 - Grammatical Error Detection benchmarking,2016-07,Bi-LSTM (unrestricted data),44.0,97.56,20.1,0.9481,45.1,0.72,F0\\.5
9,1,Grammatical Error Detection,CoNLL-2014 A2 - Grammatical Error Detection benchmarking,2017-07,Bi-LSTM + POS (unrestricted data),45.1,100.0,1.1,0.0519,45.1,0.74,F0\\.5
10,1,Grammatical Error Correction,CoNLL-2014 Shared Task - Grammatical Error Correction benchmarking,2018-01,CNN Seq2Seq,54.79,89.6,54.79,100,61.15,0.89,F0\\.5
11,1,Grammatical Error Correction,CoNLL-2014 Shared Task - Grammatical Error Correction benchmarking,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),61.15,100.0,6.36,1.0,61.15,1.0,F0\\.5
12,1,Grammatical Error Correction,Unrestricted - Grammatical Error Correction benchmarking,2018-07,CNN Seq2Seq + Fluency Boost,61.34,100.0,61.34,100,61.34,1.0,F0\\.5
13,1,Grammatical Error Correction,Restricted - Grammatical Error Correction benchmarking,2018-10,CNN Seq2Seq + Quality Estimation,56.52,100.0,56.52,100,56.52,0.92,F0\\.5
14,1,Grammatical Error Detection,JFLEG - Grammatical Error Detection benchmarking,2018-11,BiLSTM-JOINT (trained on FCE),52.52,100.0,52.52,100,52.52,0.86,F0\\.5
0,1,AMR Parsing,LDC2014T12 - AMR Parsing benchmarking,2016-08,Imitation learning,70.0,95.5,70.0,100,73.3,0.95,F1\\ Newswire
1,1,AMR Parsing,LDC2014T12 - AMR Parsing benchmarking,2016-11,Incremental joint model ,71.0,96.86,1.0,0.303,73.3,0.97,F1\\ Newswire
2,1,AMR Parsing,LDC2014T12 - AMR Parsing benchmarking,2018-10,Transition-based+improved aligner+ensemble,73.3,100.0,2.3,0.697,73.3,1.0,F1\\ Newswire
3,1,AMR Parsing,LDC2014T12: - AMR Parsing benchmarking,2016-08,Imitation learning ,0.7,93.33,0.7,100,0.75,0.01,F1\\ Newswire
4,1,AMR Parsing,LDC2014T12: - AMR Parsing benchmarking,2016-11,Incremental joint model,0.71,94.67,0.01,0.2,0.75,0.01,F1\\ Newswire
5,1,AMR Parsing,LDC2014T12: - AMR Parsing benchmarking,2018-10,Transition-based+improved aligner+ensemble,0.73,97.33,0.02,0.4,0.75,0.01,F1\\ Newswire
6,1,AMR Parsing,LDC2014T12: - AMR Parsing benchmarking,2019-05,Sequence-to-Graph Transduction,0.75,100.0,0.02,0.4,0.75,0.01,F1\\ Newswire
0,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-08,Match-LSTM with Ans-Ptr (Sentence),54.505,60.63,54.505,100,89.898,0.61,EM
1,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-08,Match-LSTM with Ans-Ptr (Boundary) (ensemble),67.901,75.53,13.396,0.3785,89.898,0.76,EM
2,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-09,ReasoNet (single model),70.555,78.48,2.654,0.075,89.898,0.78,EM
3,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2016-09,ReasoNet (ensemble),75.034,83.47,4.479,0.1266,89.898,0.83,EM
4,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2017-05,Reinforced Mnemonic Reader (ensemble model),82.283,91.53,7.249,0.2048,89.898,0.92,EM
5,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2018-10,BERT (ensemble),87.433,97.26,5.15,0.1455,89.898,0.97,EM
6,1,Question Answering,SQuAD1.1 - Question Answering benchmarking,2019-06,XLNet (single model),89.898,100.0,2.465,0.0696,89.898,1.0,EM
7,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2016-08,Match-LSTM with Bi-Ans-Ptr (Boundary+Search+b) ,64.1,71.46,64.1,100,89.7,0.71,EM
8,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2016-11,RASOR,66.4,74.02,2.3,0.0898,89.7,0.74,EM
9,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2016-11,BIDAF (single),67.7,75.47,1.3,0.0508,89.7,0.75,EM
10,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-03,SEDT-LSTM,67.89,75.69,0.19,0.0074,89.7,0.76,EM
11,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-03,FastQAExt (beam-size 5),70.3,78.37,2.41,0.0941,89.7,0.78,EM
12,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-04,Ruminating Reader,70.6,78.71,0.3,0.0117,89.7,0.79,EM
13,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2017-05,R.M-Reader (single),78.9,87.96,8.3,0.3242,89.7,0.88,EM
14,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2018-10,BERT large,84.1,93.76,5.2,0.2031,89.7,0.94,EM
15,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2018-10,BERT large (+TriviaQA),84.2,93.87,0.1,0.0039,89.7,0.94,EM
16,1,Question Answering,SQuAD1.1 dev - Question Answering benchmarking,2019-06,XLNet (single model),89.7,100.0,5.5,0.2148,89.7,1.0,EM
17,1,Open-Domain Question Answering,SQuAD1.1 - Open-Domain Question Answering benchmarking,2016-11,DCN,66.2,94.57,66.2,100,70.0,0.74,EM
18,1,Open-Domain Question Answering,SQuAD1.1 - Open-Domain Question Answering benchmarking,2017-03,DrQA,70.0,100.0,3.8,1.0,70.0,0.78,EM
19,1,Question Answering,NewsQA - Question Answering benchmarking,2017-03,FastQAExt,43.7,82.3,43.7,100,53.1,0.49,EM
20,1,Question Answering,NewsQA - Question Answering benchmarking,2018-01,AMANDA,48.4,91.15,4.7,0.5,53.1,0.54,EM
21,1,Question Answering,NewsQA - Question Answering benchmarking,2018-05,MINIMAL(Dyn),50.1,94.35,1.7,0.1809,53.1,0.56,EM
22,1,Question Answering,NewsQA - Question Answering benchmarking,2018-11,DecaProp,53.1,100.0,3.0,0.3191,53.1,0.59,EM
23,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2017-03,DrQA,41.9,67.36,41.9,100,62.2,0.47,EM
24,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-07,Denoising QA,58.8,94.53,16.9,0.8325,62.2,0.65,EM
25,1,Open-Domain Question Answering,SearchQA - Open-Domain Question Answering benchmarking,2018-11,DECAPROP,62.2,100.0,3.4,0.1675,62.2,0.69,EM
26,1,Question Answering,Quasart-T - Question Answering benchmarking,2017-03,DrQA,37.7,89.34,37.7,100,42.2,0.42,EM
27,1,Question Answering,Quasart-T - Question Answering benchmarking,2018-07,Denoising QA,42.2,100.0,4.5,1.0,42.2,0.47,EM
28,1,Question Answering,TriviaQA - Question Answering benchmarking,2017-05,Mnemonic Reader,46.94,69.84,46.94,100,67.21,0.52,EM
29,1,Question Answering,TriviaQA - Question Answering benchmarking,2017-06,Reading Twice for NLU,50.56,75.23,3.62,0.1786,67.21,0.56,EM
30,1,Question Answering,TriviaQA - Question Answering benchmarking,2017-10,S-Norm,66.37,98.75,15.81,0.78,67.21,0.74,EM
31,1,Question Answering,TriviaQA - Question Answering benchmarking,2018-10,MemoReader,67.21,100.0,0.84,0.0414,67.21,0.75,EM
32,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2017-11,FusionNet++ (ensemble),70.3,78.35,70.3,100,89.731,0.78,EM
33,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2017-12,SAN (ensemble model),71.316,79.48,1.016,0.0523,89.731,0.79,EM
34,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2018-08,Reinforced Mnemonic Reader + Answer Verifier (single model),71.767,79.98,0.451,0.0232,89.731,0.8,EM
35,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2018-10,BERT (single model),80.005,89.16,8.238,0.424,89.731,0.89,EM
36,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2019-06,XLNet (single model),87.926,97.99,7.921,0.4076,89.731,0.98,EM
37,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2019-08,XLNet + SG-Net Verifier (ensemble),88.174,98.26,0.248,0.0128,89.731,0.98,EM
38,1,Question Answering,SQuAD2.0 - Question Answering benchmarking,2019-09,ALBERT (ensemble model),89.731,100.0,1.557,0.0801,89.731,1.0,EM
39,1,Question Answering,SQuAD2.0 dev - Question Answering benchmarking,2018-08,RMR + ELMo (Model-III),72.3,82.25,72.3,100,87.9,0.8,EM
40,1,Question Answering,SQuAD2.0 dev - Question Answering benchmarking,2018-10,BERT large,78.7,89.53,6.4,0.4103,87.9,0.88,EM
41,1,Question Answering,SQuAD2.0 dev - Question Answering benchmarking,2019-06,XLNet (single model),87.9,100.0,9.2,0.5897,87.9,0.98,EM
42,1,Common Sense Reasoning,ReCoRD - Common Sense Reasoning benchmarking,2018-10,BERT-Base (single model),54.04,100.0,54.04,100,54.04,0.6,EM
43,1,Open-Domain Question Answering,DuReader - Open-Domain Question Answering benchmarking,2019-07,ERNIE 2.0 Base,61.3,95.48,61.3,100,64.2,0.68,EM
44,1,Open-Domain Question Answering,DuReader - Open-Domain Question Answering benchmarking,2019-07,ERNIE 2.0 Large,64.2,100.0,2.9,1.0,64.2,0.71,EM
0,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2016-09,SeqGAN,0.4541,72.42,0.4541,100,0.627,0.01,BLEU\\-4
1,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2017-09,LeakGAN,0.627,100.0,0.1729,1.0,0.627,0.02,BLEU\\-4
2,1,Text Generation,COCO Captions - Text Generation benchmarking,2016-09,SeqGAN,0.521,66.97,0.521,100,0.778,0.01,BLEU\\-4
3,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-05,RankGAN,0.557,71.59,0.036,0.1401,0.778,0.02,BLEU\\-4
4,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-09,LeakGAN,0.778,100.0,0.221,0.8599,0.778,0.02,BLEU\\-4
5,1,Question Answering,NarrativeQA - Question Answering benchmarking,2016-11,BiDAF,15.69,51.56,15.69,100,30.43,0.43,BLEU\\-4
6,1,Question Answering,NarrativeQA - Question Answering benchmarking,2018-09,MHPGM + NOIC,21.07,69.24,5.38,0.365,30.43,0.58,BLEU\\-4
7,1,Question Answering,NarrativeQA - Question Answering benchmarking,2018-10,ConZNet,22.49,73.91,1.42,0.0963,30.43,0.62,BLEU\\-4
8,1,Question Answering,NarrativeQA - Question Answering benchmarking,2018-11,DecaProp,27.61,90.73,5.12,0.3474,30.43,0.76,BLEU\\-4
9,1,Question Answering,NarrativeQA - Question Answering benchmarking,2019-01,Masque (NarrativeQA + MS MARCO),30.43,100.0,2.82,0.1913,30.43,0.83,BLEU\\-4
10,1,Question Generation,SQuAD1.1 - Question Generation benchmarking,2017-04,NQG++,13.27,58.25,13.27,100,22.78,0.36,BLEU\\-4
11,1,Question Generation,SQuAD1.1 - Question Generation benchmarking,2018-06,MPQG,13.91,61.06,0.64,0.0673,22.78,0.38,BLEU\\-4
12,1,Question Generation,SQuAD1.1 - Question Generation benchmarking,2019-05,UniLM,22.78,100.0,8.87,0.9327,22.78,0.62,BLEU\\-4
13,1,Text Generation,DailyDialog - Text Generation benchmarking,2018-08,AEM+Attention,2.84,100.0,2.84,100,2.84,0.08,BLEU\\-4
14,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",24.9,100.0,24.9,100,24.9,0.68,BLEU\\-4
15,1,Image Captioning,Flickr30k Captions test - Image Captioning benchmarking,2019-09,Unified VLP,30.1,100.0,30.1,100,30.1,0.82,BLEU\\-4
16,1,Image Captioning,COCO Captions test - Image Captioning benchmarking,2019-09,Unified VLP,36.5,100.0,36.5,100,36.5,1.0,BLEU\\-4
0,1,Text Generation,Chinese Poems - Text Generation benchmarking,2016-09,SeqGAN,0.738,90.89,0.738,100,0.812,0.02,BLEU\\-2
1,1,Text Generation,Chinese Poems - Text Generation benchmarking,2017-05,RankGAN,0.812,100.0,0.074,1.0,0.812,0.02,BLEU\\-2
2,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2016-09,SeqGAN,0.859,89.85,0.859,100,0.956,0.02,BLEU\\-2
3,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2017-09,LeakGAN,0.956,100.0,0.097,1.0,0.956,0.02,BLEU\\-2
4,1,Text Generation,COCO Captions - Text Generation benchmarking,2016-09,SeqGAN,0.831,87.47,0.831,100,0.95,0.02,BLEU\\-2
5,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-05,RankGAN,0.85,89.47,0.019,0.1597,0.95,0.02,BLEU\\-2
6,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-09,partGAN,0.91,95.79,0.06,0.5042,0.95,0.02,BLEU\\-2
7,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-09,LeakGAN,0.95,100.0,0.04,0.3361,0.95,0.02,BLEU\\-2
8,1,Text Generation,DailyDialog - Text Generation benchmarking,2018-08,AEM+Attention,5.69,100.0,5.69,100,5.69,0.12,BLEU\\-2
9,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",46.3,100.0,46.3,100,46.3,1.0,BLEU\\-2
0,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2016-09,SeqGAN,0.6015,73.44,0.6015,100,0.819,0.02,BLEU\\-3
1,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2017-09,LeakGAN,0.819,100.0,0.2175,1.0,0.819,0.02,BLEU\\-3
2,1,Text Generation,COCO Captions - Text Generation benchmarking,2016-09,SeqGAN,0.642,72.95,0.642,100,0.88,0.02,BLEU\\-3
3,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-05,RankGAN,0.672,76.36,0.03,0.1261,0.88,0.02,BLEU\\-3
4,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-09,partGAN,0.713,81.02,0.041,0.1723,0.88,0.02,BLEU\\-3
5,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-09,LeakGAN,0.88,100.0,0.167,0.7017,0.88,0.03,BLEU\\-3
6,1,Text Generation,DailyDialog - Text Generation benchmarking,2018-08,AEM+Attention,3.78,100.0,3.78,100,3.78,0.11,BLEU\\-3
7,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",33.6,100.0,33.6,100,33.6,1.0,BLEU\\-3
0,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2016-09,SeqGAN,0.4498,90.32,0.4498,100,0.498,0.66,BLEU\\-5
1,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2017-05,RankGAN,0.463,92.97,0.0132,0.2739,0.498,0.67,BLEU\\-5
2,1,Text Generation,EMNLP2017 WMT - Text Generation benchmarking,2017-09,LeakGAN,0.498,100.0,0.035,0.7261,0.498,0.73,BLEU\\-5
3,1,Text Generation,COCO Captions - Text Generation benchmarking,2016-09,SeqGAN,0.427,62.24,0.427,100,0.686,0.62,BLEU\\-5
4,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-05,RankGAN,0.544,79.3,0.117,0.4517,0.686,0.79,BLEU\\-5
5,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-09,partGAN,0.59,86.01,0.046,0.1776,0.686,0.86,BLEU\\-5
6,1,Text Generation,COCO Captions - Text Generation benchmarking,2017-09,LeakGAN,0.686,100.0,0.096,0.3707,0.686,1.0,BLEU\\-5
0,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2016-10,GAWWN,3.62,76.21,3.62,100,4.75,0.12,Inception\\ score
1,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2016-12,StackGAN,3.7,77.89,0.08,0.0708,4.75,0.12,Inception\\ score
2,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,3.82,80.42,0.12,0.1062,4.75,0.13,Inception\\ score
3,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2017-11,AttnGAN,4.36,91.79,0.54,0.4779,4.75,0.14,Inception\\ score
4,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2019-03,MirrorGAN,4.56,96.0,0.2,0.177,4.75,0.15,Inception\\ score
5,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2019-04,DM-GAN,4.75,100.0,0.19,0.1681,4.75,0.16,Inception\\ score
6,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2016-12,StackGAN,8.45,27.71,8.45,100,30.49,0.28,Inception\\ score
7,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2017-11,AttnGAN,25.89,84.91,17.44,0.7913,30.49,0.85,Inception\\ score
8,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-03,MirrorGAN,26.47,86.82,0.58,0.0263,30.49,0.87,Inception\\ score
9,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-04,DM-GAN,30.49,100.0,4.02,0.1824,30.49,1.0,Inception\\ score
10,1,Text-to-Image Generation,Oxford 102 Flowers - Text-to-Image Generation benchmarking,2016-12,StackGAN,3.2,98.16,3.2,100,3.26,0.1,Inception\\ score
11,1,Text-to-Image Generation,Oxford 102 Flowers - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,3.26,100.0,0.06,1.0,3.26,0.11,Inception\\ score
0,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2016-10,GAWWN,-67.22,439.35,-67.22,100,-15.3,4.39,FID
1,1,Text-to-Image Generation,CUB - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,-15.3,100.0,51.92,1.0,-15.3,1.0,FID
2,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2017-10,StackGAN-v1 ,-74.05,222.04,-74.05,100,-33.35,4.84,FID
3,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-01,AttnGAN + OP,-33.35,100.0,40.7,1.0,-33.35,2.18,FID
4,1,Text-to-Image Generation,Oxford 102 Flowers - Text-to-Image Generation benchmarking,2017-10,StackGAN-v2,-48.68,100.0,-48.68,100,-48.68,3.18,FID
5,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,-125.98,108.3,-125.98,100,-116.32,8.23,FID
6,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2019-09,ControlGAN,-116.32,100.0,9.66,1.0,-116.32,7.6,FID
0,1,Aspect-Based Sentiment Analysis,Sentihood - Aspect-Based Sentiment Analysis benchmarking,2016-10,LSTM-LOC,69.3,78.84,69.3,100,87.9,0.79,Aspect
1,1,Aspect-Based Sentiment Analysis,Sentihood - Aspect-Based Sentiment Analysis benchmarking,2018-04,Sentic LSTM + TA + SA,78.18,88.94,8.88,0.4774,87.9,0.89,Aspect
2,1,Aspect-Based Sentiment Analysis,Sentihood - Aspect-Based Sentiment Analysis benchmarking,2019-03,BERT-pair-QA-M,86.4,98.29,8.22,0.4419,87.9,0.98,Aspect
3,1,Aspect-Based Sentiment Analysis,Sentihood - Aspect-Based Sentiment Analysis benchmarking,2019-03,BERT-pair-QA-B,87.9,100.0,1.5,0.0806,87.9,1.0,Aspect
0,1,Aspect-Based Sentiment Analysis,Sentihood - Aspect-Based Sentiment Analysis benchmarking,2016-10,LSTM-LOC,-81.9,100.0,-81.9,100,-81.9,1.0,Sentiment
0,1,Hypernym Discovery,Music domain - Hypernym Discovery benchmarking,2016-11,vTE,12.41,30.04,12.41,100,41.31,0.18,P\\-at\\-5
1,1,Hypernym Discovery,Music domain - Hypernym Discovery benchmarking,2018-06,CRIM,41.31,100.0,28.9,1.0,41.31,0.6,P\\-at\\-5
2,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2016-11,vTE,20.71,56.32,20.71,100,36.77,0.3,P\\-at\\-5
3,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2018-06,300-sparsans,21.43,58.28,0.72,0.0448,36.77,0.31,P\\-at\\-5
4,1,Hypernym Discovery,Medical domain - Hypernym Discovery benchmarking,2018-06,CRIM,36.77,100.0,15.34,0.9552,36.77,0.54,P\\-at\\-5
5,1,Hypernym Discovery,General - Hypernym Discovery benchmarking,2016-11,vTE,9.91,52.08,9.91,100,19.03,0.14,P\\-at\\-5
6,1,Hypernym Discovery,General - Hypernym Discovery benchmarking,2018-06,CRIM,19.03,100.0,9.12,1.0,19.03,0.28,P\\-at\\-5
7,1,Multi-Label Text Classification,Amazon-12K - Multi-Label Text Classification benchmarking,2019-05,LAHA,63.16,100.0,63.16,100,63.16,0.92,P\\-at\\-5
8,1,Multi-Label Text Classification,Wiki-30K - Multi-Label Text Classification benchmarking,2019-05,LAHA,62.87,100.0,62.87,100,62.87,0.92,P\\-at\\-5
9,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-05,LAHA,50.71,73.81,50.71,100,68.7,0.74,P\\-at\\-5
10,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,bert-base,68.7,100.0,17.99,1.0,68.7,1.0,P\\-at\\-5
11,1,Multi-Label Text Classification,Kan-Shan Cup - Multi-Label Text Classification benchmarking,2019-05,LAHA,25.88,100.0,25.88,100,25.88,0.38,P\\-at\\-5
12,1,Multi-Label Text Classification,AAPD - Multi-Label Text Classification benchmarking,2019-05,LAHA,41.19,100.0,41.19,100,41.19,0.6,P\\-at\\-5
13,1,Text Classification,RCV1 - Text Classification benchmarking,2019-06,NLP-Cap,56.33,100.0,56.33,100,56.33,0.82,P\\-at\\-5
0,1,AMR Parsing,LDC2014T12 - AMR Parsing benchmarking,2016-11,Incremental joint model ,66.0,94.02,66.0,100,70.2,0.94,F1\\ Full
1,1,AMR Parsing,LDC2014T12 - AMR Parsing benchmarking,2018-10,Transition-based+improved aligner+ensemble,68.4,97.44,2.4,0.5714,70.2,0.97,F1\\ Full
2,1,AMR Parsing,LDC2014T12 - AMR Parsing benchmarking,2019-05,Two-stage Sequence-to-Graph Transducer,70.2,100.0,1.8,0.4286,70.2,1.0,F1\\ Full
3,1,AMR Parsing,LDC2014T12: - AMR Parsing benchmarking,2016-11,Incremental joint model,0.66,94.29,0.66,100,0.7,0.01,F1\\ Full
4,1,AMR Parsing,LDC2014T12: - AMR Parsing benchmarking,2018-10,Transition-based+improved aligner+ensemble,0.68,97.14,0.02,0.5,0.7,0.01,F1\\ Full
5,1,AMR Parsing,LDC2014T12: - AMR Parsing benchmarking,2019-05,Sequence-to-Graph Transduction,0.7,100.0,0.02,0.5,0.7,0.01,F1\\ Full
0,1,Question Answering,MS MARCO - Question Answering benchmarking,2016-11,BiDaF Baseline,23.96,45.9,23.96,100,52.2,0.4,Rouge\\-L
1,1,Question Answering,MS MARCO - Question Answering benchmarking,2018-05,VNET,51.63,98.91,27.67,0.9798,52.2,0.86,Rouge\\-L
2,1,Question Answering,MS MARCO - Question Answering benchmarking,2018-11,Deep Cascade QA,52.01,99.64,0.38,0.0135,52.2,0.87,Rouge\\-L
3,1,Question Answering,MS MARCO - Question Answering benchmarking,2019-01,Masque Q&A Style,52.2,100.0,0.19,0.0067,52.2,0.87,Rouge\\-L
4,1,Question Answering,NarrativeQA - Question Answering benchmarking,2016-11,BiDAF,36.74,61.37,36.74,100,59.87,0.61,Rouge\\-L
5,1,Question Answering,NarrativeQA - Question Answering benchmarking,2018-09,MHPGM + NOIC,44.16,73.76,7.42,0.3208,59.87,0.74,Rouge\\-L
6,1,Question Answering,NarrativeQA - Question Answering benchmarking,2018-10,ConZNet,46.67,77.95,2.51,0.1085,59.87,0.78,Rouge\\-L
7,1,Question Answering,NarrativeQA - Question Answering benchmarking,2019-01,Masque (NarrativeQA only),54.74,91.43,8.07,0.3489,59.87,0.91,Rouge\\-L
8,1,Question Answering,NarrativeQA - Question Answering benchmarking,2019-01,Masque (NarrativeQA + MS MARCO),59.87,100.0,5.13,0.2218,59.87,1.0,Rouge\\-L
0,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2016-12,Prior,25.98,35.83,25.98,100,72.5,0.36,overall
1,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2016-12,"MCB [11, 12]",62.27,85.89,36.29,0.7801,72.5,0.86,overall
2,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2017-05,MUTAN,67.4,92.97,5.13,0.1103,72.5,0.93,overall
3,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2017-07,Up-Down,70.34,97.02,2.94,0.0632,72.5,0.97,overall
4,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2018-05,BAN+Glove+Counter,70.4,97.1,0.06,0.0013,72.5,0.97,overall
5,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2019-06,MCANed-6,70.9,97.79,0.5,0.0107,72.5,0.98,overall
6,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2019-08,VisualBERT,71.0,97.93,0.1,0.0021,72.5,0.98,overall
7,1,Visual Question Answering,VQA v2 test-std - Visual Question Answering benchmarking,2019-08,LXMERT,72.5,100.0,1.5,0.0322,72.5,1.0,overall
8,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-04,Pythia v0.3 ,54.72,98.77,54.72,100,55.4,0.75,overall
9,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",55.4,100.0,0.68,1.0,55.4,0.76,overall
0,1,Dialog Generation,Amazon-5 - Dialog Generation benchmarking,2017-01,mm,5.0,100.0,5,100,5,1.0,1\\ in\\ 10\\ R\\-at\\-2
0,1,Optical Character Recognition,FSNS - Test - Optical Character Recognition benchmarking,2017-02,STREET,27.54,100.0,27.54,100,27.54,1.0,Sequence\\ error
0,1,Text Generation,Yahoo Questions - Text Generation benchmarking,2017-02,CNN-VAE,-63.9,107.04,-63.9,100,-59.7,24.11,Perplexity
1,1,Text Generation,Yahoo Questions - Text Generation benchmarking,2018-02,SA-VAE,-60.4,101.17,3.5,0.8333,-59.7,22.79,Perplexity
2,1,Text Generation,Yahoo Questions - Text Generation benchmarking,2019-01,Aggressive VAE,-59.7,100.0,0.7,0.1667,-59.7,22.53,Perplexity
3,1,Code Generation,Android Repos - Code Generation benchmarking,2018-05,Entity Type Model,-2.65,100.0,-2.65,100,-2.65,1.0,Perplexity
0,1,Text Generation,Yahoo Questions - Text Generation benchmarking,2017-02,CNN-VAE,10.0,100.0,10.0,100,10.0,1.0,KL
0,1,Text Generation,Yahoo Questions - Text Generation benchmarking,2017-02,CNN-VAE,332.1,100.0,332.1,100,332.1,1.0,NLL
0,1,Entity Disambiguation,WNED-CWEB - Entity Disambiguation benchmarking,2017-04,Global,77.9,98.73,77.9,100,78.9,0.81,Micro\\-F1
1,1,Entity Disambiguation,WNED-CWEB - Entity Disambiguation benchmarking,2019-09,confidence-order,78.9,100.0,1.0,1.0,78.9,0.82,Micro\\-F1
2,1,Entity Disambiguation,WNED-WIKI - Entity Disambiguation benchmarking,2017-04,Glonal,77.5,86.98,77.5,100,89.1,0.8,Micro\\-F1
3,1,Entity Disambiguation,WNED-WIKI - Entity Disambiguation benchmarking,2019-09,confidence-order,89.1,100.0,11.6,1.0,89.1,0.93,Micro\\-F1
4,1,Entity Disambiguation,ACE2004 - Entity Disambiguation benchmarking,2017-04,Global,88.5,96.3,88.5,100,91.9,0.92,Micro\\-F1
5,1,Entity Disambiguation,ACE2004 - Entity Disambiguation benchmarking,2019-09,MEP + pseudo entities,91.5,99.56,3.0,0.8824,91.9,0.95,Micro\\-F1
6,1,Entity Disambiguation,ACE2004 - Entity Disambiguation benchmarking,2019-09,confidence-order,91.9,100.0,0.4,0.1176,91.9,0.95,Micro\\-F1
7,1,Entity Disambiguation,MSNBC - Entity Disambiguation benchmarking,2017-04,Global,93.7,97.3,93.7,100,96.3,0.97,Micro\\-F1
8,1,Entity Disambiguation,MSNBC - Entity Disambiguation benchmarking,2019-09,MEP,94.1,97.72,0.4,0.1538,96.3,0.98,Micro\\-F1
9,1,Entity Disambiguation,MSNBC - Entity Disambiguation benchmarking,2019-09,confidence-order,96.3,100.0,2.2,0.8462,96.3,1.0,Micro\\-F1
10,1,Entity Linking,MSNBC - Entity Linking benchmarking,2018-08,E2E,72.4,100.0,72.4,100,72.4,0.75,Micro\\-F1
11,1,Entity Linking,OKE-2015 - Entity Linking benchmarking,2018-08,E2E,66.9,100.0,66.9,100,66.9,0.69,Micro\\-F1
12,1,Entity Linking,Derczynski - Entity Linking benchmarking,2018-08,E2E,42.3,100.0,42.3,100,42.3,0.44,Micro\\-F1
13,1,Entity Linking,N3-Reuters-128 - Entity Linking benchmarking,2018-08,E2E,54.6,100.0,54.6,100,54.6,0.57,Micro\\-F1
14,1,Entity Linking,OKE-2016 - Entity Linking benchmarking,2018-08,E2E,58.4,100.0,58.4,100,58.4,0.61,Micro\\-F1
15,1,Emotion Recognition in Conversation,EC - Emotion Recognition in Conversation benchmarking,2019-03,HRLCE + BERT,0.7709,99.28,0.7709,100,0.7765,0.01,Micro\\-F1
16,1,Emotion Recognition in Conversation,EC - Emotion Recognition in Conversation benchmarking,2019-04,NELEC,0.7765,100.0,0.0056,1.0,0.7765,0.01,Micro\\-F1
17,1,Entity Disambiguation,AQUAINT - Entity Disambiguation benchmarking,2019-09,confidence-order,93.5,99.79,93.5,100,93.7,0.97,Micro\\-F1
18,1,Entity Disambiguation,AQUAINT - Entity Disambiguation benchmarking,2019-09,MEP + pseudo entities,93.7,100.0,0.2,1.0,93.7,0.97,Micro\\-F1
19,1,Emotion Recognition in Conversation,DailyDialog - Emotion Recognition in Conversation benchmarking,2019-09,KET,53.37,100.0,53.37,100,53.37,0.55,Micro\\-F1
20,1,Multi-Label Text Classification,Slashdot - Multi-Label Text Classification benchmarking,2020-02,MAGNET,56.8,100.0,56.8,100,56.8,0.59,Micro\\-F1
21,1,Multi-Label Text Classification,RCV1-v2 - Multi-Label Text Classification benchmarking,2020-02,MAGNET,88.5,100.0,88.5,100,88.5,0.92,Micro\\-F1
22,1,Multi-Label Text Classification,Reuters-21578 - Multi-Label Text Classification benchmarking,2020-02,MAGNET,89.9,100.0,89.9,100,89.9,0.93,Micro\\-F1
0,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,58.1,100.0,58.1,100,58.1,1.0,NDCG\\ \\(x\\ 100\\)
0,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,58.8,84.85,58.8,100,69.3,0.85,MRR\\ \\(x\\ 100\\)
1,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),61.5,88.74,2.7,0.2571,69.3,0.89,MRR\\ \\(x\\ 100\\)
2,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,63.2,91.2,1.7,0.1619,69.3,0.91,MRR\\ \\(x\\ 100\\)
3,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,64.22,92.67,1.02,0.0971,69.3,0.93,MRR\\ \\(x\\ 100\\)
4,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),69.3,100.0,5.08,0.4838,69.3,1.0,MRR\\ \\(x\\ 100\\)
0,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2017-04,NMN,-4.4,140.13,-4.4,100,-3.14,1.4,Mean
1,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2018-09,CorefNMN (ResNet-152),-4.4,140.13,0.0,0.0,-3.14,1.4,Mean
2,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,DAN,-4.3,136.94,0.1,0.0794,-3.14,1.37,Mean
3,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-02,HACAN,-4.2,133.76,0.1,0.0794,-3.14,1.34,Mean
4,1,Visual Dialog,Visual Dialog v1.0 test-std - Visual Dialog benchmarking,2019-04,5xFGA (F-RCNNx101),-3.14,100.0,1.06,0.8413,-3.14,1.0,Mean
0,1,Sentiment Analysis,SemEval 2017 Task 4-A - Sentiment Analysis benchmarking,2017-04,LSTMs+CNNs ensemble with multiple conv. ops,0.685,100.0,0.685,100,0.685,0.76,Average\\ Recall
1,1,Sentiment Analysis,ArSAS - Sentiment Analysis benchmarking,2019-08,CNN-LSTM,0.9,100.0,0.9,100,0.9,1.0,Average\\ Recall
2,1,Sentiment Analysis,ASTD - Sentiment Analysis benchmarking,2019-08,CNN-LSTM,0.62,100.0,0.62,100,0.62,0.69,Average\\ Recall
0,1,Sentiment Analysis,SemEval - Sentiment Analysis benchmarking,2017-04,LSTMs+CNNs ensemble with multiple conv. ops,0.685,100.0,0.685,100,0.685,0.74,F1\\-score
1,1,Humor Detection,200k Short Texts for Humor Detection - Humor Detection benchmarking,2019-06,XLNet Large Cased,0.92,100.0,0.92,100,0.92,1.0,F1\\-score
0,1,Semantic Textual Similarity,SentEval - Semantic Textual Similarity benchmarking,2017-05,InferSent,86.3,98.29,86.3,100,87.8,0.98,SICK\\-E
1,1,Semantic Textual Similarity,SentEval - Semantic Textual Similarity benchmarking,2018-03,GenSen,87.8,100.0,1.5,1.0,87.8,1.0,SICK\\-E
0,1,Semantic Textual Similarity,SentEval - Semantic Textual Similarity benchmarking,2017-05,InferSent,0.884,99.55,0.884,100,0.888,1.0,SICK\\-R
1,1,Semantic Textual Similarity,SentEval - Semantic Textual Similarity benchmarking,2018-03,GenSen,0.888,100.0,0.004,1.0,0.888,1.0,SICK\\-R
0,1,Code Generation,100 sleep nights of 8 caregivers - Code Generation benchmarking,2017-05,anv,10.0,100.0,10,100,10,1.0,14\\ gestures\\ accuracy
1,1,Visual Question Answering,100 sleep nights of 8 caregivers - Visual Question Answering benchmarking,2018-10,TallyQA,10.0,100.0,10,100,10,1.0,14\\ gestures\\ accuracy
0,1,Abstract Anaphora Resolution,The ARRAU Corpus - Abstract Anaphora Resolution benchmarking,2017-06,MR-LSTM,43.83,100.0,43.83,100,43.83,1.0,Average\\ Precision
1,1,Relationship extraction using distant supervision,New York Times Corpus - Relationship extraction using distant supervision benchmarking,2018-08,BGRU-SET,0.39,100.0,0.39,100,0.39,0.01,Average\\ Precision
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,0.189,98.44,0.189,100,0.192,0.98,MAE\\ \\(Valence\\)
1,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2018-06,CMN,0.192,100.0,0.003,1.0,0.192,1.0,MAE\\ \\(Valence\\)
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,0.213,100.0,0.213,100,0.213,1.0,MAE\\ \\(Arousal\\)
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,0.19,97.44,0.19,100,0.195,0.97,MAE\\ \\(Expectancy\\)
1,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2018-06,CMN,0.195,100.0,0.005,1.0,0.195,1.0,MAE\\ \\(Expectancy\\)
0,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,8.67,99.2,8.67,100,8.74,0.99,MAE\\ \\(Power\\)
1,1,Emotion Recognition in Conversation,SEMAINE - Emotion Recognition in Conversation benchmarking,2018-06,CMN,8.74,100.0,0.07,1.0,8.74,1.0,MAE\\ \\(Power\\)
0,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,56.19,87.55,56.19,100,64.18,0.88,Weighted\\-F1
1,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-10,ICON,58.54,91.21,2.35,0.2941,64.18,0.91,Weighted\\-F1
2,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,62.75,97.77,4.21,0.5269,64.18,0.98,Weighted\\-F1
3,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,64.18,100.0,1.43,0.179,64.18,1.0,Weighted\\-F1
4,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,56.44,97.01,56.44,100,58.18,0.88,Weighted\\-F1
5,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,57.03,98.02,0.59,0.3391,58.18,0.89,Weighted\\-F1
6,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,58.1,99.86,1.07,0.6149,58.18,0.91,Weighted\\-F1
7,1,Emotion Recognition in Conversation,MELD - Emotion Recognition in Conversation benchmarking,2019-09,KET,58.18,100.0,0.08,0.046,58.18,0.91,Weighted\\-F1
0,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2017-07,bc-LSTM ,0.741,96.86,0.741,100,0.765,0.97,UA
1,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (T+V),0.759,99.22,0.018,0.75,0.765,0.99,UA
2,1,Multimodal Emotion Recognition,IEMOCAP - Multimodal Emotion Recognition benchmarking,2018-06,CHFusion (A+T+V),0.765,100.0,0.006,0.25,0.765,1.0,UA
3,1,Speech Emotion Recognition,IEMOCAP - Speech Emotion Recognition benchmarking,2018-02,CNN+LSTM,0.65,92.72,0.65,100,0.701,0.85,UA
4,1,Speech Emotion Recognition,IEMOCAP - Speech Emotion Recognition benchmarking,2019-04,Ensemble (Acoustic + Text)(Random Forests + Gradient Boosted Trees + Multi Layer Perceptron + Multinomial Naive Bayes + Logistic Regression),0.701,100.0,0.051,1.0,0.701,0.92,UA
0,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2017-07,bc-LSTM+Att,54.84,86.46,54.84,100,63.43,0.86,Macro\\-F1
1,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-10,ICON,56.52,89.11,1.68,0.1956,63.43,0.89,Macro\\-F1
2,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2018-11,DialogueRNN,60.66,95.63,4.14,0.482,63.43,0.96,Macro\\-F1
3,1,Emotion Recognition in Conversation,IEMOCAP - Emotion Recognition in Conversation benchmarking,2019-08,DialogueGCN,63.43,100.0,2.77,0.3225,63.43,1.0,Macro\\-F1
4,1,Emotion Classification,SemEval 2018 Task 1E-c - Emotion Classification benchmarking,2018-12,Transformer (finetune),0.561,100.0,0.561,100,0.561,0.01,Macro\\-F1
0,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017)",81.72,98.36,81.72,100,83.08,0.98,Weighted\\ Accuracy
1,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-12,Bhatt et al.,83.08,100.0,1.36,1.0,83.08,1.0,Weighted\\ Accuracy
0,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017)",97.9,99.86,97.9,100,98.04,1.0,Per\\-class\\ Accuracy\\ \\(Unrelated\\)
1,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-12,Bhatt et al.,98.04,100.0,0.14,1.0,98.04,1.0,Per\\-class\\ Accuracy\\ \\(Unrelated\\)
0,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017)",44.04,85.78,44.04,100,51.34,0.86,Per\\-class\\ Accuracy\\ \\(Agree\\)
1,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-12,"Baseline based on word2vec + hand-crafted features (Bhatt et al., 2017)",50.7,98.75,6.66,0.9123,51.34,0.99,Per\\-class\\ Accuracy\\ \\(Agree\\)
2,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2018-11,"Bi-LSTM (max-pooling, attention)",51.34,100.0,0.64,0.0877,51.34,1.0,Per\\-class\\ Accuracy\\ \\(Agree\\)
0,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017)",6.6,63.89,6.6,100,10.33,0.64,Per\\-class\\ Accuracy\\ \\(Disagree\\)
1,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-12,"Baseline based on word2vec + hand-crafted features (Bhatt et al., 2017)",9.61,93.03,3.01,0.807,10.33,0.93,Per\\-class\\ Accuracy\\ \\(Disagree\\)
2,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2018-11,"Bi-LSTM (max-pooling, attention)",10.33,100.0,0.72,0.193,10.33,1.0,Per\\-class\\ Accuracy\\ \\(Disagree\\)
0,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-07,"3rd place at FNC-1 - Team UCL Machine Reading (Riedel et al., 2017)",81.38,94.98,81.38,100,85.68,0.95,Per\\-class\\ Accuracy\\ \\(Discuss\\)
1,1,Fake News Detection,FNC-1 - Fake News Detection benchmarking,2017-12,Bhatt et al.,85.68,100.0,4.3,1.0,85.68,1.0,Per\\-class\\ Accuracy\\ \\(Discuss\\)
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,78.71,84.54,78.71,100,93.1,0.85,Consistency
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",89.59,96.23,10.88,0.7561,93.1,0.96,Consistency
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",93.1,100.0,3.51,0.2439,93.1,1.0,Consistency
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,84.57,99.25,84.57,100,85.21,0.99,Plausibility
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",85.21,100.0,0.64,1.0,85.21,1.0,Plausibility
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,96.18,99.81,96.18,100,96.36,1.0,Validity
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",96.35,99.99,0.17,0.9444,96.36,1.0,Validity
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",96.36,100.0,0.01,0.0556,96.36,1.0,Validity
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,5.98,93.15,5.98,100,6.42,0.93,Distribution
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",6.42,100.0,0.44,1.0,6.42,1.0,Distribution
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,66.64,83.52,66.64,100,79.79,0.84,Binary
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",77.16,96.7,10.52,0.8,79.79,0.97,Binary
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",79.79,100.0,2.63,0.2,79.79,1.0,Binary
0,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2017-07,BottomUp,34.83,73.11,34.83,100,47.64,0.73,Open
1,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Single Model",45.47,95.45,10.64,0.8306,47.64,0.95,Open
2,1,Visual Question Answering,GQA Test2019 - Visual Question Answering benchmarking,2019-08,"LXR955, Ensemble",47.64,100.0,2.17,0.1694,47.64,1.0,Open
0,1,Code Generation,WikiSQL - Code Generation benchmarking,2017-08,"Seq2Seq (Zhong et al., 2017)",35.9,40.25,35.9,100,89.2,0.4,Execution\\ Accuracy
1,1,Code Generation,WikiSQL - Code Generation benchmarking,2017-08,"Seq2SQL (Zhong et al., 2017)",59.4,66.59,23.5,0.4409,89.2,0.67,Execution\\ Accuracy
2,1,Code Generation,WikiSQL - Code Generation benchmarking,2018-03,"PT-MAML (Huang et al., 2018)",68.0,76.23,8.6,0.1614,89.2,0.76,Execution\\ Accuracy
3,1,Code Generation,WikiSQL - Code Generation benchmarking,2018-04,"STAMP+RL (Sun et al., 2018)+",74.6,83.63,6.6,0.1238,89.2,0.84,Execution\\ Accuracy
4,1,Code Generation,WikiSQL - Code Generation benchmarking,2018-04,"TypeSQL+TC (Yu et al., 2018)+",82.6,92.6,8.0,0.1501,89.2,0.93,Execution\\ Accuracy
5,1,Code Generation,WikiSQL - Code Generation benchmarking,2019-10,NL2SQL-RULE,89.2,100.0,6.6,0.1238,89.2,1.0,Execution\\ Accuracy
0,1,Code Generation,WikiSQL - Code Generation benchmarking,2017-08,"Seq2Seq (Zhong et al., 2017)",23.4,27.96,23.4,100,83.7,0.28,Exact\\ Match\\ Accuracy
1,1,Code Generation,WikiSQL - Code Generation benchmarking,2017-08,"Seq2SQL (Zhong et al., 2017)",48.3,57.71,24.9,0.4129,83.7,0.58,Exact\\ Match\\ Accuracy
2,1,Code Generation,WikiSQL - Code Generation benchmarking,2018-03,"PT-MAML (Huang et al., 2018)",62.8,75.03,14.5,0.2405,83.7,0.75,Exact\\ Match\\ Accuracy
3,1,Code Generation,WikiSQL - Code Generation benchmarking,2018-10,Tranx,68.6,81.96,5.8,0.0962,83.7,0.82,Exact\\ Match\\ Accuracy
4,1,Code Generation,WikiSQL - Code Generation benchmarking,2019-10,NL2SQL-RULE,83.7,100.0,15.1,0.2504,83.7,1.0,Exact\\ Match\\ Accuracy
0,1,Relation Extraction,Wikipedia-Wikidata relations - Relation Extraction benchmarking,2017-09,ContextAtt,0.159,100.0,0.159,100,0.159,1.0,Error\\ rate
0,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2017-09,Bi-LSTM<sub>att+LEX</sub>,72.0,90.34,72.0,100,79.7,0.9,Senseval\\ 2
1,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-05,GAS<sub>ext</sub>,72.2,90.59,0.2,0.026,79.7,0.91,Senseval\\ 2
2,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",75.15,94.29,2.95,0.3831,79.7,0.94,Senseval\\ 2
3,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",79.7,100.0,4.55,0.5909,79.7,1.0,Senseval\\ 2
0,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2017-09,Bi-LSTM<sub>att+LEX</sub>,69.4,89.2,69.4,100,77.8,0.89,Senseval\\ 3
1,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-02,ELMo,69.6,89.46,0.2,0.0238,77.8,0.89,Senseval\\ 3
2,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-05,GAS<sub>ext</sub>,70.5,90.62,0.9,0.1071,77.8,0.91,Senseval\\ 3
3,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",77.8,100.0,7.3,0.869,77.8,1.0,Senseval\\ 3
0,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2017-09,Bi-LSTM<sub>att+LEX</sub>,66.4,84.37,66.4,100,78.7,0.84,SemEval\\ 2013
1,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2017-09,Bi-LSTM<sub>att+LEX+POS</sub>,66.9,85.01,0.5,0.0407,78.7,0.85,SemEval\\ 2013
2,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-05,GAS<sub>ext</sub>,67.2,85.39,0.3,0.0244,78.7,0.85,SemEval\\ 2013
3,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",72.63,92.29,5.43,0.4415,78.7,0.92,SemEval\\ 2013
4,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",78.7,100.0,6.07,0.4935,78.7,1.0,SemEval\\ 2013
5,1,Word Sense Disambiguation,Knowledge-based: - Word Sense Disambiguation benchmarking,2018-01,WSD-TM,65.3,100.0,65.3,100,65.3,0.83,SemEval\\ 2013
0,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2017-09,Bi-LSTM<sub>att+LEX</sub>,72.4,87.65,72.4,100,82.6,0.88,SemEval\\ 2015
1,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-05,GAS<sub>ext</sub>,72.6,87.89,0.2,0.0196,82.6,0.88,SemEval\\ 2015
2,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",74.46,90.15,1.86,0.1824,82.6,0.9,SemEval\\ 2015
3,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",82.6,100.0,8.14,0.798,82.6,1.0,SemEval\\ 2015
4,1,Word Sense Disambiguation,Knowledge-based: - Word Sense Disambiguation benchmarking,2018-01,WSD-TM,69.6,100.0,69.6,100,69.6,0.84,SemEval\\ 2015
0,1,Named Entity Recognition,Long-tail emerging entities - Named Entity Recognition benchmarking,2017-09,SpinningBytes,39.33,100.0,39.33,100,39.33,1.0,F1\\ \\(surface\\ form\\)
0,1,Machine Translation,20NEWS - Machine Translation benchmarking,2017-09,tensorflow/tensor2tensor,5.0,100.0,5,100,5,0.06,1\\-of\\-100\\ Accuracy
1,1,Conversational Response Selection,PolyAI Reddit - Conversational Response Selection benchmarking,2018-02,ELMO,19.3,26.88,19.3,100,71.8,0.23,1\\-of\\-100\\ Accuracy
2,1,Conversational Response Selection,PolyAI Reddit - Conversational Response Selection benchmarking,2018-03,USE,47.7,66.43,28.4,0.541,71.8,0.57,1\\-of\\-100\\ Accuracy
3,1,Conversational Response Selection,PolyAI Reddit - Conversational Response Selection benchmarking,2019-04,PolyAI Encoder,61.3,85.38,13.6,0.259,71.8,0.73,1\\-of\\-100\\ Accuracy
4,1,Conversational Response Selection,PolyAI Reddit - Conversational Response Selection benchmarking,2019-11,Multi-context ConveRT,71.8,100.0,10.5,0.2,71.8,0.85,1\\-of\\-100\\ Accuracy
5,1,Conversational Response Selection,DSTC7 Ubuntu - Conversational Response Selection benchmarking,2018-12,Sequential Inference Models,60.8,85.39,60.8,100,71.2,0.72,1\\-of\\-100\\ Accuracy
6,1,Conversational Response Selection,DSTC7 Ubuntu - Conversational Response Selection benchmarking,2019-01,Sequential Attention-based Network,64.5,90.59,3.7,0.3558,71.2,0.77,1\\-of\\-100\\ Accuracy
7,1,Conversational Response Selection,DSTC7 Ubuntu - Conversational Response Selection benchmarking,2019-04,Bi-encoder,66.3,93.12,1.8,0.1731,71.2,0.79,1\\-of\\-100\\ Accuracy
8,1,Conversational Response Selection,DSTC7 Ubuntu - Conversational Response Selection benchmarking,2019-04,Bi-encoder (v2),70.9,99.58,4.6,0.4423,71.2,0.84,1\\-of\\-100\\ Accuracy
9,1,Conversational Response Selection,DSTC7 Ubuntu - Conversational Response Selection benchmarking,2019-11,Multi-context ConveRT,71.2,100.0,0.3,0.0288,71.2,0.84,1\\-of\\-100\\ Accuracy
10,1,Conversational Response Selection,PolyAI OpenSubtitles - Conversational Response Selection benchmarking,2019-04,PolyAI Encoder,30.6,100.0,30.6,100,30.6,0.36,1\\-of\\-100\\ Accuracy
11,1,Conversational Response Selection,PolyAI AmazonQA - Conversational Response Selection benchmarking,2019-04,PolyAI Encoder,71.3,84.58,71.3,100,84.3,0.85,1\\-of\\-100\\ Accuracy
12,1,Conversational Response Selection,PolyAI AmazonQA - Conversational Response Selection benchmarking,2019-11,ConveRT,84.3,100.0,13.0,1.0,84.3,1.0,1\\-of\\-100\\ Accuracy
0,1,Common Sense Reasoning,Visual Dialog v0.9 - Common Sense Reasoning benchmarking,2018-09,NMN [kottur2018visual],80.1,100.0,80.1,100,80.1,0.99,1\\ in\\ 10\\ R\\-at\\-5
1,1,Common Sense Reasoning,Visual Dialog  v0.9 - Common Sense Reasoning benchmarking,2019-09,PDUN,81.0,100.0,81,100,81,1.0,1\\ in\\ 10\\ R\\-at\\-5
0,1,Question Answering,WikiHop - Question Answering benchmarking,2017-10,BiDAF,42.9,60.76,42.9,100,70.6,0.48,Test
1,1,Question Answering,WikiHop - Question Answering benchmarking,2018-04,Coref-GRU,59.3,83.99,16.4,0.5921,70.6,0.66,Test
2,1,Question Answering,WikiHop - Question Answering benchmarking,2018-09,MHQA,65.4,92.63,6.1,0.2202,70.6,0.73,Test
3,1,Question Answering,WikiHop - Question Answering benchmarking,2019-01,CFC,70.6,100.0,5.2,0.1877,70.6,0.79,Test
4,1,Common Sense Reasoning,SWAG - Common Sense Reasoning benchmarking,2018-08,ESIM + ELMo,59.2,65.85,59.2,100,89.9,0.66,Test
5,1,Common Sense Reasoning,SWAG - Common Sense Reasoning benchmarking,2018-10,BERT Large,86.3,96.0,27.1,0.8827,89.9,0.96,Test
6,1,Common Sense Reasoning,SWAG - Common Sense Reasoning benchmarking,2019-07,RoBERTa,89.9,100.0,3.6,0.1173,89.9,1.0,Test
0,1,Fine-Grained Opinion Analysis,MPQA - Fine-Grained Opinion Analysis benchmarking,2017-11,FS-MTL,83.8,98.69,83.8,100,84.91,0.99,Holder\\ Binary\\ F1
1,1,Fine-Grained Opinion Analysis,MPQA - Fine-Grained Opinion Analysis benchmarking,2019-06,SRL-SAWR,84.91,100.0,1.11,1.0,84.91,1.0,Holder\\ Binary\\ F1
0,1,Fine-Grained Opinion Analysis,MPQA - Fine-Grained Opinion Analysis benchmarking,2017-11,FS-MTL,72.06,98.32,72.06,100,73.29,0.98,Target\\ Binary\\ F1
1,1,Fine-Grained Opinion Analysis,MPQA - Fine-Grained Opinion Analysis benchmarking,2019-06,SRL-SAWR,73.29,100.0,1.23,1.0,73.29,1.0,Target\\ Binary\\ F1
0,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2017-11,PRPN (tuned),47.3,84.92,47.3,100,55.7,0.85,Mean\\ F1\\ \\(WSJ\\)
1,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2018-08,DMV + invertible projector,47.9,86.0,0.6,0.0714,55.7,0.86,Mean\\ F1\\ \\(WSJ\\)
2,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2018-10,ON-LSTM (tuned),48.1,86.36,0.2,0.0238,55.7,0.86,Mean\\ F1\\ \\(WSJ\\)
3,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2019-06,DIORA (+PP),55.7,100.0,7.6,0.9048,55.7,1.0,Mean\\ F1\\ \\(WSJ\\)
0,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2017-11,PRPN (tuned),47.9,79.7,47.9,100,60.1,0.8,Max\\ F1\\ \\(WSJ\\)
1,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2018-10,ON-LSTM,49.4,82.2,1.5,0.123,60.1,0.82,Max\\ F1\\ \\(WSJ\\)
2,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2018-10,ON-LSTM (tuned),50.0,83.19,0.6,0.0492,60.1,0.83,Max\\ F1\\ \\(WSJ\\)
3,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2019-04,URNNG,52.4,87.19,2.4,0.1967,60.1,0.87,Max\\ F1\\ \\(WSJ\\)
4,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2019-06,DIORA (+PP),56.2,93.51,3.8,0.3115,60.1,0.94,Max\\ F1\\ \\(WSJ\\)
5,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2019-06,Compound PCFG,60.1,100.0,3.9,0.3197,60.1,1.0,Max\\ F1\\ \\(WSJ\\)
0,1,Chunking,CoNLL 2000 - Chunking benchmarking,2017-11,Adversarial Training,95.25,98.48,95.25,100,96.72,0.98,Exact\\ Span\\ F1
1,1,Chunking,CoNLL 2000 - Chunking benchmarking,2018-08,Flair,96.72,100.0,1.47,1.0,96.72,1.0,Exact\\ Span\\ F1
2,1,Scientific Concept Extraction,STM-corpus - Scientific Concept Extraction benchmarking,2020-01,SciBERT (full data),65.5,98.64,65.5,100,66.4,0.68,Exact\\ Span\\ F1
3,1,Scientific Concept Extraction,STM-corpus - Scientific Concept Extraction benchmarking,2020-01,SciBERT (active learning),66.4,100.0,0.9,1.0,66.4,0.69,Exact\\ Span\\ F1
0,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2017-11,DRMM,0.431,80.1,0.431,100,0.5381,0.01,nDCG\\-at\\-20
1,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2018-09,PACRR,0.445,82.7,0.014,0.1307,0.5381,0.01,nDCG\\-at\\-20
2,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2018-09,POSIT-DRMM-MV,0.464,86.23,0.019,0.1774,0.5381,0.01,nDCG\\-at\\-20
3,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2019-04,CEDR-KNRM,0.5381,100.0,0.0741,0.6919,0.5381,0.02,nDCG\\-at\\-20
4,1,Document Ranking,ClueWeb09-B - Document Ranking benchmarking,2019-06,XLNet,31.1,100.0,31.1,100,31.1,1.0,nDCG\\-at\\-20
0,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2017-11,DRMM,0.382,81.85,0.382,100,0.4667,0.82,P\\-at\\-20
1,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2018-09,POSIT-DRMM-MV,0.389,83.35,0.007,0.0826,0.4667,0.83,P\\-at\\-20
2,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2018-10,SNRM-PRF,0.3948,84.59,0.0058,0.0685,0.4667,0.85,P\\-at\\-20
3,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2018-10,NPRF-DRMM,0.4064,87.08,0.0116,0.137,0.4667,0.87,P\\-at\\-20
4,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2019-03,BERT FT(Microblog),0.4287,91.86,0.0223,0.2633,0.4667,0.92,P\\-at\\-20
5,1,Ad-Hoc Information Retrieval,TREC Robust04 - Ad-Hoc Information Retrieval benchmarking,2019-04,CEDR-KNRM,0.4667,100.0,0.038,0.4486,0.4667,1.0,P\\-at\\-20
0,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,-0.512,100.0,-0.512,100,-0.512,1.0,LPIPS
0,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,13.0,79.27,13.0,100,16.4,0.79,Acc
1,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2019-04,DM-GAN,16.4,100.0,3.4,1.0,16.4,1.0,Acc
0,1,Text-to-Image Generation,Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking,2017-11,AttnGAN,-11.9,100.0,-11.9,100,-11.9,1.0,Real
0,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2017-11,AttnGAN,25.88,77.39,25.88,100,33.44,0.77,SOA\\-C
1,1,Text-to-Image Generation,COCO - Text-to-Image Generation benchmarking,2019-04,DM-GAN,33.44,100.0,7.56,1.0,33.44,1.0,SOA\\-C
0,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2017-12,Gong,2.2721,95.87,2.2721,100,2.37,0.02,CIDEr
1,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2019-04,S_1^R,2.37,100.0,0.0979,1.0,2.37,0.02,CIDEr
2,1,Image Captioning,COCO - Image Captioning benchmarking,2019-05,"NIC (ResNet-50, CutMix)",77.6,100.0,77.6,100,77.6,0.66,CIDEr
3,1,Image Captioning,Flickr30k Captions test - Image Captioning benchmarking,2019-09,Unified VLP,67.4,100.0,67.4,100,67.4,0.58,CIDEr
4,1,Image Captioning,COCO Captions test - Image Captioning benchmarking,2019-09,Unified VLP,116.9,100.0,116.9,100,116.9,1.0,CIDEr
0,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2017-12,Gong,8.3453,95.59,8.3453,100,8.73,0.96,NIST
1,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2018-04,Sys1-Primary,8.5105,97.49,0.1652,0.4294,8.73,0.97,NIST
2,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2018-05,Slug,8.613,98.66,0.1025,0.2664,8.73,0.99,NIST
3,1,Data-to-Text Generation,E2E NLG Challenge - Data-to-Text Generation benchmarking,2019-04,S_1^R,8.73,100.0,0.117,0.3041,8.73,1.0,NIST
0,1,Word Sense Disambiguation,Knowledge-based: - Word Sense Disambiguation benchmarking,2018-01,WSD-TM,66.9,100.0,66.9,100,66.9,1.0,All
0,1,Grammatical Error Correction,_Restricted_ - Grammatical Error Correction benchmarking,2018-01,CNN Seq2Seq,57.47,100.0,57.47,100,57.47,0.92,GLEU
1,1,Grammatical Error Correction,Unrestricted - Grammatical Error Correction benchmarking,2018-07,CNN Seq2Seq + Fluency Boost and inference,62.37,100.0,62.37,100,62.37,1.0,GLEU
2,1,Grammatical Error Correction,JFLEG - Grammatical Error Correction benchmarking,2019-03,Copy-augmented Model (4 Ensemble +Denoising Autoencoder),61.0,100.0,61,100,61,0.98,GLEU
0,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-02,ELMo,62.2,84.74,62.2,100,73.4,0.85,SemEval\\ 2007
1,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2018-11,"SemCor+WNGT, vocabulary reduced, ensemble",66.81,91.02,4.61,0.4116,73.4,0.91,SemEval\\ 2007
2,1,Word Sense Disambiguation,Supervised: - Word Sense Disambiguation benchmarking,2019-05,"SemCor+WNGC, hypernyms",73.4,100.0,6.59,0.5884,73.4,1.0,SemEval\\ 2007
0,1,Question Answering,RACE - Question Answering benchmarking,2018-03,BiAttention MRU,60.2,70.45,60.2,100,85.45,0.7,RACE\\-m
1,1,Question Answering,RACE - Question Answering benchmarking,2018-06,Finetuned Transformer LM,62.9,73.61,2.7,0.1069,85.45,0.74,RACE\\-m
2,1,Question Answering,RACE - Question Answering benchmarking,2019-06,XLNet,85.45,100.0,22.55,0.8931,85.45,1.0,RACE\\-m
0,1,Question Answering,RACE - Question Answering benchmarking,2018-03,BiAttention MRU,50.3,62.71,50.3,100,80.21,0.63,RACE\\-h
1,1,Question Answering,RACE - Question Answering benchmarking,2018-06,Finetuned Transformer LM,57.4,71.56,7.1,0.2374,80.21,0.72,RACE\\-h
2,1,Question Answering,RACE - Question Answering benchmarking,2019-06,XLNet,80.21,100.0,22.81,0.7626,80.21,1.0,RACE\\-h
0,1,Question Answering,RACE - Question Answering benchmarking,2018-03,BiAttention MRU,53.3,65.2,53.3,100,81.75,0.65,RACE
1,1,Question Answering,RACE - Question Answering benchmarking,2018-06,Finetuned Transformer LM,59.0,72.17,5.7,0.2004,81.75,0.72,RACE
2,1,Question Answering,RACE - Question Answering benchmarking,2019-06,XLNet,81.75,100.0,22.75,0.7996,81.75,1.0,RACE
0,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2018-03,GenSen,71.4,78.2,71.4,100,91.3,0.78,Matched
1,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2018-04,Multi-task BiLSTM + Attn,72.2,79.08,0.8,0.0402,91.3,0.79,Matched
2,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2018-06,Finetuned Transformer LM,82.1,89.92,9.9,0.4975,91.3,0.9,Matched
3,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2019-01,MT-DNN,86.7,94.96,4.6,0.2312,91.3,0.95,Matched
4,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2019-06,XLNet (single model),90.8,99.45,4.1,0.206,91.3,0.99,Matched
5,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2019-09,ALBERT,91.3,100.0,0.5,0.0251,91.3,1.0,Matched
0,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2018-03,GenSen,71.3,79.05,71.3,100,90.2,0.79,Mismatched
1,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2018-04,Multi-task BiLSTM + Attn,72.1,79.93,0.8,0.0423,90.2,0.8,Mismatched
2,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2018-06,Finetuned Transformer LM,81.4,90.24,9.3,0.4921,90.2,0.9,Mismatched
3,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2019-01,MT-DNN,86.0,95.34,4.6,0.2434,90.2,0.95,Mismatched
4,1,Natural Language Inference,MultiNLI - Natural Language Inference benchmarking,2019-07,RoBERTa,90.2,100.0,4.2,0.2222,90.2,1.0,Mismatched
0,1,Relation Extraction,ADE Corpus - Relation Extraction benchmarking,2018-04,multi-head,74.58,96.62,74.58,100,77.19,0.97,RE\\+\\ Macro\\ F1
1,1,Relation Extraction,ADE Corpus - Relation Extraction benchmarking,2018-08,multi-head + AT,75.52,97.84,0.94,0.3602,77.19,0.98,RE\\+\\ Macro\\ F1
2,1,Relation Extraction,ADE Corpus - Relation Extraction benchmarking,2019-05,Relation-Metric,77.19,100.0,1.67,0.6398,77.19,1.0,RE\\+\\ Macro\\ F1
3,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2018-04,multi-head,62.04,96.34,62.04,100,64.4,0.8,RE\\+\\ Macro\\ F1
4,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2018-12,Biaffine attention,64.4,100.0,2.36,1.0,64.4,0.83,RE\\+\\ Macro\\ F1
0,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2018-04,multi-head,83.9,97.33,83.9,100,86.2,0.96,NER\\ Macro\\ F1
1,1,Relation Extraction,CoNLL04 - Relation Extraction benchmarking,2018-12,Biaffine attention,86.2,100.0,2.3,1.0,86.2,0.99,NER\\ Macro\\ F1
2,1,Relation Extraction,ADE Corpus - Relation Extraction benchmarking,2018-04,multi-head,86.4,99.29,86.4,100,87.02,0.99,NER\\ Macro\\ F1
3,1,Relation Extraction,ADE Corpus - Relation Extraction benchmarking,2018-08,multi-head + AT,86.73,99.67,0.33,0.5323,87.02,1.0,NER\\ Macro\\ F1
4,1,Relation Extraction,ADE Corpus - Relation Extraction benchmarking,2019-05,Relation-Metric,87.02,100.0,0.29,0.4677,87.02,1.0,NER\\ Macro\\ F1
0,1,Text Classification,LOCAL DATASET - Text Classification benchmarking,2018-05,RMDL (15 RDLs,90.79,100.0,90.79,100,90.79,1.0,Accuracy\\ \\(%\\)
0,1,Common Sense Reasoning,Event2Mind test - Common Sense Reasoning benchmarking,2018-05,BiRNN 100d,4.22,95.91,4.22,100,4.4,0.95,Average\\ Cross\\-Ent
1,1,Common Sense Reasoning,Event2Mind test - Common Sense Reasoning benchmarking,2018-05,ConvNet,4.4,100.0,0.18,1.0,4.4,0.99,Average\\ Cross\\-Ent
2,1,Common Sense Reasoning,Event2Mind dev - Common Sense Reasoning benchmarking,2018-05,ConvNet,4.44,100.0,4.44,100,4.44,1.0,Average\\ Cross\\-Ent
0,1,Language Acquisition,SLAM 2018 - Language Acquisition benchmarking,2018-06,Context Based Model,0.821,100.0,0.821,100,0.821,1.0,AUC
1,1,Relationship extraction using distant supervision,New York Times Corpus - Relationship extraction using distant supervision benchmarking,2018-08,BGRU-SET,0.39,100.0,0.39,100,0.39,0.48,AUC
0,1,Common Sense Reasoning,Winograd Schema Challenge - Common Sense Reasoning benchmarking,2018-06,Ensemble of 14 LMs,63.7,90.1,63.7,100,70.7,0.9,Score
1,1,Common Sense Reasoning,Winograd Schema Challenge - Common Sense Reasoning benchmarking,2019-02,GPT-2,70.7,100.0,7.0,1.0,70.7,1.0,Score
2,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2018-08,HAN,28.65,55.04,28.65,100,52.05,0.41,Score
3,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2019-02,MuRel,39.54,75.97,10.89,0.4654,52.05,0.56,Score
4,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2019-05,UpDn+SCR (VQA-X),49.45,95.0,9.91,0.4235,52.05,0.7,Score
5,1,Visual Question Answering,VQA-CP - Visual Question Answering benchmarking,2019-09,Learned-Mixin +H,52.05,100.0,2.6,0.1111,52.05,0.74,Score
0,1,Entity Typing,Freebase FIGER - Entity Typing benchmarking,2018-06,TextEnt-full,94.8,100.0,94.8,100,94.8,1.0,BEP
0,1,Text Classification,20NEWS - Text Classification benchmarking,2018-06,TextEnt-full,83.9,97.33,83.9,100,86.2,0.91,F\\-measure
1,1,Text Classification,20NEWS - Text Classification benchmarking,2019-09,NABoE-full,86.2,100.0,2.3,1.0,86.2,0.94,F\\-measure
2,1,Text Classification,R8 - Text Classification benchmarking,2018-06,TextEnt-full,91.0,99.24,91.0,100,91.7,0.99,F\\-measure
3,1,Text Classification,R8 - Text Classification benchmarking,2019-09,NABoE-full,91.7,100.0,0.7,1.0,91.7,1.0,F\\-measure
0,1,Entity Typing,Freebase FIGER - Entity Typing benchmarking,2018-06,TextEnt-full,84.2,100.0,84.2,100,84.2,1.0,Macro\\ F1
1,1,Entity Linking,FIGER - Entity Linking benchmarking,2019-05,ERNIE,76.51,100.0,76.51,100,76.51,0.91,Macro\\ F1
2,1,Text Classification,RCV1 - Text Classification benchmarking,2019-08,HiLAP (bow-CNN),60.1,100.0,60.1,100,60.1,0.71,Macro\\ F1
0,1,Entity Typing,Freebase FIGER - Entity Typing benchmarking,2018-06,TextEnt-full,85.7,100.0,85.7,100,85.7,0.97,Micro\\ F1
1,1,Entity Linking,FIGER - Entity Linking benchmarking,2019-05,ERNIE,73.39,100.0,73.39,100,73.39,0.83,Micro\\ F1
2,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,bert-base,73.2,100.0,73.2,100,73.2,0.83,Micro\\ F1
3,1,Text Classification,RCV1 - Text Classification benchmarking,2019-08,HiLAP (bow-CNN),83.3,94.12,83.3,100,88.5,0.94,Micro\\ F1
4,1,Text Classification,RCV1 - Text Classification benchmarking,2020-02,MAGNET,88.5,100.0,5.2,1.0,88.5,1.0,Micro\\ F1
0,1,Paraphrase Identification,2017_test set - Paraphrase Identification benchmarking,2018-06,CNN,50.0,100.0,50,100,50,1.0,10\\ fold\\ Cross\\ validation
0,1,Multimodal Sentiment Analysis,CMU-MOSEI - Multimodal Sentiment Analysis benchmarking,2018-07,Graph-MFN,-0.71,100.0,-0.71,100,-0.71,1.0,MAE
0,1,Common Sense Reasoning,SWAG - Common Sense Reasoning benchmarking,2018-08,ESIM + ELMo,59.1,68.24,59.1,100,86.6,0.68,Dev
1,1,Common Sense Reasoning,SWAG - Common Sense Reasoning benchmarking,2018-10,BERT Base,81.6,94.23,22.5,0.8182,86.6,0.94,Dev
2,1,Common Sense Reasoning,SWAG - Common Sense Reasoning benchmarking,2018-10,BERT Large,86.6,100.0,5.0,0.1818,86.6,1.0,Dev
0,1,Question Answering,CoQA - Question Answering benchmarking,2018-08,Vanilla DrQA (single model),54.5,66.06,54.5,100,82.5,0.66,In\\-domain
1,1,Question Answering,CoQA - Question Answering benchmarking,2018-08,DrQA + seq2seq with copy attention (single model),67.0,81.21,12.5,0.4464,82.5,0.81,In\\-domain
2,1,Question Answering,CoQA - Question Answering benchmarking,2018-09,BiDAF++ (single model),69.4,84.12,2.4,0.0857,82.5,0.84,In\\-domain
3,1,Question Answering,CoQA - Question Answering benchmarking,2018-10,FlowQA (single model),76.3,92.48,6.9,0.2464,82.5,0.92,In\\-domain
4,1,Question Answering,CoQA - Question Answering benchmarking,2018-10,BERT-base finetune (single model),79.8,96.73,3.5,0.125,82.5,0.97,In\\-domain
5,1,Question Answering,CoQA - Question Answering benchmarking,2018-10,BERT Large Augmented (single model),82.5,100.0,2.7,0.0964,82.5,1.0,In\\-domain
0,1,Question Answering,CoQA - Question Answering benchmarking,2018-08,Vanilla DrQA (single model),52.6,64.86,52.6,100,81.1,0.65,Overall
1,1,Question Answering,CoQA - Question Answering benchmarking,2018-08,DrQA + seq2seq with copy attention (single model),65.1,80.27,12.5,0.4386,81.1,0.8,Overall
2,1,Question Answering,CoQA - Question Answering benchmarking,2018-09,BiDAF++ (single model),67.8,83.6,2.7,0.0947,81.1,0.84,Overall
3,1,Question Answering,CoQA - Question Answering benchmarking,2018-10,FlowQA (single model),75.0,92.48,7.2,0.2526,81.1,0.92,Overall
4,1,Question Answering,CoQA - Question Answering benchmarking,2018-10,BERT Large Augmented (single model),81.1,100.0,6.1,0.214,81.1,1.0,Overall
0,1,Question Answering,CoQA - Question Answering benchmarking,2018-08,Vanilla DrQA (single model),47.9,61.73,47.9,100,77.6,0.62,Out\\-of\\-domain
1,1,Question Answering,CoQA - Question Answering benchmarking,2018-08,DrQA + seq2seq with copy attention (single model),60.4,77.84,12.5,0.4209,77.6,0.78,Out\\-of\\-domain
2,1,Question Answering,CoQA - Question Answering benchmarking,2018-09,BiDAF++ (single model),63.8,82.22,3.4,0.1145,77.6,0.82,Out\\-of\\-domain
3,1,Question Answering,CoQA - Question Answering benchmarking,2018-10,FlowQA (single model),71.8,92.53,8.0,0.2694,77.6,0.93,Out\\-of\\-domain
4,1,Question Answering,CoQA - Question Answering benchmarking,2018-10,BERT Large Augmented (single model),77.6,100.0,5.8,0.1953,77.6,1.0,Out\\-of\\-domain
0,1,Generative Question Answering,CoQA - Generative Question Answering benchmarking,2018-08,PGNet,45.4,55.03,45.4,100,82.5,0.55,F1\\-Score
1,1,Generative Question Answering,CoQA - Generative Question Answering benchmarking,2019-05,UniLM,82.5,100.0,37.1,1.0,82.5,1.0,F1\\-Score
0,1,Entity Linking,AIDA-CoNLL - Entity Linking benchmarking,2018-08,base model + att + global ,82.6,100.0,82.6,100,82.6,1.0,Micro\\-F1\\ strong
0,1,Entity Linking,AIDA-CoNLL - Entity Linking benchmarking,2018-08,base model + att + global ,82.4,100.0,82.4,100,82.4,1.0,Macro\\-F1\\ strong
0,1,Machine Translation,WMT2014 English-French - Machine Translation benchmarking,2018-08,Noisy back-translation,43.8,100.0,43.8,100,43.8,1.0,SacreBLEU
1,1,Machine Translation,WMT2014 English-German - Machine Translation benchmarking,2018-08,Noisy back-translation,33.8,100.0,33.8,100,33.8,0.77,SacreBLEU
2,1,Machine Translation,WMT2019 English-German - Machine Translation benchmarking,2019-07,Facebook FAIR (ensemble),42.7,100.0,42.7,100,42.7,0.97,SacreBLEU
0,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2018-08,DMV + invertible projector,60.2,88.92,60.2,100,67.7,0.89,Mean\\ F1\\ \\(WSJ10\\)
1,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2018-10,ON-LSTM,65.1,96.16,4.9,0.6533,67.7,0.96,Mean\\ F1\\ \\(WSJ10\\)
2,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2019-06,DIORA,67.7,100.0,2.6,0.3467,67.7,1.0,Mean\\ F1\\ \\(WSJ10\\)
0,1,Joint Entity and Relation Extraction,SciERC - Joint Entity and Relation Extraction benchmarking,2018-08,SciIE,64.2,95.11,64.2,100,67.5,0.95,Entity\\ F1
1,1,Joint Entity and Relation Extraction,SciERC - Joint Entity and Relation Extraction benchmarking,2019-04,DyGIE,65.2,96.59,1.0,0.303,67.5,0.97,Entity\\ F1
2,1,Joint Entity and Relation Extraction,SciERC - Joint Entity and Relation Extraction benchmarking,2019-09,DyGIE++,67.5,100.0,2.3,0.697,67.5,1.0,Entity\\ F1
0,1,Joint Entity and Relation Extraction,SciERC - Joint Entity and Relation Extraction benchmarking,2018-08,SciIE,39.3,81.2,39.3,100,48.4,0.81,Relation\\ F1
1,1,Joint Entity and Relation Extraction,SciERC - Joint Entity and Relation Extraction benchmarking,2019-04,DyGIE,41.6,85.95,2.3,0.2527,48.4,0.86,Relation\\ F1
2,1,Joint Entity and Relation Extraction,SciERC - Joint Entity and Relation Extraction benchmarking,2019-09,DyGIE++,48.4,100.0,6.8,0.7473,48.4,1.0,Relation\\ F1
0,1,Multi-Document Summarization,Multi-News - Multi-Document Summarization benchmarking,2018-08,CopyTransformer,17.37,100.0,17.37,100,17.37,1.0,ROUGE\\-SU4
0,1,Question Answering,QuAC - Question Answering benchmarking,2018-10,FlowQA (single model),5.8,100.0,5.8,100,5.8,1.0,HEQD
0,1,Question Answering,QuAC - Question Answering benchmarking,2018-10,FlowQA (single model),59.6,100.0,59.6,100,59.6,1.0,HEQQ
0,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2018-10,ON-LSTM,66.8,97.52,66.8,100,68.5,0.98,Max\\ F1\\ \\(WSJ10\\)
1,1,Constituency Grammar Induction,PTB - Constituency Grammar Induction benchmarking,2019-06,DIORA,68.5,100.0,1.7,1.0,68.5,1.0,Max\\ F1\\ \\(WSJ10\\)
0,1,Information Retrieval,TREC-PM - Information Retrieval benchmarking,2018-11,hpictall,0.5545,98.93,0.5545,100,0.5605,0.99,infNDCG
1,1,Information Retrieval,TREC-PM - Information Retrieval benchmarking,2018-11,hpipubcommon,0.5605,100.0,0.006,1.0,0.5605,1.0,infNDCG
0,1,Phrase Grounding,ReferIt - Phrase Grounding benchmarking,2018-11,VG_BiLSTM_VGG,62.76,100.0,62.76,100,62.76,0.91,Pointing\\ Game\\ Accuracy
1,1,Phrase Grounding,Flickr30k - Phrase Grounding benchmarking,2018-11,COCO_ELMo_PNASNet,69.19,100.0,69.19,100,69.19,1.0,Pointing\\ Game\\ Accuracy
2,1,Phrase Grounding,Visual Genome - Phrase Grounding benchmarking,2018-11,VG_ELMo_PNASNet,55.16,100.0,55.16,100,55.16,0.8,Pointing\\ Game\\ Accuracy
0,1,Intent Detection,SNIPS - Intent Detection benchmarking,2018-12,Capsule-NLU,91.8,99.53,91.8,100,92.23,1.0,Slot\\ F1\\ Score
1,1,Intent Detection,SNIPS - Intent Detection benchmarking,2019-06,SF-ID (BLSTM) network,92.23,100.0,0.43,1.0,92.23,1.0,Slot\\ F1\\ Score
0,1,Intent Detection,SNIPS - Intent Detection benchmarking,2018-12,Capsule-NLU,97.7,100.0,97.7,100,97.7,1.0,Intent\\ Accuracy
0,1,Conversational Response Selection,Advising Corpus - Conversational Response Selection benchmarking,2019-01,CtxDec & -Rev,97.8,100.0,97.8,100,97.8,1.0,R@50
0,1,Question Answering,Natural Questions - Question Answering benchmarking,2019-01,BERT-joint,66.2,100.0,66.2,100,66.2,1.0,F1\\ \\(Long\\)
0,1,Question Answering,Natural Questions - Question Answering benchmarking,2019-01,BERT-joint,52.1,100.0,52.1,100,52.1,1.0,F1\\ \\(Short\\)
0,1,Language Modelling,The Pile - Language Modelling benchmarking,2019-02,GPT-2 (Zero-Shot),1.2253,100.0,1.2253,100,1.2253,1.0,Bits\\ per\\ byte
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Subtask 4 - Aspect-Based Sentiment Analysis benchmarking,2019-03,BERT-pair-QA-B,89.9,100.0,89.9,100,89.9,1.0,Accuracy\\ \\(3\\-way\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Subtask 4 - Aspect-Based Sentiment Analysis benchmarking,2019-03,BERT-pair-QA-B,85.9,100.0,85.9,100,85.9,1.0,Accuracy\\ \\(4\\-way\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Subtask 4 - Aspect-Based Sentiment Analysis benchmarking,2019-03,BERT-pair-QA-B,95.6,100.0,95.6,100,95.6,1.0,Binary\\ Accuracy
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 1 - Aspect-Based Sentiment Analysis benchmarking,2019-04,BERT-PT,84.26,100.0,84.26,100,84.26,1.0,Laptop\\ \\(F1\\)
0,1,Aspect-Based Sentiment Analysis,SemEval 2014 Task 4 Sub Task 1 - Aspect-Based Sentiment Analysis benchmarking,2019-04,BERT-PT,77.97,100.0,77.97,100,77.97,1.0,Restaurant\\ \\(F1\\)
0,1,Text-To-Speech Synthesis,CMUDict 0.7b - Text-To-Speech Synthesis benchmarking,2019-04,Token-Level Ensemble Distillation,4.6,100.0,4.6,100,4.6,1.0,Phoneme\\ Error\\ Rate
0,1,Text-To-Speech Synthesis,CMUDict 0.7b - Text-To-Speech Synthesis benchmarking,2019-04,Token-Level Ensemble Distillation,19.88,100.0,19.88,100,19.88,1.0,Word\\ Error\\ Rate\\ \\(WER\\)
1,1,Arabic Text Diacritization,Tashkeela - Arabic Text Diacritization benchmarking,2019-04,Shakkala,0.1119,100.0,0.1119,100,0.1119,0.01,Word\\ Error\\ Rate\\ \\(WER\\)
0,1,Text Classification,IMDb - Text Classification benchmarking,2019-04,KD-LSTMreg,53.7,100.0,53.7,100,53.7,1.0,Accuracy\\ \\(10\\ classes\\)
0,1,Passage Re-Ranking,TREC-PM - Passage Re-Ranking benchmarking,2019-04,BERT + Doc2query,36.5,100.0,36.5,100,36.5,1.0,mAP
0,1,Arabic Text Diacritization,Tashkeela - Arabic Text Diacritization benchmarking,2019-04,Shakkala,0.0373,100.0,0.0373,100,0.0373,1.0,Diacritic\\ Error\\ Rate
0,1,Question Answering,HotpotQA - Question Answering benchmarking,2019-05,DFGN,0.5982,100.0,0.5982,100,0.5982,1.0,JOINT\\-F1
0,1,AMR Parsing,LDC2017T10 - AMR Parsing benchmarking,2019-05,Sequence-to-Graph Transduction,76.3,100.0,76.3,100,76.3,1.0,Smatch
0,1,Text-To-Speech Synthesis,LJSpeech - Text-To-Speech Synthesis benchmarking,2019-05,Merlin,2.4,62.5,2.4,100,3.84,0.62,Audio\\ Quality\\ MOS
1,1,Text-To-Speech Synthesis,LJSpeech - Text-To-Speech Synthesis benchmarking,2019-05,FastSpeech (Mel + WaveGlow),3.84,100.0,1.44,1.0,3.84,1.0,Audio\\ Quality\\ MOS
0,1,Multi-Label Text Classification,Kan-Shan Cup - Multi-Label Text Classification benchmarking,2019-05,LAHA,54.65,100.0,54.65,100,54.65,0.59,nDCG\\-at\\-5
1,1,Multi-Label Text Classification,Wiki-30K - Multi-Label Text Classification benchmarking,2019-05,LAHA,67.82,100.0,67.82,100,67.82,0.73,nDCG\\-at\\-5
2,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-05,LAHA,59.28,72.03,59.28,100,82.3,0.64,nDCG\\-at\\-5
3,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,bert-base,82.3,100.0,23.02,1.0,82.3,0.88,nDCG\\-at\\-5
4,1,Multi-Label Text Classification,AAPD - Multi-Label Text Classification benchmarking,2019-05,LAHA,83.7,100.0,83.7,100,83.7,0.9,nDCG\\-at\\-5
5,1,Multi-Label Text Classification,Amazon-12K - Multi-Label Text Classification benchmarking,2019-05,LAHA,87.57,100.0,87.57,100,87.57,0.94,nDCG\\-at\\-5
6,1,Text Classification,RCV1 - Text Classification benchmarking,2019-06,NLP-Cap,93.11,100.0,93.11,100,93.11,1.0,nDCG\\-at\\-5
0,1,Multi-Label Text Classification,Kan-Shan Cup - Multi-Label Text Classification benchmarking,2019-05,LAHA,51.7,100.0,51.7,100,51.7,0.56,nDCG\\-at\\-3
1,1,Multi-Label Text Classification,Wiki-30K - Multi-Label Text Classification benchmarking,2019-05,LAHA,75.64,100.0,75.64,100,75.64,0.82,nDCG\\-at\\-3
2,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-05,LAHA,64.89,91.25,64.89,100,71.11,0.7,nDCG\\-at\\-3
3,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,NLP-Cap,71.11,100.0,6.22,1.0,71.11,0.77,nDCG\\-at\\-3
4,1,Multi-Label Text Classification,AAPD - Multi-Label Text Classification benchmarking,2019-05,LAHA,80.11,100.0,80.11,100,80.11,0.87,nDCG\\-at\\-3
5,1,Multi-Label Text Classification,Amazon-12K - Multi-Label Text Classification benchmarking,2019-05,LAHA,89.13,100.0,89.13,100,89.13,0.96,nDCG\\-at\\-3
6,1,Text Classification,RCV1 - Text Classification benchmarking,2019-06,NLP-Cap,92.47,100.0,92.47,100,92.47,1.0,nDCG\\-at\\-3
0,1,Multi-Label Text Classification,AAPD - Multi-Label Text Classification benchmarking,2019-05,LAHA,60.72,100.0,60.72,100,60.72,0.75,P\\-at\\-3
1,1,Multi-Label Text Classification,Amazon-12K - Multi-Label Text Classification benchmarking,2019-05,LAHA,79.16,100.0,79.16,100,79.16,0.97,P\\-at\\-3
2,1,Multi-Label Text Classification,Kan-Shan Cup - Multi-Label Text Classification benchmarking,2019-05,LAHA,34.6,100.0,34.6,100,34.6,0.43,P\\-at\\-3
3,1,Multi-Label Text Classification,Wiki-30K - Multi-Label Text Classification benchmarking,2019-05,LAHA,73.14,100.0,73.14,100,73.14,0.9,P\\-at\\-3
4,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-05,LAHA,61.48,93.89,61.48,100,65.48,0.76,P\\-at\\-3
5,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,NLP-Cap,65.48,100.0,4.0,1.0,65.48,0.81,P\\-at\\-3
6,1,Text Classification,RCV1 - Text Classification benchmarking,2019-06,NLP-Cap,81.27,100.0,81.27,100,81.27,1.0,P\\-at\\-3
0,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,bert-base,79.6,100.0,79.6,100,79.6,1.0,RP\\-at\\-5
0,1,Multi-Label Text Classification,EUR-Lex - Multi-Label Text Classification benchmarking,2019-06,NLP-Cap,80.2,100.0,80.2,100,80.2,0.83,nDCG\\-at\\-1
1,1,Text Classification,RCV1 - Text Classification benchmarking,2019-06,NLP-Cap,97.05,100.0,97.05,100,97.05,1.0,nDCG\\-at\\-1
0,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-06,DocRED-LSTM,43.6,80.12,43.6,100,54.42,0.8,Ign\\ F1
1,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-06,DocRED-Context-Aware,43.93,80.72,0.33,0.0305,54.42,0.81,Ign\\ F1
2,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-06,DocRED-BiLSTM,44.73,82.19,0.8,0.0739,54.42,0.82,Ign\\ F1
3,1,Relation Extraction,DocRED - Relation Extraction benchmarking,2019-09,Two-Step+BERT-base,54.42,100.0,9.69,0.8956,54.42,1.0,Ign\\ F1
0,1,Aspect Term Extraction and Sentiment Classification,SemEval - Aspect Term Extraction and Sentiment Classification benchmarking,2019-06,IMN-BERT,70.72,100.0,70.72,100,70.72,1.0,Restaurant\\ 2014\\ \\(F1\\)
0,1,Aspect Term Extraction and Sentiment Classification,SemEval - Aspect Term Extraction and Sentiment Classification benchmarking,2019-06,IMN-BERT,61.73,100.0,61.73,100,61.73,1.0,Laptop\\ 2014\\ \\(F1\\)
0,1,Aspect Term Extraction and Sentiment Classification,SemEval - Aspect Term Extraction and Sentiment Classification benchmarking,2019-06,IMN-BERT,60.22,100.0,60.22,100,60.22,1.0,Restaurant\\ 2015\\ \\(F1\\)
0,1,Natural Language Inference,ANLI test - Natural Language Inference benchmarking,2019-06,XLNet (Large),70.3,97.1,70.3,100,72.4,0.97,A1
1,1,Natural Language Inference,ANLI test - Natural Language Inference benchmarking,2019-07,RoBERTa (Large),72.4,100.0,2.1,1.0,72.4,1.0,A1
0,1,Natural Language Inference,ANLI test - Natural Language Inference benchmarking,2019-06,XLNet (Large),50.9,100.0,50.9,100,50.9,1.0,A2
0,1,Natural Language Inference,ANLI test - Natural Language Inference benchmarking,2019-06,XLNet (Large),49.4,100.0,49.4,100,49.4,1.0,A3
0,1,Document Ranking,ClueWeb09-B - Document Ranking benchmarking,2019-06,XLNet,20.28,100.0,20.28,100,20.28,1.0,ERR\\-at\\-20
0,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-06,XLNet,84.0,93.33,84.0,100,90.0,0.93,Accuracy\\ \\(High\\)
1,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-09,Megatron-BERT,88.6,98.44,4.6,0.7667,90.0,0.98,Accuracy\\ \\(High\\)
2,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-09,Megatron-BERT (ensemble),90.0,100.0,1.4,0.2333,90.0,1.0,Accuracy\\ \\(High\\)
0,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-06,XLNet,88.6,95.17,88.6,100,93.1,0.95,Accuracy\\ \\(Middle\\)
1,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-09,Megatron-BERT,91.8,98.6,3.2,0.7111,93.1,0.99,Accuracy\\ \\(Middle\\)
2,1,Reading Comprehension,RACE - Reading Comprehension benchmarking,2019-09,Megatron-BERT (ensemble),93.1,100.0,1.3,0.2889,93.1,1.0,Accuracy\\ \\(Middle\\)
0,1,Coreference Resolution,GAP - Coreference Resolution benchmarking,2019-08,ProBERT,0.97,100.0,0.97,100,0.97,1.0,Bias\\ \\(F/M\\)
0,1,Coreference Resolution,GAP - Coreference Resolution benchmarking,2019-08,ProBERT,92.5,100.0,92.5,100,92.5,1.0,Overall\\ F1
0,1,Coreference Resolution,GAP - Coreference Resolution benchmarking,2019-08,ProBERT,94.0,100.0,94,100,94,1.0,Masculine\\ F1\\ \\(M\\)
0,1,Coreference Resolution,GAP - Coreference Resolution benchmarking,2019-08,ProBERT,91.1,100.0,91.1,100,91.1,1.0,Feminine\\ F1\\ \\(F\\)
0,1,Visual Reasoning,NLVR - Visual Reasoning benchmarking,2019-08,VisualBERT,67.4,100.0,67.4,100,67.4,1.0,Accuracy\\ \\(Dev\\)
0,1,Visual Reasoning,NLVR - Visual Reasoning benchmarking,2019-08,VisualBERT,67.0,100.0,67,100,67,1.0,Accuracy\\ \\(Test\\-P\\)
0,1,Visual Reasoning,NLVR - Visual Reasoning benchmarking,2019-08,VisualBERT,67.3,100.0,67.3,100,67.3,1.0,Accuracy\\ \\(Test\\-U\\)
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",24.76,100.0,24.76,100,24.76,1.0,number
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",82.26,100.0,82.26,100,82.26,1.0,unanswerable
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",74.0,100.0,74,100,74,1.0,yes/no
0,1,Visual Question Answering,VizWiz 2018 - Visual Question Answering benchmarking,2019-08,"LXR955, No Ensemble",39.0,100.0,39,100,39,1.0,other
0,1,Text Summarization,X-Sum - Text Summarization benchmarking,2019-08,BertSumExtAbs,31.27,83.95,31.27,100,37.25,0.84,ROUGE\\-3
1,1,Text Summarization,X-Sum - Text Summarization benchmarking,2019-10,BART,37.25,100.0,5.98,1.0,37.25,1.0,ROUGE\\-3
0,1,Sentiment Analysis,FiQA - Sentiment Analysis benchmarking,2019-08,FinBERT,0.55,100.0,0.55,100,0.55,1.0,R\\^2
0,1,Common Sense Reasoning,Visual Dialog  v0.9 - Common Sense Reasoning benchmarking,2019-09,PDUN,90.5,100.0,90.5,100,90.5,1.0,Recall\\-at\\-10
0,1,Image Captioning,Flickr30k Captions test - Image Captioning benchmarking,2019-09,Unified VLP,17.0,100.0,17,100,17,0.8,SPICE
1,1,Image Captioning,COCO Captions test - Image Captioning benchmarking,2019-09,Unified VLP,21.2,100.0,21.2,100,21.2,1.0,SPICE
0,1,Emotion Recognition in Conversation,EmoryNLP - Emotion Recognition in Conversation benchmarking,2019-09,KET,34.39,100.0,34.39,100,34.39,1.0,Weighted\\ Macro\\-F1
0,1,Machine Translation,IWSLT2017 French-English - Machine Translation benchmarking,2019-10,Transformer base + BPE-Dropout,38.6,100.0,38.6,100,38.6,0.97,Cased\\ sacreBLEU
1,1,Machine Translation,IWSLT2017 Arabic-English - Machine Translation benchmarking,2019-10,Transformer base + BPE-Dropout,33.0,100.0,33,100,33,0.83,Cased\\ sacreBLEU
2,1,Machine Translation,IWSLT2017 English-Arabic - Machine Translation benchmarking,2019-10,Transformer base + BPE-Dropout,15.2,100.0,15.2,100,15.2,0.38,Cased\\ sacreBLEU
3,1,Machine Translation,IWSLT2017 English-French - Machine Translation benchmarking,2019-10,Transformer base + BPE-Dropout,39.83,100.0,39.83,100,39.83,1.0,Cased\\ sacreBLEU
0,1,Bias Detection,StereoSet - Bias Detection benchmarking,2020-04,XLNet (large),72.03,98.71,72.03,100,72.97,0.99,ICAT\\ Score
1,1,Bias Detection,StereoSet - Bias Detection benchmarking,2020-04,GPT-2 (small),72.97,100.0,0.94,1.0,72.97,1.0,ICAT\\ Score
0,1,Question Answering,SCDE - Question Answering benchmarking,2020-04,bert-large-uncased + APN,0.661,100.0,0.661,100,0.661,1.0,DE
0,1,Question Answering,SCDE - Question Answering benchmarking,2020-04,bert-large-uncased + APN,0.299,100.0,0.299,100,0.299,1.0,PA
0,1,Question Answering,SCDE - Question Answering benchmarking,2020-04,bert-large-uncased + APN,0.717,100.0,0.717,100,0.717,1.0,BA
