{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh0gdl4fsOza"
   },
   "source": [
    "### A Global Map of Artificial Intelligence Benchmark Dynamics\n",
    "\n",
    "This page shows the interactive trajectories of averaged gain ratio for the performance increase of benchmarking datasets for NLP and CVP top hierarchies.\n",
    "\n",
    "**Run the chunck below** to make sure all the required modules are installed and the input files are downloaded to the local running environment.\n",
    "\n",
    "After that, run the next 4 chuncks do generate the trajectories on-the-fly.\n",
    "\n",
    "Alternativelly press Ctrl + F9 to run all chunks! It takes 1 min to render it all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 946
    },
    "id": "t_tbT69HnMs6",
    "outputId": "9b65e2f4-07a9-4e77-df44-cdac53ee8279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown) (2.25.1)\n",
      "Requirement already satisfied: six in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown) (3.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown) (4.63.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown) (4.0.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages (from tqdm->gdown) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.3.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.21.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (5.5.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from plotly) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaleido in c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\ottsi\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ottsi\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1l_HJTdkXNsVfGYRhoq-HgX5Altgrr5c0\n",
      "To: C:\\Users\\ottsi\\ITO\\notebooks\\barbosa-silva-etal-2022\\data\\get_ratio_df_all_per_global_ITO_00101.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 714k/714k [00:00<00:00, 7.26MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19uIjeF4ODFAv1RSMbG6uzpZmA6MJEw75\n",
      "To: C:\\Users\\ottsi\\ITO\\notebooks\\barbosa-silva-etal-2022\\data\\get_ratio_df_all_per_global_ITO_00141.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252k/252k [00:00<00:00, 5.59MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1hVB6J4mt-RWsP1AJsqi6R30UyjzNtUuN\n",
      "To: C:\\Users\\ottsi\\ITO\\notebooks\\barbosa-silva-etal-2022\\data\\trajectory_grouping_ITO_00101.csv\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 22.6k/22.6k [00:00<00:00, 1.62MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KFd4W69i0ck1mrZCWgvjE5klp-v8M2Vm\n",
      "To: C:\\Users\\ottsi\\ITO\\notebooks\\barbosa-silva-etal-2022\\data\\trajectory_grouping_ITO_00141.csv\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.37k/9.37k [00:00<00:00, 4.70MB/s]\n",
      "[Errno 13] Permission denied: 'data/trajectory_grouping_ITO_00141.csv'\n"
     ]
    }
   ],
   "source": [
    "#Install modules\n",
    "!pip install gdown\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install plotly\n",
    "!pip install -U kaleido\n",
    "\n",
    "#Import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplotlib\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "import gdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Dowload input files from my Google Drive to this local environment (files a cleared once the runtime is finished).\n",
    "file_id=\"1l_HJTdkXNsVfGYRhoq-HgX5Altgrr5c0\"\n",
    "url = 'https://drive.google.com/uc?id=' + file_id\n",
    "output = 'data/get_ratio_df_all_per_global_ITO_00101.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "#\n",
    "file_id=\"19uIjeF4ODFAv1RSMbG6uzpZmA6MJEw75\"\n",
    "url = 'https://drive.google.com/uc?id=' + file_id\n",
    "output = 'data/get_ratio_df_all_per_global_ITO_00141.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "#\n",
    "file_id=\"1hVB6J4mt-RWsP1AJsqi6R30UyjzNtUuN\"\n",
    "url = 'https://drive.google.com/uc?id=' + file_id\n",
    "output = 'data/trajectory_grouping_ITO_00101.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "#\n",
    "file_id=\"1KFd4W69i0ck1mrZCWgvjE5klp-v8M2Vm\"\n",
    "url = 'https://drive.google.com/uc?id=' + file_id\n",
    "output = 'data/trajectory_grouping_ITO_00141.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVNU7SbSrZ7x"
   },
   "source": [
    "### Defining Plotting Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "FIyImfK-6E0n"
   },
   "outputs": [],
   "source": [
    "#Defining Plotting Function\n",
    "def plot_task_trajectory(ito, class_label, anchor):\n",
    "    import pandas as pd\n",
    "    #Read input\n",
    "    input_file_name=\"data/get_ratio_df_all_per_global_\"+ito+\".csv\"\n",
    "    get_ratio_df_all_per_global = pd.read_csv(input_file_name)\n",
    "    #Drop first column.\n",
    "    get_ratio_df_all_per_global = get_ratio_df_all_per_global.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "\n",
    "    #Change the column to name the facets of the plot.\n",
    "    get_ratio_df_all_per_global = get_ratio_df_all_per_global.rename(\n",
    "        columns={\"merge\": \"metricName\"}\n",
    "    )\n",
    "    get_ratio_df_all_per_global[\"metricName\"] = get_ratio_df_all_per_global[\n",
    "        \"metricName\"\n",
    "    ].str.replace(\"\\\\\", \"\")\n",
    "\n",
    "    get_ratio_df_all_per_global[\"unique_ds\"] = (\n",
    "        get_ratio_df_all_per_global[\"ds\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"metricName\"]\n",
    "    )\n",
    "    get_ratio_df_all_per_global[\"unique_task\"] = (\n",
    "        get_ratio_df_all_per_global[\"task\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"metricName\"]\n",
    "    )\n",
    "\n",
    "    get_ratio_df_all_per_global[\"unique_task_ds_metric\"] = (\n",
    "        get_ratio_df_all_per_global[\"task\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"ds\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"metricName\"]\n",
    "    )\n",
    "\n",
    "    def agg(ex):\n",
    "      ex[\"ds\"] = \"  \" + ex[\"ds\"] + \": \" + ex[\"metricName\"] + \"<BR>\"\n",
    "      return pd.Series({\"ds\": \"\".join(ex[\"ds\"].sort_values().unique()), \"ratio\": ex[\"ratio\"].mean()})\n",
    "    \n",
    "    #fix dirty data\n",
    "    # get_ratio_df_all_per_global[\"task\"] = get_ratio_df_all_per_global[\"task\"].str.replace(\"Abstractive Text Summarization\", \"Text Summarization\")\n",
    "    # get_ratio_df_all_per_global[\"task\"] = get_ratio_df_all_per_global[\"task\"].str.replace(\"Document Text Summarization\", \"Text Summarization\")\n",
    "    \n",
    "\n",
    "    #Calculate the average data frame using 'RATIO' as criteria per unique_task.\n",
    "    traj = get_ratio_df_all_per_global.copy()\n",
    "\n",
    "    #This copy is necessary to add the anchors in the plot, and avoid that they influence in the calculation of the ratio.\n",
    "    traj_complete = traj.copy()\n",
    "    #Select here only the anchors.\n",
    "    traj_anchors = traj_complete.drop(traj_complete[traj_complete[\"ratio\"]!=100].index)                 ######\n",
    "    \n",
    "    # traj_anchors.to_csv(\"artefacts/traj_anchors_\"+ito+\".csv\")\n",
    "    \n",
    "    #This is wrong here, this is averaging just the anchor values, check the effect\n",
    "\n",
    "    average_summary_anchors = pd.DataFrame(traj_anchors.groupby([\"task\", \"date\"])[\"ds\", \"metricName\", \"ratio\"].apply(agg))\n",
    "    #\n",
    "    average_summary_anchors.sort_values(by=[\"date\"], ascending=True)\n",
    "    average_summary_anchors.reset_index(inplace=True)\n",
    "    average_summary_anchors[\"in_trajectory\"] = \"OUT\"\n",
    "   \n",
    "    #metricName causing some problems. Removed!\n",
    "    traj = traj.drop(traj[traj[\"metricName\"]==\"Parameters\"].index)\n",
    "\n",
    "    #This will delete from the traj data frame all the ds/tasks which the counts are equal to 1, \n",
    "    #not forming, therefore, a trajectory.\n",
    "    count_df = pd.DataFrame(traj[\"ds\"].value_counts())\n",
    "    count_df[count_df.ds == 1].index\n",
    "\n",
    "    #Use the symbol ~ to inverselly select here.\n",
    "    traj = traj[~traj[\"ds\"].isin(count_df[count_df.ds == 1].index)]\n",
    "    \n",
    "    count_df = pd.DataFrame(traj[\"task\"].value_counts())\n",
    "    count_df[count_df.task == 1].index\n",
    "\n",
    "    traj = traj[~traj[\"task\"].isin(count_df[count_df.task == 1].index)]\n",
    "\n",
    "    if(len(traj) == 0):\n",
    "        return(\"Not enough points to plot trajectory\")\n",
    "    \n",
    "    \n",
    "    #We have to drop the first anchors, because we want to calculate the ratio with only the remaining meaningful points\n",
    "    #traj = traj.drop(traj[traj[\"ratio\"]==100].index) # for the individual trajectories analysis, keep these points and make them equal to Zero in the relevant notebook.\n",
    "    traj = traj.drop(traj[traj[\"ratio\"]<0].index)\n",
    "    \n",
    "    \n",
    "    #Choose here which grouping variable to use.\n",
    "    #traj = traj.drop(traj[traj[\"ratio\"]>=100].index)                 ######  prepare average_summary , if this line is commented, then the anchors appear, but the ratio is then wrong\n",
    "    traj = traj[traj[\"ratio\"]<100].copy() \n",
    "    # traj.to_csv(\"artefacts/traj_\"+ito+\".csv\")\n",
    "\n",
    "    average_summary = pd.DataFrame(traj.groupby([\"task\", \"date\"])[\"ds\", \"metricName\", \"ratio\"].apply(agg))\n",
    "    # average_summary.to_csv(\"artefacts/average_summary_\"+ito+\".csv\")\n",
    "    #Drop anchors points.\n",
    "    #average_summary = average_summary.drop(average_summary[average_summary[\"ratio\"]>0.5].index)\n",
    "\n",
    "    average_summary.sort_values(by=[\"date\"], ascending=True)\n",
    "    average_summary.reset_index(inplace=True)\n",
    "    # average_summary[\"date\"]=pd.to_datetime(average_summary['date'])\n",
    "    # average_summary[\"date\"]=average_summary[\"date\"].dt.year\n",
    "    \n",
    "    \n",
    "\n",
    "    average_summary[\"in_trajectory\"] = 1\n",
    "\n",
    "    i = 0\n",
    "    for t in average_summary.task.unique():\n",
    "        sota_per = 0\n",
    "        #Here \"date\" can't be unique, because we want to look for the best value obtained per year.\n",
    "        for v in average_summary[average_summary[\"task\"] == t].ratio:\n",
    "            per = average_summary[\n",
    "                (average_summary[\"task\"] == t) & (average_summary[\"ratio\"] == v)\n",
    "            ].ratio.astype(float)\n",
    "            per = per.iloc[0]\n",
    "            if per == 100:\n",
    "                average_summary.loc[i, \"in_trajectory\"] = \"OUT\"\n",
    "            elif per >= sota_per:\n",
    "                # print(per)\n",
    "                sota_per = per\n",
    "                average_summary.loc[i, \"in_trajectory\"] = \"IN\"\n",
    "            else:\n",
    "                average_summary.loc[i, \"in_trajectory\"] = \"IN\"  # change back to OUT, if you want only to display the ratio average state of the art\n",
    "                # sota_per = per\n",
    "            i = i + 1\n",
    "\n",
    "    average_summary[\"ratio\"] = average_summary[\"ratio\"].apply(lambda x: round(x, 4))\n",
    "    \n",
    "    #NOW PLOT IT\n",
    "    #try using plotly\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    from plotly.validators.scatter.marker import SymbolValidator\n",
    "\n",
    "    #PLACE HOLDER FOR THE ORDERING OF THE DATA FRAME #ISSUE_01\n",
    "    #Use one column of this variable to sort: average_summary (get input from Kathrin)\n",
    "    \n",
    "    #get grouping: this tries to add the annotations by Kathrin, we need a merge here...\n",
    "    grouping_table = pd.read_csv(\"data/trajectory_grouping_\"+ito+\".csv\")\n",
    "    \n",
    "    # fix dirty data\n",
    "    grouping_table[\"Suggested_label\"] = grouping_table[\"Suggested_label\"].str.replace(\"Semantic segmenation\", \"Semantic segmentation\")\n",
    "    # grouping_table[\"Suggested_label\"] = grouping_table[\"Suggested_label\"].str.replace(\"Text summarization\", \"Natural language generation\")\n",
    "\n",
    "    grouping_table = grouping_table.rename(columns={'Class_Label': 'task'})\n",
    "    average_summary = pd.merge(average_summary, grouping_table, on = 'task', how = 'left')\n",
    "    average_summary = average_summary.drop(['id', 'Superclass_id','Superclass_label'], axis = 1)\n",
    "    average_summary = average_summary.dropna()\n",
    "    average_summary = average_summary.sort_values('Suggested_label')\n",
    "    average_summary[\"task\"] = average_summary[\"Suggested_label\"]+\": \"+average_summary[\"task\"]\n",
    "\n",
    "    average_summary_anchors = pd.merge(average_summary_anchors, grouping_table, on = 'task', how = 'left')\n",
    "    average_summary_anchors = average_summary_anchors.drop(['id', 'Superclass_id','Superclass_label'], axis = 1)\n",
    "    average_summary_anchors = average_summary_anchors.dropna()\n",
    "    average_summary_anchors = average_summary_anchors.sort_values('Suggested_label')\n",
    "    average_summary_anchors[\"task\"] = average_summary_anchors[\"Suggested_label\"]+\": \"+average_summary_anchors[\"task\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    #split the plot for the paper\n",
    "    #average_summary = average_summary.iloc[0:585]\n",
    "    #average_summary = average_summary.iloc[586:len(average_summary)]\n",
    "\n",
    "    #return(average_summary)\n",
    "    \n",
    "    average_summary_IN = average_summary[average_summary[\"in_trajectory\"] == \"IN\"]\n",
    "    \n",
    "    #anchors = average_summary[average_summary[\"in_trajectory\"] == \"OUT\"]\n",
    "    \n",
    "    #this is used to calculate the height of the plot.\n",
    "    if(anchor==1):\n",
    "        average_summary_OUT = average_summary_anchors.copy()\n",
    "        plot_title = 'Trajectory for average gain ratio (task per year). Trajectories with single arrow shown.'\n",
    "        img_file_title = \"top_classes_trajectory_plots/\"+ito+\"_with_anchors_and_single_arrows_shown.png\"\n",
    "    elif(anchor==0):\n",
    "        average_summary_OUT = average_summary_anchors.copy()\n",
    "        plot_title = 'Trajectory for average gain ratio (task per year). Trajectories with single arrow removed.'     \n",
    "        img_file_title = \"top_classes_trajectory_plots/\"+ito+\"_with_anchors_and_single_arrows_removed.png\"\n",
    "    \n",
    "    rows = len(average_summary_IN[\"task\"].unique())*20\n",
    "    \n",
    "    #This block will take the values from average_summary_IN and delete those that have only one arrow per trajectory.\n",
    "    count_df = pd.DataFrame(average_summary_IN[\"task\"].value_counts())\n",
    "    count_df[count_df.task == 1].index\n",
    "    \n",
    "    ##~~~~~ uncomment this block to filter out the trajectories with one arrow only\n",
    "    if(anchor==0): average_summary_IN = average_summary_IN[~average_summary_IN[\"task\"].isin(count_df[count_df.task == 1].index)]\n",
    "    average_summary_IN_1_point = average_summary_IN[average_summary_IN[\"task\"].isin(count_df[count_df.task == 1].index)]\n",
    "    average_summary_IN_1_point[\"Single arrow\"] = \"Yes\"\n",
    "    # average_summary_IN_1_point.to_csv(\"average_summary_IN_1_point_\"+ito+\".csv\", index=False)\n",
    "    ##~~~~~\n",
    "    #this will delete from the traj data frame, all the tasks which the average_summary_IN counts are equal to 1,\n",
    "    #meaning single arrows will be excluded from the plot.\n",
    "\n",
    "\n",
    "    #NEEDS to add a feature to goup based on the Suggested_label, detalhe, the ito file is fixed, change it to dynamic\n",
    "    #return(average_summary_IN)\n",
    "\n",
    "    fig_traj = px.line(average_summary_IN, \n",
    "                       x=\"date\", \n",
    "                       y=\"task\", \n",
    "                       color=\"task\", \n",
    "                       )   \n",
    "                                                            \n",
    "    #adding anchors on both cases...\n",
    "    #This function is declared to add the anchor dots (white dots) to the trajectory.\n",
    "    def add_white(category,anchors_to_add,average_summary_IN,fig_traj):\n",
    "        \n",
    "        #select anchors that belong to selected trajectories\n",
    "        anchors_to_add=anchors_to_add[anchors_to_add[\"task\"].isin(average_summary_IN.task)].copy()\n",
    "        \n",
    "        fig_traj.add_trace(\n",
    "            go.Scatter(\n",
    "                x=anchors_to_add[\"date\"],\n",
    "                y=anchors_to_add[category],\n",
    "                #facet_row=\"task\",\n",
    "                #facet_row_spacing=0.009, \n",
    "                mode=\"markers\",\n",
    "                name=None,\n",
    "                marker=dict(\n",
    "                    symbol=42, \n",
    "                    size=20,\n",
    "                    line=dict(\n",
    "                        width=2\n",
    "                    ),\n",
    "                    \n",
    "                ),\n",
    "                \n",
    "                hovertemplate=\n",
    "                \"<BR>task: \"\n",
    "                + anchors_to_add[category]\n",
    "                + \"<BR>date: \"\n",
    "                + anchors_to_add[\"date\"].astype(\"string\")\n",
    "                + \"<BR>Anchor.\"\n",
    "                + \"<BR>benchmarks:<BR>\"\n",
    "                + anchors_to_add[\"ds\"].astype(\"string\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    average_summary_IN = average_summary_IN.sort_values(by=\"date\")\n",
    "    add_white(\"task\",average_summary_anchors,average_summary_IN,fig_traj) # it seems wrong with average_summary_IN being from where the anchors are taken...\n",
    "    fig_traj.add_trace(\n",
    "        go.Scatter(\n",
    "            x=average_summary_IN[\"date\"],\n",
    "            y=average_summary_IN[\"task\"],\n",
    "            #facet_row=\"task\",\n",
    "            #facet_row_spacing=0.009, \n",
    "            mode=\"markers\",\n",
    "            name=None,\n",
    "            hovertemplate=\n",
    "            \"<BR>task: \"\n",
    "            + average_summary_IN[\"task\"]\n",
    "            + \"<BR>date: \"\n",
    "            + average_summary_IN[\"date\"].astype(\"string\")\n",
    "            + \"<BR>ratio: \"\n",
    "            + average_summary_IN[\"ratio\"].astype(\"string\")\n",
    "            + \"<BR>benchmarks:<BR>\"\n",
    "            + average_summary_IN[\"ds\"].astype(\"string\"),\n",
    "            marker=dict(\n",
    "                size=19,  \n",
    "                symbol=\"circle\",  # https://plotly.com/python/marker-style/\n",
    "                opacity=0.7,  # alpha ratio\n",
    "                color=average_summary_IN[\"ratio\"],  # set color equal to a variable\n",
    "                colorscale=\"YlGn\",  # one of plotly colorscales\n",
    "                colorbar=dict(title=\"ratio\", lenmode=\"pixels\", len=500, thickness=10),\n",
    "                showscale=True,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "   \n",
    "    \n",
    "    fig_traj.update_traces(\n",
    "        marker=dict(line=dict(color=\"black\", width=1)),\n",
    "        line  =dict(width=0, color=\"black\")        \n",
    "    )\n",
    "\n",
    "    fig_traj.update_xaxes(showgrid=True, gridcolor=\"lightBlue\", title=\"Year\")\n",
    "    #title=ito+\": \"+class_label\n",
    "    fig_traj.update_yaxes(showgrid=True, gridcolor=\"lightBlue\", title=None)\n",
    "\n",
    "    # font_size 14, height=rows*1.5\n",
    "    # cv, single traces --> height=rows*1.5\n",
    "    fig_traj.update_layout(\n",
    "        #title=\"Trajectory for ratio (task per year)\",\n",
    "        title_text=class_label,\n",
    "        showlegend=False,\n",
    "        font_size=21,\n",
    "        plot_bgcolor=\"white\",\n",
    "        height=rows*(1.5 if (anchor and ito == \"ITO_00101\") else 1.35),\n",
    "        width=1500,\n",
    "        xaxis=dict(\n",
    "            tickmode=\"auto\",\n",
    "        ),\n",
    "        yaxis={'side': 'left'}\n",
    "        \n",
    "    )  \n",
    "\n",
    "    \n",
    "    fig_traj.update_layout(\n",
    "        title={\n",
    "            'y':0.995,            \n",
    "            })\n",
    "\n",
    "    fig_traj.write_html(f\"artefacts/{class_label.replace(' ', '_').lower() + ('_single_arrow' if anchor else '')}.html\", include_plotlyjs=\"cdn\")\n",
    "    fig_traj.write_image(f\"artefacts/{class_label.replace(' ', '_').lower() + ('_single_arrow' if anchor else '')}.png\", scale=2)\n",
    "    fig_traj.show()\n",
    "    return(average_summary_IN)\n",
    "\n",
    "#plot_task_trajectory(\"ITO_00101\", \"Vision process\", 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwCPbdoirJ8W"
   },
   "source": [
    "### Define get statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Sy8pv9KGrJD-"
   },
   "outputs": [],
   "source": [
    "#Define get statistics function\n",
    "def get_statistics(traj_df):\n",
    "  results=pd.DataFrame(columns=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "\n",
    "  #traj_df = traj_df_bkp.copy()\n",
    "\n",
    "  traj_df['date'] = pd.to_datetime(traj_df['date'])\n",
    "  traj_df['date'] = traj_df['date'].dt.strftime('%Y')\n",
    "\n",
    "  for date in traj_df[\"date\"].unique():\n",
    "    \n",
    "    df = traj_df[traj_df[\"date\"]==date].copy()\n",
    "    ts = df.get(['date','ratio'])\n",
    "    ts['date']= pd.to_datetime(ts['date'])\n",
    "    ts['date'] = ts['date'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    \n",
    "    #print(results.index)\n",
    "    year = ts.describe()\n",
    "    year[\"ratio\"]=year[\"ratio\"].astype('float').round(3)\n",
    "\n",
    "    year = year.T\n",
    "    year.index=[date]\n",
    "    results=results.append(year)\n",
    "\n",
    "  #year[\"ratio\"].values.round(3).astype('float')\\\n",
    "  results = results.sort_index()\n",
    "  return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIZjCkz2rpmS"
   },
   "source": [
    "### Define calculate S index function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Xu_L1W7LSOhE"
   },
   "outputs": [],
   "source": [
    "def calculate_S_index(results):\n",
    "\n",
    "  data=results.T\n",
    "  count = data.loc[data.index.str.contains('count')]\n",
    "  mean = data.loc[data.index.str.contains('mean')]\n",
    "\n",
    "  corrected_means=[]\n",
    "  for year in count.columns:\n",
    "    lambda_val = count[year].values/count.values.sum()\n",
    "    corrected_means.append(mean[year].values[0]*lambda_val[0])\n",
    "\n",
    "  S_index = sum(corrected_means) / len(corrected_means)\n",
    "  return(S_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuEE9vZ3xGCO"
   },
   "source": [
    "### Define get boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "hikr_z2cxCK6"
   },
   "outputs": [],
   "source": [
    "#Define get boxplots\n",
    "def get_boxplot(traj_df):\n",
    "  import plotly.graph_objects as go\n",
    "\n",
    "  traj_df['date'] = pd.to_datetime(traj_df['date'])\n",
    "  traj_df['date'] = traj_df['date'].dt.strftime('%Y')\n",
    "\n",
    "  c = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, 20)] #here controls the colors\n",
    "\n",
    "\n",
    "  fig = go.Figure()\n",
    "  # Use x instead of y argument for horizontal plot\n",
    "  i=0\n",
    "  for date in traj_df[\"date\"].unique():\n",
    "    i=i+1\n",
    "    df = traj_df[traj_df[\"date\"]==date].copy()\n",
    "    ts = df.get(['date','ratio'])\n",
    "\n",
    "    ts['date']= pd.to_datetime(ts['date'])\n",
    "    ts['date'] = ts['date'].dt.strftime('%Y-%m')\n",
    "    #fig.add_trace(go.Box(x=ts[\"ratio\"], name=task))\n",
    "    fig.add_trace(go.Box(y=ts[\"ratio\"], \n",
    "              boxpoints='all',\n",
    "              jitter=0.8,\n",
    "              whiskerwidth=0.1,\n",
    "              marker_size=3, \n",
    "              line_width=2,\n",
    "              name=int(date), \n",
    "              marker_color=\"Blue\"))\n",
    "    \n",
    "  fig.update_layout(height=400, width=1000, showlegend=False,\n",
    "                    font_size=20,\n",
    "                    xaxis=dict(tickmode='linear'))\n",
    "  \n",
    "  fig.update_xaxes(categoryorder='array', categoryarray=list(range(2011,2021)))\n",
    "                  \n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tHwnN77q1h6"
   },
   "source": [
    "## Plot the global maps for selected top hierarchies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "yGkrVsfqgx0q"
   },
   "outputs": [],
   "source": [
    "anchor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DLlSDx9JpaH9",
    "outputId": "66efa52e-81cb-4663-8098-77ffcc55973b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Action localization: Temporal Action Localization",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Action localization: Temporal Action Localization",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-03",
          "2018-04",
          "2017-05",
          "2019-04",
          "2016-09",
          "2016-01",
          "2019-06",
          "2019-07",
          "2019-09",
          "2019-11",
          "2018-06",
          "2019-03"
         ],
         "xaxis": "x",
         "y": [
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Action localization: Action Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Action localization: Action Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-11",
          "2018-06",
          "2019-03",
          "2020-03"
         ],
         "xaxis": "x",
         "y": [
          "Action localization: Action Segmentation",
          "Action localization: Action Segmentation",
          "Action localization: Action Segmentation",
          "Action localization: Action Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity detection: Action Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity detection: Action Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-03",
          "2017-12",
          "2018-03",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Activity detection: Action Detection",
          "Activity detection: Action Detection",
          "Activity detection: Action Detection",
          "Activity detection: Action Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity localization: Weakly Supervised Action Localization",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity localization: Weakly Supervised Action Localization",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-06",
          "2017-12",
          "2018-07",
          "2019-05",
          "2019-08",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity localization: Temporal Action Proposal Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity localization: Temporal Action Proposal Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-07",
          "2018-11",
          "2018-06"
         ],
         "xaxis": "x",
         "y": [
          "Activity localization: Temporal Action Proposal Generation",
          "Activity localization: Temporal Action Proposal Generation",
          "Activity localization: Temporal Action Proposal Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition: Action Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition: Action Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-06",
          "2019-05",
          "2019-04",
          "2018-11",
          "2018-12",
          "2018-10",
          "2018-07",
          "2018-06",
          "2017-12",
          "2017-11",
          "2017-05",
          "2016-12"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition: Action Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition: Action Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-06",
          "2016-04",
          "2015-05",
          "2015-12",
          "2017-03",
          "2016-08",
          "2016-01",
          "2014-12",
          "2017-04",
          "2015-03",
          "2017-05",
          "2019-12",
          "2019-08",
          "2019-07",
          "2019-06",
          "2019-05",
          "2019-04",
          "2019-01",
          "2018-12",
          "2018-11",
          "2018-10",
          "2018-07",
          "2018-06",
          "2018-01",
          "2017-12",
          "2017-11"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition: Group Activity Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition: Group Activity Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition: Group Activity Recognition",
          "Activity recognition: Group Activity Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition: Human Interaction Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition: Human Interaction Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-06",
          "2018-11"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition: Human Interaction Recognition",
          "Activity recognition: Human Interaction Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition: Egocentric Activity Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition: Egocentric Activity Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2018-12",
          "2019-05",
          "2019-08",
          "2020-02"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Egocentric Activity Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition: Multimodal Activity Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition: Multimodal Activity Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-03",
          "2016-08",
          "2017-04",
          "2018-01",
          "2019-01"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition: Skeleton Based Action Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition: Skeleton Based Action Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2019-07",
          "2019-06",
          "2019-04",
          "2019-01",
          "2018-12",
          "2019-11",
          "2018-11",
          "2018-05",
          "2018-04",
          "2017-03",
          "2017-04",
          "2018-02",
          "2018-01",
          "2018-06",
          "2019-12",
          "2016-11",
          "2017-05",
          "2016-09",
          "2016-06",
          "2016-04",
          "2013-02",
          "2017-08"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Emotion recognition: Emotion Recognition in Conversation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Emotion recognition: Emotion Recognition in Conversation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-04",
          "2019-08",
          "2019-09",
          "2018-11",
          "2018-06",
          "2018-10"
         ],
         "xaxis": "x",
         "y": [
          "Emotion recognition: Emotion Recognition in Conversation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Emotion recognition: Emotion Recognition in Conversation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling: Facial Landmark Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling: Facial Landmark Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-02",
          "2018-03",
          "2015-11"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling: Facial Landmark Detection",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Facial recognition and modelling: Facial Landmark Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling: Face Verification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling: Face Verification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-03",
          "2017-03",
          "2016-03",
          "2015-11",
          "2018-09",
          "2018-12",
          "2019-03",
          "2019-04",
          "2019-08",
          "2019-10",
          "2017-12",
          "2014-06",
          "2014-12",
          "2015-02",
          "2015-03",
          "2018-01",
          "2015-08",
          "2017-10",
          "2017-04"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling: Facial Expression Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling: Facial Expression Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-11",
          "2019-05",
          "2019-02",
          "2013-07",
          "2017-08",
          "2018-05"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling: Face Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling: Face Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-08",
          "2018-10",
          "2018-09",
          "2017-09",
          "2016-03"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling: Face Detection",
          "Facial recognition and modelling: Face Detection",
          "Facial recognition and modelling: Face Detection",
          "Facial recognition and modelling: Face Detection",
          "Facial recognition and modelling: Face Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling: Face Alignment",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling: Face Alignment",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-07",
          "2019-08",
          "2019-06",
          "2019-04",
          "2019-02",
          "2018-05",
          "2018-04",
          "2018-03",
          "2017-11"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling: Face Identification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling: Face Identification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-11",
          "2017-04",
          "2018-01",
          "2018-03",
          "2018-12"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Identification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-05",
          "2017-06",
          "2018-04",
          "2018-06",
          "2018-08",
          "2019-08"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Gesture recognition: Hand Gesture Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Gesture recognition: Hand Gesture Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-05",
          "2018-04",
          "2019-01",
          "2018-12"
         ],
         "xaxis": "x",
         "y": [
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification: Image Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification: Image Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-12",
          "2020-01",
          "2013-12",
          "2013-11",
          "2013-02",
          "2012-12",
          "2014-09",
          "2019-10",
          "2019-08",
          "2019-06",
          "2019-05",
          "2014-12",
          "2015-02",
          "2015-06",
          "2015-11",
          "2015-12",
          "2016-02",
          "2016-03",
          "2016-05",
          "2016-08",
          "2016-10",
          "2014-06",
          "2016-11",
          "2017-08",
          "2017-09",
          "2017-10",
          "2017-12",
          "2018-02",
          "2018-05",
          "2014-04",
          "2018-11",
          "2019-01",
          "2019-04",
          "2017-07",
          "2018-07"
         ],
         "xaxis": "x",
         "y": [
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification: Unsupervised Image Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification: Unsupervised Image Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-02",
          "2017-02"
         ],
         "xaxis": "x",
         "y": [
          "Image classification: Unsupervised Image Classification",
          "Image classification: Unsupervised Image Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification: Hyperspectral Image Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification: Hyperspectral Image Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-02",
          "2018-07"
         ],
         "xaxis": "x",
         "y": [
          "Image classification: Hyperspectral Image Classification",
          "Image classification: Hyperspectral Image Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification: Retinal OCT Disease Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification: Retinal OCT Disease Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-12",
          "2018-01"
         ],
         "xaxis": "x",
         "y": [
          "Image classification: Retinal OCT Disease Classification",
          "Image classification: Retinal OCT Disease Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification: Satellite Image Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification: Satellite Image Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-12",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Image classification: Satellite Image Classification",
          "Image classification: Satellite Image Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification: Sequential Image Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification: Sequential Image Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-10",
          "2018-03",
          "2017-10",
          "2016-03",
          "2015-11"
         ],
         "xaxis": "x",
         "y": [
          "Image classification: Sequential Image Classification",
          "Image classification: Sequential Image Classification",
          "Image classification: Sequential Image Classification",
          "Image classification: Sequential Image Classification",
          "Image classification: Sequential Image Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification: Document Image Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification: Document Image Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-08",
          "2018-01",
          "2017-04"
         ],
         "xaxis": "x",
         "y": [
          "Image classification: Document Image Classification",
          "Image classification: Document Image Classification",
          "Image classification: Document Image Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image generation: Image Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image generation: Image Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-06",
          "2019-12",
          "2019-11",
          "2019-07",
          "2019-04",
          "2019-03",
          "2018-12",
          "2018-11",
          "2018-09",
          "2018-07",
          "2018-03",
          "2018-02",
          "2017-10",
          "2017-09",
          "2017-06",
          "2017-03",
          "2016-01",
          "2019-08",
          "2017-02"
         ],
         "xaxis": "x",
         "y": [
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image generation: Conditional Image Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image generation: Conditional Image Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-09",
          "2018-05",
          "2018-02",
          "2017-09",
          "2017-03",
          "2016-12",
          "2016-10",
          "2016-06"
         ],
         "xaxis": "x",
         "y": [
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image generation: Pose Transfer",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image generation: Pose Transfer",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-12",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Image generation: Pose Transfer",
          "Image generation: Pose Transfer"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image-to-image translation: Fundus to Angiography Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image-to-image translation: Fundus to Angiography Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-07",
          "2017-11"
         ],
         "xaxis": "x",
         "y": [
          "Image-to-image translation: Fundus to Angiography Generation",
          "Image-to-image translation: Fundus to Angiography Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: RGB Salient Object Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: RGB Salient Object Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-06",
          "2019-06",
          "2019-04",
          "2018-06",
          "2017-08",
          "2017-07",
          "2017-04",
          "2016-11",
          "2017-10"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: Video Object Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: Video Object Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2019-07",
          "2020-03"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: Video Object Detection",
          "Object detection: Video Object Detection",
          "Object detection: Video Object Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: 3D Object Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: 3D Object Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-03",
          "2019-04",
          "2019-07",
          "2020-01",
          "2018-02",
          "2016-06",
          "2017-11",
          "2017-12",
          "2018-12",
          "2020-03"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: Lane Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: Lane Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-12",
          "2018-06",
          "2019-08",
          "2017-10"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: Lane Detection",
          "Object detection: Lane Detection",
          "Object detection: Lane Detection",
          "Object detection: Lane Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: Birds Eye View Object Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: Birds Eye View Object Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-12",
          "2017-12",
          "2017-11",
          "2016-11",
          "2016-08",
          "2019-07",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: Weakly Supervised Object Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: Weakly Supervised Object Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-11",
          "2019-10",
          "2019-04",
          "2018-11",
          "2018-07",
          "2018-06",
          "2018-04",
          "2018-03",
          "2018-02",
          "2017-11",
          "2017-08",
          "2015-11",
          "2016-03",
          "2016-09",
          "2016-11",
          "2017-04",
          "2017-06",
          "2017-07"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: Pedestrian Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: Pedestrian Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-12",
          "2017-02",
          "2018-07"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: Pedestrian Detection",
          "Object detection: Pedestrian Detection",
          "Object detection: Pedestrian Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: Object Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: Object Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-11",
          "2019-10",
          "2019-09",
          "2015-06",
          "2015-12",
          "2016-12",
          "2017-03",
          "2017-07",
          "2017-08",
          "2017-11",
          "2018-03",
          "2018-05",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-06",
          "2017-12",
          "2019-08"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection: Dense Object Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection: Dense Object Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-04",
          "2017-08",
          "2016-12"
         ],
         "xaxis": "x",
         "y": [
          "Object detection: Dense Object Detection",
          "Object detection: Dense Object Detection",
          "Object detection: Dense Object Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object recognition: Traffic Sign Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object recognition: Traffic Sign Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-04",
          "2018-06"
         ],
         "xaxis": "x",
         "y": [
          "Object recognition: Traffic Sign Recognition",
          "Object recognition: Traffic Sign Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object recognition: Pedestrian Attribute Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object recognition: Pedestrian Attribute Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-08",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Object recognition: Pedestrian Attribute Recognition",
          "Object recognition: Pedestrian Attribute Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object tracking: Visual Object Tracking",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object tracking: Visual Object Tracking",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-07",
          "2018-12",
          "2019-06",
          "2017-06",
          "2017-04",
          "2016-11",
          "2018-03",
          "2017-10",
          "2018-02",
          "2018-11",
          "2018-06"
         ],
         "xaxis": "x",
         "y": [
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object tracking: Multiple Object Tracking",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object tracking: Multiple Object Tracking",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2019-09",
          "2018-02"
         ],
         "xaxis": "x",
         "y": [
          "Object tracking: Multiple Object Tracking",
          "Object tracking: Multiple Object Tracking",
          "Object tracking: Multiple Object Tracking"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other 3D task: 3D Point Cloud Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other 3D task: 3D Point Cloud Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-12",
          "2018-03",
          "2018-01",
          "2017-06",
          "2017-04",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Other 3D task: 3D Point Cloud Classification",
          "Other 3D task: 3D Point Cloud Classification",
          "Other 3D task: 3D Point Cloud Classification",
          "Other 3D task: 3D Point Cloud Classification",
          "Other 3D task: 3D Point Cloud Classification",
          "Other 3D task: 3D Point Cloud Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other 3D task: 3D Object Reconstruction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other 3D task: 3D Object Reconstruction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-01",
          "2018-02",
          "2016-12"
         ],
         "xaxis": "x",
         "y": [
          "Other 3D task: 3D Object Reconstruction",
          "Other 3D task: 3D Object Reconstruction",
          "Other 3D task: 3D Object Reconstruction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other 3D task: 3D Reconstruction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other 3D task: 3D Reconstruction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-12",
          "2018-11",
          "2018-08"
         ],
         "xaxis": "x",
         "y": [
          "Other 3D task: 3D Reconstruction",
          "Other 3D task: 3D Reconstruction",
          "Other 3D task: 3D Reconstruction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other 3D task: 3D Room Layouts From A Single RGB Panorama",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other 3D task: 3D Room Layouts From A Single RGB Panorama",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2019-01"
         ],
         "xaxis": "x",
         "y": [
          "Other 3D task: 3D Room Layouts From A Single RGB Panorama",
          "Other 3D task: 3D Room Layouts From A Single RGB Panorama"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other 3D task: 3D Shape Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other 3D task: 3D Shape Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-04",
          "2017-11"
         ],
         "xaxis": "x",
         "y": [
          "Other 3D task: 3D Shape Classification",
          "Other 3D task: 3D Shape Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other image process: Color Image Denoising",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other image process: Color Image Denoising",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-07",
          "2019-04",
          "2018-02",
          "2017-10",
          "2017-04"
         ],
         "xaxis": "x",
         "y": [
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other image process: Image Clustering",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other image process: Image Clustering",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-04",
          "2018-10",
          "2012-08",
          "2015-11",
          "2016-04",
          "2018-04",
          "2017-03",
          "2017-04",
          "2018-07",
          "2017-09",
          "2017-10",
          "2018-11",
          "2018-12",
          "2019-01"
         ],
         "xaxis": "x",
         "y": [
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other image process: Aesthetics Quality Assessment",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other image process: Aesthetics Quality Assessment",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-10",
          "2017-04",
          "2016-04"
         ],
         "xaxis": "x",
         "y": [
          "Other image process: Aesthetics Quality Assessment",
          "Other image process: Aesthetics Quality Assessment",
          "Other image process: Aesthetics Quality Assessment"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other image process: Grayscale Image Denoising",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other image process: Grayscale Image Denoising",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-10",
          "2018-06",
          "2018-05",
          "2017-10",
          "2017-04",
          "2016-08"
         ],
         "xaxis": "x",
         "y": [
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other image process: Image Retrieval",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other image process: Image Retrieval",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-03",
          "2017-12",
          "2016-12",
          "2016-11",
          "2016-08",
          "2016-04",
          "2015-11",
          "2015-04",
          "2018-01",
          "2018-11",
          "2019-02",
          "2019-03",
          "2019-09",
          "2018-04"
         ],
         "xaxis": "x",
         "y": [
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other image process: Image Reconstruction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other image process: Image Reconstruction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-03",
          "2017-06"
         ],
         "xaxis": "x",
         "y": [
          "Other image process: Image Reconstruction",
          "Other image process: Image Reconstruction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other video process: Video Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other video process: Video Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-07",
          "2019-12",
          "2016-11"
         ],
         "xaxis": "x",
         "y": [
          "Other video process: Video Generation",
          "Other video process: Video Generation",
          "Other video process: Video Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other video process: Video Frame Interpolation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other video process: Video Frame Interpolation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2020-03",
          "2019-04",
          "2018-10"
         ],
         "xaxis": "x",
         "y": [
          "Other video process: Video Frame Interpolation",
          "Other video process: Video Frame Interpolation",
          "Other video process: Video Frame Interpolation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other video process: Video Retrieval",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other video process: Video Retrieval",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-04",
          "2016-12",
          "2017-07",
          "2018-06",
          "2018-08",
          "2019-06",
          "2019-07"
         ],
         "xaxis": "x",
         "y": [
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Formation Energy",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Formation Energy",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-05",
          "2017-09",
          "2018-06",
          "2017-06",
          "2017-04"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Formation Energy",
          "Other vision process: Formation Energy",
          "Other vision process: Formation Energy",
          "Other vision process: Formation Energy",
          "Other vision process: Formation Energy"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Domain Adaptation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Domain Adaptation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-08",
          "2016-05",
          "2015-02",
          "2014-09",
          "2015-05",
          "2019-11",
          "2017-04",
          "2017-05",
          "2017-11",
          "2017-12",
          "2018-07",
          "2018-11",
          "2019-01",
          "2019-03",
          "2019-05",
          "2019-06",
          "2019-08",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Visual Question Answering",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Visual Question Answering",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-11",
          "2017-05",
          "2016-06",
          "2016-03",
          "2018-05",
          "2017-08",
          "2018-03",
          "2016-05",
          "2019-02",
          "2019-05",
          "2020-02",
          "2019-09",
          "2019-08",
          "2019-07",
          "2019-04",
          "2017-07",
          "2019-06",
          "2017-04"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Scene Text Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Scene Text Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-04",
          "2017-07",
          "2017-08",
          "2017-09",
          "2018-01",
          "2018-02",
          "2018-04",
          "2018-06",
          "2018-07",
          "2018-11",
          "2019-03",
          "2019-04",
          "2019-10",
          "2019-11",
          "2017-04",
          "2017-03",
          "2015-04"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Domain Generalization",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Domain Generalization",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-08",
          "2017-10",
          "2019-03",
          "2019-05"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Domain Generalization",
          "Other vision process: Domain Generalization",
          "Other vision process: Domain Generalization",
          "Other vision process: Domain Generalization"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Scene Graph Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Scene Graph Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-06",
          "2018-08",
          "2018-12",
          "2020-02"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Scene Graph Generation",
          "Other vision process: Scene Graph Generation",
          "Other vision process: Scene Graph Generation",
          "Other vision process: Scene Graph Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Depth Completion",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Depth Completion",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-02",
          "2018-08",
          "2018-11",
          "2019-05"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Depth Completion",
          "Other vision process: Depth Completion",
          "Other vision process: Depth Completion",
          "Other vision process: Depth Completion"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Multivariate Time Series Imputation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Multivariate Time Series Imputation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-06",
          "2018-12"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Multivariate Time Series Imputation",
          "Other vision process: Multivariate Time Series Imputation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Curved Text Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Curved Text Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-05",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Curved Text Detection",
          "Other vision process: Curved Text Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Crowd Counting",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Crowd Counting",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-08",
          "2019-06",
          "2016-08",
          "2015-12"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Crowd Counting",
          "Other vision process: Crowd Counting",
          "Other vision process: Crowd Counting",
          "Other vision process: Crowd Counting"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Monocular Depth Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Monocular Depth Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-07",
          "2019-03",
          "2018-12",
          "2018-06",
          "2018-05",
          "2018-03",
          "2017-04",
          "2016-07"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Metric Learning",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Metric Learning",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-08",
          "2018-04",
          "2017-06",
          "2016-11"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Metric Learning",
          "Other vision process: Metric Learning",
          "Other vision process: Metric Learning",
          "Other vision process: Metric Learning"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Video Prediction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Video Prediction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2019-05"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Video Prediction",
          "Other vision process: Video Prediction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Visual Dialog",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Visual Dialog",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-09",
          "2017-11",
          "2018-09",
          "2019-02",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Visual Dialog",
          "Other vision process: Visual Dialog",
          "Other vision process: Visual Dialog",
          "Other vision process: Visual Dialog",
          "Other vision process: Visual Dialog"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Unsupervised Domain Adaptation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Unsupervised Domain Adaptation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-02",
          "2015-05",
          "2018-11",
          "2018-12",
          "2019-11",
          "2020-01"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Horizon Line Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Horizon Line Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-08",
          "2019-05"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Horizon Line Estimation",
          "Other vision process: Horizon Line Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Object Counting",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Object Counting",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-04",
          "2016-09",
          "2017-07",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Object Counting",
          "Other vision process: Object Counting",
          "Other vision process: Object Counting",
          "Other vision process: Object Counting"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process: Denoising",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process: Denoising",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-05",
          "2019-04",
          "2018-07"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process: Denoising",
          "Other vision process: Denoising",
          "Other vision process: Denoising"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-04",
          "2019-03",
          "2018-11"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: Pose Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: Pose Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-11",
          "2017-12",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-02",
          "2020-01",
          "2020-02",
          "2017-08",
          "2017-07",
          "2017-04",
          "2015-11",
          "2016-01",
          "2016-03",
          "2016-09",
          "2016-12",
          "2017-01",
          "2017-02",
          "2017-05",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: Head Pose Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: Head Pose Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-01",
          "2018-12",
          "2017-10",
          "2019-06"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Head Pose Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: Keypoint Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: Keypoint Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-03",
          "2017-11",
          "2018-04",
          "2018-12",
          "2019-01",
          "2019-02",
          "2016-12",
          "2016-11",
          "2016-08",
          "2017-01"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: Hand Pose Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: Hand Pose Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-12",
          "2017-08",
          "2017-07"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: Hand Pose Estimation",
          "Pose estimation: Hand Pose Estimation",
          "Pose estimation: Hand Pose Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: 3D Human Pose Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: 3D Human Pose Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2020-04",
          "2020-02",
          "2019-12",
          "2019-09",
          "2018-06",
          "2018-04",
          "2017-09",
          "2017-05"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: 6D Pose Estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: 6D Pose Estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-01",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: 6D Pose Estimation",
          "Pose estimation: 6D Pose Estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: 6D Pose Estimation using RGB",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: 6D Pose Estimation using RGB",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-11",
          "2018-03",
          "2018-12",
          "2019-02",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation using RGB"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation: 6D Pose Estimation using RGBD",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation: 6D Pose Estimation using RGBD",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-03",
          "2019-01",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation using RGBD"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose tracking: Pose Tracking",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose tracking: Pose Tracking",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-05",
          "2019-02",
          "2018-04",
          "2018-02",
          "2017-12"
         ],
         "xaxis": "x",
         "y": [
          "Pose tracking: Pose Tracking",
          "Pose tracking: Pose Tracking",
          "Pose tracking: Pose Tracking",
          "Pose tracking: Pose Tracking",
          "Pose tracking: Pose Tracking"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Instance Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Instance Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-04",
          "2017-11",
          "2017-12",
          "2018-03",
          "2019-02",
          "2019-06",
          "2020-01",
          "2019-08",
          "2019-09",
          "2019-11",
          "2016-11",
          "2017-03"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Video Semantic Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Video Semantic Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-12",
          "2020-04"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Video Semantic Segmentation",
          "Semantic segmentation: Video Semantic Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: 3D Instance Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: 3D Instance Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-03",
          "2020-03",
          "2019-12",
          "2019-06"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: 3D Instance Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Electron Microscopy Image Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Electron Microscopy Image Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-06",
          "2019-08"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Electron Microscopy Image Segmentation",
          "Semantic segmentation: Electron Microscopy Image Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Human Part Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Human Part Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2017-08",
          "2018-05",
          "2018-09",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Human Part Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Brain Tumor Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Brain Tumor Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-08",
          "2019-06"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Brain Tumor Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: 3D Semantic Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: 3D Semantic Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-04",
          "2018-09",
          "2018-07",
          "2017-10",
          "2017-06"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: 3D Semantic Segmentation",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Semantic segmentation: 3D Semantic Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: 3D Semantic Instance Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: 3D Semantic Instance Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2020-03",
          "2018-12"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: 3D Semantic Instance Segmentation",
          "Semantic segmentation: 3D Semantic Instance Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: 3D Part Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: 3D Part Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2019-04",
          "2018-01",
          "2017-11",
          "2017-06",
          "2016-12"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Part Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Semantic Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Semantic Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-11",
          "2019-08",
          "2019-09",
          "2019-10",
          "2019-11",
          "2020-04",
          "2016-05",
          "2019-06",
          "2016-03",
          "2015-09",
          "2015-04",
          "2015-03",
          "2015-02",
          "2014-12",
          "2014-11",
          "2015-11",
          "2019-04",
          "2019-01",
          "2016-12",
          "2017-03",
          "2017-04",
          "2017-06",
          "2017-10",
          "2017-11",
          "2019-03",
          "2017-12",
          "2018-03",
          "2018-04",
          "2018-06",
          "2018-08",
          "2018-09",
          "2018-12",
          "2018-02",
          "2016-06"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Lesion Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Lesion Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-10",
          "2020-03"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Lesion Segmentation",
          "Semantic segmentation: Lesion Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Scene Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Scene Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-08",
          "2018-03"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Scene Segmentation",
          "Semantic segmentation: Scene Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Retinal Vessel Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Retinal Vessel Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-07",
          "2019-12",
          "2018-10",
          "2018-06",
          "2018-02",
          "2017-11"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Lung Nodule Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Lung Nodule Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-11",
          "2019-08"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: Lung Nodule Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Medical Image Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Medical Image Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-11",
          "2018-07"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Medical Image Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Multi-tissue Nucleus Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Multi-tissue Nucleus Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-05",
          "2017-03"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Multi-tissue Nucleus Segmentation",
          "Semantic segmentation: Multi-tissue Nucleus Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Nuclear Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Nuclear Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-11",
          "2017-03",
          "2018-09"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Nuclear Segmentation",
          "Semantic segmentation: Nuclear Segmentation",
          "Semantic segmentation: Nuclear Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Pancreas Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Pancreas Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-04",
          "2017-09"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Pancreas Segmentation",
          "Semantic segmentation: Pancreas Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Panoptic Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Panoptic Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-01",
          "2018-12",
          "2019-11",
          "2019-09",
          "2019-05"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Real-time Instance Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Real-time Instance Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2020-01",
          "2019-11",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Real-time Instance Segmentation",
          "Semantic segmentation: Real-time Instance Segmentation",
          "Semantic segmentation: Real-time Instance Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation: Skin Cancer Segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation: Skin Cancer Segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-11",
          "2018-02"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation: Skin Cancer Segmentation",
          "Semantic segmentation: Skin Cancer Segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": [
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Temporal Action Localization benchmarking: Frame-mAP<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>",
          "<BR>task: Action localization: Action Segmentation<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>",
          "<BR>task: Action localization: Action Segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>",
          "<BR>task: Action localization: Action Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  50 Salads - Action Segmentation benchmarking: Acc<BR>  50 Salads - Action Segmentation benchmarking: Edit<BR>  50 Salads - Action Segmentation benchmarking: F1@10%<BR>  50 Salads - Action Segmentation benchmarking: F1@25%<BR>  50 Salads - Action Segmentation benchmarking: F1@50%<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  Breakfast - Action Segmentation benchmarking: F1@10%<BR>  Breakfast - Action Segmentation benchmarking: F1@25%<BR>  Breakfast - Action Segmentation benchmarking: F1@50%<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Video-mAP 0.5<BR>  UCF101-24 - Temporal Action Localization benchmarking: Video-mAP 0.5<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  MEXaction2 - Temporal Action Localization benchmarking: mAP<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: mAP<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.1<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.2<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>",
          "<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@1000<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@200<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@500<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@50<BR>",
          "<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (test)<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.5<BR>  THUMOS’14 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: Mean mAP<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2012-07<BR>Anchor.<BR>benchmarks:<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  ActionNet-VE - Action Recognition benchmarking: F-measure (%)<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  UTD-MHAD - Multimodal Activity Recognition benchmarking: Accuracy (CS)<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 5 Accuracy<BR>  Something-Something V1 - Action Recognition benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Action Classification benchmarking: mAP<BR>  THUMOS’14 - Action Classification benchmarking: mAP<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-5 Accuracy<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  YouCook2 - Action Classification benchmarking: Object Top 5 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Object Top-1 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Verb Top-1 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Verb Top-5 Accuracy<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  MiniKinetics - Action Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  THUMOS'14 - Action Classification benchmarking: mAP<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-1 (%)<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-5 (%)<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>  VIRAT Ground 2.0 - Action Recognition benchmarking: Average Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet - Action Recognition benchmarking: mAP<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CS<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CV1<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CV2<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ICVL-4 - Action Recognition benchmarking: Accuracy<BR>  IRD - Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  miniSports - Action Recognition benchmarking: Clip Hit-at-1<BR>  miniSports - Action Recognition benchmarking: Video hit-at-1<BR>  miniSports - Action Recognition benchmarking: Video hit-at-5<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  miniSports - Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2012-10<BR>Anchor.<BR>benchmarks:<BR>  CAD-120 - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UT-Kinect - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  SBU - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.1<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  J-HMBD Early Action - Skeleton Based Action Recognition benchmarking: 10%<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  UAV-Human - Skeleton Based Action Recognition benchmarking: Average Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: Speed  (FPS)<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  MSR Action3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UPenn Action - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: No. parameters<BR>",
          "<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S1)<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Volleyball - Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking: Accuracy<BR>  Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking: Train F-measure<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  LboroHAR - Multimodal Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Action Recognition benchmarking: Top-1 Accuracy<BR>  EgoGesture - Action Recognition benchmarking: Top-1 Accuracy<BR>  EgoGesture - Action Recognition benchmarking: Top-5 Accuracy<BR>",
          "<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: GFlops<BR>  AVA v2.1 - Action Recognition benchmarking: Params (M)<BR>  AVA v2.2 - Action Recognition benchmarking: mAP<BR>  Diving-48 - Action Recognition benchmarking: Accuracy<BR>  UTD-MHAD - Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: mAP (Val)<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  DailyDialog - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>  EmoryNLP - Emotion Recognition in Conversation benchmarking: Weighted Macro-F1<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Arousal)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Cohn-Kanade - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000-3D - Face Alignment benchmarking: Mean NME<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>  WFLW - Face Alignment benchmarking: FR-at-0.1(%, all)<BR>  WFLW - Face Alignment benchmarking: ME (%, all)<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000 - Face Alignment benchmarking: Error rate<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  SFEW - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  FERPlus - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Real-World Affective Faces - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  3DFAW - Face Alignment benchmarking: CVGTCE<BR>  3DFAW - Face Alignment benchmarking: GTE<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  FERG - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  RAF-DB - Facial Expression Recognition benchmarking: Overall Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: Mean Error Rate<BR>",
          "<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  AFLW-Front - Facial Landmark Detection benchmarking: Mean NME<BR>  AFLW-Full - Facial Landmark Detection benchmarking: Mean NME<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (7 emotion)<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Oulu-CASIA - Facial Expression Recognition benchmarking: Accuracy (10-fold)<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  AFLW-Full - Face Alignment benchmarking: Mean NME<BR>  LS3D-W Balanced - Face Alignment benchmarking: AUC0.07<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Identification benchmarking: Accuracy<BR>  IJB-B - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Oulu-CASIA - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.01<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-B - Face Verification benchmarking: TAR at FAR=0.01<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.1<BR>  IJB-B - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.001<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  IIIT-D Viewed Sketch - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  AgeDB-30 - Face Verification benchmarking: Accuracy<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  CelebA Aligned - Face Alignment benchmarking: MOS<BR>  CelebA Aligned - Face Alignment benchmarking: MS-SSIM<BR>  CelebA Aligned - Face Alignment benchmarking: PSNR<BR>  CelebA Aligned - Face Alignment benchmarking: SSIM<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  IBUG - Face Alignment benchmarking: Mean Error Rate<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  AFLW - Face Alignment benchmarking: Mean NME<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  COFW - Face Alignment benchmarking: Mean Error Rate<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  AFLW-LFPA - Face Alignment benchmarking: Mean NME<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>  300W - Face Alignment benchmarking: Failure private<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>  300W - Face Alignment benchmarking: Mean Error Rate private<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2013-07<BR>Anchor.<BR>benchmarks:<BR>  FER2013 - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  JAFFE - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  MMI - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>  AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ChaLean test - Hand Gesture Recognition benchmarking: Accuracy<BR>  Jester val - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>  Jester val - Hand Gesture Recognition benchmarking: Top 5 Accuracy<BR>  NVGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Northwestern University - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  DHG-14 - Hand Gesture Recognition benchmarking: Accuracy<BR>  DHG-28 - Hand Gesture Recognition benchmarking: Accuracy<BR>  SHREC 2017 - Hand Gesture Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 - Hand Gesture Recognition benchmarking: 28 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Hand Gesture Recognition benchmarking: 14 gestures accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  ChaLearn val - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  BUAA - Hand Gesture Recognition benchmarking: Accuracy<BR>  MGB - Hand Gesture Recognition benchmarking: Accuracy<BR>  SmartWatch - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2013-03<BR>Anchor.<BR>benchmarks:<BR>  Cambridge - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Jester test - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Image classification: Document Image Classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Noisy Bangla Characters - Document Image Classification benchmarking: Accuracy<BR>  Noisy Bangla Numeral - Document Image Classification benchmarking: Accuracy<BR>  n-MNIST - Document Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Document Image Classification<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Indian Pines - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>  Pavia University - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Salinas Scene - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  EMNIST-Letters - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  CINIC-10 - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Clothing1M - Image Classification benchmarking: Accuracy<BR>  Food-101N - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  EMNIST-Balanced - Image Classification benchmarking: Accuracy<BR>  MultiMNIST - Image Classification benchmarking: Percentage error<BR>  smallNORB - Image Classification benchmarking: Classification Error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Flowers-102 - Image Classification benchmarking: Accuracy<BR>  ObjectNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ObjectNet - Image Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2013-01<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  iNaturalist - Image Classification benchmarking: Top 1 Accuracy<BR>  iNaturalist - Image Classification benchmarking: Top 5 Accuracy<BR>  iNaturalist 2018 - Image Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ImageNet ReaL - Image Classification benchmarking: Params<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Unsupervised Image Classification benchmarking: Accuracy<BR>  CIFAR-20 - Unsupervised Image Classification benchmarking: Accuracy<BR>  STL-10 - Unsupervised Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sequential CIFAR-10 - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>",
          "<BR>task: Image classification: Satellite Image Classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>  SAT-6 - Satellite Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Sequential Image Classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>",
          "<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  MNIST - Unsupervised Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>  Srinivasan2014 - Retinal OCT Disease Classification benchmarking: Acc<BR>",
          "<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: # of clusters (k)<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  CelebA 128 x 128 - Image Generation benchmarking: FID<BR>  Stacked MNIST - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Pose Transfer<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: FID<BR>",
          "<BR>task: Image generation: Pose Transfer<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: DS<BR>  Deep-Fashion - Pose Transfer benchmarking: PCKh<BR>  Market-1501 - Pose Transfer benchmarking: DS<BR>  Market-1501 - Pose Transfer benchmarking: IS<BR>  Market-1501 - Pose Transfer benchmarking: PCKh<BR>  Market-1501 - Pose Transfer benchmarking: SSIM<BR>  Market-1501 - Pose Transfer benchmarking: mask-IS<BR>  Market-1501 - Pose Transfer benchmarking: mask-SSIM<BR>",
          "<BR>task: Image generation: Pose Transfer<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: LPIPS<BR>  Deep-Fashion - Pose Transfer benchmarking: Retrieval Top10 Recall<BR>",
          "<BR>task: Image generation: Pose Transfer<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: IS<BR>  Deep-Fashion - Pose Transfer benchmarking: SSIM<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  LSUN Car 512 x 384 - Image Generation benchmarking: FID<BR>  LSUN Horse 256 x 256 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  ADE-Indoor - Image Generation benchmarking: FID<BR>  CIFAR-100 - Image Generation benchmarking: FID<BR>  Cityscapes-25K 256x512 - Image Generation benchmarking: FID<BR>  Cityscapes-5K 256x512 - Image Generation benchmarking: FID<BR>  ImageNet 32x32 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: FID<BR>  Fashion-MNIST - Image Generation benchmarking: FID<BR>  MNIST - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 64x64 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: bits/dimension<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Binarized MNIST - Image Generation benchmarking: nats<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  CUB 128 x 128 - Image Generation benchmarking: FID<BR>  CUB 128 x 128 - Image Generation benchmarking: Inception score<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>  Stanford Cars - Image Generation benchmarking: FID<BR>  Stanford Cars - Image Generation benchmarking: Inception score<BR>  Stanford Dogs - Image Generation benchmarking: FID<BR>  Stanford Dogs - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  LSUN Bedroom - Image Generation benchmarking: FID-50k<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 128x128 - Image Generation benchmarking: FID<BR>  MNIST - Image Generation benchmarking: bits/dimension<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  CAT 256x256 - Image Generation benchmarking: FID<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  LSUN Bedroom 64 x 64 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  ImageNet 128x128 - Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Image Generation benchmarking: IS<BR>  ImageNet 256x256 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: bpd<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 1024x1024 - Image Generation benchmarking: FID<BR>  CelebA-HQ 256x256 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Cat 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Churches 256 x 256 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: Kernel Inception Distance<BR>",
          "<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ImageNet - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>  Watercolor2k - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ISTD - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  UCF - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Dense Object Detection<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Clipart1k - Weakly Supervised Object Detection benchmarking: MAP<BR>  Comic2k - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  SOC - RGB Salient Object Detection benchmarking: Average MAE<BR>  SOC - RGB Salient Object Detection benchmarking: S-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: mean E-Measure<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: MAE<BR>  SOD - RGB Salient Object Detection benchmarking: MAE<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: F-measure<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-test - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-test - RGB Salient Object Detection benchmarking: MAE<BR>  ECSSD - RGB Salient Object Detection benchmarking: F-measure<BR>  ECSSD - RGB Salient Object Detection benchmarking: MAE<BR>  HKU-IS - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: MAE<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: F-measure<BR>  SOD - RGB Salient Object Detection benchmarking: F-measure<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  IconArt - Weakly Supervised Object Detection benchmarking: MAP<BR>  PeopleArt - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  nuScenes - 3D Object Detection benchmarking: mAP<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  nuScenes-F - 3D Object Detection benchmarking: AP50<BR>  nuScenes-F - 3D Object Detection benchmarking: AP75<BR>  nuScenes-F - 3D Object Detection benchmarking: AP<BR>  nuScenes-F - 3D Object Detection benchmarking: AR<BR>  nuScenes-F - 3D Object Detection benchmarking: ARI<BR>  nuScenes-F - 3D Object Detection benchmarking: ARm<BR>  nuScenes-F - 3D Object Detection benchmarking: ARs<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP50<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP75<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP<BR>  nuScenes-FB - 3D Object Detection benchmarking: AR<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARI<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARm<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARs<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  nuScenes - 3D Object Detection benchmarking: NDS<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  NYU Depth v2 - 3D Object Detection benchmarking: MAP<BR>  SUN-RGBD - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>",
          "<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Bare MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Heavy MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Partial MR^-2<BR>",
          "<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Large MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Medium MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Small MR^-2<BR>",
          "<BR>task: Object detection: Pedestrian Detection<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Caltech - Pedestrian Detection benchmarking: Reasonable Miss Rate<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Caltech Lanes Cordova - Lane Detection benchmarking: F1<BR>  Caltech Lanes Washington - Lane Detection benchmarking: F1<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  CULane - Lane Detection benchmarking: F1 score<BR>  TuSimple - Lane Detection benchmarking: F1 score<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  BDD100K - Lane Detection benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  COCO 2017 - Object Detection benchmarking: Mean mAP<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: Video Object Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>  ImageNet VID - Video Object Detection benchmarking: runtime (ms)<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cyclists Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  BDD100K - Object Detection benchmarking: mAP-at-0.5<BR>  India Driving Dataset - Object Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  iSAID - Object Detection benchmarking: Average Precision<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP50<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>  COCO minival - Object Detection benchmarking: box AP<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  PeopleArt - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Backpack<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Gender<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Hat<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCS<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCS<BR>",
          "<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  GTSRB - Traffic Sign Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking: MAP<BR>  Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking: MAP<BR>",
          "<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP at-0.5:0.95<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP-at-0.50<BR>",
          "<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  PA-100K - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  PETA - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  RAP - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: Precision<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Unseen)<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  GOT-10k - Visual Object Tracking benchmarking: Average Overlap<BR>  GOT-10k - Visual Object Tracking benchmarking: Success Rate 0.5<BR>  LaSOT - Visual Object Tracking benchmarking: AUC<BR>",
          "<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Unseen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: O (Average of Measures)<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  VOT2017 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  VOT2019 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>  VOT2016 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>",
          "<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>",
          "<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Scan2CAD - 3D Reconstruction benchmarking: Average Accuracy<BR>",
          "<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Data3D−R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>",
          "<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>",
          "<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Data3D−R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D−R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Mean Accuracy<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  Sydney Urban Objects - 3D Point Cloud Classification benchmarking: F1<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  INRIA Holidays - Image Retrieval benchmarking: Mean mAP<BR>  NUS-WIDE - Image Retrieval benchmarking: MAP<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  DeepFashion - Image Retrieval benchmarking: Recall-at-20<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  street2shop - topwear - Image Retrieval benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CUB-200-2011 - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>",
          "<BR>task: Other image process: Image Reconstruction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: HP<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: MMD<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: HP<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: MMD<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  FRGC - Image Clustering benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  FRGC - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>  UMist - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  CMU-PIE - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: Accuracy<BR>  YouTube Faces DB - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2012-08<BR>Anchor.<BR>benchmarks:<BR>  Coil-20 - Image Clustering benchmarking: Accuracy<BR>  Coil-20 - Image Clustering benchmarking: NMI<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>  Fashion-MNIST - Image Clustering benchmarking: Accuracy<BR>  Fashion-MNIST - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: Accuracy<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: Accuracy<BR>  coil-100 - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2012-03<BR>Anchor.<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Set12 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: SSIM<BR>  Urban100 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Set12 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ARL Polarimetric Thermal Face Dataset - Image Clustering benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma35 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma75 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma35 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma60 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: SSIM<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: PSNR (sRGB)<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: SSIM (sRGB)<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  CBSD68 sigma50 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma5 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  CBSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma75 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma35 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma50 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma75 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma15 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma25 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma35 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma50 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma75 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Image Reconstruction<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>",
          "<BR>task: Other image process: Image Reconstruction<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Clothes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Clothes - Image Reconstruction benchmarking: LPIPS<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  LetterA-J - Image Clustering benchmarking: Accuracy<BR>  LetterA-J - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Other video process: Video Generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  UCF-101 16 frames, 64x64, Unconditional - Video Generation benchmarking: Inception Score<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video Median Rank<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-10<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-1<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video Median Rank<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video Median Rank<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-10<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-1<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-50<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-5<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video Median Rank<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-10<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-1<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-50<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-50<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Other video process: Video Generation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  BAIR Robot Pushing - Video Generation benchmarking: FVD score<BR>  Kinetics-600 12 frames, 128x128 - Video Generation benchmarking: FID<BR>  Kinetics-600 12 frames, 64x64 - Video Generation benchmarking: FID<BR>  Kinetics-600 12 frames, 64x64 - Video Generation benchmarking: Inception Score<BR>  Kinetics-600 48 frames, 64x64 - Video Generation benchmarking: FID<BR>  Kinetics-600 48 frames, 64x64 - Video Generation benchmarking: Inception Score<BR>",
          "<BR>task: Other video process: Video Frame Interpolation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Middlebury - Video Frame Interpolation benchmarking: Interpolation Error<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>",
          "<BR>task: Other video process: Video Frame Interpolation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  UCF101 - Video Frame Interpolation benchmarking: PSNR<BR>  UCF101 - Video Frame Interpolation benchmarking: SSIM<BR>  Vimeo90k - Video Frame Interpolation benchmarking: SSIM<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: PSNR<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: SSIM<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: tOF<BR>",
          "<BR>task: Other video process: Video Frame Interpolation<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Middlebury - Video Frame Interpolation benchmarking: PSNR<BR>  Middlebury - Video Frame Interpolation benchmarking: SSIM<BR>",
          "<BR>task: Other video process: Video Generation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  TrailerFaces - Video Generation benchmarking: FID<BR>",
          "<BR>task: Other video process: Video Generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text Mean Rank<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text Median Rank<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-1<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  HMDBfull-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Synth Objects-to-LINEMOD - Domain Adaptation benchmarking: Classification Accuracy<BR>  Synth Objects-to-LINEMOD - Domain Adaptation benchmarking: Mean Angle Error<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  TDIUC - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  USPS-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Office-Caltech-10 - Domain Adaptation benchmarking: Accuracy (%)<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking: mIoU<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Generalization<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>  ImageNet-R - Domain Generalization benchmarking: Top-1 Error Rate<BR>",
          "<BR>task: Other vision process: Domain Generalization<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  ImageNet-C - Domain Generalization benchmarking: mean Corruption Error (mCE)<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  SYNSIG-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>  Synth Signs-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Crowd Counting<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech A - Crowd Counting benchmarking: MSE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>  UCF CC 50 - Crowd Counting benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Depth Completion<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  VOID - Depth Completion benchmarking: MAE<BR>  VOID - Depth Completion benchmarking: RMSE<BR>  VOID - Depth Completion benchmarking: iMAE<BR>  VOID - Depth Completion benchmarking: iRMSE<BR>",
          "<BR>task: Other vision process: Depth Completion<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: iMAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: iRMSE<BR>",
          "<BR>task: Other vision process: Depth Completion<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>  KITTI Depth Completion - Depth Completion benchmarking: Runtime [ms]<BR>",
          "<BR>task: Other vision process: Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  IC19-ReCTs - Scene Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: H-Mean<BR>  ICDAR 2015 - Scene Text Detection benchmarking: H-Mean<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: H-Mean<BR>  Total-Text - Scene Text Detection benchmarking: H-Mean<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: TIoU<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: H-Mean<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: H-Mean<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Other vision process: Curved Text Detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Other vision process: Scene Graph Generation<BR>date: 2020-02<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: mean Recall @20<BR>",
          "<BR>task: Other vision process: Scene Graph Generation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>",
          "<BR>task: Other vision process: Scene Graph Generation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  VRD - Scene Graph Generation benchmarking: Recall-at-50<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>",
          "<BR>task: Other vision process: Crowd Counting<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: OOB Rate (10^−3)<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Difference<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Length<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Player Distance<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Step Change (10^−3)<BR>  PEMS-SF - Multivariate Time Series Imputation benchmarking: L2 Loss (10^-4)<BR>",
          "<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  MuJoCo - Multivariate Time Series Imputation benchmarking: MSE (10^2, 50% missing)<BR>",
          "<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2010-11<BR>Anchor.<BR>benchmarks:<BR>  Beijing Air Quality - Multivariate Time Series Imputation benchmarking: MAE (PM2.5)<BR>  KDD CUP Challenge 2018 - Multivariate Time Series Imputation benchmarking: MSE (10% missing)<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: MAE (10% of data as GT)<BR>  UCI localization data - Multivariate Time Series Imputation benchmarking: MAE (10% missing)<BR>",
          "<BR>task: Other vision process: Video Prediction<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - Video Prediction benchmarking: MAE<BR>  Human3.6M - Video Prediction benchmarking: MSE<BR>  Human3.6M - Video Prediction benchmarking: SSIM<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: number<BR>  VizWiz 2018 - Visual Question Answering benchmarking: other<BR>  VizWiz 2018 - Visual Question Answering benchmarking: unanswerable<BR>  VizWiz 2018 - Visual Question Answering benchmarking: yes/no<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  GQA test-dev - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Other vision process: Video Prediction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  CMU Mocap-1 - Video Prediction benchmarking: Test Error<BR>  CMU Mocap-2 - Video Prediction benchmarking: Test Error<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Make3D - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Make3D - Monocular Depth Estimation benchmarking: RMSE<BR>  Make3D - Monocular Depth Estimation benchmarking: Sq Rel<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  KITTI Eigen split unsupervised - Monocular Depth Estimation benchmarking: absolute relative error<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE log<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: SQ Rel<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>",
          "<BR>task: Other vision process: Metric Learning<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>",
          "<BR>task: Other vision process: Metric Learning<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>",
          "<BR>task: Other vision process: Visual Dialog<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: Mean Rank<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Other vision process: Visual Dialog<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: NDCG (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CLEVR - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  100 sleep nights of 8 caregivers - Visual Question Answering benchmarking: 14 gestures accuracy<BR>  HowmanyQA - Visual Question Answering benchmarking: Accuracy<BR>  TallyQA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Object Counting<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>",
          "<BR>task: Other vision process: Object Counting<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE<BR>  COCO count-test - Object Counting benchmarking: mRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: mRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-relRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE<BR>",
          "<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: mse (10^-3)<BR>",
          "<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Eurasian Cities Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  Horizon Lines in the Wild - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  York Urban Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  SIM10K to BDD100K - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Materials Project - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  OQMD v1.2 - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking: AP-at-0.7<BR>",
          "<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: ATV<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: AUC<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: MSE<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-Olympic - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: PCK3D<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking: MPJAE<BR>  3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking: MPJPE<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  CHALL H80K - 3D Human Pose Estimation benchmarking: MPJPE<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>  3DPW - 3D Human Pose Estimation benchmarking: PA-MPJPE<BR>  3DPW - 3D Human Pose Estimation benchmarking: acceleration error<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: 3DPCK<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: MJPE<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2010-03<BR>Anchor.<BR>benchmarks:<BR>  HumanEva-I - 3D Human Pose Estimation benchmarking: Mean Reconstruction Error (mm)<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGB benchmarking: Mean Recall<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGB benchmarking: Recall (VSD)<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean IoU<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: IoU-2D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: IoU-3D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: VSS-2D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: VSS-3D<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADI<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 10cm<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 5cm<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-25<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-50<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 5, 5cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 10cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 5cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-25<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-50<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 5, 5cm<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGBD benchmarking: Mean Recall<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: MPJPE (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: MPJPE (CS)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CS)<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Total Capture - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean AUC<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation benchmarking: Accuracy (ADD)<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean IoU<BR>  OCCLUSION - 6D Pose Estimation using RGB benchmarking: MAP<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: 5°5 cm<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: IOU25<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: Rerr<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: Terr<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPVPE<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADI<BR>",
          "<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Frames Per View<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Views<BR>",
          "<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>  BJUT-3D - Head Pose Estimation benchmarking: MAE<BR>  Pointing'04 - Head Pose Estimation benchmarking: MAE<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  FLIC Elbows - Pose Estimation benchmarking: PCK-at-0.2<BR>  FLIC Wrists - Pose Estimation benchmarking: PCK-at-0.2<BR>  J-HMDB - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>  ITOP top-view - Pose Estimation benchmarking: Mean mAP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  UAV-Human - Pose Estimation benchmarking: mAP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  UPenn Action - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  DensePose-COCO - Pose Estimation benchmarking: AP<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: AP<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: FPS<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Pose Estimation benchmarking: AP<BR>",
          "<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>  BIWI - Head Pose Estimation benchmarking: MAE (trained with other data)<BR>",
          "<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>  MSRA Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>  NYU Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>",
          "<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  HANDS 2017 - Hand Pose Estimation benchmarking: Average 3D Error<BR>",
          "<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  BIWI - Head Pose Estimation benchmarking: MAE (trained with BIWI data)<BR>",
          "<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: FPS<BR>  K2HPD - Hand Pose Estimation benchmarking: PDJ@5mm<BR>  NYU Hands - Hand Pose Estimation benchmarking: FPS<BR>",
          "<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  HANDS 2019 - Hand Pose Estimation benchmarking: Average 3D Error<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Multi-Person PoseTrack - Pose Tracking benchmarking: MOTA<BR>  Multi-Person PoseTrack - Pose Tracking benchmarking: MOTP<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  PoseTrack2018 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2018 - Pose Tracking benchmarking: mAP<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: VInfo<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: VRand<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: Dice Score<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: MSD<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: VS<BR>  HSVM - Medical Image Segmentation benchmarking: Dice Score<BR>  HSVM - Medical Image Segmentation benchmarking: MSD<BR>  HSVM - Medical Image Segmentation benchmarking: VS<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Medical Image Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Dice<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  MHP v2.0 - Human Part Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  2018 Data Science Bowl - Medical Image Segmentation benchmarking: Dice<BR>  2018 Data Science Bowl - Medical Image Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Cell - Medical Image Segmentation benchmarking: IoU<BR>  EM - Medical Image Segmentation benchmarking: IoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>  NYU Depth v2 - Instance Segmentation benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  iSAID - Instance Segmentation benchmarking: Average Precision<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  LVIS v1.0 - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mAcc<BR>  S3DIS - 3D Instance Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  ISLES-2015 - Lesion Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ISIC 2017 - Lesion Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BUS 2017 Dataset B - Lesion Segmentation benchmarking: Dice Score<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  iSEG 2017 Challenge - Medical Image Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: F1-Score<BR>",
          "<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  NIH - Lung Nodule Segmentation benchmarking: AVD<BR>  NIH - Lung Nodule Segmentation benchmarking: Dice Score<BR>  NIH - Lung Nodule Segmentation benchmarking: Precision<BR>  NIH - Lung Nodule Segmentation benchmarking: Recall<BR>  NIH - Lung Nodule Segmentation benchmarking: VS<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>  S3DIS - 3D Instance Segmentation benchmarking: mRec<BR>  ScanNet - 3D Instance Segmentation benchmarking: mAP<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  SceneNN - 3D Instance Segmentation benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mCov<BR>  S3DIS - 3D Instance Segmentation benchmarking: mWCov<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  ScanNetV1 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - 3D Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: mIoU<BR>  Montgomery County - Lung Nodule Segmentation benchmarking: Accuracy<BR>  Montgomery County - Lung Nodule Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CVC-ClinicDB - Medical Image Segmentation benchmarking: mean Dice<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: Warping Error<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: Average MAE<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: S-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: max E-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: mean Dice<BR>  RITE - Medical Image Segmentation benchmarking: Dice<BR>  RITE - Medical Image Segmentation benchmarking: Jaccard Index<BR>",
          "<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  LIDC-IDRI - Lung Nodule Segmentation benchmarking: Dice<BR>  LIDC-IDRI - Lung Nodule Segmentation benchmarking: IoU<BR>  Lung Nodule  - Lung Nodule Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CT-150 - Pancreas Segmentation benchmarking: Dice Score<BR>  CT-150 - Pancreas Segmentation benchmarking: Precision<BR>  CT-150 - Pancreas Segmentation benchmarking: Recall<BR>  TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD - Scene Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: 3DIoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>  SkyScapes-Lane - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Kvasir-Instrument - Semantic Segmentation benchmarking: DSC<BR>  Kvasir-Instrument - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Global Accuracy<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  HRF - Retinal Vessel Segmentation benchmarking: AUC<BR>  HRF - Retinal Vessel Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  NYU Depth v2 - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2011 - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  BDD - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Category mIoU<BR>  GTAV-to-Cityscapes Labels - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ParisLille3D - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: Pixel Accuracy<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Semantic Segmentation - Semantic Segmentation benchmarking: Mean IoU (class)<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Freiburg Forest - Semantic Segmentation benchmarking: Mean IoU<BR>  SUN-RGBD - Semantic Segmentation benchmarking: Mean IoU<BR>  SYNTHIA-CVPR’16 - Semantic Segmentation benchmarking: Mean IoU<BR>  ScanNetV2 - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: oAcc<BR>  Semantic3D - Semantic Segmentation benchmarking: oAcc<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: AUC<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: Frame (fps)<BR>",
          "<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes test - Panoptic Segmentation benchmarking: PQ<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: AP<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQst<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQth<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO panoptic - Panoptic Segmentation benchmarking: PQ<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Mapillary val - Panoptic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: Total Variation of Information<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Merge<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Split<BR>",
          "<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: AUC<BR>",
          "<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2015 - Brain Tumor Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2014 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2017 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: MSD<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: VS<BR>  BRATS 2018 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP50<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP75<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APL<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  PH2 - Skin Cancer Segmentation benchmarking: IoU<BR>",
          "<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2013 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2013 leaderboard - Brain Tumor Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: Average Accuracy<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 20,
          "symbol": 42
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2016-09",
          "2020-01",
          "2016-02",
          "2018-06",
          "2019-03",
          "2018-04",
          "2017-05",
          "2017-03",
          "2016-04",
          "2016-01",
          "2015-11",
          "2015-06",
          "2019-04",
          "2017-03",
          "2016-12",
          "2015-07",
          "2018-06",
          "2017-07",
          "2019-11",
          "2019-08",
          "2018-07",
          "2017-12",
          "2017-03",
          "2012-07",
          "2017-11",
          "2017-08",
          "2017-06",
          "2018-06",
          "2014-06",
          "2016-08",
          "2017-12",
          "2017-03",
          "2017-12",
          "2018-12",
          "2019-04",
          "2019-06",
          "2019-08",
          "2018-07",
          "2018-01",
          "2015-11",
          "2014-03",
          "2012-12",
          "2014-06",
          "2015-11",
          "2015-12",
          "2017-05",
          "2018-01",
          "2019-05",
          "2019-04",
          "2017-04",
          "2015-11",
          "2012-10",
          "2014-06",
          "2014-11",
          "2016-04",
          "2016-06",
          "2016-12",
          "2017-04",
          "2017-10",
          "2018-01",
          "2018-02",
          "2019-01",
          "2019-06",
          "2019-07",
          "2014-11",
          "2018-12",
          "2016-11",
          "2018-11",
          "2019-09",
          "2019-01",
          "2020-04",
          "2013-06",
          "2018-12",
          "2017-05",
          "2019-09",
          "2017-07",
          "2019-03",
          "2016-10",
          "2017-01",
          "2017-08",
          "2015-11",
          "2015-06",
          "2014-06",
          "2017-10",
          "2018-08",
          "2018-05",
          "2016-09",
          "2019-02",
          "2019-05",
          "2015-11",
          "2017-08",
          "2018-03",
          "2018-04",
          "2016-07",
          "2016-04",
          "2017-03",
          "2017-08",
          "2014-04",
          "2014-12",
          "2015-03",
          "2015-07",
          "2015-11",
          "2016-04",
          "2017-08",
          "2017-10",
          "2019-03",
          "2019-07",
          "2015-03",
          "2019-08",
          "2018-12",
          "2018-04",
          "2018-02",
          "2017-09",
          "2017-06",
          "2013-07",
          "2015-05",
          "2014-08",
          "2015-09",
          "2017-05",
          "2014-08",
          "2014-06",
          "2018-04",
          "2019-01",
          "2019-07",
          "2017-05",
          "2017-01",
          "2017-07",
          "2013-03",
          "2017-11",
          "2018-06",
          "2015-02",
          "2016-12",
          "2019-02",
          "2019-04",
          "2019-01",
          "2018-10",
          "2018-05",
          "2017-11",
          "2017-10",
          "2019-12",
          "2017-08",
          "2016-03",
          "2013-12",
          "2013-06",
          "2013-01",
          "2012-12",
          "2012-02",
          "2017-07",
          "2019-06",
          "2019-10",
          "2018-07",
          "2018-03",
          "2015-09",
          "2015-04",
          "2018-02",
          "2015-12",
          "2015-11",
          "2016-10",
          "2015-11",
          "2019-10",
          "2019-10",
          "2019-04",
          "2017-12",
          "2017-05",
          "2019-12",
          "2019-11",
          "2018-02",
          "2019-05",
          "2019-03",
          "2014-10",
          "2016-01",
          "2016-06",
          "2018-12",
          "2018-11",
          "2017-03",
          "2017-06",
          "2018-09",
          "2018-07",
          "2017-09",
          "2017-10",
          "2018-02",
          "2017-11",
          "2016-11",
          "2016-11",
          "2014-07",
          "2015-11",
          "2016-09",
          "2014-06",
          "2016-11",
          "2015-05",
          "2015-06",
          "2016-03",
          "2018-03",
          "2017-07",
          "2017-08",
          "2018-06",
          "2019-04",
          "2018-10",
          "2014-03",
          "2015-11",
          "2016-11",
          "2016-08",
          "2019-08",
          "2019-05",
          "2019-04",
          "2019-03",
          "2017-11",
          "2016-11",
          "2015-11",
          "2017-11",
          "2017-02",
          "2014-11",
          "2015-04",
          "2017-08",
          "2017-12",
          "2019-08",
          "2019-12",
          "2015-12",
          "2017-03",
          "2017-11",
          "2017-12",
          "2019-07",
          "2019-09",
          "2017-03",
          "2016-12",
          "2016-10",
          "2015-12",
          "2018-12",
          "2015-12",
          "2012-02",
          "2018-06",
          "2019-04",
          "2017-09",
          "2019-09",
          "2016-06",
          "2017-06",
          "2018-11",
          "2015-04",
          "2015-12",
          "2016-11",
          "2019-01",
          "2019-07",
          "2017-04",
          "2016-10",
          "2016-03",
          "2016-04",
          "2018-03",
          "2016-04",
          "2016-12",
          "2016-04",
          "2019-02",
          "2019-08",
          "2016-11",
          "2019-01",
          "2018-11",
          "2018-04",
          "2017-06",
          "2016-04",
          "2019-03",
          "2015-04",
          "2017-04",
          "2017-03",
          "2016-04",
          "2015-11",
          "2013-12",
          "2012-08",
          "2012-03",
          "2019-08",
          "2018-10",
          "2018-05",
          "2018-04",
          "2017-10",
          "2016-06",
          "2015-08",
          "2015-12",
          "2015-08",
          "2016-08",
          "2016-11",
          "2017-04",
          "2017-10",
          "2015-11",
          "2017-04",
          "2018-10",
          "2016-08",
          "2016-11",
          "2019-10",
          "2018-12",
          "2014-12",
          "2019-12",
          "2015-06",
          "2016-09",
          "2016-10",
          "2018-08",
          "2019-07",
          "2019-07",
          "2017-08",
          "2019-04",
          "2020-03",
          "2019-04",
          "2016-09",
          "2018-06",
          "2016-05",
          "2016-08",
          "2019-02",
          "2017-05",
          "2018-07",
          "2018-11",
          "2019-04",
          "2015-12",
          "2018-11",
          "2012-12",
          "2017-11",
          "2017-02",
          "2015-02",
          "2017-07",
          "2019-05",
          "2019-02",
          "2017-08",
          "2015-08",
          "2014-09",
          "2019-06",
          "2019-04",
          "2018-11",
          "2018-06",
          "2018-01",
          "2017-09",
          "2017-04",
          "2017-03",
          "2016-06",
          "2016-04",
          "2014-12",
          "2015-05",
          "2020-02",
          "2017-07",
          "2016-07",
          "2014-12",
          "2015-12",
          "2015-11",
          "2018-03",
          "2018-01",
          "2016-06",
          "2010-11",
          "2017-12",
          "2019-08",
          "2019-07",
          "2019-04",
          "2019-05",
          "2018-06",
          "2018-03",
          "2016-09",
          "2014-11",
          "2017-06",
          "2016-10",
          "2016-05",
          "2017-04",
          "2015-11",
          "2016-06",
          "2016-12",
          "2017-04",
          "2017-07",
          "2018-08",
          "2018-10",
          "2015-06",
          "2016-04",
          "2018-06",
          "2016-04",
          "2018-12",
          "2017-02",
          "2018-06",
          "2019-05",
          "2019-04",
          "2019-05",
          "2019-07",
          "2014-06",
          "2020-04",
          "2019-07",
          "2018-09",
          "2017-12",
          "2017-05",
          "2010-03",
          "2013-12",
          "2019-02",
          "2019-08",
          "2017-11",
          "2017-11",
          "2018-03",
          "2019-01",
          "2019-02",
          "2017-04",
          "2016-01",
          "2018-12",
          "2019-01",
          "2017-11",
          "2017-03",
          "2019-10",
          "2019-09",
          "2018-03",
          "2017-01",
          "2016-11",
          "2016-01",
          "2016-03",
          "2016-11",
          "2016-12",
          "2017-12",
          "2018-02",
          "2019-01",
          "2017-01",
          "2016-12",
          "2016-11",
          "2016-05",
          "2019-01",
          "2015-11",
          "2017-02",
          "2017-08",
          "2017-10",
          "2019-08",
          "2020-01",
          "2014-07",
          "2014-11",
          "2016-11",
          "2018-04",
          "2017-10",
          "2019-03",
          "2019-06",
          "2019-08",
          "2014-11",
          "2018-11",
          "2018-08",
          "2015-11",
          "2016-03",
          "2018-07",
          "2019-12",
          "2017-06",
          "2016-05",
          "2015-12",
          "2016-04",
          "2016-11",
          "2017-03",
          "2017-11",
          "2018-03",
          "2019-12",
          "2018-01",
          "2016-03",
          "2017-03",
          "2018-10",
          "2018-04",
          "2019-08",
          "2015-05",
          "2019-03",
          "2019-04",
          "2019-02",
          "2019-04",
          "2019-12",
          "2016-06",
          "2016-12",
          "2017-11",
          "2016-12",
          "2019-07",
          "2019-07",
          "2015-05",
          "2019-08",
          "2015-05",
          "2019-03",
          "2019-07",
          "2014-12",
          "2019-04",
          "2014-07",
          "2014-11",
          "2014-12",
          "2015-05",
          "2015-11",
          "2015-12",
          "2018-06",
          "2016-03",
          "2016-11",
          "2016-12",
          "2017-02",
          "2019-12",
          "2019-04",
          "2019-01",
          "2018-12",
          "2018-08",
          "2017-11",
          "2017-07",
          "2016-06",
          "2015-05",
          "2020-01",
          "2015-05",
          "2017-03",
          "2017-03",
          "2017-04",
          "2018-01",
          "2018-08",
          "2018-09",
          "2019-01",
          "2019-09",
          "2019-06",
          "2015-05",
          "2016-03",
          "2017-09",
          "2018-10",
          "2019-06",
          "2019-04",
          "2019-11",
          "2015-05",
          "2016-12"
         ],
         "y": [
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Action Segmentation",
          "Action localization: Action Segmentation",
          "Action localization: Action Segmentation",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Action localization: Temporal Action Localization",
          "Activity detection: Action Detection",
          "Activity detection: Action Detection",
          "Activity detection: Action Detection",
          "Activity detection: Action Detection",
          "Activity localization: Temporal Action Proposal Generation",
          "Activity localization: Temporal Action Proposal Generation",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Classification",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Group Activity Recognition",
          "Activity recognition: Group Activity Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Human Interaction Recognition",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Human Interaction Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Recognition",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Face Detection",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Image classification: Document Image Classification",
          "Image classification: Document Image Classification",
          "Image classification: Hyperspectral Image Classification",
          "Image classification: Hyperspectral Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Unsupervised Image Classification",
          "Image classification: Sequential Image Classification",
          "Image classification: Satellite Image Classification",
          "Image classification: Sequential Image Classification",
          "Image classification: Unsupervised Image Classification",
          "Image classification: Retinal OCT Disease Classification",
          "Image classification: Unsupervised Image Classification",
          "Image generation: Conditional Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Image Generation",
          "Image generation: Pose Transfer",
          "Image generation: Pose Transfer",
          "Image generation: Pose Transfer",
          "Image generation: Pose Transfer",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Conditional Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image generation: Image Generation",
          "Image-to-image translation: Fundus to Angiography Generation",
          "Image-to-image translation: Fundus to Angiography Generation",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Dense Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: 3D Object Detection",
          "Object detection: Pedestrian Detection",
          "Object detection: Pedestrian Detection",
          "Object detection: Pedestrian Detection",
          "Object detection: Lane Detection",
          "Object detection: Lane Detection",
          "Object detection: Lane Detection",
          "Object detection: Lane Detection",
          "Object detection: Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Video Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: Object Detection",
          "Object detection: 3D Object Detection",
          "Object recognition: Pedestrian Attribute Recognition",
          "Object recognition: Traffic Sign Recognition",
          "Object recognition: Traffic Sign Recognition",
          "Object recognition: Traffic Sign Recognition",
          "Object recognition: Pedestrian Attribute Recognition",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Multiple Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Other 3D task: 3D Shape Classification",
          "Other 3D task: 3D Reconstruction",
          "Other 3D task: 3D Reconstruction",
          "Other 3D task: 3D Room Layouts From A Single RGB Panorama",
          "Other 3D task: 3D Object Reconstruction",
          "Other 3D task: 3D Point Cloud Classification",
          "Other 3D task: 3D Point Cloud Classification",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Retrieval",
          "Other image process: Image Reconstruction",
          "Other image process: Image Retrieval",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Image Clustering",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Image Clustering",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Aesthetics Quality Assessment",
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising",
          "Other image process: Color Image Denoising",
          "Other image process: Image Retrieval",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Image Clustering",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Image Reconstruction",
          "Other image process: Image Reconstruction",
          "Other image process: Image Clustering",
          "Other image process: Image Retrieval",
          "Other video process: Video Generation",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Retrieval",
          "Other video process: Video Generation",
          "Other video process: Video Frame Interpolation",
          "Other video process: Video Frame Interpolation",
          "Other video process: Video Frame Interpolation",
          "Other video process: Video Generation",
          "Other video process: Video Generation",
          "Other video process: Video Retrieval",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Visual Question Answering",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Generalization",
          "Other vision process: Domain Generalization",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Crowd Counting",
          "Other vision process: Depth Completion",
          "Other vision process: Depth Completion",
          "Other vision process: Depth Completion",
          "Other vision process: Denoising",
          "Other vision process: Domain Adaptation",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Scene Text Detection",
          "Other vision process: Curved Text Detection",
          "Other vision process: Scene Graph Generation",
          "Other vision process: Scene Graph Generation",
          "Other vision process: Scene Graph Generation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Crowd Counting",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Multivariate Time Series Imputation",
          "Other vision process: Multivariate Time Series Imputation",
          "Other vision process: Multivariate Time Series Imputation",
          "Other vision process: Video Prediction",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Video Prediction",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Metric Learning",
          "Other vision process: Metric Learning",
          "Other vision process: Visual Dialog",
          "Other vision process: Visual Dialog",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Question Answering",
          "Other vision process: Object Counting",
          "Other vision process: Object Counting",
          "Other vision process: Multivariate Time Series Imputation",
          "Other vision process: Horizon Line Estimation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Formation Energy",
          "Other vision process: Formation Energy",
          "Other vision process: Formation Energy",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Horizon Line Estimation",
          "Other vision process: Domain Adaptation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Hand Pose Estimation",
          "Pose estimation: Hand Pose Estimation",
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Hand Pose Estimation",
          "Pose estimation: Hand Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose tracking: Pose Tracking",
          "Pose tracking: Pose Tracking",
          "Pose tracking: Pose Tracking",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Multi-tissue Nucleus Segmentation",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Nuclear Segmentation",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Video Semantic Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: Lesion Segmentation",
          "Semantic segmentation: Lesion Segmentation",
          "Semantic segmentation: Lesion Segmentation",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Lesion Segmentation",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Semantic Instance Segmentation",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: Pancreas Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Scene Segmentation",
          "Semantic segmentation: Scene Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Real-time Instance Segmentation",
          "Semantic segmentation: Skin Cancer Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Electron Microscopy Image Segmentation",
          "Semantic segmentation: Electron Microscopy Image Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Semantic segmentation: Real-time Instance Segmentation",
          "Semantic segmentation: Skin Cancer Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Semantic segmentation: Scene Segmentation"
         ]
        },
        {
         "hovertemplate": [
          "<BR>task: Other image process: Image Clustering<BR>date: 2012-08<BR>ratio: 0.5546<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2012-12<BR>ratio: 0.0189<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2013-02<BR>ratio: 0.1073<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2013-02<BR>ratio: 1.0<BR>benchmarks:<BR>  CAD-120 - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2013-07<BR>ratio: 1.0<BR>benchmarks:<BR>  FER2013 - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2013-11<BR>ratio: 0.0397<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2013-12<BR>ratio: 0.0553<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2014-04<BR>ratio: 0.021<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2014-06<BR>ratio: 0.8122<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-06<BR>ratio: 0.4737<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2014-06<BR>ratio: 0.289<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2014-09<BR>ratio: 0.1446<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-09<BR>ratio: 0.9752<BR>benchmarks:<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-Olympic - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-11<BR>ratio: 0.2834<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Object detection: Pedestrian Detection<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Caltech - Pedestrian Detection benchmarking: Reasonable Miss Rate<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2014-12<BR>ratio: 0.2375<BR>benchmarks:<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-12<BR>ratio: 0.2513<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2014-12<BR>ratio: 0.1392<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-02<BR>ratio: 0.0877<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2015-02<BR>ratio: 0.8858<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-02<BR>ratio: 0.0451<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-02<BR>ratio: 0.3958<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2015-02<BR>ratio: 0.0134<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2015-03<BR>ratio: 0.3753<BR>benchmarks:<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-03<BR>ratio: 0.2327<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-03<BR>ratio: 0.0741<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2015-04<BR>ratio: 0.426<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2015-04<BR>ratio: 0.1007<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-04<BR>ratio: 0.2878<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2015-05<BR>ratio: 0.8426<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-05<BR>ratio: 0.3991<BR>benchmarks:<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Curved Text Detection<BR>date: 2015-05<BR>ratio: 0.2321<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2015-05<BR>ratio: 0.1675<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2015-05<BR>ratio: 0.1142<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2015-06<BR>ratio: 0.4459<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2015-06<BR>ratio: 0.2457<BR>benchmarks:<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-08<BR>ratio: 0.4321<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-09<BR>ratio: 0.5<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2015-11<BR>ratio: 0.117<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Sequential Image Classification<BR>date: 2015-11<BR>ratio: 0.4701<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-11<BR>ratio: 0.0854<BR>benchmarks:<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2015-11<BR>ratio: 1.0<BR>benchmarks:<BR>  RITE - Medical Image Segmentation benchmarking: Jaccard Index<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2015-11<BR>ratio: 0.1888<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>",
          "<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2015-11<BR>ratio: 0.3222<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2015-11<BR>ratio: 0.0331<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2015-11<BR>ratio: 0.15<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-11<BR>ratio: 0.1785<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-11<BR>ratio: 0.4339<BR>benchmarks:<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2015-11<BR>ratio: 0.1637<BR>benchmarks:<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2015-12<BR>ratio: 0.0779<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Crowd Counting<BR>date: 2015-12<BR>ratio: 0.7477<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>",
          "<BR>task: Image classification: Satellite Image Classification<BR>date: 2015-12<BR>ratio: 0.4051<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2015-12<BR>ratio: 1.0<BR>benchmarks:<BR>  ActivityNet - Action Recognition benchmarking: mAP<BR>",
          "<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2015-12<BR>ratio: 0.9509<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2015-12<BR>ratio: 0.5016<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2016-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Binarized MNIST - Image Generation benchmarking: nats<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2016-01<BR>ratio: 0.0707<BR>benchmarks:<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-01<BR>ratio: 0.6524<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-01<BR>ratio: 0.0478<BR>benchmarks:<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2016-02<BR>ratio: 0.056<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2016-03<BR>ratio: 0.0258<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2016-03<BR>ratio: 1.0<BR>benchmarks:<BR>  MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-03<BR>ratio: 0.3359<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2016-03<BR>ratio: 0.2119<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Image classification: Sequential Image Classification<BR>date: 2016-03<BR>ratio: 0.4252<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2016-03<BR>ratio: 0.5954<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-03<BR>ratio: 0.7322<BR>benchmarks:<BR>  FLIC Elbows - Pose Estimation benchmarking: PCK-at-0.2<BR>  FLIC Wrists - Pose Estimation benchmarking: PCK-at-0.2<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-03<BR>ratio: 0.1471<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2016-04<BR>ratio: 0.4868<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2016-04<BR>ratio: 0.4832<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-04<BR>ratio: 0.4271<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2016-04<BR>ratio: 0.6898<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: NMI<BR>  Coil-20 - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2016-04<BR>ratio: 0.0092<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Other vision process: Object Counting<BR>date: 2016-04<BR>ratio: 0.8889<BR>benchmarks:<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE<BR>  COCO count-test - Object Counting benchmarking: mRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: mRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-relRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-04<BR>ratio: 0.3005<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-05<BR>ratio: 0.0418<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2016-05<BR>ratio: 0.1069<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-05<BR>ratio: 0.072<BR>benchmarks:<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-06<BR>ratio: 0.0741<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-06<BR>ratio: 0.572<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-06<BR>ratio: 0.3146<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-06<BR>ratio: 0.061<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2016-06<BR>ratio: 0.3235<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2016-06<BR>ratio: 0.3625<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2016-07<BR>ratio: 0.0241<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2016-08<BR>ratio: 0.8371<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Eurasian Cities Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  York Urban Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-08<BR>ratio: 0.1233<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-08<BR>ratio: 0.2999<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-08<BR>ratio: 0.6547<BR>benchmarks:<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>  Synth Signs-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Backpack<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Gender<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Hat<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCS<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCS<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2016-08<BR>ratio: 0.0251<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Other vision process: Crowd Counting<BR>date: 2016-08<BR>ratio: 0.2523<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-08<BR>ratio: 0.5511<BR>benchmarks:<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2016-08<BR>ratio: 0.1736<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2016-08<BR>ratio: 0.0915<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-09<BR>ratio: 0.0144<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-09<BR>ratio: 0.3082<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>",
          "<BR>task: Other vision process: Object Counting<BR>date: 2016-09<BR>ratio: 0.4115<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-09<BR>ratio: 0.2784<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-09<BR>ratio: 0.0093<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2016-10<BR>ratio: 0.0142<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-10<BR>ratio: 0.0606<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2016-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2016-11<BR>ratio: 0.3326<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>",
          "<BR>task: Action localization: Action Segmentation<BR>date: 2016-11<BR>ratio: 0.3571<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-11<BR>ratio: 0.0<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Metric Learning<BR>date: 2016-11<BR>ratio: 0.2824<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>",
          "<BR>task: Other video process: Video Generation<BR>date: 2016-11<BR>ratio: 0.2492<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2016-11<BR>ratio: 0.2441<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-11<BR>ratio: 0.096<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-11<BR>ratio: 0.3399<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-11<BR>ratio: 0.2329<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-11<BR>ratio: 0.2215<BR>benchmarks:<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-11<BR>ratio: 0.5084<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-11<BR>ratio: 0.6006<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-11<BR>ratio: 0.674<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-11<BR>ratio: 0.2304<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Global Accuracy<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-12<BR>ratio: 0.6019<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-12<BR>ratio: 0.3137<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2016-12<BR>ratio: 0.1022<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-12<BR>ratio: 0.5259<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>",
          "<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2016-12<BR>ratio: 0.4898<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2016-12<BR>ratio: 0.1001<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>",
          "<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2016-12<BR>ratio: 0.5647<BR>benchmarks:<BR>  Data3D−R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D−R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>",
          "<BR>task: Object detection: Dense Object Detection<BR>date: 2016-12<BR>ratio: 0.1125<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-12<BR>ratio: 0.1929<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2016-12<BR>ratio: 0.0388<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-12<BR>ratio: 0.1288<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2016-12<BR>ratio: 0.1429<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>",
          "<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-12<BR>ratio: 0.9756<BR>benchmarks:<BR>  Data3D−R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-01<BR>ratio: 0.1328<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-01<BR>ratio: 0.2025<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>",
          "<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2017-02<BR>ratio: 0.6995<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-02<BR>ratio: 0.0692<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-02<BR>ratio: 0.0541<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-02<BR>ratio: 0.9524<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Medium MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Small MR^-2<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2017-03<BR>ratio: 0.5645<BR>benchmarks:<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2017-03<BR>ratio: 0.1574<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-03<BR>ratio: 0.3612<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2017-03<BR>ratio: 0.1565<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-03<BR>ratio: 0.2235<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2017-03<BR>ratio: 0.0303<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2017-03<BR>ratio: 0.6528<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-03<BR>ratio: 0.5668<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-03<BR>ratio: 0.4081<BR>benchmarks:<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-03<BR>ratio: 0.2484<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-03<BR>ratio: 0.1096<BR>benchmarks:<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-03<BR>ratio: 0.2099<BR>benchmarks:<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2017-03<BR>ratio: 0.2205<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-03<BR>ratio: 0.1193<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-03<BR>ratio: 0.5877<BR>benchmarks:<BR>  SBU - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-04<BR>ratio: 0.2105<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-04<BR>ratio: 0.0331<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2017-04<BR>ratio: 0.4378<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  FRGC - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-04<BR>ratio: 0.2112<BR>benchmarks:<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2017-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-04<BR>ratio: 0.2041<BR>benchmarks:<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-04<BR>ratio: 0.1026<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2017-04<BR>ratio: 0.3846<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-04<BR>ratio: 0.7172<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-04<BR>ratio: 0.103<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2017-04<BR>ratio: 0.1968<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2017-04<BR>ratio: 0.1629<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-04<BR>ratio: 0.2194<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2017-04<BR>ratio: 0.1332<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2017-04<BR>ratio: 0.8514<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  Sydney Urban Objects - 3D Point Cloud Classification benchmarking: F1<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-04<BR>ratio: 0.1767<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>",
          "<BR>task: Image classification: Document Image Classification<BR>date: 2017-04<BR>ratio: 0.4855<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2017-04<BR>ratio: 1.0<BR>benchmarks:<BR>  BSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-04<BR>ratio: 0.4565<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-04<BR>ratio: 0.1679<BR>benchmarks:<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-04<BR>ratio: 0.3172<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.5<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-04<BR>ratio: 0.0758<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>",
          "<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2017-04<BR>ratio: 0.4474<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2017-05<BR>ratio: 0.4983<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-05<BR>ratio: 0.528<BR>benchmarks:<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-05<BR>ratio: 0.2366<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Other vision process: Denoising<BR>date: 2017-05<BR>ratio: 0.7853<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-05<BR>ratio: 0.4284<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CS)<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-05<BR>ratio: 0.8352<BR>benchmarks:<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-05<BR>ratio: 0.2561<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>  UCF101-24 - Temporal Action Localization benchmarking: Frame-mAP<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-05<BR>ratio: 0.0559<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-05<BR>ratio: 0.0607<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-05<BR>ratio: 0.2867<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>",
          "<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2017-06<BR>ratio: 0.6277<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-06<BR>ratio: 0.2514<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-06<BR>ratio: 0.6175<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Seen)<BR>",
          "<BR>task: Other vision process: Metric Learning<BR>date: 2017-06<BR>ratio: 0.3412<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2017-06<BR>ratio: 0.1244<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2017-06<BR>ratio: 0.4844<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2017-06<BR>ratio: 0.4142<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2017-06<BR>ratio: 0.5909<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-06<BR>ratio: 0.4898<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-06<BR>ratio: 0.6641<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-06<BR>ratio: 0.1161<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>",
          "<BR>task: Other image process: Image Reconstruction<BR>date: 2017-06<BR>ratio: 0.7104<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Visual Genome - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-07<BR>ratio: 0.02<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-07<BR>ratio: 0.6614<BR>benchmarks:<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-07<BR>ratio: 0.0095<BR>benchmarks:<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>",
          "<BR>task: Other video process: Video Generation<BR>date: 2017-07<BR>ratio: 0.0387<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2017-07<BR>ratio: 0.4581<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video Median Rank<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-07<BR>ratio: 0.2874<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Other vision process: Object Counting<BR>date: 2017-07<BR>ratio: 0.1758<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>",
          "<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  NYU Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-07<BR>ratio: 0.3268<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-07<BR>ratio: 0.6997<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>  WFLW - Face Alignment benchmarking: ME (%, all)<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-07<BR>ratio: 0.0363<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Generalization<BR>date: 2017-08<BR>ratio: 0.0645<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>",
          "<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2017-08<BR>ratio: 0.2688<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-08<BR>ratio: 0.5913<BR>benchmarks:<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-08<BR>ratio: 0.0083<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-08<BR>ratio: 1.0<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-08<BR>ratio: 0.219<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2017-08<BR>ratio: 0.0624<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-08<BR>ratio: 0.88<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-08<BR>ratio: 0.3456<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-08<BR>ratio: 0.0951<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-08<BR>ratio: 0.1108<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2017-08<BR>ratio: 0.4921<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Object detection: Dense Object Detection<BR>date: 2017-08<BR>ratio: 0.6932<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-08<BR>ratio: 0.5449<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  SOC - RGB Salient Object Detection benchmarking: Average MAE<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2017-09<BR>ratio: 0.8514<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2017-09<BR>ratio: 0.0551<BR>benchmarks:<BR>  FDDB - Face Detection benchmarking: AP<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2017-09<BR>ratio: 0.1598<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-09<BR>ratio: 0.474<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Other vision process: Visual Dialog<BR>date: 2017-09<BR>ratio: 0.3904<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-09<BR>ratio: 0.0103<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Total Capture - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>",
          "<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2017-09<BR>ratio: 0.0758<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-09<BR>ratio: 0.0809<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2017-10<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-10<BR>ratio: 0.6429<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2017-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Caltech Lanes Cordova - Lane Detection benchmarking: F1<BR>  Caltech Lanes Washington - Lane Detection benchmarking: F1<BR>",
          "<BR>task: Image classification: Sequential Image Classification<BR>date: 2017-10<BR>ratio: 0.0909<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-10<BR>ratio: 0.9952<BR>benchmarks:<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  UCF - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-10<BR>ratio: 0.4958<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2017-10<BR>ratio: 0.2127<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-10<BR>ratio: 0.9847<BR>benchmarks:<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2017-10<BR>ratio: 0.4337<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-10<BR>ratio: 0.2492<BR>benchmarks:<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-10<BR>ratio: 0.2941<BR>benchmarks:<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other vision process: Domain Generalization<BR>date: 2017-10<BR>ratio: 0.7097<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2017-10<BR>ratio: 0.5297<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2017-10<BR>ratio: 0.656<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>",
          "<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2017-11<BR>ratio: 0.2656<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-11<BR>ratio: 0.6273<BR>benchmarks:<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>  ShapeNet - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-11<BR>ratio: 0.435<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-11<BR>ratio: 0.2438<BR>benchmarks:<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-11<BR>ratio: 0.619<BR>benchmarks:<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2017-11<BR>ratio: 0.5452<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2017-11<BR>ratio: 0.1491<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-11<BR>ratio: 0.6998<BR>benchmarks:<BR>  KITTI Cars Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2017-11<BR>ratio: 0.789<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2017-11<BR>ratio: 0.1976<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP50<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2017-11<BR>ratio: 0.5062<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2017-11<BR>ratio: 0.6503<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CS<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-11<BR>ratio: 0.3521<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2017-11<BR>ratio: 0.1702<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-11<BR>ratio: 0.3953<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>  ITOP top-view - Pose Estimation benchmarking: Mean mAP<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-11<BR>ratio: 0.069<BR>benchmarks:<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Visual Dialog<BR>date: 2017-11<BR>ratio: 0.2578<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-11<BR>ratio: 0.383<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>",
          "<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2017-11<BR>ratio: 0.2407<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-11<BR>ratio: 0.6857<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2017-12<BR>ratio: 0.0238<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-12<BR>ratio: 0.0546<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-12<BR>ratio: 0.0332<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2017-12<BR>ratio: 0.046<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2017-12<BR>ratio: 0.1898<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  J-HMDB - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2017-12<BR>ratio: 0.5028<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>",
          "<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  HANDS 2017 - Hand Pose Estimation benchmarking: Average 3D Error<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Image generation: Pose Transfer<BR>date: 2017-12<BR>ratio: 0.5<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: IS<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2017-12<BR>ratio: 0.1097<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2017-12<BR>ratio: 0.5417<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2017-12<BR>ratio: 0.0083<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-12<BR>ratio: 0.613<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  SYNSIG-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2017-12<BR>ratio: 0.3524<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2017-12<BR>ratio: 0.3532<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-12<BR>ratio: 0.0247<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-12<BR>ratio: 0.7094<BR>benchmarks:<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-01<BR>ratio: 0.0583<BR>benchmarks:<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Image classification: Document Image Classification<BR>date: 2018-01<BR>ratio: 0.5145<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-01<BR>ratio: 0.4342<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2018-01<BR>ratio: 0.3197<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-01<BR>ratio: 0.0837<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2018-01<BR>ratio: 0.3661<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>  Srinivasan2014 - Retinal OCT Disease Classification benchmarking: Acc<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-01<BR>ratio: 0.3059<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-01<BR>ratio: 0.4232<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-01<BR>ratio: 0.6486<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2018-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-1 (%)<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-5 (%)<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-02<BR>ratio: 0.9695<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  J-HMBD Early Action - Skeleton Based Action Recognition benchmarking: 10%<BR>",
          "<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2018-02<BR>ratio: 0.9199<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-02<BR>ratio: 0.6229<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-02<BR>ratio: 0.1321<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2018-02<BR>ratio: 0.0351<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2018-02<BR>ratio: 0.628<BR>benchmarks:<BR>  Data3D−R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>",
          "<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2018-02<BR>ratio: 0.4938<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-02<BR>ratio: 0.2543<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-02<BR>ratio: 0.0865<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-02<BR>ratio: 0.3996<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2018-02<BR>ratio: 0.2075<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>",
          "<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-02<BR>ratio: 0.3005<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-02<BR>ratio: 0.6875<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: AUC<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2018-02<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma50 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-02<BR>ratio: 0.2155<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-02<BR>ratio: 0.5545<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2018-03<BR>ratio: 0.387<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-03<BR>ratio: 0.1771<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-03<BR>ratio: 0.1202<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-03<BR>ratio: 0.6418<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-03<BR>ratio: 0.1351<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2018-03<BR>ratio: 0.1099<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2018-03<BR>ratio: 0.2131<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-03<BR>ratio: 0.2249<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-03<BR>ratio: 0.0083<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2018-03<BR>ratio: 0.0929<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>",
          "<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: Average Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-03<BR>ratio: 0.5083<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-03<BR>ratio: 0.0203<BR>benchmarks:<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2018-03<BR>ratio: 0.4588<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-03<BR>ratio: 0.3293<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Watercolor2k - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  IJB-A - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2018-03<BR>ratio: 0.2555<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  iSAID - Object Detection benchmarking: Average Precision<BR>",
          "<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-03<BR>ratio: 0.1184<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-04<BR>ratio: 0.0985<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-04<BR>ratio: 0.5245<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-04<BR>ratio: 0.0647<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>",
          "<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  CT-150 - Pancreas Segmentation benchmarking: Dice Score<BR>  CT-150 - Pancreas Segmentation benchmarking: Precision<BR>  CT-150 - Pancreas Segmentation benchmarking: Recall<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2018-04<BR>ratio: 0.1925<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-04<BR>ratio: 0.0067<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-04<BR>ratio: 0.3804<BR>benchmarks:<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-04<BR>ratio: 0.6801<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  ChaLearn val - Hand Gesture Recognition benchmarking: Accuracy<BR>  Jester test - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2018-04<BR>ratio: 0.211<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2018-04<BR>ratio: 0.1429<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-04<BR>ratio: 0.3034<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-04<BR>ratio: 0.2694<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-04<BR>ratio: 0.5604<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-04<BR>ratio: 0.5045<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Other vision process: Metric Learning<BR>date: 2018-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-04<BR>ratio: 0.1608<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2018-04<BR>ratio: 0.6147<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-05<BR>ratio: 0.6543<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UAV-Human - Skeleton Based Action Recognition benchmarking: Average Accuracy<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-05<BR>ratio: 0.0149<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2018-05<BR>ratio: 0.6951<BR>benchmarks:<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-05<BR>ratio: 0.714<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-05<BR>ratio: 0.2306<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2018-05<BR>ratio: 0.145<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-05<BR>ratio: 0.028<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-05<BR>ratio: 0.2865<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-05<BR>ratio: 0.4536<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-05<BR>ratio: 0.6039<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  MuJoCo - Multivariate Time Series Imputation benchmarking: MSE (10^2, 50% missing)<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: mse (10^-3)<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-06<BR>ratio: 0.1822<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-06<BR>ratio: 0.3698<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: F1 score<BR>",
          "<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-06<BR>ratio: 0.7435<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (test)<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-06<BR>ratio: 0.1992<BR>benchmarks:<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-06<BR>ratio: 0.5603<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-06<BR>ratio: 0.2047<BR>benchmarks:<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-06<BR>ratio: 0.1128<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-06<BR>ratio: 0.5515<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  UT-Kinect - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-06<BR>ratio: 0.2077<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-06<BR>ratio: 0.9967<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: 3DPCK<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: MJPE<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-06<BR>ratio: 0.1235<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-06<BR>ratio: 0.4686<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2018-06<BR>ratio: 0.297<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-06<BR>ratio: 0.7711<BR>benchmarks:<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE log<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: SQ Rel<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2018-06<BR>ratio: 0.5207<BR>benchmarks:<BR>  Materials Project - Formation Energy benchmarking: MAE<BR>  QM9 - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2018-06<BR>ratio: 0.3576<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>",
          "<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  VRD - Scene Graph Generation benchmarking: Recall-at-50<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2018-06<BR>ratio: 0.3129<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>",
          "<BR>task: Action localization: Action Segmentation<BR>date: 2018-06<BR>ratio: 0.2185<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>",
          "<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking: MAP<BR>  Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking: MAP<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-07<BR>ratio: 0.3854<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2018-07<BR>ratio: 0.4436<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  ActivityNet-1.2 - Action Classification benchmarking: mAP<BR>  THUMOS’14 - Action Classification benchmarking: mAP<BR>",
          "<BR>task: Object detection: Pedestrian Detection<BR>date: 2018-07<BR>ratio: 0.5357<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Bare MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Partial MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-07<BR>ratio: 0.9856<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-07<BR>ratio: 0.4007<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>  ImageNet - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-07<BR>ratio: 0.1059<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2018-07<BR>ratio: 0.1448<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Pavia University - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2018-07<BR>ratio: 0.5<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: PSNR (sRGB)<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: SSIM (sRGB)<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2018-07<BR>ratio: 0.0631<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Other vision process: Denoising<BR>date: 2018-07<BR>ratio: 0.1158<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-07<BR>ratio: 0.7291<BR>benchmarks:<BR>  CAT 256x256 - Image Generation benchmarking: FID<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-07<BR>ratio: 0.5625<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>",
          "<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: S-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: max E-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: mean Dice<BR>",
          "<BR>task: Other vision process: Depth Completion<BR>date: 2018-08<BR>ratio: 0.4543<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>  KITTI Depth Completion - Depth Completion benchmarking: Runtime [ms]<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2018-08<BR>ratio: 0.3547<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>",
          "<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-08<BR>ratio: 0.0321<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-08<BR>ratio: 0.6807<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-08<BR>ratio: 0.6698<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  Freiburg Forest - Semantic Segmentation benchmarking: Mean IoU<BR>  ScanNetV2 - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2018-08<BR>ratio: 0.0244<BR>benchmarks:<BR>  Data3D−R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-09<BR>ratio: 0.2687<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2018-09<BR>ratio: 0.1255<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>",
          "<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-09<BR>ratio: 0.6183<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: FID<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-09<BR>ratio: 0.1346<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-09<BR>ratio: 0.8995<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.01<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-09<BR>ratio: 0.1071<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2018-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2018-09<BR>ratio: 0.086<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other vision process: Visual Dialog<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Sequential CIFAR-10 - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>",
          "<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2018-10<BR>ratio: 0.4091<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2018-10<BR>ratio: 0.0658<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-10<BR>ratio: 0.1139<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>",
          "<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2018-10<BR>ratio: 0.0079<BR>benchmarks:<BR>  FDDB - Face Detection benchmarking: AP<BR>",
          "<BR>task: Other video process: Video Frame Interpolation<BR>date: 2018-10<BR>ratio: 0.2609<BR>benchmarks:<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-10<BR>ratio: 0.2227<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2018-10<BR>ratio: 0.0225<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-10<BR>ratio: 0.0087<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Scan2CAD - 3D Reconstruction benchmarking: Average Accuracy<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-11<BR>ratio: 0.6675<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-11<BR>ratio: 0.4542<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Object detection: Video Object Detection<BR>date: 2018-11<BR>ratio: 0.6415<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-11<BR>ratio: 0.7361<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-11<BR>ratio: 0.34<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2018-11<BR>ratio: 0.0808<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2018-11<BR>ratio: 0.0423<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2018-11<BR>ratio: 0.3352<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-11<BR>ratio: 0.1431<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>",
          "<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2018-11<BR>ratio: 0.7527<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>",
          "<BR>task: Other vision process: Depth Completion<BR>date: 2018-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-11<BR>ratio: 0.2788<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-11<BR>ratio: 0.5387<BR>benchmarks:<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2018-11<BR>ratio: 0.2295<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  DensePose-COCO - Pose Estimation benchmarking: AP<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2018-11<BR>ratio: 0.7924<BR>benchmarks:<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  CUB 128 x 128 - Image Generation benchmarking: FID<BR>  CUB 128 x 128 - Image Generation benchmarking: Inception score<BR>  Stanford Cars - Image Generation benchmarking: FID<BR>  Stanford Cars - Image Generation benchmarking: Inception score<BR>  Stanford Dogs - Image Generation benchmarking: FID<BR>  Stanford Dogs - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-11<BR>ratio: 0.5789<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>",
          "<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2018-11<BR>ratio: 0.3724<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2018-11<BR>ratio: 0.3967<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-11<BR>ratio: 0.0755<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-11<BR>ratio: 0.2526<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Frames Per View<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-11<BR>ratio: 0.0286<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Video Prediction<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Video Prediction benchmarking: MAE<BR>  Human3.6M - Video Prediction benchmarking: MSE<BR>  Human3.6M - Video Prediction benchmarking: SSIM<BR>",
          "<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-12<BR>ratio: 0.7779<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>",
          "<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: OOB Rate (10^−3)<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Difference<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Length<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Step Change (10^−3)<BR>  PEMS-SF - Multivariate Time Series Imputation benchmarking: L2 Loss (10^-4)<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-12<BR>ratio: 0.1767<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2018-12<BR>ratio: 0.5<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: mAP (Val)<BR>  AVA v2.2 - Action Recognition benchmarking: mAP<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2018-12<BR>ratio: 0.5107<BR>benchmarks:<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-12<BR>ratio: 0.5811<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQth<BR>  Cityscapes val - Panoptic Segmentation benchmarking: AP<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-12<BR>ratio: 0.1675<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-12<BR>ratio: 0.027<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2018-12<BR>ratio: 0.4227<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2018-12<BR>ratio: 0.1343<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-12<BR>ratio: 0.2902<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-12<BR>ratio: 0.8571<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-12<BR>ratio: 0.6789<BR>benchmarks:<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>  NVGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-12<BR>ratio: 0.2451<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2018-12<BR>ratio: 0.2741<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2018-12<BR>ratio: 0.1735<BR>benchmarks:<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2018-12<BR>ratio: 0.4718<BR>benchmarks:<BR>  Fashion-MNIST - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: Accuracy<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-12<BR>ratio: 0.5095<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>",
          "<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2018-12<BR>ratio: 0.221<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-12<BR>ratio: 0.0597<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2018-12<BR>ratio: 0.6087<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: bpd<BR>  CelebA-HQ 1024x1024 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2018-12<BR>ratio: 0.3083<BR>benchmarks:<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-12<BR>ratio: 0.3195<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-12<BR>ratio: 0.549<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-01<BR>ratio: 0.0894<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-01<BR>ratio: 0.1641<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>",
          "<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2019-01<BR>ratio: 0.3272<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>",
          "<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  LboroHAR - Multimodal Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-01<BR>ratio: 0.4<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-01<BR>ratio: 0.0323<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-01<BR>ratio: 0.3127<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2019-01<BR>ratio: 0.1212<BR>benchmarks:<BR>  Data3D−R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D−R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  ICVL-4 - Action Recognition benchmarking: Accuracy<BR>  IRD - Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-01<BR>ratio: 0.2126<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-01<BR>ratio: 0.7353<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-01<BR>ratio: 0.3141<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-01<BR>ratio: 0.564<BR>benchmarks:<BR>  Cambridge - Hand Gesture Recognition benchmarking: Accuracy<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2019-01<BR>ratio: 0.371<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-01<BR>ratio: 0.1086<BR>benchmarks:<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2019-01<BR>ratio: 0.1323<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2019-02<BR>ratio: 0.0183<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  INRIA Holidays - Image Retrieval benchmarking: Mean mAP<BR>",
          "<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Indian Pines - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-02<BR>ratio: 0.0018<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-02<BR>ratio: 0.1<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-02<BR>ratio: 0.036<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (7 emotion)<BR>  JAFFE - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-02<BR>ratio: 0.068<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2019-02<BR>ratio: 0.6096<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>  AFLW-Full - Facial Landmark Detection benchmarking: Mean NME<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-02<BR>ratio: 0.1324<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-02<BR>ratio: 0.4587<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Other vision process: Visual Dialog<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Other vision process: Depth Completion<BR>date: 2019-02<BR>ratio: 0.069<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-03<BR>ratio: 0.098<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2019-03<BR>ratio: 0.2573<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  nuScenes - 3D Object Detection benchmarking: NDS<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  CelebA-HQ 128x128 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2019-03<BR>ratio: 0.5464<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>",
          "<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Views<BR>",
          "<BR>task: Other vision process: Domain Generalization<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ImageNet-C - Domain Generalization benchmarking: mean Corruption Error (mCE)<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2019-03<BR>ratio: 0.6399<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>  CUB-200-2011 - Image Retrieval benchmarking: R-at-1<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>",
          "<BR>task: Action localization: Action Segmentation<BR>date: 2019-03<BR>ratio: 0.3213<BR>benchmarks:<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-03<BR>ratio: 0.3109<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-03<BR>ratio: 0.1256<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Other image process: Image Reconstruction<BR>date: 2019-03<BR>ratio: 0.2896<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-03<BR>ratio: 0.4466<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mRec<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-03<BR>ratio: 0.9015<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2019-04<BR>ratio: 0.162<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>",
          "<BR>task: Other image process: Image Clustering<BR>date: 2019-04<BR>ratio: 0.3733<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>",
          "<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2019-04<BR>ratio: 0.1081<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-04<BR>ratio: 0.3059<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2019-04<BR>ratio: 0.6033<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Dense Object Detection<BR>date: 2019-04<BR>ratio: 0.1943<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-04<BR>ratio: 0.6718<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>",
          "<BR>task: Other video process: Video Frame Interpolation<BR>date: 2019-04<BR>ratio: 0.1348<BR>benchmarks:<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-04<BR>ratio: 0.2881<BR>benchmarks:<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.1<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>",
          "<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2019-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP at-0.5:0.95<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP-at-0.50<BR>",
          "<BR>task: Other image process: Color Image Denoising<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma75 - Color Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-04<BR>ratio: 0.155<BR>benchmarks:<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2019-04<BR>ratio: 0.1123<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Image generation: Pose Transfer<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: SSIM<BR>",
          "<BR>task: Other vision process: Visual Dialog<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-04<BR>ratio: 0.0663<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>",
          "<BR>task: Activity detection: Action Detection<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.1<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.2<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2019-04<BR>ratio: 0.4321<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other vision process: Object Counting<BR>date: 2019-04<BR>ratio: 0.3247<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>",
          "<BR>task: Other vision process: Curved Text Detection<BR>date: 2019-04<BR>ratio: 0.7679<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2019-04<BR>ratio: 0.3574<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Other vision process: Denoising<BR>date: 2019-04<BR>ratio: 0.0989<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-04<BR>ratio: 0.1341<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>",
          "<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-04<BR>ratio: 0.6555<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP50<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP75<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-04<BR>ratio: 0.7784<BR>benchmarks:<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: F-measure<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  ECSSD - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: MAE<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: F-measure<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: MAE<BR>  SOD - RGB Salient Object Detection benchmarking: MAE<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-04<BR>ratio: 0.6714<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-04<BR>ratio: 0.0519<BR>benchmarks:<BR>  FFHQ - Image Generation benchmarking: FID<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-04<BR>ratio: 0.339<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: H-Mean<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: H-Mean<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-04<BR>ratio: 0.0382<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>",
          "<BR>task: Other vision process: Video Prediction<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  CMU Mocap-2 - Video Prediction benchmarking: Test Error<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-05<BR>ratio: 0.0144<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-05<BR>ratio: 0.4235<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-05<BR>ratio: 0.7854<BR>benchmarks:<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-05<BR>ratio: 0.7002<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-05<BR>ratio: 0.1623<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>",
          "<BR>task: Other vision process: Depth Completion<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: iMAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: iRMSE<BR>",
          "<BR>task: Pose tracking: Pose Tracking<BR>date: 2019-05<BR>ratio: 0.0083<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-05<BR>ratio: 0.4063<BR>benchmarks:<BR>  HMDBfull-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2019-05<BR>ratio: 0.0225<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>",
          "<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Horizon Lines in the Wild - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2019-05<BR>ratio: 0.4905<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>",
          "<BR>task: Other vision process: Domain Generalization<BR>date: 2019-05<BR>ratio: 0.2258<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-05<BR>ratio: 0.7067<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>  FERPlus - Facial Expression Recognition benchmarking: Accuracy<BR>  SFEW - Facial Expression Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Formation Energy<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  OQMD v1.2 - Formation Energy benchmarking: MAE<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-06<BR>ratio: 0.2194<BR>benchmarks:<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>",
          "<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  BRATS-2015 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2017 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-06<BR>ratio: 0.0589<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-06<BR>ratio: 0.0493<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: Total Variation of Information<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Merge<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Split<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-06<BR>ratio: 0.2305<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>  iNaturalist - Image Classification benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-06<BR>ratio: 0.0932<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-06<BR>ratio: 0.0985<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-06<BR>ratio: 0.2174<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-06<BR>ratio: 0.2823<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-06<BR>ratio: 0.066<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2019-06<BR>ratio: 0.6079<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-10<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-1<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-06<BR>ratio: 0.9629<BR>benchmarks:<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2019-06<BR>ratio: 0.1399<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>",
          "<BR>task: Activity recognition: Action Classification<BR>date: 2019-06<BR>ratio: 0.4736<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>  Kinetics-600 - Action Classification benchmarking: Top-5 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-06<BR>ratio: 0.0171<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-06<BR>ratio: 0.7183<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: S-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: mean E-Measure<BR>",
          "<BR>task: Other vision process: Crowd Counting<BR>date: 2019-06<BR>ratio: 0.9021<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>  UCF CC 50 - Crowd Counting benchmarking: MAE<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-06<BR>ratio: 0.1952<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-06<BR>ratio: 0.0318<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2019-07<BR>ratio: 0.5126<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - 3D Object Detection benchmarking: AP<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-07<BR>ratio: 0.0989<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2019-07<BR>ratio: 0.2932<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>",
          "<BR>task: Object detection: Video Object Detection<BR>date: 2019-07<BR>ratio: 0.1509<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-07<BR>ratio: 0.066<BR>benchmarks:<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS’14 - Action Recognition benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-07<BR>ratio: 0.5481<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2019-07<BR>ratio: 0.2415<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>",
          "<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2019-07<BR>ratio: 0.7593<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>",
          "<BR>task: Other video process: Video Retrieval<BR>date: 2019-07<BR>ratio: 0.4898<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-07<BR>ratio: 1.0<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-07<BR>ratio: 0.2647<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>",
          "<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-07<BR>ratio: 0.835<BR>benchmarks:<BR>  VOT2016 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  VOT2017 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Unseen)<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-07<BR>ratio: 0.3796<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-07<BR>ratio: 0.6173<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-08<BR>ratio: 0.3316<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-08<BR>ratio: 0.3395<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Something-Something V1 - Action Recognition benchmarking: Top 5 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>",
          "<BR>task: Other vision process: Metric Learning<BR>date: 2019-08<BR>ratio: 0.3549<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>",
          "<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: AUC<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-08<BR>ratio: 0.1793<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-08<BR>ratio: 0.0978<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Other vision process: Crowd Counting<BR>date: 2019-08<BR>ratio: 0.1469<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>",
          "<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-08<BR>ratio: 0.0324<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>",
          "<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-08<BR>ratio: 0.7344<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking: mIoU<BR>",
          "<BR>task: Object detection: Lane Detection<BR>date: 2019-08<BR>ratio: 0.4583<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2019-08<BR>ratio: 0.6596<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>  AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>",
          "<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SUN-RGBD - Scene Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S1)<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-08<BR>ratio: 0.2284<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-08<BR>ratio: 0.3737<BR>benchmarks:<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-08<BR>ratio: 0.3<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-08<BR>ratio: 0.1982<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Image classification: Document Image Classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Noisy Bangla Characters - Document Image Classification benchmarking: Accuracy<BR>  Noisy Bangla Numeral - Document Image Classification benchmarking: Accuracy<BR>  n-MNIST - Document Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  BRATS-2013 - Brain Tumor Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-08<BR>ratio: 0.4622<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-09<BR>ratio: 0.2029<BR>benchmarks:<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>",
          "<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Other image process: Image Retrieval<BR>date: 2019-09<BR>ratio: 0.1412<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-09<BR>ratio: 0.26<BR>benchmarks:<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS’14 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>",
          "<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2019-09<BR>ratio: 0.1818<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-09<BR>ratio: 0.1053<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-09<BR>ratio: 0.0137<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-09<BR>ratio: 0.071<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-09<BR>ratio: 0.9068<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  PASCAL VOC 2007 - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-09<BR>ratio: 0.4198<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-09<BR>ratio: 0.1111<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-09<BR>ratio: 0.5071<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  USPS-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2019-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-10<BR>ratio: 0.5032<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>  SkyScapes-Lane - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-10<BR>ratio: 0.0388<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-10<BR>ratio: 0.0466<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-10<BR>ratio: 0.0205<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>",
          "<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2019-10<BR>ratio: 0.2772<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-10<BR>ratio: 0.1955<BR>benchmarks:<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-10<BR>ratio: 0.5483<BR>benchmarks:<BR>  AgeDB-30 - Face Verification benchmarking: Accuracy<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>",
          "<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2019-10<BR>ratio: 0.9715<BR>benchmarks:<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>",
          "<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-10<BR>ratio: 0.2791<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>",
          "<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2019-10<BR>ratio: 1.0<BR>benchmarks:<BR>  PA-100K - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  PETA - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  RAP - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-10<BR>ratio: 0.0533<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Image classification: Satellite Image Classification<BR>date: 2019-11<BR>ratio: 0.7974<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>  SAT-6 - Satellite Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-11<BR>ratio: 0.471<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes test - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: mIoU<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>",
          "<BR>task: Object detection: Object Detection<BR>date: 2019-11<BR>ratio: 0.0635<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APS<BR>",
          "<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Oulu-CASIA - Facial Expression Recognition benchmarking: Accuracy (10-fold)<BR>",
          "<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-11<BR>ratio: 0.0838<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>",
          "<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-11<BR>ratio: 0.3797<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-11<BR>ratio: 0.057<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-11<BR>ratio: 0.0114<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-11<BR>ratio: 0.0848<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking: AP-at-0.7<BR>",
          "<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-11<BR>ratio: 0.6248<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APL<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-11<BR>ratio: 0.2438<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-11<BR>ratio: 0.8357<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>",
          "<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-11<BR>ratio: 0.9839<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation benchmarking: Accuracy (ADD)<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-11<BR>ratio: 0.2369<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: oAcc<BR>",
          "<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-11<BR>ratio: 0.0283<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>",
          "<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-11<BR>ratio: 0.3356<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>",
          "<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-12<BR>ratio: 0.172<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>",
          "<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-12<BR>ratio: 0.04<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-12<BR>ratio: 0.0932<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>",
          "<BR>task: Activity recognition: Action Recognition<BR>date: 2019-12<BR>ratio: 0.1636<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2019-12<BR>ratio: 0.2424<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Params<BR>  ObjectNet - Image Classification benchmarking: Top 5 Accuracy<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-12<BR>ratio: 0.1413<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>",
          "<BR>task: Image generation: Image Generation<BR>date: 2019-12<BR>ratio: 0.8135<BR>benchmarks:<BR>  CIFAR-100 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Cat 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Churches 256 x 256 - Image Generation benchmarking: FID<BR>",
          "<BR>task: Other video process: Video Generation<BR>date: 2019-12<BR>ratio: 0.7122<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>",
          "<BR>task: Image classification: Image Classification<BR>date: 2020-01<BR>ratio: 0.25<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>",
          "<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2020-01<BR>ratio: 0.0444<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  SUN-RGBD - 3D Object Detection benchmarking: mAP-at-0.25<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  UPenn Action - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>",
          "<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2020-01<BR>ratio: 0.4159<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>",
          "<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2020-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>",
          "<BR>task: Other vision process: Visual Question Answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2020-02<BR>ratio: 0.4211<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-02<BR>ratio: 0.0098<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>",
          "<BR>task: Other vision process: Scene Graph Generation<BR>date: 2020-02<BR>ratio: 0.19<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>",
          "<BR>task: Pose estimation: Pose Estimation<BR>date: 2020-02<BR>ratio: 0.0152<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>",
          "<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2020-03<BR>ratio: 0.5909<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>",
          "<BR>task: Object detection: 3D Object Detection<BR>date: 2020-03<BR>ratio: 0.3135<BR>benchmarks:<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Action localization: Action Segmentation<BR>date: 2020-03<BR>ratio: 0.6966<BR>benchmarks:<BR>  50 Salads - Action Segmentation benchmarking: Acc<BR>  50 Salads - Action Segmentation benchmarking: Edit<BR>  50 Salads - Action Segmentation benchmarking: F1@10%<BR>  50 Salads - Action Segmentation benchmarking: F1@25%<BR>  50 Salads - Action Segmentation benchmarking: F1@50%<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  Breakfast - Action Segmentation benchmarking: F1@10%<BR>  Breakfast - Action Segmentation benchmarking: F1@25%<BR>  Breakfast - Action Segmentation benchmarking: F1@50%<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>",
          "<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2020-03<BR>ratio: 0.8206<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>  SceneNN - 3D Instance Segmentation benchmarking: mAP-at-0.5<BR>",
          "<BR>task: Object detection: Video Object Detection<BR>date: 2020-03<BR>ratio: 0.2075<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>",
          "<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2020-03<BR>ratio: 0.4893<BR>benchmarks:<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>",
          "<BR>task: Other video process: Video Frame Interpolation<BR>date: 2020-03<BR>ratio: 0.8021<BR>benchmarks:<BR>  UCF101 - Video Frame Interpolation benchmarking: PSNR<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>",
          "<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2020-04<BR>ratio: 0.4197<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  NYU Depth v2 - Semantic Segmentation benchmarking: Mean IoU<BR>",
          "<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2020-04<BR>ratio: 0.0204<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>",
          "<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-04<BR>ratio: 0.4396<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "color": [
           0.5546,
           0.0189,
           0.1073,
           1,
           1,
           0.0397,
           0.0553,
           0.021,
           0.8122,
           0.4737,
           0.289,
           0.1446,
           0.9752,
           0.2834,
           1,
           0.2375,
           0.2513,
           0.2406,
           0.1392,
           0.0877,
           0.8858,
           0.0451,
           0.3958,
           0.0134,
           0.3753,
           0.2327,
           0.0741,
           0.426,
           0.1007,
           0.2878,
           0.8426,
           0.3991,
           0.2321,
           0.1675,
           0.1142,
           0.4459,
           0.2457,
           0.4321,
           0.5,
           0.117,
           0.4701,
           0.0854,
           1,
           0.1888,
           0.3222,
           0.0331,
           0.15,
           0.1785,
           0.4339,
           0.1637,
           0.0779,
           0.7477,
           0.4051,
           1,
           0.9509,
           0.5016,
           1,
           0.0707,
           0.6524,
           0.0478,
           0.056,
           0.2757,
           0.0258,
           1,
           0.3359,
           0.2119,
           0.4252,
           0.5954,
           0.7322,
           0.1471,
           0.4868,
           0.4832,
           0.4271,
           0.6898,
           0.0092,
           0.8889,
           0.3005,
           0.0418,
           0.1069,
           0.2762,
           0.072,
           0.2999,
           0.0741,
           0.572,
           0.3146,
           0.061,
           0.3235,
           0.3625,
           0.0241,
           0.8371,
           1,
           0.1233,
           0.2999,
           0.6547,
           1,
           0.0251,
           0.2523,
           0.5511,
           0.1736,
           0.0915,
           0.0144,
           0.3082,
           0.4115,
           0.2784,
           0.0093,
           0.0142,
           0.0606,
           0.0684,
           0.3326,
           0.3571,
           0,
           0.2824,
           0.2492,
           0.2441,
           0.096,
           0.3399,
           0.2329,
           0.2215,
           0.5084,
           0.6006,
           0.674,
           0.2304,
           0.6019,
           0.3137,
           0.1022,
           0.5259,
           0.4898,
           0.1001,
           0.5647,
           0.1125,
           0.1929,
           0.0388,
           0.1288,
           0.1429,
           0.9756,
           0.1328,
           0.2025,
           0.6995,
           0.0692,
           0.0541,
           0.9524,
           0.5645,
           0.1574,
           0.3612,
           0.1565,
           0.2235,
           0.0303,
           0.6528,
           0.5668,
           0.4081,
           0.2484,
           0.1096,
           0.2099,
           0.2205,
           0.1193,
           0.5877,
           0.2105,
           0.0331,
           0.4378,
           0.2112,
           1,
           0.2041,
           0.1026,
           0.3846,
           0.7172,
           0.103,
           0.1968,
           0.1629,
           0.2194,
           0.1332,
           0.8514,
           0.1767,
           0.4855,
           1,
           0.4565,
           0.1679,
           0.3172,
           0.0758,
           0.4474,
           0.4983,
           0.528,
           0.2366,
           0.7853,
           0.4284,
           1,
           0.8352,
           0.2561,
           0.0559,
           0.0607,
           0.2867,
           0.6277,
           0.2514,
           0.6175,
           0.3412,
           0.1244,
           0.4844,
           0.4142,
           0.5909,
           0.4898,
           0.6641,
           0.1161,
           0.7104,
           1,
           0.02,
           0.6614,
           0.0095,
           0.0387,
           0.4581,
           0.2874,
           0.1758,
           1,
           0.3268,
           0.6997,
           0.0363,
           0.0645,
           0.2688,
           0.5913,
           0.0083,
           1,
           0.219,
           0.0624,
           0.88,
           0.3456,
           0.0951,
           0.1108,
           0.4921,
           0.6932,
           0.5449,
           0.8514,
           0.0551,
           0.1598,
           0.474,
           0.3904,
           0.0103,
           1,
           1,
           0.0758,
           0.0809,
           1,
           0.6429,
           1,
           0.0909,
           0.9952,
           0.4958,
           0.2127,
           0.9847,
           0.4337,
           0.2492,
           0.2941,
           0.7097,
           0.5297,
           0.656,
           0.2656,
           0.6273,
           0.435,
           0.2438,
           0.619,
           0.5452,
           0.1491,
           0.6998,
           0.789,
           0.1976,
           0.5062,
           0.6503,
           0.3521,
           0.1702,
           0.3953,
           0.069,
           0.2578,
           0.383,
           0.2407,
           0.6857,
           0.0238,
           0.0546,
           0.0332,
           0.046,
           0.1898,
           1,
           0.5028,
           1,
           0.2406,
           0.5,
           0.1097,
           0.5417,
           0.0083,
           0.613,
           0.3524,
           0.3532,
           0.0247,
           0.7094,
           0.0583,
           0.5145,
           0.4342,
           0.3197,
           0.0837,
           0.3661,
           0.3059,
           0.4232,
           0.6486,
           0.2233,
           0.3333,
           0.9695,
           0.9199,
           0.6229,
           0.1321,
           0.0351,
           0.628,
           0.4938,
           0.2543,
           0.0865,
           0.3996,
           0.2075,
           0.3005,
           0.6875,
           1,
           0.2155,
           0.5545,
           0.387,
           0.1771,
           0.1202,
           0.6418,
           0.1351,
           0.1099,
           0.2131,
           0.2249,
           0.0083,
           0.1587,
           0.0929,
           1,
           0.5083,
           0.0203,
           0.4588,
           0.3293,
           1,
           1,
           0.2555,
           0.1184,
           0.0985,
           0.5245,
           0.0647,
           1,
           0.1925,
           0.0067,
           0.3804,
           0.6801,
           1,
           0.211,
           0.1429,
           0.3034,
           0.2694,
           0.5604,
           0.5045,
           0.6667,
           0.1608,
           0.6147,
           1,
           0.6543,
           0.0149,
           0.6951,
           0.714,
           0.2306,
           0.145,
           0.028,
           0.2865,
           0.4536,
           0.6039,
           1,
           0.1822,
           0.3698,
           1,
           0.7435,
           0.1992,
           0.5603,
           0.2047,
           0.1128,
           0.5515,
           0.2077,
           0.9967,
           0.1235,
           0.4686,
           0.297,
           0.7711,
           0.7567,
           0.5207,
           0.3576,
           1,
           0.3129,
           0.2185,
           1,
           0.3854,
           0.4436,
           1,
           0.5357,
           0.9856,
           0.4007,
           0.1059,
           0.1448,
           1,
           0.5,
           0.0631,
           0.1158,
           0.7291,
           0.5625,
           1,
           0.4543,
           0.3547,
           0.0321,
           0.6807,
           0.6698,
           0.0244,
           0.2687,
           0.1255,
           0.6183,
           0.1346,
           0.8995,
           0.1071,
           0.0378,
           0.086,
           0.1442,
           1,
           0.4091,
           0.0658,
           0.1139,
           0.0079,
           0.2609,
           0.2227,
           0.0225,
           0.0087,
           0.2577,
           1,
           0.6675,
           0.4542,
           0.6415,
           0.7361,
           0.34,
           0.5661,
           0.0808,
           0.0423,
           0.3352,
           0.1431,
           0.7527,
           0.0684,
           0.2788,
           0.5387,
           0.2295,
           1,
           0.7924,
           1,
           0.5789,
           0.3724,
           0.3967,
           0.0755,
           0.2526,
           1,
           0.0286,
           1,
           0.7779,
           1,
           0.1767,
           0.5,
           0.5107,
           0.5811,
           0.1675,
           0.027,
           0.4227,
           0.1343,
           0.2902,
           0.8571,
           0.6789,
           0.2451,
           0.2741,
           1,
           0.1735,
           0.4718,
           0.5095,
           0.221,
           0.0597,
           0.6087,
           0.3083,
           0.3195,
           0.549,
           0.0894,
           0.1641,
           0.3272,
           1,
           0.4,
           0.0323,
           0.3127,
           0.1212,
           1,
           0.2126,
           0.7353,
           0.0882,
           0.3141,
           0.564,
           0.371,
           0.1086,
           0.1323,
           0.0183,
           1,
           1,
           0.0018,
           0.1,
           0.036,
           1,
           0.068,
           0.6096,
           0.1324,
           0.4587,
           0.1426,
           0.069,
           0.098,
           0.2573,
           1,
           0.5464,
           1,
           1,
           0.6399,
           0.3213,
           0.3109,
           0.1256,
           0.2896,
           0.4466,
           1,
           0.9015,
           0.162,
           0.3733,
           0.1081,
           0.3059,
           0.6033,
           0.1943,
           0.6718,
           0.1348,
           0.2881,
           0.6667,
           1,
           0.155,
           1,
           0.1123,
           0.3147,
           1,
           0.4213,
           0.0663,
           1,
           0.4321,
           0.3247,
           0.7679,
           0.3574,
           0.0989,
           0.1341,
           0.6555,
           0.7784,
           0.6714,
           0.0519,
           0.339,
           0.0382,
           1,
           0.0144,
           0.4235,
           0.7854,
           0.7002,
           0.1623,
           1,
           0.0083,
           0.4063,
           0.0225,
           1,
           0.4905,
           0.2258,
           0.7067,
           1,
           0.2194,
           1,
           0.0589,
           0.0493,
           1,
           0.2305,
           0.0932,
           0.0985,
           0.2174,
           0.2823,
           0.066,
           0.6079,
           0.9629,
           0.1399,
           0.4736,
           0.0171,
           0.7183,
           0.9021,
           0.1952,
           0.0318,
           0.5126,
           0.0989,
           0.2932,
           0.1509,
           0.066,
           0.5481,
           0.2415,
           0.7593,
           0.4898,
           1,
           0.2647,
           0.835,
           0.3796,
           0.6173,
           0.3316,
           0.3395,
           0.3549,
           1,
           0.1793,
           0.0978,
           0.1469,
           0.0324,
           0.7344,
           1,
           0.4583,
           0.3309,
           0.6596,
           1,
           1,
           0.2284,
           0.3737,
           0.3,
           0.1982,
           1,
           1,
           0.4622,
           0.2029,
           0.046,
           0.1412,
           0.26,
           0.1818,
           0.1053,
           0.0137,
           0.071,
           0.9068,
           1,
           0.4198,
           0.1111,
           0.5071,
           0.0378,
           0.5032,
           0.0388,
           0.0466,
           0.0205,
           0.2772,
           0.1955,
           0.5483,
           0.9715,
           0.2791,
           1,
           0.0533,
           0.7974,
           0.471,
           0.0635,
           1,
           0.0838,
           0.3797,
           0.057,
           0.0114,
           0.0848,
           1,
           0.6248,
           0.2438,
           0.8357,
           0.9839,
           0.2369,
           0.0283,
           0.3356,
           0.172,
           0.04,
           0.0932,
           0.1636,
           0.2424,
           0.1413,
           0.8135,
           0.7122,
           0.25,
           0.0444,
           1,
           1,
           0.4159,
           0.2233,
           0.5266,
           0.4211,
           0.0098,
           0.19,
           0.0152,
           0.5909,
           0.3135,
           0.6966,
           0.8206,
           0.2075,
           0.4893,
           0.8021,
           0.4197,
           0.0204,
           0.4396
          ],
          "colorbar": {
           "len": 500,
           "lenmode": "pixels",
           "thickness": 10,
           "title": {
            "text": "ratio"
           }
          },
          "colorscale": [
           [
            0,
            "rgb(255,255,229)"
           ],
           [
            0.125,
            "rgb(247,252,185)"
           ],
           [
            0.25,
            "rgb(217,240,163)"
           ],
           [
            0.375,
            "rgb(173,221,142)"
           ],
           [
            0.5,
            "rgb(120,198,121)"
           ],
           [
            0.625,
            "rgb(65,171,93)"
           ],
           [
            0.75,
            "rgb(35,132,67)"
           ],
           [
            0.875,
            "rgb(0,104,55)"
           ],
           [
            1,
            "rgb(0,69,41)"
           ]
          ],
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "showscale": true,
          "size": 19,
          "symbol": "circle"
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2012-08",
          "2012-12",
          "2013-02",
          "2013-02",
          "2013-07",
          "2013-11",
          "2013-12",
          "2014-04",
          "2014-06",
          "2014-06",
          "2014-06",
          "2014-09",
          "2014-09",
          "2014-11",
          "2014-12",
          "2014-12",
          "2014-12",
          "2014-12",
          "2014-12",
          "2015-02",
          "2015-02",
          "2015-02",
          "2015-02",
          "2015-02",
          "2015-03",
          "2015-03",
          "2015-03",
          "2015-04",
          "2015-04",
          "2015-04",
          "2015-05",
          "2015-05",
          "2015-05",
          "2015-05",
          "2015-05",
          "2015-06",
          "2015-06",
          "2015-08",
          "2015-09",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-12",
          "2015-12",
          "2015-12",
          "2015-12",
          "2015-12",
          "2015-12",
          "2016-01",
          "2016-01",
          "2016-01",
          "2016-01",
          "2016-02",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-04",
          "2016-04",
          "2016-04",
          "2016-04",
          "2016-04",
          "2016-04",
          "2016-04",
          "2016-05",
          "2016-05",
          "2016-05",
          "2016-05",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-07",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-10",
          "2016-10",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2017-01",
          "2017-01",
          "2017-02",
          "2017-02",
          "2017-02",
          "2017-02",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2020-01",
          "2020-01",
          "2020-01",
          "2020-01",
          "2020-01",
          "2020-01",
          "2020-02",
          "2020-02",
          "2020-02",
          "2020-02",
          "2020-02",
          "2020-03",
          "2020-03",
          "2020-03",
          "2020-03",
          "2020-03",
          "2020-03",
          "2020-03",
          "2020-04",
          "2020-04",
          "2020-04"
         ],
         "y": [
          "Other image process: Image Clustering",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Activity recognition: Skeleton Based Action Recognition",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Activity recognition: Action Recognition",
          "Facial recognition and modelling: Face Verification",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Other vision process: Domain Adaptation",
          "Semantic segmentation: Semantic Segmentation",
          "Object detection: Pedestrian Detection",
          "Activity recognition: Action Recognition",
          "Semantic segmentation: Semantic Segmentation",
          "Facial recognition and modelling: Face Verification",
          "Image classification: Image Classification",
          "Semantic segmentation: Semantic Segmentation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Facial recognition and modelling: Face Verification",
          "Other vision process: Domain Adaptation",
          "Image classification: Image Classification",
          "Activity recognition: Action Recognition",
          "Facial recognition and modelling: Face Verification",
          "Semantic segmentation: Semantic Segmentation",
          "Other image process: Image Retrieval",
          "Other vision process: Scene Text Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Multi-tissue Nucleus Segmentation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Curved Text Detection",
          "Activity recognition: Action Recognition",
          "Other vision process: Unsupervised Domain Adaptation",
          "Image classification: Image Classification",
          "Object detection: Object Detection",
          "Facial recognition and modelling: Face Verification",
          "Semantic segmentation: Semantic Segmentation",
          "Facial recognition and modelling: Face Identification",
          "Image classification: Sequential Image Classification",
          "Facial recognition and modelling: Face Verification",
          "Semantic segmentation: Medical Image Segmentation",
          "Other image process: Image Retrieval",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Pose estimation: Pose Estimation",
          "Other image process: Image Clustering",
          "Semantic segmentation: Semantic Segmentation",
          "Object detection: Weakly Supervised Object Detection",
          "Image classification: Image Classification",
          "Image classification: Image Classification",
          "Other vision process: Crowd Counting",
          "Image classification: Satellite Image Classification",
          "Activity recognition: Action Recognition",
          "Image classification: Retinal OCT Disease Classification",
          "Object detection: Object Detection",
          "Image generation: Image Generation",
          "Activity recognition: Action Recognition",
          "Pose estimation: Pose Estimation",
          "Action localization: Temporal Action Localization",
          "Image classification: Image Classification",
          "Other vision process: Visual Question Answering",
          "Image classification: Image Classification",
          "Activity recognition: Multimodal Activity Recognition",
          "Object detection: Weakly Supervised Object Detection",
          "Facial recognition and modelling: Face Verification",
          "Image classification: Sequential Image Classification",
          "Facial recognition and modelling: Face Detection",
          "Pose estimation: Pose Estimation",
          "Semantic segmentation: Semantic Segmentation",
          "Other image process: Aesthetics Quality Assessment",
          "Other image process: Image Retrieval",
          "Activity recognition: Skeleton Based Action Recognition",
          "Other image process: Image Clustering",
          "Activity recognition: Action Recognition",
          "Other vision process: Object Counting",
          "Other vision process: Scene Text Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Image classification: Image Classification",
          "Other vision process: Visual Question Answering",
          "Other vision process: Domain Adaptation",
          "Other vision process: Visual Question Answering",
          "Semantic segmentation: Semantic Segmentation",
          "Image generation: Conditional Image Generation",
          "Object detection: RGB Salient Object Detection",
          "Activity recognition: Skeleton Based Action Recognition",
          "Object detection: 3D Object Detection",
          "Image generation: Image Generation",
          "Other vision process: Monocular Depth Estimation",
          "Activity recognition: Multimodal Activity Recognition",
          "Other vision process: Horizon Line Estimation",
          "Pose estimation: Keypoint Detection",
          "Object detection: Birds Eye View Object Detection",
          "Other vision process: Domain Adaptation",
          "Object recognition: Pedestrian Attribute Recognition",
          "Image classification: Image Classification",
          "Other vision process: Crowd Counting",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Image Retrieval",
          "Activity recognition: Action Recognition",
          "Object detection: Weakly Supervised Object Detection",
          "Activity recognition: Skeleton Based Action Recognition",
          "Other vision process: Object Counting",
          "Action localization: Temporal Action Localization",
          "Pose estimation: Pose Estimation",
          "Image classification: Image Classification",
          "Image generation: Conditional Image Generation",
          "Image classification: Image Classification",
          "Semantic segmentation: Nuclear Segmentation",
          "Action localization: Action Segmentation",
          "Object detection: RGB Salient Object Detection",
          "Other vision process: Metric Learning",
          "Other video process: Video Generation",
          "Other image process: Image Retrieval",
          "Object tracking: Visual Object Tracking",
          "Activity recognition: Skeleton Based Action Recognition",
          "Semantic segmentation: Instance Segmentation",
          "Object detection: Weakly Supervised Object Detection",
          "Other vision process: Visual Question Answering",
          "Object detection: Birds Eye View Object Detection",
          "Pose estimation: Keypoint Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Pose estimation: Keypoint Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Other video process: Video Retrieval",
          "Pose estimation: Pose Estimation",
          "Semantic segmentation: Video Semantic Segmentation",
          "Other image process: Image Retrieval",
          "Other 3D task: 3D Object Reconstruction",
          "Object detection: Dense Object Detection",
          "Semantic segmentation: 3D Part Segmentation",
          "Object detection: Object Detection",
          "Image generation: Conditional Image Generation",
          "Activity recognition: Action Classification",
          "Other 3D task: 3D Reconstruction",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Pose Estimation",
          "Image classification: Unsupervised Image Classification",
          "Pose estimation: Pose Estimation",
          "Image generation: Image Generation",
          "Object detection: Pedestrian Detection",
          "Other image process: Image Clustering",
          "Semantic segmentation: Multi-tissue Nucleus Segmentation",
          "Other vision process: Scene Text Detection",
          "Object detection: Object Detection",
          "Pose estimation: Keypoint Detection",
          "Image generation: Conditional Image Generation",
          "Semantic segmentation: Nuclear Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Image generation: Image Generation",
          "Action localization: Temporal Action Localization",
          "Activity recognition: Action Recognition",
          "Activity detection: Action Detection",
          "Facial recognition and modelling: Face Verification",
          "Activity recognition: Skeleton Based Action Recognition",
          "Other vision process: Scene Text Detection",
          "Pose estimation: Pose Estimation",
          "Other image process: Image Clustering",
          "Semantic segmentation: Semantic Segmentation",
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
          "Semantic segmentation: Instance Segmentation",
          "Other vision process: Visual Question Answering",
          "Other vision process: Formation Energy",
          "Other vision process: Domain Adaptation",
          "Object tracking: Visual Object Tracking",
          "Other vision process: Monocular Depth Estimation",
          "Activity recognition: Multimodal Activity Recognition",
          "Object detection: Weakly Supervised Object Detection",
          "Facial recognition and modelling: Face Identification",
          "Other 3D task: 3D Point Cloud Classification",
          "Activity recognition: Skeleton Based Action Recognition",
          "Image classification: Document Image Classification",
          "Other image process: Color Image Denoising",
          "Other image process: Grayscale Image Denoising",
          "Facial recognition and modelling: Face Verification",
          "Activity recognition: Action Recognition",
          "Object detection: RGB Salient Object Detection",
          "Other image process: Aesthetics Quality Assessment",
          "Activity recognition: Action Classification",
          "Other vision process: Domain Adaptation",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Other vision process: Denoising",
          "Other vision process: Visual Question Answering",
          "Pose estimation: 3D Human Pose Estimation",
          "Gesture recognition: Hand Gesture Recognition",
          "Action localization: Temporal Action Localization",
          "Activity recognition: Action Recognition",
          "Pose estimation: Pose Estimation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Human Interaction Recognition",
          "Image generation: Image Generation",
          "Object tracking: Visual Object Tracking",
          "Other vision process: Metric Learning",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Other 3D task: 3D Point Cloud Classification",
          "Other vision process: Formation Energy",
          "Semantic segmentation: 3D Part Segmentation",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Other image process: Image Reconstruction",
          "Object detection: Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Pose estimation: Pose Estimation",
          "Object detection: RGB Salient Object Detection",
          "Other video process: Video Generation",
          "Other video process: Video Retrieval",
          "Other vision process: Visual Question Answering",
          "Other vision process: Object Counting",
          "Pose estimation: Hand Pose Estimation",
          "Other vision process: Scene Text Detection",
          "Facial recognition and modelling: Face Alignment",
          "Image classification: Image Classification",
          "Other vision process: Domain Generalization",
          "Facial recognition and modelling: Face Detection",
          "Activity recognition: Skeleton Based Action Recognition",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Hand Pose Estimation",
          "Image classification: Image Classification",
          "Object detection: Object Detection",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Other vision process: Visual Question Answering",
          "Object detection: Weakly Supervised Object Detection",
          "Other vision process: Scene Text Detection",
          "Semantic segmentation: Human Part Segmentation",
          "Object detection: Dense Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Other image process: Image Clustering",
          "Facial recognition and modelling: Face Detection",
          "Other vision process: Formation Energy",
          "Other vision process: Scene Text Detection",
          "Other vision process: Visual Dialog",
          "Image generation: Image Generation",
          "Pose estimation: 3D Human Pose Estimation",
          "Semantic segmentation: Pancreas Segmentation",
          "Image generation: Conditional Image Generation",
          "Image classification: Image Classification",
          "Other image process: Color Image Denoising",
          "Object tracking: Visual Object Tracking",
          "Object detection: Lane Detection",
          "Image classification: Sequential Image Classification",
          "Object detection: RGB Salient Object Detection",
          "Image classification: Image Classification",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Facial recognition and modelling: Face Verification",
          "Other image process: Image Clustering",
          "Semantic segmentation: Semantic Segmentation",
          "Other image process: Grayscale Image Denoising",
          "Other vision process: Domain Generalization",
          "Image generation: Image Generation",
          "Pose estimation: Head Pose Estimation",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Facial recognition and modelling: Face Alignment",
          "Activity recognition: Action Recognition",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: 3D Object Detection",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Object detection: Birds Eye View Object Detection",
          "Other 3D task: 3D Shape Classification",
          "Object detection: Object Detection",
          "Semantic segmentation: Skin Cancer Segmentation",
          "Activity recognition: Action Classification",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Semantic segmentation: 3D Part Segmentation",
          "Pose estimation: Pose Estimation",
          "Other vision process: Domain Adaptation",
          "Other vision process: Visual Dialog",
          "Pose estimation: Keypoint Detection",
          "Image-to-image translation: Fundus to Angiography Generation",
          "Semantic segmentation: Instance Segmentation",
          "Activity recognition: Action Classification",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Other image process: Image Retrieval",
          "Pose tracking: Pose Tracking",
          "Pose estimation: Pose Estimation",
          "Activity detection: Action Detection",
          "Pose estimation: Hand Pose Estimation",
          "Activity localization: Weakly Supervised Action Localization",
          "Image generation: Pose Transfer",
          "Object detection: 3D Object Detection",
          "Object detection: Lane Detection",
          "Image classification: Image Classification",
          "Other vision process: Domain Adaptation",
          "Object detection: Object Detection",
          "Activity recognition: Action Recognition",
          "Facial recognition and modelling: Face Verification",
          "Object detection: Birds Eye View Object Detection",
          "Other 3D task: 3D Point Cloud Classification",
          "Image classification: Document Image Classification",
          "Activity recognition: Skeleton Based Action Recognition",
          "Other image process: Image Retrieval",
          "Activity recognition: Action Recognition",
          "Image classification: Retinal OCT Disease Classification",
          "Other vision process: Scene Text Detection",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Face Identification",
          "Semantic segmentation: 3D Part Segmentation",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Object tracking: Multiple Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object detection: Weakly Supervised Object Detection",
          "Image classification: Image Classification",
          "Other 3D task: 3D Object Reconstruction",
          "Semantic segmentation: Skin Cancer Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Image generation: Conditional Image Generation",
          "Other vision process: Scene Text Detection",
          "Object detection: 3D Object Detection",
          "Image classification: Unsupervised Image Classification",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Other image process: Color Image Denoising",
          "Pose tracking: Pose Tracking",
          "Image generation: Image Generation",
          "Activity detection: Action Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Object tracking: Visual Object Tracking",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Other 3D task: 3D Point Cloud Classification",
          "Other image process: Image Retrieval",
          "Semantic segmentation: Instance Segmentation",
          "Other vision process: Monocular Depth Estimation",
          "Pose estimation: Pose Estimation",
          "Other vision process: Visual Question Answering",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Semantic segmentation: Scene Segmentation",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Image generation: Image Generation",
          "Object detection: Weakly Supervised Object Detection",
          "Facial recognition and modelling: Face Identification",
          "Object detection: Object Detection",
          "Image classification: Sequential Image Classification",
          "Other vision process: Scene Text Detection",
          "Activity recognition: Skeleton Based Action Recognition",
          "Pose estimation: Pose Estimation",
          "Semantic segmentation: Pancreas Segmentation",
          "Pose estimation: Keypoint Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Action localization: Temporal Action Localization",
          "Pose tracking: Pose Tracking",
          "Gesture recognition: Hand Gesture Recognition",
          "Other 3D task: 3D Shape Classification",
          "Other image process: Image Retrieval",
          "Semantic segmentation: Semantic Segmentation",
          "Facial recognition and modelling: Face Alignment",
          "Pose estimation: 3D Human Pose Estimation",
          "Other image process: Image Clustering",
          "Other vision process: Metric Learning",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Other video process: Video Retrieval",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Activity recognition: Skeleton Based Action Recognition",
          "Other vision process: Visual Question Answering",
          "Object detection: Object Detection",
          "Facial recognition and modelling: Face Alignment",
          "Semantic segmentation: Human Part Segmentation",
          "Image classification: Image Classification",
          "Pose estimation: Pose Estimation",
          "Image generation: Conditional Image Generation",
          "Other vision process: Monocular Depth Estimation",
          "Other image process: Grayscale Image Denoising",
          "Other vision process: Multivariate Time Series Imputation",
          "Object detection: Weakly Supervised Object Detection",
          "Other vision process: Scene Text Detection",
          "Object detection: Lane Detection",
          "Activity localization: Temporal Action Proposal Generation",
          "Activity recognition: Action Recognition",
          "Action localization: Temporal Action Localization",
          "Other image process: Grayscale Image Denoising",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Activity recognition: Skeleton Based Action Recognition",
          "Object tracking: Visual Object Tracking",
          "Pose estimation: 3D Human Pose Estimation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Activity recognition: Action Classification",
          "Other vision process: Monocular Depth Estimation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Other vision process: Formation Energy",
          "Object detection: RGB Salient Object Detection",
          "Other vision process: Scene Graph Generation",
          "Other video process: Video Retrieval",
          "Action localization: Action Segmentation",
          "Object recognition: Traffic Sign Recognition",
          "Other image process: Image Clustering",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity recognition: Action Classification",
          "Object detection: Pedestrian Detection",
          "Activity recognition: Action Recognition",
          "Object detection: Weakly Supervised Object Detection",
          "Other vision process: Scene Text Detection",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Image classification: Hyperspectral Image Classification",
          "Other image process: Color Image Denoising",
          "Image classification: Image Classification",
          "Other vision process: Denoising",
          "Image generation: Image Generation",
          "Other vision process: Domain Adaptation",
          "Semantic segmentation: Medical Image Segmentation",
          "Other vision process: Depth Completion",
          "Other video process: Video Retrieval",
          "Other vision process: Scene Graph Generation",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Other 3D task: 3D Reconstruction",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Nuclear Segmentation",
          "Image generation: Conditional Image Generation",
          "Image generation: Image Generation",
          "Facial recognition and modelling: Face Verification",
          "Semantic segmentation: Semantic Segmentation",
          "Facial recognition and modelling: Face Detection",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Other vision process: Visual Dialog",
          "Image classification: Sequential Image Classification",
          "Semantic segmentation: Lesion Segmentation",
          "Other image process: Aesthetics Quality Assessment",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Facial recognition and modelling: Face Detection",
          "Other video process: Video Frame Interpolation",
          "Other image process: Image Clustering",
          "Activity recognition: Action Classification",
          "Activity recognition: Action Recognition",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Other 3D task: 3D Reconstruction",
          "Other vision process: Unsupervised Domain Adaptation",
          "Semantic segmentation: Human Part Segmentation",
          "Object detection: Video Object Detection",
          "Object tracking: Visual Object Tracking",
          "Other vision process: Domain Adaptation",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Image classification: Image Classification",
          "Object tracking: Multiple Object Tracking",
          "Other image process: Image Retrieval",
          "Activity localization: Temporal Action Proposal Generation",
          "Other 3D task: 3D Room Layouts From A Single RGB Panorama",
          "Other vision process: Depth Completion",
          "Activity recognition: Action Recognition",
          "Other image process: Image Clustering",
          "Object detection: Object Detection",
          "Pose estimation: Pose Estimation",
          "Activity recognition: Action Classification",
          "Image generation: Image Generation",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Human Interaction Recognition",
          "Activity recognition: Group Activity Recognition",
          "Object detection: Weakly Supervised Object Detection",
          "Other vision process: Scene Text Detection",
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Other vision process: Video Prediction",
          "Other vision process: Scene Graph Generation",
          "Other vision process: Multivariate Time Series Imputation",
          "Other vision process: Monocular Depth Estimation",
          "Activity recognition: Action Recognition",
          "Semantic segmentation: 3D Semantic Instance Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Object tracking: Visual Object Tracking",
          "Other 3D task: 3D Point Cloud Classification",
          "Object detection: Object Detection",
          "Activity recognition: Action Classification",
          "Facial recognition and modelling: Face Verification",
          "Activity recognition: Skeleton Based Action Recognition",
          "Gesture recognition: Hand Gesture Recognition",
          "Semantic segmentation: Semantic Segmentation",
          "Pose estimation: Keypoint Detection",
          "Other vision process: Unsupervised Domain Adaptation",
          "Object detection: 3D Object Detection",
          "Other image process: Image Clustering",
          "Activity recognition: Egocentric Activity Recognition",
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Image generation: Image Generation",
          "Object detection: Birds Eye View Object Detection",
          "Facial recognition and modelling: Face Identification",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Head Pose Estimation",
          "Activity recognition: Multimodal Activity Recognition",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation",
          "Other vision process: Domain Adaptation",
          "Other 3D task: 3D Object Reconstruction",
          "Activity recognition: Action Recognition",
          "Semantic segmentation: Panoptic Segmentation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Object detection: Object Detection",
          "Image classification: Image Classification",
          "Gesture recognition: Hand Gesture Recognition",
          "Other 3D task: 3D Room Layouts From A Single RGB Panorama",
          "Semantic segmentation: Semantic Segmentation",
          "Other image process: Image Clustering",
          "Pose tracking: Pose Tracking",
          "Other image process: Image Retrieval",
          "Image classification: Hyperspectral Image Classification",
          "Facial recognition and modelling: Face Alignment",
          "Semantic segmentation: Instance Segmentation",
          "Pose estimation: Keypoint Detection",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Pose estimation: Pose Estimation",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Other vision process: Visual Question Answering",
          "Other vision process: Visual Dialog",
          "Other vision process: Depth Completion",
          "Semantic segmentation: Semantic Segmentation",
          "Object detection: 3D Object Detection",
          "Image generation: Image Generation",
          "Other vision process: Monocular Depth Estimation",
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
          "Other vision process: Domain Generalization",
          "Other image process: Image Retrieval",
          "Action localization: Action Segmentation",
          "Other vision process: Scene Text Detection",
          "Facial recognition and modelling: Face Verification",
          "Other image process: Image Reconstruction",
          "Other vision process: Domain Adaptation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Action localization: Temporal Action Localization",
          "Activity recognition: Action Classification",
          "Other image process: Image Clustering",
          "Other 3D task: 3D Point Cloud Classification",
          "Image classification: Image Classification",
          "Activity recognition: Group Activity Recognition",
          "Object detection: Dense Object Detection",
          "Facial recognition and modelling: Face Verification",
          "Other video process: Video Frame Interpolation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Object recognition: Traffic Sign Recognition",
          "Other image process: Color Image Denoising",
          "Action localization: Temporal Action Localization",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Semantic segmentation: 3D Part Segmentation",
          "Other vision process: Visual Question Answering",
          "Image generation: Pose Transfer",
          "Other vision process: Visual Dialog",
          "Activity recognition: Action Recognition",
          "Activity detection: Action Detection",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Other vision process: Object Counting",
          "Other vision process: Curved Text Detection",
          "Object detection: 3D Object Detection",
          "Other vision process: Denoising",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Real-time Instance Segmentation",
          "Object detection: RGB Salient Object Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Image generation: Image Generation",
          "Other vision process: Scene Text Detection",
          "Facial recognition and modelling: Face Alignment",
          "Other vision process: Video Prediction",
          "Activity recognition: Action Recognition",
          "Other vision process: Visual Question Answering",
          "Semantic segmentation: Panoptic Segmentation",
          "Activity localization: Weakly Supervised Action Localization",
          "Image classification: Image Classification",
          "Other vision process: Depth Completion",
          "Pose tracking: Pose Tracking",
          "Other vision process: Domain Adaptation",
          "Activity recognition: Action Classification",
          "Other vision process: Horizon Line Estimation",
          "Activity recognition: Egocentric Activity Recognition",
          "Other vision process: Domain Generalization",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Other vision process: Formation Energy",
          "Semantic segmentation: Instance Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Activity localization: Weakly Supervised Action Localization",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Electron Microscopy Image Segmentation",
          "Image classification: Image Classification",
          "Facial recognition and modelling: Face Alignment",
          "Action localization: Temporal Action Localization",
          "Semantic segmentation: 3D Instance Segmentation",
          "Object tracking: Visual Object Tracking",
          "Other vision process: Visual Question Answering",
          "Other video process: Video Retrieval",
          "Other vision process: Domain Adaptation",
          "Pose estimation: Head Pose Estimation",
          "Activity recognition: Action Classification",
          "Activity recognition: Skeleton Based Action Recognition",
          "Object detection: RGB Salient Object Detection",
          "Other vision process: Crowd Counting",
          "Object detection: Object Detection",
          "Activity recognition: Action Recognition",
          "Object detection: 3D Object Detection",
          "Image generation: Image Generation",
          "Other vision process: Monocular Depth Estimation",
          "Object detection: Video Object Detection",
          "Activity recognition: Action Recognition",
          "Other vision process: Visual Question Answering",
          "Activity localization: Temporal Action Proposal Generation",
          "Image-to-image translation: Fundus to Angiography Generation",
          "Other video process: Video Retrieval",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Object tracking: Visual Object Tracking",
          "Action localization: Temporal Action Localization",
          "Object detection: Birds Eye View Object Detection",
          "Image classification: Image Classification",
          "Activity recognition: Action Recognition",
          "Other vision process: Metric Learning",
          "Semantic segmentation: Electron Microscopy Image Segmentation",
          "Object detection: Object Detection",
          "Semantic segmentation: Semantic Segmentation",
          "Other vision process: Crowd Counting",
          "Facial recognition and modelling: Face Alignment",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Other vision process: Domain Adaptation",
          "Object detection: Lane Detection",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Semantic segmentation: Scene Segmentation",
          "Activity recognition: Egocentric Activity Recognition",
          "Semantic segmentation: Instance Segmentation",
          "Facial recognition and modelling: Face Verification",
          "Image generation: Image Generation",
          "Activity localization: Weakly Supervised Action Localization",
          "Image classification: Document Image Classification",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Other vision process: Visual Question Answering",
          "Activity recognition: Skeleton Based Action Recognition",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Other image process: Image Retrieval",
          "Action localization: Temporal Action Localization",
          "Semantic segmentation: 3D Part Segmentation",
          "Object detection: Object Detection",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Semantic segmentation: Instance Segmentation",
          "Pose estimation: 3D Human Pose Estimation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Other vision process: Visual Question Answering",
          "Other vision process: Domain Adaptation",
          "Object tracking: Multiple Object Tracking",
          "Semantic segmentation: Semantic Segmentation",
          "Other vision process: Scene Text Detection",
          "Object detection: Object Detection",
          "Pose estimation: Pose Estimation",
          "Semantic segmentation: Human Part Segmentation",
          "Image classification: Image Classification",
          "Facial recognition and modelling: Face Verification",
          "Other image process: Grayscale Image Denoising",
          "Object detection: Birds Eye View Object Detection",
          "Object recognition: Pedestrian Attribute Recognition",
          "Object detection: Weakly Supervised Object Detection",
          "Image classification: Satellite Image Classification",
          "Semantic segmentation: Panoptic Segmentation",
          "Object detection: Object Detection",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Action localization: Temporal Action Localization",
          "Other vision process: Scene Text Detection",
          "Image generation: Image Generation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Other vision process: Domain Adaptation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Semantic segmentation: Real-time Instance Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation",
          "Semantic segmentation: Semantic Segmentation",
          "Object detection: Weakly Supervised Object Detection",
          "Activity localization: Weakly Supervised Action Localization",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Pose estimation: 3D Human Pose Estimation",
          "Activity recognition: Action Recognition",
          "Image classification: Image Classification",
          "Semantic segmentation: 3D Instance Segmentation",
          "Image generation: Image Generation",
          "Other video process: Video Generation",
          "Image classification: Image Classification",
          "Semantic segmentation: Instance Segmentation",
          "Object detection: 3D Object Detection",
          "Pose estimation: Pose Estimation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Semantic segmentation: Real-time Instance Segmentation",
          "Other vision process: Visual Question Answering",
          "Activity recognition: Egocentric Activity Recognition",
          "Pose estimation: 3D Human Pose Estimation",
          "Other vision process: Scene Graph Generation",
          "Pose estimation: Pose Estimation",
          "Semantic segmentation: Lesion Segmentation",
          "Object detection: 3D Object Detection",
          "Action localization: Action Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Object detection: Video Object Detection",
          "Semantic segmentation: 3D Semantic Instance Segmentation",
          "Other video process: Video Frame Interpolation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: Video Semantic Segmentation",
          "Pose estimation: 3D Human Pose Estimation"
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 21
        },
        "height": 4023.0000000000005,
        "legend": {
         "title": {
          "text": "task"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Vision process",
         "y": 0.995
        },
        "width": 1500,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "tickmode": "auto",
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "categoryarray": [
          "Semantic segmentation: Skin Cancer Segmentation",
          "Semantic segmentation: Real-time Instance Segmentation",
          "Semantic segmentation: Panoptic Segmentation",
          "Semantic segmentation: Pancreas Segmentation",
          "Semantic segmentation: Nuclear Segmentation",
          "Semantic segmentation: Multi-tissue Nucleus Segmentation",
          "Semantic segmentation: Medical Image Segmentation",
          "Semantic segmentation: Lung Nodule Segmentation",
          "Semantic segmentation: Retinal Vessel Segmentation",
          "Semantic segmentation: Scene Segmentation",
          "Semantic segmentation: Lesion Segmentation",
          "Semantic segmentation: Semantic Segmentation",
          "Semantic segmentation: 3D Part Segmentation",
          "Semantic segmentation: 3D Semantic Instance Segmentation",
          "Semantic segmentation: 3D Semantic Segmentation",
          "Semantic segmentation: Brain Tumor Segmentation",
          "Semantic segmentation: Human Part Segmentation",
          "Semantic segmentation: Electron Microscopy Image Segmentation",
          "Semantic segmentation: 3D Instance Segmentation",
          "Semantic segmentation: Video Semantic Segmentation",
          "Semantic segmentation: Instance Segmentation",
          "Pose tracking: Pose Tracking",
          "Pose estimation: 6D Pose Estimation using RGBD",
          "Pose estimation: 6D Pose Estimation using RGB",
          "Pose estimation: 6D Pose Estimation",
          "Pose estimation: 3D Human Pose Estimation",
          "Pose estimation: Hand Pose Estimation",
          "Pose estimation: Keypoint Detection",
          "Pose estimation: Head Pose Estimation",
          "Pose estimation: Pose Estimation",
          "Pose estimation: Weakly-supervised 3D Human Pose Estimation",
          "Other vision process: Denoising",
          "Other vision process: Object Counting",
          "Other vision process: Horizon Line Estimation",
          "Other vision process: Unsupervised Domain Adaptation",
          "Other vision process: Visual Dialog",
          "Other vision process: Video Prediction",
          "Other vision process: Metric Learning",
          "Other vision process: Monocular Depth Estimation",
          "Other vision process: Crowd Counting",
          "Other vision process: Curved Text Detection",
          "Other vision process: Multivariate Time Series Imputation",
          "Other vision process: Depth Completion",
          "Other vision process: Scene Graph Generation",
          "Other vision process: Domain Generalization",
          "Other vision process: Scene Text Detection",
          "Other vision process: Visual Question Answering",
          "Other vision process: Domain Adaptation",
          "Other vision process: Formation Energy",
          "Other video process: Video Retrieval",
          "Other video process: Video Frame Interpolation",
          "Other video process: Video Generation",
          "Other image process: Image Reconstruction",
          "Other image process: Image Retrieval",
          "Other image process: Grayscale Image Denoising",
          "Other image process: Aesthetics Quality Assessment",
          "Other image process: Image Clustering",
          "Other image process: Color Image Denoising",
          "Other 3D task: 3D Shape Classification",
          "Other 3D task: 3D Room Layouts From A Single RGB Panorama",
          "Other 3D task: 3D Reconstruction",
          "Other 3D task: 3D Object Reconstruction",
          "Other 3D task: 3D Point Cloud Classification",
          "Object tracking: Multiple Object Tracking",
          "Object tracking: Visual Object Tracking",
          "Object recognition: Pedestrian Attribute Recognition",
          "Object recognition: Traffic Sign Recognition",
          "Object detection: Dense Object Detection",
          "Object detection: Object Detection",
          "Object detection: Pedestrian Detection",
          "Object detection: Weakly Supervised Object Detection",
          "Object detection: Birds Eye View Object Detection",
          "Object detection: Lane Detection",
          "Object detection: 3D Object Detection",
          "Object detection: Video Object Detection",
          "Object detection: RGB Salient Object Detection",
          "Image-to-image translation: Fundus to Angiography Generation",
          "Image generation: Pose Transfer",
          "Image generation: Conditional Image Generation",
          "Image generation: Image Generation",
          "Image classification: Document Image Classification",
          "Image classification: Sequential Image Classification",
          "Image classification: Satellite Image Classification",
          "Image classification: Retinal OCT Disease Classification",
          "Image classification: Hyperspectral Image Classification",
          "Image classification: Unsupervised Image Classification",
          "Image classification: Image Classification",
          "Gesture recognition: Hand Gesture Recognition",
          "Facial recognition and modelling: Unsupervised Facial Landmark Detection",
          "Facial recognition and modelling: Face Identification",
          "Facial recognition and modelling: Face Alignment",
          "Facial recognition and modelling: Face Detection",
          "Facial recognition and modelling: Facial Expression Recognition",
          "Facial recognition and modelling: Face Verification",
          "Facial recognition and modelling: Facial Landmark Detection",
          "Emotion recognition: Emotion Recognition in Conversation",
          "Activity recognition: Skeleton Based Action Recognition",
          "Activity recognition: Multimodal Activity Recognition",
          "Activity recognition: Egocentric Activity Recognition",
          "Activity recognition: Human Interaction Recognition",
          "Activity recognition: Group Activity Recognition",
          "Activity recognition: Action Recognition",
          "Activity recognition: Action Classification",
          "Activity localization: Temporal Action Proposal Generation",
          "Activity localization: Weakly Supervised Action Localization",
          "Activity detection: Action Detection",
          "Action localization: Action Segmentation",
          "Action localization: Temporal Action Localization"
         ],
         "categoryorder": "array",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "side": "left",
         "title": {}
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"a5ad99f9-6f07-4d14-bd02-557ff03a930a\" class=\"plotly-graph-div\" style=\"height:4023.0000000000005px; width:1500px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a5ad99f9-6f07-4d14-bd02-557ff03a930a\")) {                    Plotly.newPlot(                        \"a5ad99f9-6f07-4d14-bd02-557ff03a930a\",                        [{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Action localization: Temporal Action Localization\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Action localization: Temporal Action Localization\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-03\",\"2018-04\",\"2017-05\",\"2019-04\",\"2016-09\",\"2016-01\",\"2019-06\",\"2019-07\",\"2019-09\",\"2019-11\",\"2018-06\",\"2019-03\"],\"xaxis\":\"x\",\"y\":[\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Action localization: Action Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Action localization: Action Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-11\",\"2018-06\",\"2019-03\",\"2020-03\"],\"xaxis\":\"x\",\"y\":[\"Action localization: Action Segmentation\",\"Action localization: Action Segmentation\",\"Action localization: Action Segmentation\",\"Action localization: Action Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity detection: Action Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity detection: Action Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-03\",\"2017-12\",\"2018-03\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Activity detection: Action Detection\",\"Activity detection: Action Detection\",\"Activity detection: Action Detection\",\"Activity detection: Action Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity localization: Weakly Supervised Action Localization\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity localization: Weakly Supervised Action Localization\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-06\",\"2017-12\",\"2018-07\",\"2019-05\",\"2019-08\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity localization: Temporal Action Proposal Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity localization: Temporal Action Proposal Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-07\",\"2018-11\",\"2018-06\"],\"xaxis\":\"x\",\"y\":[\"Activity localization: Temporal Action Proposal Generation\",\"Activity localization: Temporal Action Proposal Generation\",\"Activity localization: Temporal Action Proposal Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition: Action Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition: Action Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-06\",\"2019-05\",\"2019-04\",\"2018-11\",\"2018-12\",\"2018-10\",\"2018-07\",\"2018-06\",\"2017-12\",\"2017-11\",\"2017-05\",\"2016-12\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition: Action Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition: Action Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-06\",\"2016-04\",\"2015-05\",\"2015-12\",\"2017-03\",\"2016-08\",\"2016-01\",\"2014-12\",\"2017-04\",\"2015-03\",\"2017-05\",\"2019-12\",\"2019-08\",\"2019-07\",\"2019-06\",\"2019-05\",\"2019-04\",\"2019-01\",\"2018-12\",\"2018-11\",\"2018-10\",\"2018-07\",\"2018-06\",\"2018-01\",\"2017-12\",\"2017-11\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition: Group Activity Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition: Group Activity Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition: Group Activity Recognition\",\"Activity recognition: Group Activity Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition: Human Interaction Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition: Human Interaction Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-06\",\"2018-11\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition: Human Interaction Recognition\",\"Activity recognition: Human Interaction Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition: Egocentric Activity Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition: Egocentric Activity Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2018-12\",\"2019-05\",\"2019-08\",\"2020-02\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Egocentric Activity Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition: Multimodal Activity Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition: Multimodal Activity Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-03\",\"2016-08\",\"2017-04\",\"2018-01\",\"2019-01\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition: Skeleton Based Action Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition: Skeleton Based Action Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2019-07\",\"2019-06\",\"2019-04\",\"2019-01\",\"2018-12\",\"2019-11\",\"2018-11\",\"2018-05\",\"2018-04\",\"2017-03\",\"2017-04\",\"2018-02\",\"2018-01\",\"2018-06\",\"2019-12\",\"2016-11\",\"2017-05\",\"2016-09\",\"2016-06\",\"2016-04\",\"2013-02\",\"2017-08\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Emotion recognition: Emotion Recognition in Conversation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Emotion recognition: Emotion Recognition in Conversation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-04\",\"2019-08\",\"2019-09\",\"2018-11\",\"2018-06\",\"2018-10\"],\"xaxis\":\"x\",\"y\":[\"Emotion recognition: Emotion Recognition in Conversation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Emotion recognition: Emotion Recognition in Conversation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling: Facial Landmark Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling: Facial Landmark Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-02\",\"2018-03\",\"2015-11\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling: Facial Landmark Detection\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Facial recognition and modelling: Facial Landmark Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling: Face Verification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling: Face Verification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-03\",\"2017-03\",\"2016-03\",\"2015-11\",\"2018-09\",\"2018-12\",\"2019-03\",\"2019-04\",\"2019-08\",\"2019-10\",\"2017-12\",\"2014-06\",\"2014-12\",\"2015-02\",\"2015-03\",\"2018-01\",\"2015-08\",\"2017-10\",\"2017-04\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling: Facial Expression Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling: Facial Expression Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-11\",\"2019-05\",\"2019-02\",\"2013-07\",\"2017-08\",\"2018-05\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling: Face Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling: Face Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-08\",\"2018-10\",\"2018-09\",\"2017-09\",\"2016-03\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling: Face Detection\",\"Facial recognition and modelling: Face Detection\",\"Facial recognition and modelling: Face Detection\",\"Facial recognition and modelling: Face Detection\",\"Facial recognition and modelling: Face Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling: Face Alignment\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling: Face Alignment\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-07\",\"2019-08\",\"2019-06\",\"2019-04\",\"2019-02\",\"2018-05\",\"2018-04\",\"2018-03\",\"2017-11\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling: Face Identification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling: Face Identification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-11\",\"2017-04\",\"2018-01\",\"2018-03\",\"2018-12\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Identification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-05\",\"2017-06\",\"2018-04\",\"2018-06\",\"2018-08\",\"2019-08\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Gesture recognition: Hand Gesture Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Gesture recognition: Hand Gesture Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-05\",\"2018-04\",\"2019-01\",\"2018-12\"],\"xaxis\":\"x\",\"y\":[\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification: Image Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification: Image Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-12\",\"2020-01\",\"2013-12\",\"2013-11\",\"2013-02\",\"2012-12\",\"2014-09\",\"2019-10\",\"2019-08\",\"2019-06\",\"2019-05\",\"2014-12\",\"2015-02\",\"2015-06\",\"2015-11\",\"2015-12\",\"2016-02\",\"2016-03\",\"2016-05\",\"2016-08\",\"2016-10\",\"2014-06\",\"2016-11\",\"2017-08\",\"2017-09\",\"2017-10\",\"2017-12\",\"2018-02\",\"2018-05\",\"2014-04\",\"2018-11\",\"2019-01\",\"2019-04\",\"2017-07\",\"2018-07\"],\"xaxis\":\"x\",\"y\":[\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification: Unsupervised Image Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification: Unsupervised Image Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-02\",\"2017-02\"],\"xaxis\":\"x\",\"y\":[\"Image classification: Unsupervised Image Classification\",\"Image classification: Unsupervised Image Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification: Hyperspectral Image Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification: Hyperspectral Image Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-02\",\"2018-07\"],\"xaxis\":\"x\",\"y\":[\"Image classification: Hyperspectral Image Classification\",\"Image classification: Hyperspectral Image Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification: Retinal OCT Disease Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification: Retinal OCT Disease Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-12\",\"2018-01\"],\"xaxis\":\"x\",\"y\":[\"Image classification: Retinal OCT Disease Classification\",\"Image classification: Retinal OCT Disease Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification: Satellite Image Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification: Satellite Image Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-12\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Image classification: Satellite Image Classification\",\"Image classification: Satellite Image Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification: Sequential Image Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification: Sequential Image Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-10\",\"2018-03\",\"2017-10\",\"2016-03\",\"2015-11\"],\"xaxis\":\"x\",\"y\":[\"Image classification: Sequential Image Classification\",\"Image classification: Sequential Image Classification\",\"Image classification: Sequential Image Classification\",\"Image classification: Sequential Image Classification\",\"Image classification: Sequential Image Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification: Document Image Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification: Document Image Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-08\",\"2018-01\",\"2017-04\"],\"xaxis\":\"x\",\"y\":[\"Image classification: Document Image Classification\",\"Image classification: Document Image Classification\",\"Image classification: Document Image Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image generation: Image Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image generation: Image Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-06\",\"2019-12\",\"2019-11\",\"2019-07\",\"2019-04\",\"2019-03\",\"2018-12\",\"2018-11\",\"2018-09\",\"2018-07\",\"2018-03\",\"2018-02\",\"2017-10\",\"2017-09\",\"2017-06\",\"2017-03\",\"2016-01\",\"2019-08\",\"2017-02\"],\"xaxis\":\"x\",\"y\":[\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image generation: Conditional Image Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image generation: Conditional Image Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-09\",\"2018-05\",\"2018-02\",\"2017-09\",\"2017-03\",\"2016-12\",\"2016-10\",\"2016-06\"],\"xaxis\":\"x\",\"y\":[\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image generation: Pose Transfer\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image generation: Pose Transfer\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-12\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Image generation: Pose Transfer\",\"Image generation: Pose Transfer\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image-to-image translation: Fundus to Angiography Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image-to-image translation: Fundus to Angiography Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-07\",\"2017-11\"],\"xaxis\":\"x\",\"y\":[\"Image-to-image translation: Fundus to Angiography Generation\",\"Image-to-image translation: Fundus to Angiography Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: RGB Salient Object Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: RGB Salient Object Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-06\",\"2019-06\",\"2019-04\",\"2018-06\",\"2017-08\",\"2017-07\",\"2017-04\",\"2016-11\",\"2017-10\"],\"xaxis\":\"x\",\"y\":[\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: Video Object Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: Video Object Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2019-07\",\"2020-03\"],\"xaxis\":\"x\",\"y\":[\"Object detection: Video Object Detection\",\"Object detection: Video Object Detection\",\"Object detection: Video Object Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: 3D Object Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: 3D Object Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-03\",\"2019-04\",\"2019-07\",\"2020-01\",\"2018-02\",\"2016-06\",\"2017-11\",\"2017-12\",\"2018-12\",\"2020-03\"],\"xaxis\":\"x\",\"y\":[\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: Lane Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: Lane Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-12\",\"2018-06\",\"2019-08\",\"2017-10\"],\"xaxis\":\"x\",\"y\":[\"Object detection: Lane Detection\",\"Object detection: Lane Detection\",\"Object detection: Lane Detection\",\"Object detection: Lane Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: Birds Eye View Object Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: Birds Eye View Object Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-12\",\"2017-12\",\"2017-11\",\"2016-11\",\"2016-08\",\"2019-07\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: Weakly Supervised Object Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: Weakly Supervised Object Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-11\",\"2019-10\",\"2019-04\",\"2018-11\",\"2018-07\",\"2018-06\",\"2018-04\",\"2018-03\",\"2018-02\",\"2017-11\",\"2017-08\",\"2015-11\",\"2016-03\",\"2016-09\",\"2016-11\",\"2017-04\",\"2017-06\",\"2017-07\"],\"xaxis\":\"x\",\"y\":[\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: Pedestrian Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: Pedestrian Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-12\",\"2017-02\",\"2018-07\"],\"xaxis\":\"x\",\"y\":[\"Object detection: Pedestrian Detection\",\"Object detection: Pedestrian Detection\",\"Object detection: Pedestrian Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: Object Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: Object Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-11\",\"2019-10\",\"2019-09\",\"2015-06\",\"2015-12\",\"2016-12\",\"2017-03\",\"2017-07\",\"2017-08\",\"2017-11\",\"2018-03\",\"2018-05\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-06\",\"2017-12\",\"2019-08\"],\"xaxis\":\"x\",\"y\":[\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection: Dense Object Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection: Dense Object Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-04\",\"2017-08\",\"2016-12\"],\"xaxis\":\"x\",\"y\":[\"Object detection: Dense Object Detection\",\"Object detection: Dense Object Detection\",\"Object detection: Dense Object Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object recognition: Traffic Sign Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object recognition: Traffic Sign Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-04\",\"2018-06\"],\"xaxis\":\"x\",\"y\":[\"Object recognition: Traffic Sign Recognition\",\"Object recognition: Traffic Sign Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object recognition: Pedestrian Attribute Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object recognition: Pedestrian Attribute Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-08\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Object recognition: Pedestrian Attribute Recognition\",\"Object recognition: Pedestrian Attribute Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object tracking: Visual Object Tracking\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object tracking: Visual Object Tracking\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-07\",\"2018-12\",\"2019-06\",\"2017-06\",\"2017-04\",\"2016-11\",\"2018-03\",\"2017-10\",\"2018-02\",\"2018-11\",\"2018-06\"],\"xaxis\":\"x\",\"y\":[\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object tracking: Multiple Object Tracking\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object tracking: Multiple Object Tracking\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2019-09\",\"2018-02\"],\"xaxis\":\"x\",\"y\":[\"Object tracking: Multiple Object Tracking\",\"Object tracking: Multiple Object Tracking\",\"Object tracking: Multiple Object Tracking\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other 3D task: 3D Point Cloud Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other 3D task: 3D Point Cloud Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-12\",\"2018-03\",\"2018-01\",\"2017-06\",\"2017-04\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Other 3D task: 3D Point Cloud Classification\",\"Other 3D task: 3D Point Cloud Classification\",\"Other 3D task: 3D Point Cloud Classification\",\"Other 3D task: 3D Point Cloud Classification\",\"Other 3D task: 3D Point Cloud Classification\",\"Other 3D task: 3D Point Cloud Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other 3D task: 3D Object Reconstruction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other 3D task: 3D Object Reconstruction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-01\",\"2018-02\",\"2016-12\"],\"xaxis\":\"x\",\"y\":[\"Other 3D task: 3D Object Reconstruction\",\"Other 3D task: 3D Object Reconstruction\",\"Other 3D task: 3D Object Reconstruction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other 3D task: 3D Reconstruction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other 3D task: 3D Reconstruction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-12\",\"2018-11\",\"2018-08\"],\"xaxis\":\"x\",\"y\":[\"Other 3D task: 3D Reconstruction\",\"Other 3D task: 3D Reconstruction\",\"Other 3D task: 3D Reconstruction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2019-01\"],\"xaxis\":\"x\",\"y\":[\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\",\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other 3D task: 3D Shape Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other 3D task: 3D Shape Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-04\",\"2017-11\"],\"xaxis\":\"x\",\"y\":[\"Other 3D task: 3D Shape Classification\",\"Other 3D task: 3D Shape Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other image process: Color Image Denoising\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other image process: Color Image Denoising\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-07\",\"2019-04\",\"2018-02\",\"2017-10\",\"2017-04\"],\"xaxis\":\"x\",\"y\":[\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other image process: Image Clustering\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other image process: Image Clustering\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-04\",\"2018-10\",\"2012-08\",\"2015-11\",\"2016-04\",\"2018-04\",\"2017-03\",\"2017-04\",\"2018-07\",\"2017-09\",\"2017-10\",\"2018-11\",\"2018-12\",\"2019-01\"],\"xaxis\":\"x\",\"y\":[\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other image process: Aesthetics Quality Assessment\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other image process: Aesthetics Quality Assessment\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-10\",\"2017-04\",\"2016-04\"],\"xaxis\":\"x\",\"y\":[\"Other image process: Aesthetics Quality Assessment\",\"Other image process: Aesthetics Quality Assessment\",\"Other image process: Aesthetics Quality Assessment\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other image process: Grayscale Image Denoising\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other image process: Grayscale Image Denoising\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-10\",\"2018-06\",\"2018-05\",\"2017-10\",\"2017-04\",\"2016-08\"],\"xaxis\":\"x\",\"y\":[\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other image process: Image Retrieval\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other image process: Image Retrieval\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-03\",\"2017-12\",\"2016-12\",\"2016-11\",\"2016-08\",\"2016-04\",\"2015-11\",\"2015-04\",\"2018-01\",\"2018-11\",\"2019-02\",\"2019-03\",\"2019-09\",\"2018-04\"],\"xaxis\":\"x\",\"y\":[\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other image process: Image Reconstruction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other image process: Image Reconstruction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-03\",\"2017-06\"],\"xaxis\":\"x\",\"y\":[\"Other image process: Image Reconstruction\",\"Other image process: Image Reconstruction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other video process: Video Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other video process: Video Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-07\",\"2019-12\",\"2016-11\"],\"xaxis\":\"x\",\"y\":[\"Other video process: Video Generation\",\"Other video process: Video Generation\",\"Other video process: Video Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other video process: Video Frame Interpolation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other video process: Video Frame Interpolation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2020-03\",\"2019-04\",\"2018-10\"],\"xaxis\":\"x\",\"y\":[\"Other video process: Video Frame Interpolation\",\"Other video process: Video Frame Interpolation\",\"Other video process: Video Frame Interpolation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other video process: Video Retrieval\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other video process: Video Retrieval\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-04\",\"2016-12\",\"2017-07\",\"2018-06\",\"2018-08\",\"2019-06\",\"2019-07\"],\"xaxis\":\"x\",\"y\":[\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Formation Energy\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Formation Energy\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-05\",\"2017-09\",\"2018-06\",\"2017-06\",\"2017-04\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Formation Energy\",\"Other vision process: Formation Energy\",\"Other vision process: Formation Energy\",\"Other vision process: Formation Energy\",\"Other vision process: Formation Energy\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Domain Adaptation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Domain Adaptation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-08\",\"2016-05\",\"2015-02\",\"2014-09\",\"2015-05\",\"2019-11\",\"2017-04\",\"2017-05\",\"2017-11\",\"2017-12\",\"2018-07\",\"2018-11\",\"2019-01\",\"2019-03\",\"2019-05\",\"2019-06\",\"2019-08\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Visual Question Answering\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Visual Question Answering\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-11\",\"2017-05\",\"2016-06\",\"2016-03\",\"2018-05\",\"2017-08\",\"2018-03\",\"2016-05\",\"2019-02\",\"2019-05\",\"2020-02\",\"2019-09\",\"2019-08\",\"2019-07\",\"2019-04\",\"2017-07\",\"2019-06\",\"2017-04\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Scene Text Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Scene Text Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-04\",\"2017-07\",\"2017-08\",\"2017-09\",\"2018-01\",\"2018-02\",\"2018-04\",\"2018-06\",\"2018-07\",\"2018-11\",\"2019-03\",\"2019-04\",\"2019-10\",\"2019-11\",\"2017-04\",\"2017-03\",\"2015-04\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Domain Generalization\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Domain Generalization\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-08\",\"2017-10\",\"2019-03\",\"2019-05\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Domain Generalization\",\"Other vision process: Domain Generalization\",\"Other vision process: Domain Generalization\",\"Other vision process: Domain Generalization\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Scene Graph Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Scene Graph Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-06\",\"2018-08\",\"2018-12\",\"2020-02\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Scene Graph Generation\",\"Other vision process: Scene Graph Generation\",\"Other vision process: Scene Graph Generation\",\"Other vision process: Scene Graph Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Depth Completion\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Depth Completion\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-02\",\"2018-08\",\"2018-11\",\"2019-05\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Depth Completion\",\"Other vision process: Depth Completion\",\"Other vision process: Depth Completion\",\"Other vision process: Depth Completion\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Multivariate Time Series Imputation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Multivariate Time Series Imputation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-06\",\"2018-12\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Multivariate Time Series Imputation\",\"Other vision process: Multivariate Time Series Imputation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Curved Text Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Curved Text Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-05\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Curved Text Detection\",\"Other vision process: Curved Text Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Crowd Counting\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Crowd Counting\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-08\",\"2019-06\",\"2016-08\",\"2015-12\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Crowd Counting\",\"Other vision process: Crowd Counting\",\"Other vision process: Crowd Counting\",\"Other vision process: Crowd Counting\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Monocular Depth Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Monocular Depth Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-07\",\"2019-03\",\"2018-12\",\"2018-06\",\"2018-05\",\"2018-03\",\"2017-04\",\"2016-07\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Metric Learning\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Metric Learning\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-08\",\"2018-04\",\"2017-06\",\"2016-11\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Metric Learning\",\"Other vision process: Metric Learning\",\"Other vision process: Metric Learning\",\"Other vision process: Metric Learning\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Video Prediction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Video Prediction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2019-05\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Video Prediction\",\"Other vision process: Video Prediction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Visual Dialog\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Visual Dialog\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-09\",\"2017-11\",\"2018-09\",\"2019-02\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Visual Dialog\",\"Other vision process: Visual Dialog\",\"Other vision process: Visual Dialog\",\"Other vision process: Visual Dialog\",\"Other vision process: Visual Dialog\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Unsupervised Domain Adaptation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Unsupervised Domain Adaptation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-02\",\"2015-05\",\"2018-11\",\"2018-12\",\"2019-11\",\"2020-01\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Horizon Line Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Horizon Line Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-08\",\"2019-05\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Horizon Line Estimation\",\"Other vision process: Horizon Line Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Object Counting\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Object Counting\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-04\",\"2016-09\",\"2017-07\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Object Counting\",\"Other vision process: Object Counting\",\"Other vision process: Object Counting\",\"Other vision process: Object Counting\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process: Denoising\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process: Denoising\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-05\",\"2019-04\",\"2018-07\"],\"xaxis\":\"x\",\"y\":[\"Other vision process: Denoising\",\"Other vision process: Denoising\",\"Other vision process: Denoising\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-04\",\"2019-03\",\"2018-11\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: Pose Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: Pose Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-11\",\"2017-12\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-02\",\"2020-01\",\"2020-02\",\"2017-08\",\"2017-07\",\"2017-04\",\"2015-11\",\"2016-01\",\"2016-03\",\"2016-09\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-05\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: Head Pose Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: Head Pose Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-01\",\"2018-12\",\"2017-10\",\"2019-06\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Head Pose Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: Keypoint Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: Keypoint Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-03\",\"2017-11\",\"2018-04\",\"2018-12\",\"2019-01\",\"2019-02\",\"2016-12\",\"2016-11\",\"2016-08\",\"2017-01\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: Hand Pose Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: Hand Pose Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-12\",\"2017-08\",\"2017-07\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: Hand Pose Estimation\",\"Pose estimation: Hand Pose Estimation\",\"Pose estimation: Hand Pose Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: 3D Human Pose Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: 3D Human Pose Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2020-04\",\"2020-02\",\"2019-12\",\"2019-09\",\"2018-06\",\"2018-04\",\"2017-09\",\"2017-05\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: 6D Pose Estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: 6D Pose Estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-01\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: 6D Pose Estimation\",\"Pose estimation: 6D Pose Estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: 6D Pose Estimation using RGB\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: 6D Pose Estimation using RGB\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-11\",\"2018-03\",\"2018-12\",\"2019-02\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation using RGB\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation: 6D Pose Estimation using RGBD\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation: 6D Pose Estimation using RGBD\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-03\",\"2019-01\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation using RGBD\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose tracking: Pose Tracking\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose tracking: Pose Tracking\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-05\",\"2019-02\",\"2018-04\",\"2018-02\",\"2017-12\"],\"xaxis\":\"x\",\"y\":[\"Pose tracking: Pose Tracking\",\"Pose tracking: Pose Tracking\",\"Pose tracking: Pose Tracking\",\"Pose tracking: Pose Tracking\",\"Pose tracking: Pose Tracking\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Instance Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Instance Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-04\",\"2017-11\",\"2017-12\",\"2018-03\",\"2019-02\",\"2019-06\",\"2020-01\",\"2019-08\",\"2019-09\",\"2019-11\",\"2016-11\",\"2017-03\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Video Semantic Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Video Semantic Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-12\",\"2020-04\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Video Semantic Segmentation\",\"Semantic segmentation: Video Semantic Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: 3D Instance Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: 3D Instance Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-03\",\"2020-03\",\"2019-12\",\"2019-06\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-06\",\"2019-08\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"Semantic segmentation: Electron Microscopy Image Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Human Part Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Human Part Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2017-08\",\"2018-05\",\"2018-09\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Human Part Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Brain Tumor Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Brain Tumor Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-08\",\"2019-06\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Brain Tumor Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: 3D Semantic Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: 3D Semantic Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-04\",\"2018-09\",\"2018-07\",\"2017-10\",\"2017-06\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: 3D Semantic Segmentation\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Semantic segmentation: 3D Semantic Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: 3D Semantic Instance Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: 3D Semantic Instance Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2020-03\",\"2018-12\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: 3D Semantic Instance Segmentation\",\"Semantic segmentation: 3D Semantic Instance Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: 3D Part Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: 3D Part Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2019-04\",\"2018-01\",\"2017-11\",\"2017-06\",\"2016-12\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Part Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Semantic Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Semantic Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-11\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2020-04\",\"2016-05\",\"2019-06\",\"2016-03\",\"2015-09\",\"2015-04\",\"2015-03\",\"2015-02\",\"2014-12\",\"2014-11\",\"2015-11\",\"2019-04\",\"2019-01\",\"2016-12\",\"2017-03\",\"2017-04\",\"2017-06\",\"2017-10\",\"2017-11\",\"2019-03\",\"2017-12\",\"2018-03\",\"2018-04\",\"2018-06\",\"2018-08\",\"2018-09\",\"2018-12\",\"2018-02\",\"2016-06\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Lesion Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Lesion Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-10\",\"2020-03\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Lesion Segmentation\",\"Semantic segmentation: Lesion Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Scene Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Scene Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-08\",\"2018-03\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Scene Segmentation\",\"Semantic segmentation: Scene Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Retinal Vessel Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Retinal Vessel Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-07\",\"2019-12\",\"2018-10\",\"2018-06\",\"2018-02\",\"2017-11\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Lung Nodule Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Lung Nodule Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-11\",\"2019-08\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: Lung Nodule Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Medical Image Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Medical Image Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-11\",\"2018-07\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Medical Image Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Multi-tissue Nucleus Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Multi-tissue Nucleus Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-05\",\"2017-03\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Multi-tissue Nucleus Segmentation\",\"Semantic segmentation: Multi-tissue Nucleus Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Nuclear Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Nuclear Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-11\",\"2017-03\",\"2018-09\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Nuclear Segmentation\",\"Semantic segmentation: Nuclear Segmentation\",\"Semantic segmentation: Nuclear Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Pancreas Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Pancreas Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-04\",\"2017-09\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Pancreas Segmentation\",\"Semantic segmentation: Pancreas Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Panoptic Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Panoptic Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-01\",\"2018-12\",\"2019-11\",\"2019-09\",\"2019-05\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Real-time Instance Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Real-time Instance Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2020-01\",\"2019-11\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Real-time Instance Segmentation\",\"Semantic segmentation: Real-time Instance Segmentation\",\"Semantic segmentation: Real-time Instance Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation: Skin Cancer Segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation: Skin Cancer Segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-11\",\"2018-02\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation: Skin Cancer Segmentation\",\"Semantic segmentation: Skin Cancer Segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":[\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Temporal Action Localization benchmarking: Frame-mAP<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>\",\"<BR>task: Action localization: Action Segmentation<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>\",\"<BR>task: Action localization: Action Segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>\",\"<BR>task: Action localization: Action Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  50 Salads - Action Segmentation benchmarking: Acc<BR>  50 Salads - Action Segmentation benchmarking: Edit<BR>  50 Salads - Action Segmentation benchmarking: F1@10%<BR>  50 Salads - Action Segmentation benchmarking: F1@25%<BR>  50 Salads - Action Segmentation benchmarking: F1@50%<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  Breakfast - Action Segmentation benchmarking: F1@10%<BR>  Breakfast - Action Segmentation benchmarking: F1@25%<BR>  Breakfast - Action Segmentation benchmarking: F1@50%<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Video-mAP 0.5<BR>  UCF101-24 - Temporal Action Localization benchmarking: Video-mAP 0.5<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  MEXaction2 - Temporal Action Localization benchmarking: mAP<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: mAP<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.1<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.2<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>\",\"<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@1000<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@200<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@500<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@50<BR>\",\"<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (test)<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.5<BR>  THUMOS\\u201914 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: Mean mAP<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2012-07<BR>Anchor.<BR>benchmarks:<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  ActionNet-VE - Action Recognition benchmarking: F-measure (%)<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  UTD-MHAD - Multimodal Activity Recognition benchmarking: Accuracy (CS)<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 5 Accuracy<BR>  Something-Something V1 - Action Recognition benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Action Classification benchmarking: mAP<BR>  THUMOS\\u201914 - Action Classification benchmarking: mAP<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-5 Accuracy<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  YouCook2 - Action Classification benchmarking: Object Top 5 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Object Top-1 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Verb Top-1 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Verb Top-5 Accuracy<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  MiniKinetics - Action Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  THUMOS'14 - Action Classification benchmarking: mAP<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-1 (%)<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-5 (%)<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>  VIRAT Ground 2.0 - Action Recognition benchmarking: Average Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet - Action Recognition benchmarking: mAP<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CS<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CV1<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CV2<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ICVL-4 - Action Recognition benchmarking: Accuracy<BR>  IRD - Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  miniSports - Action Recognition benchmarking: Clip Hit-at-1<BR>  miniSports - Action Recognition benchmarking: Video hit-at-1<BR>  miniSports - Action Recognition benchmarking: Video hit-at-5<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  miniSports - Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2012-10<BR>Anchor.<BR>benchmarks:<BR>  CAD-120 - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UT-Kinect - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  SBU - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.1<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  J-HMBD Early Action - Skeleton Based Action Recognition benchmarking: 10%<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  UAV-Human - Skeleton Based Action Recognition benchmarking: Average Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: Speed  (FPS)<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  MSR Action3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UPenn Action - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: No. parameters<BR>\",\"<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S1)<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Volleyball - Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking: Accuracy<BR>  Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking: Train F-measure<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  LboroHAR - Multimodal Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Action Recognition benchmarking: Top-1 Accuracy<BR>  EgoGesture - Action Recognition benchmarking: Top-1 Accuracy<BR>  EgoGesture - Action Recognition benchmarking: Top-5 Accuracy<BR>\",\"<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: GFlops<BR>  AVA v2.1 - Action Recognition benchmarking: Params (M)<BR>  AVA v2.2 - Action Recognition benchmarking: mAP<BR>  Diving-48 - Action Recognition benchmarking: Accuracy<BR>  UTD-MHAD - Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: mAP (Val)<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  DailyDialog - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>  EmoryNLP - Emotion Recognition in Conversation benchmarking: Weighted Macro-F1<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Arousal)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Cohn-Kanade - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000-3D - Face Alignment benchmarking: Mean NME<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>  WFLW - Face Alignment benchmarking: FR-at-0.1(%, all)<BR>  WFLW - Face Alignment benchmarking: ME (%, all)<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000 - Face Alignment benchmarking: Error rate<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  SFEW - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  FERPlus - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Real-World Affective Faces - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  3DFAW - Face Alignment benchmarking: CVGTCE<BR>  3DFAW - Face Alignment benchmarking: GTE<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  FERG - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  RAF-DB - Facial Expression Recognition benchmarking: Overall Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: Mean Error Rate<BR>\",\"<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  AFLW-Front - Facial Landmark Detection benchmarking: Mean NME<BR>  AFLW-Full - Facial Landmark Detection benchmarking: Mean NME<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (7 emotion)<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Oulu-CASIA - Facial Expression Recognition benchmarking: Accuracy (10-fold)<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  AFLW-Full - Face Alignment benchmarking: Mean NME<BR>  LS3D-W Balanced - Face Alignment benchmarking: AUC0.07<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Identification benchmarking: Accuracy<BR>  IJB-B - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Oulu-CASIA - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.01<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-B - Face Verification benchmarking: TAR at FAR=0.01<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.1<BR>  IJB-B - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.001<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  IIIT-D Viewed Sketch - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  AgeDB-30 - Face Verification benchmarking: Accuracy<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  CelebA Aligned - Face Alignment benchmarking: MOS<BR>  CelebA Aligned - Face Alignment benchmarking: MS-SSIM<BR>  CelebA Aligned - Face Alignment benchmarking: PSNR<BR>  CelebA Aligned - Face Alignment benchmarking: SSIM<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  IBUG - Face Alignment benchmarking: Mean Error Rate<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  AFLW - Face Alignment benchmarking: Mean NME<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  COFW - Face Alignment benchmarking: Mean Error Rate<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  AFLW-LFPA - Face Alignment benchmarking: Mean NME<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>  300W - Face Alignment benchmarking: Failure private<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>  300W - Face Alignment benchmarking: Mean Error Rate private<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2013-07<BR>Anchor.<BR>benchmarks:<BR>  FER2013 - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  JAFFE - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  MMI - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>  AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ChaLean test - Hand Gesture Recognition benchmarking: Accuracy<BR>  Jester val - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>  Jester val - Hand Gesture Recognition benchmarking: Top 5 Accuracy<BR>  NVGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Northwestern University - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  DHG-14 - Hand Gesture Recognition benchmarking: Accuracy<BR>  DHG-28 - Hand Gesture Recognition benchmarking: Accuracy<BR>  SHREC 2017 - Hand Gesture Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 - Hand Gesture Recognition benchmarking: 28 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Hand Gesture Recognition benchmarking: 14 gestures accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  ChaLearn val - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  BUAA - Hand Gesture Recognition benchmarking: Accuracy<BR>  MGB - Hand Gesture Recognition benchmarking: Accuracy<BR>  SmartWatch - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2013-03<BR>Anchor.<BR>benchmarks:<BR>  Cambridge - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Jester test - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Image classification: Document Image Classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Noisy Bangla Characters - Document Image Classification benchmarking: Accuracy<BR>  Noisy Bangla Numeral - Document Image Classification benchmarking: Accuracy<BR>  n-MNIST - Document Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Document Image Classification<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Indian Pines - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>  Pavia University - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Salinas Scene - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  EMNIST-Letters - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  CINIC-10 - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Clothing1M - Image Classification benchmarking: Accuracy<BR>  Food-101N - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  EMNIST-Balanced - Image Classification benchmarking: Accuracy<BR>  MultiMNIST - Image Classification benchmarking: Percentage error<BR>  smallNORB - Image Classification benchmarking: Classification Error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Flowers-102 - Image Classification benchmarking: Accuracy<BR>  ObjectNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ObjectNet - Image Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2013-01<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  iNaturalist - Image Classification benchmarking: Top 1 Accuracy<BR>  iNaturalist - Image Classification benchmarking: Top 5 Accuracy<BR>  iNaturalist 2018 - Image Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ImageNet ReaL - Image Classification benchmarking: Params<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Unsupervised Image Classification benchmarking: Accuracy<BR>  CIFAR-20 - Unsupervised Image Classification benchmarking: Accuracy<BR>  STL-10 - Unsupervised Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sequential CIFAR-10 - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>\",\"<BR>task: Image classification: Satellite Image Classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>  SAT-6 - Satellite Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Sequential Image Classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>\",\"<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  MNIST - Unsupervised Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>  Srinivasan2014 - Retinal OCT Disease Classification benchmarking: Acc<BR>\",\"<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: # of clusters (k)<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  CelebA 128 x 128 - Image Generation benchmarking: FID<BR>  Stacked MNIST - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Pose Transfer<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: FID<BR>\",\"<BR>task: Image generation: Pose Transfer<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: DS<BR>  Deep-Fashion - Pose Transfer benchmarking: PCKh<BR>  Market-1501 - Pose Transfer benchmarking: DS<BR>  Market-1501 - Pose Transfer benchmarking: IS<BR>  Market-1501 - Pose Transfer benchmarking: PCKh<BR>  Market-1501 - Pose Transfer benchmarking: SSIM<BR>  Market-1501 - Pose Transfer benchmarking: mask-IS<BR>  Market-1501 - Pose Transfer benchmarking: mask-SSIM<BR>\",\"<BR>task: Image generation: Pose Transfer<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: LPIPS<BR>  Deep-Fashion - Pose Transfer benchmarking: Retrieval Top10 Recall<BR>\",\"<BR>task: Image generation: Pose Transfer<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: IS<BR>  Deep-Fashion - Pose Transfer benchmarking: SSIM<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  LSUN Car 512 x 384 - Image Generation benchmarking: FID<BR>  LSUN Horse 256 x 256 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  ADE-Indoor - Image Generation benchmarking: FID<BR>  CIFAR-100 - Image Generation benchmarking: FID<BR>  Cityscapes-25K 256x512 - Image Generation benchmarking: FID<BR>  Cityscapes-5K 256x512 - Image Generation benchmarking: FID<BR>  ImageNet 32x32 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: FID<BR>  Fashion-MNIST - Image Generation benchmarking: FID<BR>  MNIST - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 64x64 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: bits/dimension<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Binarized MNIST - Image Generation benchmarking: nats<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  CUB 128 x 128 - Image Generation benchmarking: FID<BR>  CUB 128 x 128 - Image Generation benchmarking: Inception score<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>  Stanford Cars - Image Generation benchmarking: FID<BR>  Stanford Cars - Image Generation benchmarking: Inception score<BR>  Stanford Dogs - Image Generation benchmarking: FID<BR>  Stanford Dogs - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  LSUN Bedroom - Image Generation benchmarking: FID-50k<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 128x128 - Image Generation benchmarking: FID<BR>  MNIST - Image Generation benchmarking: bits/dimension<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  CAT 256x256 - Image Generation benchmarking: FID<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  LSUN Bedroom 64 x 64 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  ImageNet 128x128 - Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Image Generation benchmarking: IS<BR>  ImageNet 256x256 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: bpd<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 1024x1024 - Image Generation benchmarking: FID<BR>  CelebA-HQ 256x256 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Cat 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Churches 256 x 256 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: Kernel Inception Distance<BR>\",\"<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ImageNet - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>  Watercolor2k - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ISTD - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  UCF - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Dense Object Detection<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Clipart1k - Weakly Supervised Object Detection benchmarking: MAP<BR>  Comic2k - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  SOC - RGB Salient Object Detection benchmarking: Average MAE<BR>  SOC - RGB Salient Object Detection benchmarking: S-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: mean E-Measure<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: MAE<BR>  SOD - RGB Salient Object Detection benchmarking: MAE<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: F-measure<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-test - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-test - RGB Salient Object Detection benchmarking: MAE<BR>  ECSSD - RGB Salient Object Detection benchmarking: F-measure<BR>  ECSSD - RGB Salient Object Detection benchmarking: MAE<BR>  HKU-IS - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: MAE<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: F-measure<BR>  SOD - RGB Salient Object Detection benchmarking: F-measure<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  IconArt - Weakly Supervised Object Detection benchmarking: MAP<BR>  PeopleArt - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  nuScenes - 3D Object Detection benchmarking: mAP<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  nuScenes-F - 3D Object Detection benchmarking: AP50<BR>  nuScenes-F - 3D Object Detection benchmarking: AP75<BR>  nuScenes-F - 3D Object Detection benchmarking: AP<BR>  nuScenes-F - 3D Object Detection benchmarking: AR<BR>  nuScenes-F - 3D Object Detection benchmarking: ARI<BR>  nuScenes-F - 3D Object Detection benchmarking: ARm<BR>  nuScenes-F - 3D Object Detection benchmarking: ARs<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP50<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP75<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP<BR>  nuScenes-FB - 3D Object Detection benchmarking: AR<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARI<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARm<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARs<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  nuScenes - 3D Object Detection benchmarking: NDS<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  NYU Depth v2 - 3D Object Detection benchmarking: MAP<BR>  SUN-RGBD - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>\",\"<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Bare MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Heavy MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Partial MR^-2<BR>\",\"<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Large MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Medium MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Small MR^-2<BR>\",\"<BR>task: Object detection: Pedestrian Detection<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Caltech - Pedestrian Detection benchmarking: Reasonable Miss Rate<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Caltech Lanes Cordova - Lane Detection benchmarking: F1<BR>  Caltech Lanes Washington - Lane Detection benchmarking: F1<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  CULane - Lane Detection benchmarking: F1 score<BR>  TuSimple - Lane Detection benchmarking: F1 score<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  BDD100K - Lane Detection benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  COCO 2017 - Object Detection benchmarking: Mean mAP<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: Video Object Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>  ImageNet VID - Video Object Detection benchmarking: runtime (ms)<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cyclists Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  BDD100K - Object Detection benchmarking: mAP-at-0.5<BR>  India Driving Dataset - Object Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  iSAID - Object Detection benchmarking: Average Precision<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP50<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>  COCO minival - Object Detection benchmarking: box AP<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  PeopleArt - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Backpack<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Gender<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Hat<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCS<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCS<BR>\",\"<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  GTSRB - Traffic Sign Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking: MAP<BR>  Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking: MAP<BR>\",\"<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP at-0.5:0.95<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP-at-0.50<BR>\",\"<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  PA-100K - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  PETA - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  RAP - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: Precision<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Unseen)<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  GOT-10k - Visual Object Tracking benchmarking: Average Overlap<BR>  GOT-10k - Visual Object Tracking benchmarking: Success Rate 0.5<BR>  LaSOT - Visual Object Tracking benchmarking: AUC<BR>\",\"<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Unseen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: O (Average of Measures)<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  VOT2017 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  VOT2019 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>  VOT2016 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>\",\"<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>\",\"<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Scan2CAD - 3D Reconstruction benchmarking: Average Accuracy<BR>\",\"<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Data3D\\u2212R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>\",\"<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>\",\"<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Mean Accuracy<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  Sydney Urban Objects - 3D Point Cloud Classification benchmarking: F1<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  INRIA Holidays - Image Retrieval benchmarking: Mean mAP<BR>  NUS-WIDE - Image Retrieval benchmarking: MAP<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  DeepFashion - Image Retrieval benchmarking: Recall-at-20<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  street2shop - topwear - Image Retrieval benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CUB-200-2011 - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>\",\"<BR>task: Other image process: Image Reconstruction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: HP<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: MMD<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: HP<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: MMD<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  FRGC - Image Clustering benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  FRGC - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>  UMist - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  CMU-PIE - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: Accuracy<BR>  YouTube Faces DB - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2012-08<BR>Anchor.<BR>benchmarks:<BR>  Coil-20 - Image Clustering benchmarking: Accuracy<BR>  Coil-20 - Image Clustering benchmarking: NMI<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>  Fashion-MNIST - Image Clustering benchmarking: Accuracy<BR>  Fashion-MNIST - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: Accuracy<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: Accuracy<BR>  coil-100 - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2012-03<BR>Anchor.<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Set12 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: SSIM<BR>  Urban100 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Set12 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ARL Polarimetric Thermal Face Dataset - Image Clustering benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma35 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma75 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma35 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma60 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: SSIM<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: PSNR (sRGB)<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: SSIM (sRGB)<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  CBSD68 sigma50 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma5 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  CBSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma75 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma35 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma50 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma75 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma15 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma25 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma35 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma50 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma75 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Image Reconstruction<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>\",\"<BR>task: Other image process: Image Reconstruction<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Clothes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Clothes - Image Reconstruction benchmarking: LPIPS<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  LetterA-J - Image Clustering benchmarking: Accuracy<BR>  LetterA-J - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Other video process: Video Generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  UCF-101 16 frames, 64x64, Unconditional - Video Generation benchmarking: Inception Score<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video Median Rank<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-10<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-1<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video Median Rank<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video Median Rank<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-10<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-1<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-50<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-5<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video Median Rank<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-10<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-1<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-50<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-50<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Other video process: Video Generation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  BAIR Robot Pushing - Video Generation benchmarking: FVD score<BR>  Kinetics-600 12 frames, 128x128 - Video Generation benchmarking: FID<BR>  Kinetics-600 12 frames, 64x64 - Video Generation benchmarking: FID<BR>  Kinetics-600 12 frames, 64x64 - Video Generation benchmarking: Inception Score<BR>  Kinetics-600 48 frames, 64x64 - Video Generation benchmarking: FID<BR>  Kinetics-600 48 frames, 64x64 - Video Generation benchmarking: Inception Score<BR>\",\"<BR>task: Other video process: Video Frame Interpolation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Middlebury - Video Frame Interpolation benchmarking: Interpolation Error<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>\",\"<BR>task: Other video process: Video Frame Interpolation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  UCF101 - Video Frame Interpolation benchmarking: PSNR<BR>  UCF101 - Video Frame Interpolation benchmarking: SSIM<BR>  Vimeo90k - Video Frame Interpolation benchmarking: SSIM<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: PSNR<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: SSIM<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: tOF<BR>\",\"<BR>task: Other video process: Video Frame Interpolation<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Middlebury - Video Frame Interpolation benchmarking: PSNR<BR>  Middlebury - Video Frame Interpolation benchmarking: SSIM<BR>\",\"<BR>task: Other video process: Video Generation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  TrailerFaces - Video Generation benchmarking: FID<BR>\",\"<BR>task: Other video process: Video Generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text Mean Rank<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text Median Rank<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-1<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  HMDBfull-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Synth Objects-to-LINEMOD - Domain Adaptation benchmarking: Classification Accuracy<BR>  Synth Objects-to-LINEMOD - Domain Adaptation benchmarking: Mean Angle Error<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  TDIUC - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  USPS-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Office-Caltech-10 - Domain Adaptation benchmarking: Accuracy (%)<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking: mIoU<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Generalization<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>  ImageNet-R - Domain Generalization benchmarking: Top-1 Error Rate<BR>\",\"<BR>task: Other vision process: Domain Generalization<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  ImageNet-C - Domain Generalization benchmarking: mean Corruption Error (mCE)<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  SYNSIG-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>  Synth Signs-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Crowd Counting<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech A - Crowd Counting benchmarking: MSE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>  UCF CC 50 - Crowd Counting benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Depth Completion<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  VOID - Depth Completion benchmarking: MAE<BR>  VOID - Depth Completion benchmarking: RMSE<BR>  VOID - Depth Completion benchmarking: iMAE<BR>  VOID - Depth Completion benchmarking: iRMSE<BR>\",\"<BR>task: Other vision process: Depth Completion<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: iMAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: iRMSE<BR>\",\"<BR>task: Other vision process: Depth Completion<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>  KITTI Depth Completion - Depth Completion benchmarking: Runtime [ms]<BR>\",\"<BR>task: Other vision process: Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  IC19-ReCTs - Scene Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: H-Mean<BR>  ICDAR 2015 - Scene Text Detection benchmarking: H-Mean<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: H-Mean<BR>  Total-Text - Scene Text Detection benchmarking: H-Mean<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: TIoU<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: H-Mean<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: H-Mean<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Other vision process: Curved Text Detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Other vision process: Scene Graph Generation<BR>date: 2020-02<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: mean Recall @20<BR>\",\"<BR>task: Other vision process: Scene Graph Generation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>\",\"<BR>task: Other vision process: Scene Graph Generation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  VRD - Scene Graph Generation benchmarking: Recall-at-50<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>\",\"<BR>task: Other vision process: Crowd Counting<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: OOB Rate (10^\\u22123)<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Difference<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Length<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Player Distance<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Step Change (10^\\u22123)<BR>  PEMS-SF - Multivariate Time Series Imputation benchmarking: L2 Loss (10^-4)<BR>\",\"<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  MuJoCo - Multivariate Time Series Imputation benchmarking: MSE (10^2, 50% missing)<BR>\",\"<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2010-11<BR>Anchor.<BR>benchmarks:<BR>  Beijing Air Quality - Multivariate Time Series Imputation benchmarking: MAE (PM2.5)<BR>  KDD CUP Challenge 2018 - Multivariate Time Series Imputation benchmarking: MSE (10% missing)<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: MAE (10% of data as GT)<BR>  UCI localization data - Multivariate Time Series Imputation benchmarking: MAE (10% missing)<BR>\",\"<BR>task: Other vision process: Video Prediction<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - Video Prediction benchmarking: MAE<BR>  Human3.6M - Video Prediction benchmarking: MSE<BR>  Human3.6M - Video Prediction benchmarking: SSIM<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: number<BR>  VizWiz 2018 - Visual Question Answering benchmarking: other<BR>  VizWiz 2018 - Visual Question Answering benchmarking: unanswerable<BR>  VizWiz 2018 - Visual Question Answering benchmarking: yes/no<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  GQA test-dev - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Other vision process: Video Prediction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  CMU Mocap-1 - Video Prediction benchmarking: Test Error<BR>  CMU Mocap-2 - Video Prediction benchmarking: Test Error<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Make3D - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Make3D - Monocular Depth Estimation benchmarking: RMSE<BR>  Make3D - Monocular Depth Estimation benchmarking: Sq Rel<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  KITTI Eigen split unsupervised - Monocular Depth Estimation benchmarking: absolute relative error<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE log<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: SQ Rel<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>\",\"<BR>task: Other vision process: Metric Learning<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>\",\"<BR>task: Other vision process: Metric Learning<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>\",\"<BR>task: Other vision process: Visual Dialog<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: Mean Rank<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Other vision process: Visual Dialog<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: NDCG (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CLEVR - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  100 sleep nights of 8 caregivers - Visual Question Answering benchmarking: 14 gestures accuracy<BR>  HowmanyQA - Visual Question Answering benchmarking: Accuracy<BR>  TallyQA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Object Counting<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>\",\"<BR>task: Other vision process: Object Counting<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE<BR>  COCO count-test - Object Counting benchmarking: mRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: mRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-relRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE<BR>\",\"<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: mse (10^-3)<BR>\",\"<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Eurasian Cities Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  Horizon Lines in the Wild - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  York Urban Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  SIM10K to BDD100K - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Materials Project - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  OQMD v1.2 - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking: AP-at-0.7<BR>\",\"<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: ATV<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: AUC<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: MSE<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-Olympic - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: PCK3D<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking: MPJAE<BR>  3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking: MPJPE<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  CHALL H80K - 3D Human Pose Estimation benchmarking: MPJPE<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>  3DPW - 3D Human Pose Estimation benchmarking: PA-MPJPE<BR>  3DPW - 3D Human Pose Estimation benchmarking: acceleration error<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: 3DPCK<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: MJPE<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2010-03<BR>Anchor.<BR>benchmarks:<BR>  HumanEva-I - 3D Human Pose Estimation benchmarking: Mean Reconstruction Error (mm)<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGB benchmarking: Mean Recall<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGB benchmarking: Recall (VSD)<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean IoU<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: IoU-2D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: IoU-3D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: VSS-2D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: VSS-3D<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADI<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 10cm<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 5cm<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-25<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-50<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 5, 5cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 10cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 5cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-25<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-50<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 5, 5cm<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGBD benchmarking: Mean Recall<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: MPJPE (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: MPJPE (CS)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CS)<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Total Capture - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean AUC<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation benchmarking: Accuracy (ADD)<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean IoU<BR>  OCCLUSION - 6D Pose Estimation using RGB benchmarking: MAP<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: 5\\u00b05 cm<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: IOU25<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: Rerr<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: Terr<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPVPE<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADI<BR>\",\"<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Frames Per View<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Views<BR>\",\"<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>  BJUT-3D - Head Pose Estimation benchmarking: MAE<BR>  Pointing'04 - Head Pose Estimation benchmarking: MAE<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  FLIC Elbows - Pose Estimation benchmarking: PCK-at-0.2<BR>  FLIC Wrists - Pose Estimation benchmarking: PCK-at-0.2<BR>  J-HMDB - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>  ITOP top-view - Pose Estimation benchmarking: Mean mAP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  UAV-Human - Pose Estimation benchmarking: mAP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  UPenn Action - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  DensePose-COCO - Pose Estimation benchmarking: AP<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: AP<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: FPS<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Pose Estimation benchmarking: AP<BR>\",\"<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>  BIWI - Head Pose Estimation benchmarking: MAE (trained with other data)<BR>\",\"<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>  MSRA Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>  NYU Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>\",\"<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  HANDS 2017 - Hand Pose Estimation benchmarking: Average 3D Error<BR>\",\"<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  BIWI - Head Pose Estimation benchmarking: MAE (trained with BIWI data)<BR>\",\"<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: FPS<BR>  K2HPD - Hand Pose Estimation benchmarking: PDJ@5mm<BR>  NYU Hands - Hand Pose Estimation benchmarking: FPS<BR>\",\"<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  HANDS 2019 - Hand Pose Estimation benchmarking: Average 3D Error<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Multi-Person PoseTrack - Pose Tracking benchmarking: MOTA<BR>  Multi-Person PoseTrack - Pose Tracking benchmarking: MOTP<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  PoseTrack2018 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2018 - Pose Tracking benchmarking: mAP<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: VInfo<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: VRand<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: Dice Score<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: MSD<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: VS<BR>  HSVM - Medical Image Segmentation benchmarking: Dice Score<BR>  HSVM - Medical Image Segmentation benchmarking: MSD<BR>  HSVM - Medical Image Segmentation benchmarking: VS<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Medical Image Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Dice<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  MHP v2.0 - Human Part Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  2018 Data Science Bowl - Medical Image Segmentation benchmarking: Dice<BR>  2018 Data Science Bowl - Medical Image Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Cell - Medical Image Segmentation benchmarking: IoU<BR>  EM - Medical Image Segmentation benchmarking: IoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>  NYU Depth v2 - Instance Segmentation benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  iSAID - Instance Segmentation benchmarking: Average Precision<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  LVIS v1.0 - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mAcc<BR>  S3DIS - 3D Instance Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  ISLES-2015 - Lesion Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ISIC 2017 - Lesion Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BUS 2017 Dataset B - Lesion Segmentation benchmarking: Dice Score<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  iSEG 2017 Challenge - Medical Image Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: F1-Score<BR>\",\"<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  NIH - Lung Nodule Segmentation benchmarking: AVD<BR>  NIH - Lung Nodule Segmentation benchmarking: Dice Score<BR>  NIH - Lung Nodule Segmentation benchmarking: Precision<BR>  NIH - Lung Nodule Segmentation benchmarking: Recall<BR>  NIH - Lung Nodule Segmentation benchmarking: VS<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>  S3DIS - 3D Instance Segmentation benchmarking: mRec<BR>  ScanNet - 3D Instance Segmentation benchmarking: mAP<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  SceneNN - 3D Instance Segmentation benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mCov<BR>  S3DIS - 3D Instance Segmentation benchmarking: mWCov<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  ScanNetV1 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - 3D Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: mIoU<BR>  Montgomery County - Lung Nodule Segmentation benchmarking: Accuracy<BR>  Montgomery County - Lung Nodule Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CVC-ClinicDB - Medical Image Segmentation benchmarking: mean Dice<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: Warping Error<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: Average MAE<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: S-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: max E-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: mean Dice<BR>  RITE - Medical Image Segmentation benchmarking: Dice<BR>  RITE - Medical Image Segmentation benchmarking: Jaccard Index<BR>\",\"<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  LIDC-IDRI - Lung Nodule Segmentation benchmarking: Dice<BR>  LIDC-IDRI - Lung Nodule Segmentation benchmarking: IoU<BR>  Lung Nodule  - Lung Nodule Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CT-150 - Pancreas Segmentation benchmarking: Dice Score<BR>  CT-150 - Pancreas Segmentation benchmarking: Precision<BR>  CT-150 - Pancreas Segmentation benchmarking: Recall<BR>  TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD - Scene Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: 3DIoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>  SkyScapes-Lane - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Kvasir-Instrument - Semantic Segmentation benchmarking: DSC<BR>  Kvasir-Instrument - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Global Accuracy<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  HRF - Retinal Vessel Segmentation benchmarking: AUC<BR>  HRF - Retinal Vessel Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  NYU Depth v2 - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2011 - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  BDD - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Category mIoU<BR>  GTAV-to-Cityscapes Labels - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ParisLille3D - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: Pixel Accuracy<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Semantic Segmentation - Semantic Segmentation benchmarking: Mean IoU (class)<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Freiburg Forest - Semantic Segmentation benchmarking: Mean IoU<BR>  SUN-RGBD - Semantic Segmentation benchmarking: Mean IoU<BR>  SYNTHIA-CVPR\\u201916 - Semantic Segmentation benchmarking: Mean IoU<BR>  ScanNetV2 - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: oAcc<BR>  Semantic3D - Semantic Segmentation benchmarking: oAcc<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: AUC<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: Frame (fps)<BR>\",\"<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes test - Panoptic Segmentation benchmarking: PQ<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: AP<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQst<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQth<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO panoptic - Panoptic Segmentation benchmarking: PQ<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Mapillary val - Panoptic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: Total Variation of Information<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Merge<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Split<BR>\",\"<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: AUC<BR>\",\"<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2015 - Brain Tumor Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2014 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2017 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: MSD<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: VS<BR>  BRATS 2018 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP50<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP75<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APL<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  PH2 - Skin Cancer Segmentation benchmarking: IoU<BR>\",\"<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2013 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2013 leaderboard - Brain Tumor Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: Average Accuracy<BR>\"],\"marker\":{\"line\":{\"width\":1,\"color\":\"black\"},\"size\":20,\"symbol\":42},\"mode\":\"markers\",\"x\":[\"2016-09\",\"2020-01\",\"2016-02\",\"2018-06\",\"2019-03\",\"2018-04\",\"2017-05\",\"2017-03\",\"2016-04\",\"2016-01\",\"2015-11\",\"2015-06\",\"2019-04\",\"2017-03\",\"2016-12\",\"2015-07\",\"2018-06\",\"2017-07\",\"2019-11\",\"2019-08\",\"2018-07\",\"2017-12\",\"2017-03\",\"2012-07\",\"2017-11\",\"2017-08\",\"2017-06\",\"2018-06\",\"2014-06\",\"2016-08\",\"2017-12\",\"2017-03\",\"2017-12\",\"2018-12\",\"2019-04\",\"2019-06\",\"2019-08\",\"2018-07\",\"2018-01\",\"2015-11\",\"2014-03\",\"2012-12\",\"2014-06\",\"2015-11\",\"2015-12\",\"2017-05\",\"2018-01\",\"2019-05\",\"2019-04\",\"2017-04\",\"2015-11\",\"2012-10\",\"2014-06\",\"2014-11\",\"2016-04\",\"2016-06\",\"2016-12\",\"2017-04\",\"2017-10\",\"2018-01\",\"2018-02\",\"2019-01\",\"2019-06\",\"2019-07\",\"2014-11\",\"2018-12\",\"2016-11\",\"2018-11\",\"2019-09\",\"2019-01\",\"2020-04\",\"2013-06\",\"2018-12\",\"2017-05\",\"2019-09\",\"2017-07\",\"2019-03\",\"2016-10\",\"2017-01\",\"2017-08\",\"2015-11\",\"2015-06\",\"2014-06\",\"2017-10\",\"2018-08\",\"2018-05\",\"2016-09\",\"2019-02\",\"2019-05\",\"2015-11\",\"2017-08\",\"2018-03\",\"2018-04\",\"2016-07\",\"2016-04\",\"2017-03\",\"2017-08\",\"2014-04\",\"2014-12\",\"2015-03\",\"2015-07\",\"2015-11\",\"2016-04\",\"2017-08\",\"2017-10\",\"2019-03\",\"2019-07\",\"2015-03\",\"2019-08\",\"2018-12\",\"2018-04\",\"2018-02\",\"2017-09\",\"2017-06\",\"2013-07\",\"2015-05\",\"2014-08\",\"2015-09\",\"2017-05\",\"2014-08\",\"2014-06\",\"2018-04\",\"2019-01\",\"2019-07\",\"2017-05\",\"2017-01\",\"2017-07\",\"2013-03\",\"2017-11\",\"2018-06\",\"2015-02\",\"2016-12\",\"2019-02\",\"2019-04\",\"2019-01\",\"2018-10\",\"2018-05\",\"2017-11\",\"2017-10\",\"2019-12\",\"2017-08\",\"2016-03\",\"2013-12\",\"2013-06\",\"2013-01\",\"2012-12\",\"2012-02\",\"2017-07\",\"2019-06\",\"2019-10\",\"2018-07\",\"2018-03\",\"2015-09\",\"2015-04\",\"2018-02\",\"2015-12\",\"2015-11\",\"2016-10\",\"2015-11\",\"2019-10\",\"2019-10\",\"2019-04\",\"2017-12\",\"2017-05\",\"2019-12\",\"2019-11\",\"2018-02\",\"2019-05\",\"2019-03\",\"2014-10\",\"2016-01\",\"2016-06\",\"2018-12\",\"2018-11\",\"2017-03\",\"2017-06\",\"2018-09\",\"2018-07\",\"2017-09\",\"2017-10\",\"2018-02\",\"2017-11\",\"2016-11\",\"2016-11\",\"2014-07\",\"2015-11\",\"2016-09\",\"2014-06\",\"2016-11\",\"2015-05\",\"2015-06\",\"2016-03\",\"2018-03\",\"2017-07\",\"2017-08\",\"2018-06\",\"2019-04\",\"2018-10\",\"2014-03\",\"2015-11\",\"2016-11\",\"2016-08\",\"2019-08\",\"2019-05\",\"2019-04\",\"2019-03\",\"2017-11\",\"2016-11\",\"2015-11\",\"2017-11\",\"2017-02\",\"2014-11\",\"2015-04\",\"2017-08\",\"2017-12\",\"2019-08\",\"2019-12\",\"2015-12\",\"2017-03\",\"2017-11\",\"2017-12\",\"2019-07\",\"2019-09\",\"2017-03\",\"2016-12\",\"2016-10\",\"2015-12\",\"2018-12\",\"2015-12\",\"2012-02\",\"2018-06\",\"2019-04\",\"2017-09\",\"2019-09\",\"2016-06\",\"2017-06\",\"2018-11\",\"2015-04\",\"2015-12\",\"2016-11\",\"2019-01\",\"2019-07\",\"2017-04\",\"2016-10\",\"2016-03\",\"2016-04\",\"2018-03\",\"2016-04\",\"2016-12\",\"2016-04\",\"2019-02\",\"2019-08\",\"2016-11\",\"2019-01\",\"2018-11\",\"2018-04\",\"2017-06\",\"2016-04\",\"2019-03\",\"2015-04\",\"2017-04\",\"2017-03\",\"2016-04\",\"2015-11\",\"2013-12\",\"2012-08\",\"2012-03\",\"2019-08\",\"2018-10\",\"2018-05\",\"2018-04\",\"2017-10\",\"2016-06\",\"2015-08\",\"2015-12\",\"2015-08\",\"2016-08\",\"2016-11\",\"2017-04\",\"2017-10\",\"2015-11\",\"2017-04\",\"2018-10\",\"2016-08\",\"2016-11\",\"2019-10\",\"2018-12\",\"2014-12\",\"2019-12\",\"2015-06\",\"2016-09\",\"2016-10\",\"2018-08\",\"2019-07\",\"2019-07\",\"2017-08\",\"2019-04\",\"2020-03\",\"2019-04\",\"2016-09\",\"2018-06\",\"2016-05\",\"2016-08\",\"2019-02\",\"2017-05\",\"2018-07\",\"2018-11\",\"2019-04\",\"2015-12\",\"2018-11\",\"2012-12\",\"2017-11\",\"2017-02\",\"2015-02\",\"2017-07\",\"2019-05\",\"2019-02\",\"2017-08\",\"2015-08\",\"2014-09\",\"2019-06\",\"2019-04\",\"2018-11\",\"2018-06\",\"2018-01\",\"2017-09\",\"2017-04\",\"2017-03\",\"2016-06\",\"2016-04\",\"2014-12\",\"2015-05\",\"2020-02\",\"2017-07\",\"2016-07\",\"2014-12\",\"2015-12\",\"2015-11\",\"2018-03\",\"2018-01\",\"2016-06\",\"2010-11\",\"2017-12\",\"2019-08\",\"2019-07\",\"2019-04\",\"2019-05\",\"2018-06\",\"2018-03\",\"2016-09\",\"2014-11\",\"2017-06\",\"2016-10\",\"2016-05\",\"2017-04\",\"2015-11\",\"2016-06\",\"2016-12\",\"2017-04\",\"2017-07\",\"2018-08\",\"2018-10\",\"2015-06\",\"2016-04\",\"2018-06\",\"2016-04\",\"2018-12\",\"2017-02\",\"2018-06\",\"2019-05\",\"2019-04\",\"2019-05\",\"2019-07\",\"2014-06\",\"2020-04\",\"2019-07\",\"2018-09\",\"2017-12\",\"2017-05\",\"2010-03\",\"2013-12\",\"2019-02\",\"2019-08\",\"2017-11\",\"2017-11\",\"2018-03\",\"2019-01\",\"2019-02\",\"2017-04\",\"2016-01\",\"2018-12\",\"2019-01\",\"2017-11\",\"2017-03\",\"2019-10\",\"2019-09\",\"2018-03\",\"2017-01\",\"2016-11\",\"2016-01\",\"2016-03\",\"2016-11\",\"2016-12\",\"2017-12\",\"2018-02\",\"2019-01\",\"2017-01\",\"2016-12\",\"2016-11\",\"2016-05\",\"2019-01\",\"2015-11\",\"2017-02\",\"2017-08\",\"2017-10\",\"2019-08\",\"2020-01\",\"2014-07\",\"2014-11\",\"2016-11\",\"2018-04\",\"2017-10\",\"2019-03\",\"2019-06\",\"2019-08\",\"2014-11\",\"2018-11\",\"2018-08\",\"2015-11\",\"2016-03\",\"2018-07\",\"2019-12\",\"2017-06\",\"2016-05\",\"2015-12\",\"2016-04\",\"2016-11\",\"2017-03\",\"2017-11\",\"2018-03\",\"2019-12\",\"2018-01\",\"2016-03\",\"2017-03\",\"2018-10\",\"2018-04\",\"2019-08\",\"2015-05\",\"2019-03\",\"2019-04\",\"2019-02\",\"2019-04\",\"2019-12\",\"2016-06\",\"2016-12\",\"2017-11\",\"2016-12\",\"2019-07\",\"2019-07\",\"2015-05\",\"2019-08\",\"2015-05\",\"2019-03\",\"2019-07\",\"2014-12\",\"2019-04\",\"2014-07\",\"2014-11\",\"2014-12\",\"2015-05\",\"2015-11\",\"2015-12\",\"2018-06\",\"2016-03\",\"2016-11\",\"2016-12\",\"2017-02\",\"2019-12\",\"2019-04\",\"2019-01\",\"2018-12\",\"2018-08\",\"2017-11\",\"2017-07\",\"2016-06\",\"2015-05\",\"2020-01\",\"2015-05\",\"2017-03\",\"2017-03\",\"2017-04\",\"2018-01\",\"2018-08\",\"2018-09\",\"2019-01\",\"2019-09\",\"2019-06\",\"2015-05\",\"2016-03\",\"2017-09\",\"2018-10\",\"2019-06\",\"2019-04\",\"2019-11\",\"2015-05\",\"2016-12\"],\"y\":[\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Action Segmentation\",\"Action localization: Action Segmentation\",\"Action localization: Action Segmentation\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Action localization: Temporal Action Localization\",\"Activity detection: Action Detection\",\"Activity detection: Action Detection\",\"Activity detection: Action Detection\",\"Activity detection: Action Detection\",\"Activity localization: Temporal Action Proposal Generation\",\"Activity localization: Temporal Action Proposal Generation\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Classification\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Group Activity Recognition\",\"Activity recognition: Group Activity Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Human Interaction Recognition\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Human Interaction Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Recognition\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Face Detection\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Image classification: Document Image Classification\",\"Image classification: Document Image Classification\",\"Image classification: Hyperspectral Image Classification\",\"Image classification: Hyperspectral Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Unsupervised Image Classification\",\"Image classification: Sequential Image Classification\",\"Image classification: Satellite Image Classification\",\"Image classification: Sequential Image Classification\",\"Image classification: Unsupervised Image Classification\",\"Image classification: Retinal OCT Disease Classification\",\"Image classification: Unsupervised Image Classification\",\"Image generation: Conditional Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Image Generation\",\"Image generation: Pose Transfer\",\"Image generation: Pose Transfer\",\"Image generation: Pose Transfer\",\"Image generation: Pose Transfer\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Conditional Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image generation: Image Generation\",\"Image-to-image translation: Fundus to Angiography Generation\",\"Image-to-image translation: Fundus to Angiography Generation\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Dense Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: 3D Object Detection\",\"Object detection: Pedestrian Detection\",\"Object detection: Pedestrian Detection\",\"Object detection: Pedestrian Detection\",\"Object detection: Lane Detection\",\"Object detection: Lane Detection\",\"Object detection: Lane Detection\",\"Object detection: Lane Detection\",\"Object detection: Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Video Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: Object Detection\",\"Object detection: 3D Object Detection\",\"Object recognition: Pedestrian Attribute Recognition\",\"Object recognition: Traffic Sign Recognition\",\"Object recognition: Traffic Sign Recognition\",\"Object recognition: Traffic Sign Recognition\",\"Object recognition: Pedestrian Attribute Recognition\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Multiple Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Other 3D task: 3D Shape Classification\",\"Other 3D task: 3D Reconstruction\",\"Other 3D task: 3D Reconstruction\",\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\",\"Other 3D task: 3D Object Reconstruction\",\"Other 3D task: 3D Point Cloud Classification\",\"Other 3D task: 3D Point Cloud Classification\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Retrieval\",\"Other image process: Image Reconstruction\",\"Other image process: Image Retrieval\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Image Clustering\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Image Clustering\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Aesthetics Quality Assessment\",\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\",\"Other image process: Color Image Denoising\",\"Other image process: Image Retrieval\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Image Clustering\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Image Reconstruction\",\"Other image process: Image Reconstruction\",\"Other image process: Image Clustering\",\"Other image process: Image Retrieval\",\"Other video process: Video Generation\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Retrieval\",\"Other video process: Video Generation\",\"Other video process: Video Frame Interpolation\",\"Other video process: Video Frame Interpolation\",\"Other video process: Video Frame Interpolation\",\"Other video process: Video Generation\",\"Other video process: Video Generation\",\"Other video process: Video Retrieval\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Visual Question Answering\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Generalization\",\"Other vision process: Domain Generalization\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Crowd Counting\",\"Other vision process: Depth Completion\",\"Other vision process: Depth Completion\",\"Other vision process: Depth Completion\",\"Other vision process: Denoising\",\"Other vision process: Domain Adaptation\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Scene Text Detection\",\"Other vision process: Curved Text Detection\",\"Other vision process: Scene Graph Generation\",\"Other vision process: Scene Graph Generation\",\"Other vision process: Scene Graph Generation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Crowd Counting\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Multivariate Time Series Imputation\",\"Other vision process: Multivariate Time Series Imputation\",\"Other vision process: Multivariate Time Series Imputation\",\"Other vision process: Video Prediction\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Video Prediction\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Metric Learning\",\"Other vision process: Metric Learning\",\"Other vision process: Visual Dialog\",\"Other vision process: Visual Dialog\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Question Answering\",\"Other vision process: Object Counting\",\"Other vision process: Object Counting\",\"Other vision process: Multivariate Time Series Imputation\",\"Other vision process: Horizon Line Estimation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Formation Energy\",\"Other vision process: Formation Energy\",\"Other vision process: Formation Energy\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Horizon Line Estimation\",\"Other vision process: Domain Adaptation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Hand Pose Estimation\",\"Pose estimation: Hand Pose Estimation\",\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Hand Pose Estimation\",\"Pose estimation: Hand Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose tracking: Pose Tracking\",\"Pose tracking: Pose Tracking\",\"Pose tracking: Pose Tracking\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Multi-tissue Nucleus Segmentation\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Nuclear Segmentation\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Video Semantic Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: Lesion Segmentation\",\"Semantic segmentation: Lesion Segmentation\",\"Semantic segmentation: Lesion Segmentation\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Lesion Segmentation\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Semantic Instance Segmentation\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: Pancreas Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Scene Segmentation\",\"Semantic segmentation: Scene Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Real-time Instance Segmentation\",\"Semantic segmentation: Skin Cancer Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Semantic segmentation: Real-time Instance Segmentation\",\"Semantic segmentation: Skin Cancer Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Semantic segmentation: Scene Segmentation\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}},{\"hovertemplate\":[\"<BR>task: Other image process: Image Clustering<BR>date: 2012-08<BR>ratio: 0.5546<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2012-12<BR>ratio: 0.0189<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2013-02<BR>ratio: 0.1073<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2013-02<BR>ratio: 1.0<BR>benchmarks:<BR>  CAD-120 - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2013-07<BR>ratio: 1.0<BR>benchmarks:<BR>  FER2013 - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2013-11<BR>ratio: 0.0397<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2013-12<BR>ratio: 0.0553<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2014-04<BR>ratio: 0.021<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2014-06<BR>ratio: 0.8122<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-06<BR>ratio: 0.4737<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2014-06<BR>ratio: 0.289<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2014-09<BR>ratio: 0.1446<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-09<BR>ratio: 0.9752<BR>benchmarks:<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-Olympic - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-11<BR>ratio: 0.2834<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Object detection: Pedestrian Detection<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Caltech - Pedestrian Detection benchmarking: Reasonable Miss Rate<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2014-12<BR>ratio: 0.2375<BR>benchmarks:<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-12<BR>ratio: 0.2513<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2014-12<BR>ratio: 0.1392<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-02<BR>ratio: 0.0877<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2015-02<BR>ratio: 0.8858<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-02<BR>ratio: 0.0451<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-02<BR>ratio: 0.3958<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2015-02<BR>ratio: 0.0134<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2015-03<BR>ratio: 0.3753<BR>benchmarks:<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-03<BR>ratio: 0.2327<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-03<BR>ratio: 0.0741<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2015-04<BR>ratio: 0.426<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2015-04<BR>ratio: 0.1007<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-04<BR>ratio: 0.2878<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2015-05<BR>ratio: 0.8426<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-05<BR>ratio: 0.3991<BR>benchmarks:<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Curved Text Detection<BR>date: 2015-05<BR>ratio: 0.2321<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2015-05<BR>ratio: 0.1675<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2015-05<BR>ratio: 0.1142<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2015-06<BR>ratio: 0.4459<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2015-06<BR>ratio: 0.2457<BR>benchmarks:<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-08<BR>ratio: 0.4321<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-09<BR>ratio: 0.5<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2015-11<BR>ratio: 0.117<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Sequential Image Classification<BR>date: 2015-11<BR>ratio: 0.4701<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-11<BR>ratio: 0.0854<BR>benchmarks:<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2015-11<BR>ratio: 1.0<BR>benchmarks:<BR>  RITE - Medical Image Segmentation benchmarking: Jaccard Index<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2015-11<BR>ratio: 0.1888<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>\",\"<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2015-11<BR>ratio: 0.3222<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2015-11<BR>ratio: 0.0331<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2015-11<BR>ratio: 0.15<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-11<BR>ratio: 0.1785<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-11<BR>ratio: 0.4339<BR>benchmarks:<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2015-11<BR>ratio: 0.1637<BR>benchmarks:<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2015-12<BR>ratio: 0.0779<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Crowd Counting<BR>date: 2015-12<BR>ratio: 0.7477<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>\",\"<BR>task: Image classification: Satellite Image Classification<BR>date: 2015-12<BR>ratio: 0.4051<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2015-12<BR>ratio: 1.0<BR>benchmarks:<BR>  ActivityNet - Action Recognition benchmarking: mAP<BR>\",\"<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2015-12<BR>ratio: 0.9509<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2015-12<BR>ratio: 0.5016<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2016-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Binarized MNIST - Image Generation benchmarking: nats<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2016-01<BR>ratio: 0.0707<BR>benchmarks:<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-01<BR>ratio: 0.6524<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-01<BR>ratio: 0.0478<BR>benchmarks:<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2016-02<BR>ratio: 0.056<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2016-03<BR>ratio: 0.0258<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2016-03<BR>ratio: 1.0<BR>benchmarks:<BR>  MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-03<BR>ratio: 0.3359<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2016-03<BR>ratio: 0.2119<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Image classification: Sequential Image Classification<BR>date: 2016-03<BR>ratio: 0.4252<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2016-03<BR>ratio: 0.5954<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-03<BR>ratio: 0.7322<BR>benchmarks:<BR>  FLIC Elbows - Pose Estimation benchmarking: PCK-at-0.2<BR>  FLIC Wrists - Pose Estimation benchmarking: PCK-at-0.2<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-03<BR>ratio: 0.1471<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2016-04<BR>ratio: 0.4868<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2016-04<BR>ratio: 0.4832<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-04<BR>ratio: 0.4271<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2016-04<BR>ratio: 0.6898<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: NMI<BR>  Coil-20 - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2016-04<BR>ratio: 0.0092<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Other vision process: Object Counting<BR>date: 2016-04<BR>ratio: 0.8889<BR>benchmarks:<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE<BR>  COCO count-test - Object Counting benchmarking: mRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: mRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-relRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-04<BR>ratio: 0.3005<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-05<BR>ratio: 0.0418<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2016-05<BR>ratio: 0.1069<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-05<BR>ratio: 0.072<BR>benchmarks:<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-06<BR>ratio: 0.0741<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-06<BR>ratio: 0.572<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-06<BR>ratio: 0.3146<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-06<BR>ratio: 0.061<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2016-06<BR>ratio: 0.3235<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2016-06<BR>ratio: 0.3625<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2016-07<BR>ratio: 0.0241<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2016-08<BR>ratio: 0.8371<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Eurasian Cities Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  York Urban Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-08<BR>ratio: 0.1233<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-08<BR>ratio: 0.2999<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-08<BR>ratio: 0.6547<BR>benchmarks:<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>  Synth Signs-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Backpack<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Gender<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Hat<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCS<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCS<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2016-08<BR>ratio: 0.0251<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Other vision process: Crowd Counting<BR>date: 2016-08<BR>ratio: 0.2523<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-08<BR>ratio: 0.5511<BR>benchmarks:<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2016-08<BR>ratio: 0.1736<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2016-08<BR>ratio: 0.0915<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-09<BR>ratio: 0.0144<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-09<BR>ratio: 0.3082<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>\",\"<BR>task: Other vision process: Object Counting<BR>date: 2016-09<BR>ratio: 0.4115<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-09<BR>ratio: 0.2784<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-09<BR>ratio: 0.0093<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2016-10<BR>ratio: 0.0142<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-10<BR>ratio: 0.0606<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2016-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2016-11<BR>ratio: 0.3326<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>\",\"<BR>task: Action localization: Action Segmentation<BR>date: 2016-11<BR>ratio: 0.3571<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-11<BR>ratio: 0.0<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Metric Learning<BR>date: 2016-11<BR>ratio: 0.2824<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>\",\"<BR>task: Other video process: Video Generation<BR>date: 2016-11<BR>ratio: 0.2492<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2016-11<BR>ratio: 0.2441<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-11<BR>ratio: 0.096<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-11<BR>ratio: 0.3399<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-11<BR>ratio: 0.2329<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-11<BR>ratio: 0.2215<BR>benchmarks:<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-11<BR>ratio: 0.5084<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-11<BR>ratio: 0.6006<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-11<BR>ratio: 0.674<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-11<BR>ratio: 0.2304<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Global Accuracy<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-12<BR>ratio: 0.6019<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-12<BR>ratio: 0.3137<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2016-12<BR>ratio: 0.1022<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-12<BR>ratio: 0.5259<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>\",\"<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2016-12<BR>ratio: 0.4898<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2016-12<BR>ratio: 0.1001<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>\",\"<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2016-12<BR>ratio: 0.5647<BR>benchmarks:<BR>  Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>\",\"<BR>task: Object detection: Dense Object Detection<BR>date: 2016-12<BR>ratio: 0.1125<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-12<BR>ratio: 0.1929<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2016-12<BR>ratio: 0.0388<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-12<BR>ratio: 0.1288<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2016-12<BR>ratio: 0.1429<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>\",\"<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-12<BR>ratio: 0.9756<BR>benchmarks:<BR>  Data3D\\u2212R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-01<BR>ratio: 0.1328<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-01<BR>ratio: 0.2025<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>\",\"<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2017-02<BR>ratio: 0.6995<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-02<BR>ratio: 0.0692<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-02<BR>ratio: 0.0541<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-02<BR>ratio: 0.9524<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Medium MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Small MR^-2<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2017-03<BR>ratio: 0.5645<BR>benchmarks:<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2017-03<BR>ratio: 0.1574<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-03<BR>ratio: 0.3612<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2017-03<BR>ratio: 0.1565<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-03<BR>ratio: 0.2235<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2017-03<BR>ratio: 0.0303<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2017-03<BR>ratio: 0.6528<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-03<BR>ratio: 0.5668<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-03<BR>ratio: 0.4081<BR>benchmarks:<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-03<BR>ratio: 0.2484<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-03<BR>ratio: 0.1096<BR>benchmarks:<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-03<BR>ratio: 0.2099<BR>benchmarks:<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2017-03<BR>ratio: 0.2205<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-03<BR>ratio: 0.1193<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-03<BR>ratio: 0.5877<BR>benchmarks:<BR>  SBU - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-04<BR>ratio: 0.2105<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-04<BR>ratio: 0.0331<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2017-04<BR>ratio: 0.4378<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  FRGC - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-04<BR>ratio: 0.2112<BR>benchmarks:<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2017-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-04<BR>ratio: 0.2041<BR>benchmarks:<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-04<BR>ratio: 0.1026<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2017-04<BR>ratio: 0.3846<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-04<BR>ratio: 0.7172<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-04<BR>ratio: 0.103<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2017-04<BR>ratio: 0.1968<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2017-04<BR>ratio: 0.1629<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-04<BR>ratio: 0.2194<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2017-04<BR>ratio: 0.1332<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2017-04<BR>ratio: 0.8514<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  Sydney Urban Objects - 3D Point Cloud Classification benchmarking: F1<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-04<BR>ratio: 0.1767<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>\",\"<BR>task: Image classification: Document Image Classification<BR>date: 2017-04<BR>ratio: 0.4855<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2017-04<BR>ratio: 1.0<BR>benchmarks:<BR>  BSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-04<BR>ratio: 0.4565<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-04<BR>ratio: 0.1679<BR>benchmarks:<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-04<BR>ratio: 0.3172<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-04<BR>ratio: 0.0758<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>\",\"<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2017-04<BR>ratio: 0.4474<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2017-05<BR>ratio: 0.4983<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-05<BR>ratio: 0.528<BR>benchmarks:<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-05<BR>ratio: 0.2366<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Other vision process: Denoising<BR>date: 2017-05<BR>ratio: 0.7853<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-05<BR>ratio: 0.4284<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CS)<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-05<BR>ratio: 0.8352<BR>benchmarks:<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-05<BR>ratio: 0.2561<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>  UCF101-24 - Temporal Action Localization benchmarking: Frame-mAP<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-05<BR>ratio: 0.0559<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-05<BR>ratio: 0.0607<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-05<BR>ratio: 0.2867<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>\",\"<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2017-06<BR>ratio: 0.6277<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-06<BR>ratio: 0.2514<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-06<BR>ratio: 0.6175<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Seen)<BR>\",\"<BR>task: Other vision process: Metric Learning<BR>date: 2017-06<BR>ratio: 0.3412<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2017-06<BR>ratio: 0.1244<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2017-06<BR>ratio: 0.4844<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2017-06<BR>ratio: 0.4142<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2017-06<BR>ratio: 0.5909<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-06<BR>ratio: 0.4898<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-06<BR>ratio: 0.6641<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-06<BR>ratio: 0.1161<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>\",\"<BR>task: Other image process: Image Reconstruction<BR>date: 2017-06<BR>ratio: 0.7104<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Visual Genome - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-07<BR>ratio: 0.02<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-07<BR>ratio: 0.6614<BR>benchmarks:<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-07<BR>ratio: 0.0095<BR>benchmarks:<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>\",\"<BR>task: Other video process: Video Generation<BR>date: 2017-07<BR>ratio: 0.0387<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2017-07<BR>ratio: 0.4581<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video Median Rank<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-07<BR>ratio: 0.2874<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Other vision process: Object Counting<BR>date: 2017-07<BR>ratio: 0.1758<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>\",\"<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  NYU Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-07<BR>ratio: 0.3268<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-07<BR>ratio: 0.6997<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>  WFLW - Face Alignment benchmarking: ME (%, all)<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-07<BR>ratio: 0.0363<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Generalization<BR>date: 2017-08<BR>ratio: 0.0645<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>\",\"<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2017-08<BR>ratio: 0.2688<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-08<BR>ratio: 0.5913<BR>benchmarks:<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-08<BR>ratio: 0.0083<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-08<BR>ratio: 1.0<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-08<BR>ratio: 0.219<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2017-08<BR>ratio: 0.0624<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-08<BR>ratio: 0.88<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-08<BR>ratio: 0.3456<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-08<BR>ratio: 0.0951<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-08<BR>ratio: 0.1108<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2017-08<BR>ratio: 0.4921<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Object detection: Dense Object Detection<BR>date: 2017-08<BR>ratio: 0.6932<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-08<BR>ratio: 0.5449<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  SOC - RGB Salient Object Detection benchmarking: Average MAE<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2017-09<BR>ratio: 0.8514<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2017-09<BR>ratio: 0.0551<BR>benchmarks:<BR>  FDDB - Face Detection benchmarking: AP<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2017-09<BR>ratio: 0.1598<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-09<BR>ratio: 0.474<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Other vision process: Visual Dialog<BR>date: 2017-09<BR>ratio: 0.3904<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-09<BR>ratio: 0.0103<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Total Capture - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>\",\"<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2017-09<BR>ratio: 0.0758<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-09<BR>ratio: 0.0809<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2017-10<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-10<BR>ratio: 0.6429<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2017-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Caltech Lanes Cordova - Lane Detection benchmarking: F1<BR>  Caltech Lanes Washington - Lane Detection benchmarking: F1<BR>\",\"<BR>task: Image classification: Sequential Image Classification<BR>date: 2017-10<BR>ratio: 0.0909<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-10<BR>ratio: 0.9952<BR>benchmarks:<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  UCF - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-10<BR>ratio: 0.4958<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2017-10<BR>ratio: 0.2127<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-10<BR>ratio: 0.9847<BR>benchmarks:<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2017-10<BR>ratio: 0.4337<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-10<BR>ratio: 0.2492<BR>benchmarks:<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-10<BR>ratio: 0.2941<BR>benchmarks:<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other vision process: Domain Generalization<BR>date: 2017-10<BR>ratio: 0.7097<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2017-10<BR>ratio: 0.5297<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2017-10<BR>ratio: 0.656<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>\",\"<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2017-11<BR>ratio: 0.2656<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-11<BR>ratio: 0.6273<BR>benchmarks:<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>  ShapeNet - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-11<BR>ratio: 0.435<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-11<BR>ratio: 0.2438<BR>benchmarks:<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-11<BR>ratio: 0.619<BR>benchmarks:<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2017-11<BR>ratio: 0.5452<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2017-11<BR>ratio: 0.1491<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-11<BR>ratio: 0.6998<BR>benchmarks:<BR>  KITTI Cars Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2017-11<BR>ratio: 0.789<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2017-11<BR>ratio: 0.1976<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP50<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2017-11<BR>ratio: 0.5062<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2017-11<BR>ratio: 0.6503<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CS<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-11<BR>ratio: 0.3521<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2017-11<BR>ratio: 0.1702<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-11<BR>ratio: 0.3953<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>  ITOP top-view - Pose Estimation benchmarking: Mean mAP<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-11<BR>ratio: 0.069<BR>benchmarks:<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Visual Dialog<BR>date: 2017-11<BR>ratio: 0.2578<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-11<BR>ratio: 0.383<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>\",\"<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2017-11<BR>ratio: 0.2407<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-11<BR>ratio: 0.6857<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2017-12<BR>ratio: 0.0238<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-12<BR>ratio: 0.0546<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-12<BR>ratio: 0.0332<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2017-12<BR>ratio: 0.046<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2017-12<BR>ratio: 0.1898<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  J-HMDB - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2017-12<BR>ratio: 0.5028<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>\",\"<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  HANDS 2017 - Hand Pose Estimation benchmarking: Average 3D Error<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Image generation: Pose Transfer<BR>date: 2017-12<BR>ratio: 0.5<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: IS<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2017-12<BR>ratio: 0.1097<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2017-12<BR>ratio: 0.5417<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2017-12<BR>ratio: 0.0083<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-12<BR>ratio: 0.613<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  SYNSIG-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2017-12<BR>ratio: 0.3524<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2017-12<BR>ratio: 0.3532<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-12<BR>ratio: 0.0247<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-12<BR>ratio: 0.7094<BR>benchmarks:<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-01<BR>ratio: 0.0583<BR>benchmarks:<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Image classification: Document Image Classification<BR>date: 2018-01<BR>ratio: 0.5145<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-01<BR>ratio: 0.4342<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2018-01<BR>ratio: 0.3197<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-01<BR>ratio: 0.0837<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2018-01<BR>ratio: 0.3661<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>  Srinivasan2014 - Retinal OCT Disease Classification benchmarking: Acc<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-01<BR>ratio: 0.3059<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-01<BR>ratio: 0.4232<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-01<BR>ratio: 0.6486<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2018-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-1 (%)<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-5 (%)<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-02<BR>ratio: 0.9695<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  J-HMBD Early Action - Skeleton Based Action Recognition benchmarking: 10%<BR>\",\"<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2018-02<BR>ratio: 0.9199<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-02<BR>ratio: 0.6229<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-02<BR>ratio: 0.1321<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2018-02<BR>ratio: 0.0351<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2018-02<BR>ratio: 0.628<BR>benchmarks:<BR>  Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>\",\"<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2018-02<BR>ratio: 0.4938<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-02<BR>ratio: 0.2543<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-02<BR>ratio: 0.0865<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-02<BR>ratio: 0.3996<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2018-02<BR>ratio: 0.2075<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>\",\"<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-02<BR>ratio: 0.3005<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-02<BR>ratio: 0.6875<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: AUC<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2018-02<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma50 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-02<BR>ratio: 0.2155<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-02<BR>ratio: 0.5545<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2018-03<BR>ratio: 0.387<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-03<BR>ratio: 0.1771<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-03<BR>ratio: 0.1202<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-03<BR>ratio: 0.6418<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-03<BR>ratio: 0.1351<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2018-03<BR>ratio: 0.1099<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2018-03<BR>ratio: 0.2131<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-03<BR>ratio: 0.2249<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-03<BR>ratio: 0.0083<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2018-03<BR>ratio: 0.0929<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>\",\"<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: Average Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-03<BR>ratio: 0.5083<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-03<BR>ratio: 0.0203<BR>benchmarks:<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2018-03<BR>ratio: 0.4588<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-03<BR>ratio: 0.3293<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Watercolor2k - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  IJB-A - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2018-03<BR>ratio: 0.2555<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  iSAID - Object Detection benchmarking: Average Precision<BR>\",\"<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-03<BR>ratio: 0.1184<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-04<BR>ratio: 0.0985<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-04<BR>ratio: 0.5245<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-04<BR>ratio: 0.0647<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>\",\"<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  CT-150 - Pancreas Segmentation benchmarking: Dice Score<BR>  CT-150 - Pancreas Segmentation benchmarking: Precision<BR>  CT-150 - Pancreas Segmentation benchmarking: Recall<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2018-04<BR>ratio: 0.1925<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-04<BR>ratio: 0.0067<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-04<BR>ratio: 0.3804<BR>benchmarks:<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-04<BR>ratio: 0.6801<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  ChaLearn val - Hand Gesture Recognition benchmarking: Accuracy<BR>  Jester test - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2018-04<BR>ratio: 0.211<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2018-04<BR>ratio: 0.1429<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-04<BR>ratio: 0.3034<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-04<BR>ratio: 0.2694<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-04<BR>ratio: 0.5604<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-04<BR>ratio: 0.5045<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Other vision process: Metric Learning<BR>date: 2018-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-04<BR>ratio: 0.1608<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2018-04<BR>ratio: 0.6147<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-05<BR>ratio: 0.6543<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UAV-Human - Skeleton Based Action Recognition benchmarking: Average Accuracy<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-05<BR>ratio: 0.0149<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2018-05<BR>ratio: 0.6951<BR>benchmarks:<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-05<BR>ratio: 0.714<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-05<BR>ratio: 0.2306<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2018-05<BR>ratio: 0.145<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-05<BR>ratio: 0.028<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-05<BR>ratio: 0.2865<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-05<BR>ratio: 0.4536<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-05<BR>ratio: 0.6039<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  MuJoCo - Multivariate Time Series Imputation benchmarking: MSE (10^2, 50% missing)<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: mse (10^-3)<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-06<BR>ratio: 0.1822<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-06<BR>ratio: 0.3698<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: F1 score<BR>\",\"<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-06<BR>ratio: 0.7435<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (test)<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-06<BR>ratio: 0.1992<BR>benchmarks:<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-06<BR>ratio: 0.5603<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-06<BR>ratio: 0.2047<BR>benchmarks:<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-06<BR>ratio: 0.1128<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-06<BR>ratio: 0.5515<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  UT-Kinect - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-06<BR>ratio: 0.2077<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-06<BR>ratio: 0.9967<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: 3DPCK<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: MJPE<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-06<BR>ratio: 0.1235<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-06<BR>ratio: 0.4686<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2018-06<BR>ratio: 0.297<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-06<BR>ratio: 0.7711<BR>benchmarks:<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE log<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: SQ Rel<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2018-06<BR>ratio: 0.5207<BR>benchmarks:<BR>  Materials Project - Formation Energy benchmarking: MAE<BR>  QM9 - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2018-06<BR>ratio: 0.3576<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>\",\"<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  VRD - Scene Graph Generation benchmarking: Recall-at-50<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2018-06<BR>ratio: 0.3129<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>\",\"<BR>task: Action localization: Action Segmentation<BR>date: 2018-06<BR>ratio: 0.2185<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>\",\"<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking: MAP<BR>  Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking: MAP<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-07<BR>ratio: 0.3854<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2018-07<BR>ratio: 0.4436<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  ActivityNet-1.2 - Action Classification benchmarking: mAP<BR>  THUMOS\\u201914 - Action Classification benchmarking: mAP<BR>\",\"<BR>task: Object detection: Pedestrian Detection<BR>date: 2018-07<BR>ratio: 0.5357<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Bare MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Partial MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-07<BR>ratio: 0.9856<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-07<BR>ratio: 0.4007<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>  ImageNet - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-07<BR>ratio: 0.1059<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2018-07<BR>ratio: 0.1448<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Pavia University - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2018-07<BR>ratio: 0.5<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: PSNR (sRGB)<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: SSIM (sRGB)<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2018-07<BR>ratio: 0.0631<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Other vision process: Denoising<BR>date: 2018-07<BR>ratio: 0.1158<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-07<BR>ratio: 0.7291<BR>benchmarks:<BR>  CAT 256x256 - Image Generation benchmarking: FID<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-07<BR>ratio: 0.5625<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>\",\"<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: S-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: max E-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: mean Dice<BR>\",\"<BR>task: Other vision process: Depth Completion<BR>date: 2018-08<BR>ratio: 0.4543<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>  KITTI Depth Completion - Depth Completion benchmarking: Runtime [ms]<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2018-08<BR>ratio: 0.3547<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>\",\"<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-08<BR>ratio: 0.0321<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-08<BR>ratio: 0.6807<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-08<BR>ratio: 0.6698<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  Freiburg Forest - Semantic Segmentation benchmarking: Mean IoU<BR>  ScanNetV2 - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2018-08<BR>ratio: 0.0244<BR>benchmarks:<BR>  Data3D\\u2212R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-09<BR>ratio: 0.2687<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2018-09<BR>ratio: 0.1255<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>\",\"<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-09<BR>ratio: 0.6183<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: FID<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-09<BR>ratio: 0.1346<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-09<BR>ratio: 0.8995<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.01<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-09<BR>ratio: 0.1071<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2018-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2018-09<BR>ratio: 0.086<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other vision process: Visual Dialog<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Sequential CIFAR-10 - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>\",\"<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2018-10<BR>ratio: 0.4091<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2018-10<BR>ratio: 0.0658<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-10<BR>ratio: 0.1139<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>\",\"<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2018-10<BR>ratio: 0.0079<BR>benchmarks:<BR>  FDDB - Face Detection benchmarking: AP<BR>\",\"<BR>task: Other video process: Video Frame Interpolation<BR>date: 2018-10<BR>ratio: 0.2609<BR>benchmarks:<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-10<BR>ratio: 0.2227<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2018-10<BR>ratio: 0.0225<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-10<BR>ratio: 0.0087<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Scan2CAD - 3D Reconstruction benchmarking: Average Accuracy<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-11<BR>ratio: 0.6675<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-11<BR>ratio: 0.4542<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Object detection: Video Object Detection<BR>date: 2018-11<BR>ratio: 0.6415<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-11<BR>ratio: 0.7361<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-11<BR>ratio: 0.34<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2018-11<BR>ratio: 0.0808<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2018-11<BR>ratio: 0.0423<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2018-11<BR>ratio: 0.3352<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-11<BR>ratio: 0.1431<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>\",\"<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2018-11<BR>ratio: 0.7527<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>\",\"<BR>task: Other vision process: Depth Completion<BR>date: 2018-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-11<BR>ratio: 0.2788<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-11<BR>ratio: 0.5387<BR>benchmarks:<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2018-11<BR>ratio: 0.2295<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  DensePose-COCO - Pose Estimation benchmarking: AP<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2018-11<BR>ratio: 0.7924<BR>benchmarks:<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  CUB 128 x 128 - Image Generation benchmarking: FID<BR>  CUB 128 x 128 - Image Generation benchmarking: Inception score<BR>  Stanford Cars - Image Generation benchmarking: FID<BR>  Stanford Cars - Image Generation benchmarking: Inception score<BR>  Stanford Dogs - Image Generation benchmarking: FID<BR>  Stanford Dogs - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-11<BR>ratio: 0.5789<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>\",\"<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2018-11<BR>ratio: 0.3724<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2018-11<BR>ratio: 0.3967<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-11<BR>ratio: 0.0755<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-11<BR>ratio: 0.2526<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Frames Per View<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-11<BR>ratio: 0.0286<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Video Prediction<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Video Prediction benchmarking: MAE<BR>  Human3.6M - Video Prediction benchmarking: MSE<BR>  Human3.6M - Video Prediction benchmarking: SSIM<BR>\",\"<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-12<BR>ratio: 0.7779<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>\",\"<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: OOB Rate (10^\\u22123)<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Difference<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Length<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Step Change (10^\\u22123)<BR>  PEMS-SF - Multivariate Time Series Imputation benchmarking: L2 Loss (10^-4)<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-12<BR>ratio: 0.1767<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2018-12<BR>ratio: 0.5<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: mAP (Val)<BR>  AVA v2.2 - Action Recognition benchmarking: mAP<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2018-12<BR>ratio: 0.5107<BR>benchmarks:<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-12<BR>ratio: 0.5811<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQth<BR>  Cityscapes val - Panoptic Segmentation benchmarking: AP<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-12<BR>ratio: 0.1675<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-12<BR>ratio: 0.027<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2018-12<BR>ratio: 0.4227<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2018-12<BR>ratio: 0.1343<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-12<BR>ratio: 0.2902<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-12<BR>ratio: 0.8571<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-12<BR>ratio: 0.6789<BR>benchmarks:<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>  NVGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-12<BR>ratio: 0.2451<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2018-12<BR>ratio: 0.2741<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2018-12<BR>ratio: 0.1735<BR>benchmarks:<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2018-12<BR>ratio: 0.4718<BR>benchmarks:<BR>  Fashion-MNIST - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: Accuracy<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-12<BR>ratio: 0.5095<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>\",\"<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2018-12<BR>ratio: 0.221<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-12<BR>ratio: 0.0597<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2018-12<BR>ratio: 0.6087<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: bpd<BR>  CelebA-HQ 1024x1024 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2018-12<BR>ratio: 0.3083<BR>benchmarks:<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-12<BR>ratio: 0.3195<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-12<BR>ratio: 0.549<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-01<BR>ratio: 0.0894<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-01<BR>ratio: 0.1641<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>\",\"<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2019-01<BR>ratio: 0.3272<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>\",\"<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  LboroHAR - Multimodal Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-01<BR>ratio: 0.4<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-01<BR>ratio: 0.0323<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-01<BR>ratio: 0.3127<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2019-01<BR>ratio: 0.1212<BR>benchmarks:<BR>  Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  ICVL-4 - Action Recognition benchmarking: Accuracy<BR>  IRD - Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-01<BR>ratio: 0.2126<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-01<BR>ratio: 0.7353<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-01<BR>ratio: 0.3141<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-01<BR>ratio: 0.564<BR>benchmarks:<BR>  Cambridge - Hand Gesture Recognition benchmarking: Accuracy<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2019-01<BR>ratio: 0.371<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-01<BR>ratio: 0.1086<BR>benchmarks:<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2019-01<BR>ratio: 0.1323<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2019-02<BR>ratio: 0.0183<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  INRIA Holidays - Image Retrieval benchmarking: Mean mAP<BR>\",\"<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Indian Pines - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-02<BR>ratio: 0.0018<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-02<BR>ratio: 0.1<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-02<BR>ratio: 0.036<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (7 emotion)<BR>  JAFFE - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-02<BR>ratio: 0.068<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2019-02<BR>ratio: 0.6096<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>  AFLW-Full - Facial Landmark Detection benchmarking: Mean NME<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-02<BR>ratio: 0.1324<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-02<BR>ratio: 0.4587<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Other vision process: Visual Dialog<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Other vision process: Depth Completion<BR>date: 2019-02<BR>ratio: 0.069<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-03<BR>ratio: 0.098<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2019-03<BR>ratio: 0.2573<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  nuScenes - 3D Object Detection benchmarking: NDS<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  CelebA-HQ 128x128 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2019-03<BR>ratio: 0.5464<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>\",\"<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Views<BR>\",\"<BR>task: Other vision process: Domain Generalization<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ImageNet-C - Domain Generalization benchmarking: mean Corruption Error (mCE)<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2019-03<BR>ratio: 0.6399<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>  CUB-200-2011 - Image Retrieval benchmarking: R-at-1<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>\",\"<BR>task: Action localization: Action Segmentation<BR>date: 2019-03<BR>ratio: 0.3213<BR>benchmarks:<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-03<BR>ratio: 0.3109<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-03<BR>ratio: 0.1256<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Other image process: Image Reconstruction<BR>date: 2019-03<BR>ratio: 0.2896<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-03<BR>ratio: 0.4466<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mRec<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-03<BR>ratio: 0.9015<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2019-04<BR>ratio: 0.162<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>\",\"<BR>task: Other image process: Image Clustering<BR>date: 2019-04<BR>ratio: 0.3733<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>\",\"<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2019-04<BR>ratio: 0.1081<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-04<BR>ratio: 0.3059<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2019-04<BR>ratio: 0.6033<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Dense Object Detection<BR>date: 2019-04<BR>ratio: 0.1943<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-04<BR>ratio: 0.6718<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>\",\"<BR>task: Other video process: Video Frame Interpolation<BR>date: 2019-04<BR>ratio: 0.1348<BR>benchmarks:<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-04<BR>ratio: 0.2881<BR>benchmarks:<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.1<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>\",\"<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2019-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP at-0.5:0.95<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP-at-0.50<BR>\",\"<BR>task: Other image process: Color Image Denoising<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma75 - Color Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-04<BR>ratio: 0.155<BR>benchmarks:<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2019-04<BR>ratio: 0.1123<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Image generation: Pose Transfer<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: SSIM<BR>\",\"<BR>task: Other vision process: Visual Dialog<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-04<BR>ratio: 0.0663<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>\",\"<BR>task: Activity detection: Action Detection<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.1<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.2<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2019-04<BR>ratio: 0.4321<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other vision process: Object Counting<BR>date: 2019-04<BR>ratio: 0.3247<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>\",\"<BR>task: Other vision process: Curved Text Detection<BR>date: 2019-04<BR>ratio: 0.7679<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2019-04<BR>ratio: 0.3574<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Other vision process: Denoising<BR>date: 2019-04<BR>ratio: 0.0989<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-04<BR>ratio: 0.1341<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>\",\"<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-04<BR>ratio: 0.6555<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP50<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP75<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-04<BR>ratio: 0.7784<BR>benchmarks:<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: F-measure<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  ECSSD - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: MAE<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: F-measure<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: MAE<BR>  SOD - RGB Salient Object Detection benchmarking: MAE<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-04<BR>ratio: 0.6714<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-04<BR>ratio: 0.0519<BR>benchmarks:<BR>  FFHQ - Image Generation benchmarking: FID<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-04<BR>ratio: 0.339<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: H-Mean<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: H-Mean<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-04<BR>ratio: 0.0382<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>\",\"<BR>task: Other vision process: Video Prediction<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  CMU Mocap-2 - Video Prediction benchmarking: Test Error<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-05<BR>ratio: 0.0144<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-05<BR>ratio: 0.4235<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-05<BR>ratio: 0.7854<BR>benchmarks:<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-05<BR>ratio: 0.7002<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-05<BR>ratio: 0.1623<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>\",\"<BR>task: Other vision process: Depth Completion<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: iMAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: iRMSE<BR>\",\"<BR>task: Pose tracking: Pose Tracking<BR>date: 2019-05<BR>ratio: 0.0083<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-05<BR>ratio: 0.4063<BR>benchmarks:<BR>  HMDBfull-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2019-05<BR>ratio: 0.0225<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>\",\"<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Horizon Lines in the Wild - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2019-05<BR>ratio: 0.4905<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>\",\"<BR>task: Other vision process: Domain Generalization<BR>date: 2019-05<BR>ratio: 0.2258<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-05<BR>ratio: 0.7067<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>  FERPlus - Facial Expression Recognition benchmarking: Accuracy<BR>  SFEW - Facial Expression Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Formation Energy<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  OQMD v1.2 - Formation Energy benchmarking: MAE<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-06<BR>ratio: 0.2194<BR>benchmarks:<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>\",\"<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  BRATS-2015 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2017 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-06<BR>ratio: 0.0589<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-06<BR>ratio: 0.0493<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: Total Variation of Information<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Merge<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Split<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-06<BR>ratio: 0.2305<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>  iNaturalist - Image Classification benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-06<BR>ratio: 0.0932<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-06<BR>ratio: 0.0985<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-06<BR>ratio: 0.2174<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-06<BR>ratio: 0.2823<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-06<BR>ratio: 0.066<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2019-06<BR>ratio: 0.6079<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-10<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-1<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-06<BR>ratio: 0.9629<BR>benchmarks:<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2019-06<BR>ratio: 0.1399<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>\",\"<BR>task: Activity recognition: Action Classification<BR>date: 2019-06<BR>ratio: 0.4736<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>  Kinetics-600 - Action Classification benchmarking: Top-5 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-06<BR>ratio: 0.0171<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-06<BR>ratio: 0.7183<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: S-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: mean E-Measure<BR>\",\"<BR>task: Other vision process: Crowd Counting<BR>date: 2019-06<BR>ratio: 0.9021<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>  UCF CC 50 - Crowd Counting benchmarking: MAE<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-06<BR>ratio: 0.1952<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-06<BR>ratio: 0.0318<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2019-07<BR>ratio: 0.5126<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - 3D Object Detection benchmarking: AP<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-07<BR>ratio: 0.0989<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2019-07<BR>ratio: 0.2932<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>\",\"<BR>task: Object detection: Video Object Detection<BR>date: 2019-07<BR>ratio: 0.1509<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-07<BR>ratio: 0.066<BR>benchmarks:<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-07<BR>ratio: 0.5481<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2019-07<BR>ratio: 0.2415<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>\",\"<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2019-07<BR>ratio: 0.7593<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>\",\"<BR>task: Other video process: Video Retrieval<BR>date: 2019-07<BR>ratio: 0.4898<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-07<BR>ratio: 1.0<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-07<BR>ratio: 0.2647<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>\",\"<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-07<BR>ratio: 0.835<BR>benchmarks:<BR>  VOT2016 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  VOT2017 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Unseen)<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-07<BR>ratio: 0.3796<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-07<BR>ratio: 0.6173<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-08<BR>ratio: 0.3316<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-08<BR>ratio: 0.3395<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Something-Something V1 - Action Recognition benchmarking: Top 5 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>\",\"<BR>task: Other vision process: Metric Learning<BR>date: 2019-08<BR>ratio: 0.3549<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>\",\"<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: AUC<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-08<BR>ratio: 0.1793<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-08<BR>ratio: 0.0978<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Other vision process: Crowd Counting<BR>date: 2019-08<BR>ratio: 0.1469<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>\",\"<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-08<BR>ratio: 0.0324<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>\",\"<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-08<BR>ratio: 0.7344<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking: mIoU<BR>\",\"<BR>task: Object detection: Lane Detection<BR>date: 2019-08<BR>ratio: 0.4583<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2019-08<BR>ratio: 0.6596<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>  AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>\",\"<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SUN-RGBD - Scene Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S1)<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-08<BR>ratio: 0.2284<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-08<BR>ratio: 0.3737<BR>benchmarks:<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-08<BR>ratio: 0.3<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-08<BR>ratio: 0.1982<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Image classification: Document Image Classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Noisy Bangla Characters - Document Image Classification benchmarking: Accuracy<BR>  Noisy Bangla Numeral - Document Image Classification benchmarking: Accuracy<BR>  n-MNIST - Document Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  BRATS-2013 - Brain Tumor Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-08<BR>ratio: 0.4622<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-09<BR>ratio: 0.2029<BR>benchmarks:<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>\",\"<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Other image process: Image Retrieval<BR>date: 2019-09<BR>ratio: 0.1412<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-09<BR>ratio: 0.26<BR>benchmarks:<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>\",\"<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2019-09<BR>ratio: 0.1818<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-09<BR>ratio: 0.1053<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-09<BR>ratio: 0.0137<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-09<BR>ratio: 0.071<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-09<BR>ratio: 0.9068<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  PASCAL VOC 2007 - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-09<BR>ratio: 0.4198<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-09<BR>ratio: 0.1111<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-09<BR>ratio: 0.5071<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  USPS-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2019-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-10<BR>ratio: 0.5032<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>  SkyScapes-Lane - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-10<BR>ratio: 0.0388<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-10<BR>ratio: 0.0466<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-10<BR>ratio: 0.0205<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>\",\"<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2019-10<BR>ratio: 0.2772<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-10<BR>ratio: 0.1955<BR>benchmarks:<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-10<BR>ratio: 0.5483<BR>benchmarks:<BR>  AgeDB-30 - Face Verification benchmarking: Accuracy<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>\",\"<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2019-10<BR>ratio: 0.9715<BR>benchmarks:<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>\",\"<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-10<BR>ratio: 0.2791<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>\",\"<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2019-10<BR>ratio: 1.0<BR>benchmarks:<BR>  PA-100K - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  PETA - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  RAP - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-10<BR>ratio: 0.0533<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Image classification: Satellite Image Classification<BR>date: 2019-11<BR>ratio: 0.7974<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>  SAT-6 - Satellite Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-11<BR>ratio: 0.471<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes test - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: mIoU<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>\",\"<BR>task: Object detection: Object Detection<BR>date: 2019-11<BR>ratio: 0.0635<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APS<BR>\",\"<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Oulu-CASIA - Facial Expression Recognition benchmarking: Accuracy (10-fold)<BR>\",\"<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-11<BR>ratio: 0.0838<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>\",\"<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-11<BR>ratio: 0.3797<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-11<BR>ratio: 0.057<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-11<BR>ratio: 0.0114<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-11<BR>ratio: 0.0848<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking: AP-at-0.7<BR>\",\"<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-11<BR>ratio: 0.6248<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APL<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-11<BR>ratio: 0.2438<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-11<BR>ratio: 0.8357<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>\",\"<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-11<BR>ratio: 0.9839<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation benchmarking: Accuracy (ADD)<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-11<BR>ratio: 0.2369<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: oAcc<BR>\",\"<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-11<BR>ratio: 0.0283<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>\",\"<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-11<BR>ratio: 0.3356<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>\",\"<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-12<BR>ratio: 0.172<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>\",\"<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-12<BR>ratio: 0.04<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-12<BR>ratio: 0.0932<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>\",\"<BR>task: Activity recognition: Action Recognition<BR>date: 2019-12<BR>ratio: 0.1636<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2019-12<BR>ratio: 0.2424<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Params<BR>  ObjectNet - Image Classification benchmarking: Top 5 Accuracy<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2019-12<BR>ratio: 0.1413<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>\",\"<BR>task: Image generation: Image Generation<BR>date: 2019-12<BR>ratio: 0.8135<BR>benchmarks:<BR>  CIFAR-100 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Cat 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Churches 256 x 256 - Image Generation benchmarking: FID<BR>\",\"<BR>task: Other video process: Video Generation<BR>date: 2019-12<BR>ratio: 0.7122<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>\",\"<BR>task: Image classification: Image Classification<BR>date: 2020-01<BR>ratio: 0.25<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>\",\"<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2020-01<BR>ratio: 0.0444<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  SUN-RGBD - 3D Object Detection benchmarking: mAP-at-0.25<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  UPenn Action - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>\",\"<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2020-01<BR>ratio: 0.4159<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>\",\"<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2020-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>\",\"<BR>task: Other vision process: Visual Question Answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2020-02<BR>ratio: 0.4211<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-02<BR>ratio: 0.0098<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>\",\"<BR>task: Other vision process: Scene Graph Generation<BR>date: 2020-02<BR>ratio: 0.19<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>\",\"<BR>task: Pose estimation: Pose Estimation<BR>date: 2020-02<BR>ratio: 0.0152<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>\",\"<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2020-03<BR>ratio: 0.5909<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>\",\"<BR>task: Object detection: 3D Object Detection<BR>date: 2020-03<BR>ratio: 0.3135<BR>benchmarks:<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Action localization: Action Segmentation<BR>date: 2020-03<BR>ratio: 0.6966<BR>benchmarks:<BR>  50 Salads - Action Segmentation benchmarking: Acc<BR>  50 Salads - Action Segmentation benchmarking: Edit<BR>  50 Salads - Action Segmentation benchmarking: F1@10%<BR>  50 Salads - Action Segmentation benchmarking: F1@25%<BR>  50 Salads - Action Segmentation benchmarking: F1@50%<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  Breakfast - Action Segmentation benchmarking: F1@10%<BR>  Breakfast - Action Segmentation benchmarking: F1@25%<BR>  Breakfast - Action Segmentation benchmarking: F1@50%<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>\",\"<BR>task: Semantic segmentation: 3D Instance Segmentation<BR>date: 2020-03<BR>ratio: 0.8206<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>  SceneNN - 3D Instance Segmentation benchmarking: mAP-at-0.5<BR>\",\"<BR>task: Object detection: Video Object Detection<BR>date: 2020-03<BR>ratio: 0.2075<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>\",\"<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2020-03<BR>ratio: 0.4893<BR>benchmarks:<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>\",\"<BR>task: Other video process: Video Frame Interpolation<BR>date: 2020-03<BR>ratio: 0.8021<BR>benchmarks:<BR>  UCF101 - Video Frame Interpolation benchmarking: PSNR<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>\",\"<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2020-04<BR>ratio: 0.4197<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  NYU Depth v2 - Semantic Segmentation benchmarking: Mean IoU<BR>\",\"<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2020-04<BR>ratio: 0.0204<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>\",\"<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-04<BR>ratio: 0.4396<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>\"],\"marker\":{\"color\":[0.5546,0.0189,0.1073,1.0,1.0,0.0397,0.0553,0.021,0.8122,0.4737,0.289,0.1446,0.9752,0.2834,1.0,0.2375,0.2513,0.2406,0.1392,0.0877,0.8858,0.0451,0.3958,0.0134,0.3753,0.2327,0.0741,0.426,0.1007,0.2878,0.8426,0.3991,0.2321,0.1675,0.1142,0.4459,0.2457,0.4321,0.5,0.117,0.4701,0.0854,1.0,0.1888,0.3222,0.0331,0.15,0.1785,0.4339,0.1637,0.0779,0.7477,0.4051,1.0,0.9509,0.5016,1.0,0.0707,0.6524,0.0478,0.056,0.2757,0.0258,1.0,0.3359,0.2119,0.4252,0.5954,0.7322,0.1471,0.4868,0.4832,0.4271,0.6898,0.0092,0.8889,0.3005,0.0418,0.1069,0.2762,0.072,0.2999,0.0741,0.572,0.3146,0.061,0.3235,0.3625,0.0241,0.8371,1.0,0.1233,0.2999,0.6547,1.0,0.0251,0.2523,0.5511,0.1736,0.0915,0.0144,0.3082,0.4115,0.2784,0.0093,0.0142,0.0606,0.0684,0.3326,0.3571,0.0,0.2824,0.2492,0.2441,0.096,0.3399,0.2329,0.2215,0.5084,0.6006,0.674,0.2304,0.6019,0.3137,0.1022,0.5259,0.4898,0.1001,0.5647,0.1125,0.1929,0.0388,0.1288,0.1429,0.9756,0.1328,0.2025,0.6995,0.0692,0.0541,0.9524,0.5645,0.1574,0.3612,0.1565,0.2235,0.0303,0.6528,0.5668,0.4081,0.2484,0.1096,0.2099,0.2205,0.1193,0.5877,0.2105,0.0331,0.4378,0.2112,1.0,0.2041,0.1026,0.3846,0.7172,0.103,0.1968,0.1629,0.2194,0.1332,0.8514,0.1767,0.4855,1.0,0.4565,0.1679,0.3172,0.0758,0.4474,0.4983,0.528,0.2366,0.7853,0.4284,1.0,0.8352,0.2561,0.0559,0.0607,0.2867,0.6277,0.2514,0.6175,0.3412,0.1244,0.4844,0.4142,0.5909,0.4898,0.6641,0.1161,0.7104,1.0,0.02,0.6614,0.0095,0.0387,0.4581,0.2874,0.1758,1.0,0.3268,0.6997,0.0363,0.0645,0.2688,0.5913,0.0083,1.0,0.219,0.0624,0.88,0.3456,0.0951,0.1108,0.4921,0.6932,0.5449,0.8514,0.0551,0.1598,0.474,0.3904,0.0103,1.0,1.0,0.0758,0.0809,1.0,0.6429,1.0,0.0909,0.9952,0.4958,0.2127,0.9847,0.4337,0.2492,0.2941,0.7097,0.5297,0.656,0.2656,0.6273,0.435,0.2438,0.619,0.5452,0.1491,0.6998,0.789,0.1976,0.5062,0.6503,0.3521,0.1702,0.3953,0.069,0.2578,0.383,0.2407,0.6857,0.0238,0.0546,0.0332,0.046,0.1898,1.0,0.5028,1.0,0.2406,0.5,0.1097,0.5417,0.0083,0.613,0.3524,0.3532,0.0247,0.7094,0.0583,0.5145,0.4342,0.3197,0.0837,0.3661,0.3059,0.4232,0.6486,0.2233,0.3333,0.9695,0.9199,0.6229,0.1321,0.0351,0.628,0.4938,0.2543,0.0865,0.3996,0.2075,0.3005,0.6875,1.0,0.2155,0.5545,0.387,0.1771,0.1202,0.6418,0.1351,0.1099,0.2131,0.2249,0.0083,0.1587,0.0929,1.0,0.5083,0.0203,0.4588,0.3293,1.0,1.0,0.2555,0.1184,0.0985,0.5245,0.0647,1.0,0.1925,0.0067,0.3804,0.6801,1.0,0.211,0.1429,0.3034,0.2694,0.5604,0.5045,0.6667,0.1608,0.6147,1.0,0.6543,0.0149,0.6951,0.714,0.2306,0.145,0.028,0.2865,0.4536,0.6039,1.0,0.1822,0.3698,1.0,0.7435,0.1992,0.5603,0.2047,0.1128,0.5515,0.2077,0.9967,0.1235,0.4686,0.297,0.7711,0.7567,0.5207,0.3576,1.0,0.3129,0.2185,1.0,0.3854,0.4436,1.0,0.5357,0.9856,0.4007,0.1059,0.1448,1.0,0.5,0.0631,0.1158,0.7291,0.5625,1.0,0.4543,0.3547,0.0321,0.6807,0.6698,0.0244,0.2687,0.1255,0.6183,0.1346,0.8995,0.1071,0.0378,0.086,0.1442,1.0,0.4091,0.0658,0.1139,0.0079,0.2609,0.2227,0.0225,0.0087,0.2577,1.0,0.6675,0.4542,0.6415,0.7361,0.34,0.5661,0.0808,0.0423,0.3352,0.1431,0.7527,0.0684,0.2788,0.5387,0.2295,1.0,0.7924,1.0,0.5789,0.3724,0.3967,0.0755,0.2526,1.0,0.0286,1.0,0.7779,1.0,0.1767,0.5,0.5107,0.5811,0.1675,0.027,0.4227,0.1343,0.2902,0.8571,0.6789,0.2451,0.2741,1.0,0.1735,0.4718,0.5095,0.221,0.0597,0.6087,0.3083,0.3195,0.549,0.0894,0.1641,0.3272,1.0,0.4,0.0323,0.3127,0.1212,1.0,0.2126,0.7353,0.0882,0.3141,0.564,0.371,0.1086,0.1323,0.0183,1.0,1.0,0.0018,0.1,0.036,1.0,0.068,0.6096,0.1324,0.4587,0.1426,0.069,0.098,0.2573,1.0,0.5464,1.0,1.0,0.6399,0.3213,0.3109,0.1256,0.2896,0.4466,1.0,0.9015,0.162,0.3733,0.1081,0.3059,0.6033,0.1943,0.6718,0.1348,0.2881,0.6667,1.0,0.155,1.0,0.1123,0.3147,1.0,0.4213,0.0663,1.0,0.4321,0.3247,0.7679,0.3574,0.0989,0.1341,0.6555,0.7784,0.6714,0.0519,0.339,0.0382,1.0,0.0144,0.4235,0.7854,0.7002,0.1623,1.0,0.0083,0.4063,0.0225,1.0,0.4905,0.2258,0.7067,1.0,0.2194,1.0,0.0589,0.0493,1.0,0.2305,0.0932,0.0985,0.2174,0.2823,0.066,0.6079,0.9629,0.1399,0.4736,0.0171,0.7183,0.9021,0.1952,0.0318,0.5126,0.0989,0.2932,0.1509,0.066,0.5481,0.2415,0.7593,0.4898,1.0,0.2647,0.835,0.3796,0.6173,0.3316,0.3395,0.3549,1.0,0.1793,0.0978,0.1469,0.0324,0.7344,1.0,0.4583,0.3309,0.6596,1.0,1.0,0.2284,0.3737,0.3,0.1982,1.0,1.0,0.4622,0.2029,0.046,0.1412,0.26,0.1818,0.1053,0.0137,0.071,0.9068,1.0,0.4198,0.1111,0.5071,0.0378,0.5032,0.0388,0.0466,0.0205,0.2772,0.1955,0.5483,0.9715,0.2791,1.0,0.0533,0.7974,0.471,0.0635,1.0,0.0838,0.3797,0.057,0.0114,0.0848,1.0,0.6248,0.2438,0.8357,0.9839,0.2369,0.0283,0.3356,0.172,0.04,0.0932,0.1636,0.2424,0.1413,0.8135,0.7122,0.25,0.0444,1.0,1.0,0.4159,0.2233,0.5266,0.4211,0.0098,0.19,0.0152,0.5909,0.3135,0.6966,0.8206,0.2075,0.4893,0.8021,0.4197,0.0204,0.4396],\"colorbar\":{\"len\":500,\"lenmode\":\"pixels\",\"thickness\":10,\"title\":{\"text\":\"ratio\"}},\"colorscale\":[[0.0,\"rgb(255,255,229)\"],[0.125,\"rgb(247,252,185)\"],[0.25,\"rgb(217,240,163)\"],[0.375,\"rgb(173,221,142)\"],[0.5,\"rgb(120,198,121)\"],[0.625,\"rgb(65,171,93)\"],[0.75,\"rgb(35,132,67)\"],[0.875,\"rgb(0,104,55)\"],[1.0,\"rgb(0,69,41)\"]],\"opacity\":0.7,\"showscale\":true,\"size\":19,\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"markers\",\"x\":[\"2012-08\",\"2012-12\",\"2013-02\",\"2013-02\",\"2013-07\",\"2013-11\",\"2013-12\",\"2014-04\",\"2014-06\",\"2014-06\",\"2014-06\",\"2014-09\",\"2014-09\",\"2014-11\",\"2014-12\",\"2014-12\",\"2014-12\",\"2014-12\",\"2014-12\",\"2015-02\",\"2015-02\",\"2015-02\",\"2015-02\",\"2015-02\",\"2015-03\",\"2015-03\",\"2015-03\",\"2015-04\",\"2015-04\",\"2015-04\",\"2015-05\",\"2015-05\",\"2015-05\",\"2015-05\",\"2015-05\",\"2015-06\",\"2015-06\",\"2015-08\",\"2015-09\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-12\",\"2015-12\",\"2015-12\",\"2015-12\",\"2015-12\",\"2015-12\",\"2016-01\",\"2016-01\",\"2016-01\",\"2016-01\",\"2016-02\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-04\",\"2016-04\",\"2016-04\",\"2016-04\",\"2016-04\",\"2016-04\",\"2016-04\",\"2016-05\",\"2016-05\",\"2016-05\",\"2016-05\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-07\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-10\",\"2016-10\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2017-01\",\"2017-01\",\"2017-02\",\"2017-02\",\"2017-02\",\"2017-02\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2020-01\",\"2020-01\",\"2020-01\",\"2020-01\",\"2020-01\",\"2020-01\",\"2020-02\",\"2020-02\",\"2020-02\",\"2020-02\",\"2020-02\",\"2020-03\",\"2020-03\",\"2020-03\",\"2020-03\",\"2020-03\",\"2020-03\",\"2020-03\",\"2020-04\",\"2020-04\",\"2020-04\"],\"y\":[\"Other image process: Image Clustering\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Activity recognition: Skeleton Based Action Recognition\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Activity recognition: Action Recognition\",\"Facial recognition and modelling: Face Verification\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Other vision process: Domain Adaptation\",\"Semantic segmentation: Semantic Segmentation\",\"Object detection: Pedestrian Detection\",\"Activity recognition: Action Recognition\",\"Semantic segmentation: Semantic Segmentation\",\"Facial recognition and modelling: Face Verification\",\"Image classification: Image Classification\",\"Semantic segmentation: Semantic Segmentation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Facial recognition and modelling: Face Verification\",\"Other vision process: Domain Adaptation\",\"Image classification: Image Classification\",\"Activity recognition: Action Recognition\",\"Facial recognition and modelling: Face Verification\",\"Semantic segmentation: Semantic Segmentation\",\"Other image process: Image Retrieval\",\"Other vision process: Scene Text Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Multi-tissue Nucleus Segmentation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Curved Text Detection\",\"Activity recognition: Action Recognition\",\"Other vision process: Unsupervised Domain Adaptation\",\"Image classification: Image Classification\",\"Object detection: Object Detection\",\"Facial recognition and modelling: Face Verification\",\"Semantic segmentation: Semantic Segmentation\",\"Facial recognition and modelling: Face Identification\",\"Image classification: Sequential Image Classification\",\"Facial recognition and modelling: Face Verification\",\"Semantic segmentation: Medical Image Segmentation\",\"Other image process: Image Retrieval\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Pose estimation: Pose Estimation\",\"Other image process: Image Clustering\",\"Semantic segmentation: Semantic Segmentation\",\"Object detection: Weakly Supervised Object Detection\",\"Image classification: Image Classification\",\"Image classification: Image Classification\",\"Other vision process: Crowd Counting\",\"Image classification: Satellite Image Classification\",\"Activity recognition: Action Recognition\",\"Image classification: Retinal OCT Disease Classification\",\"Object detection: Object Detection\",\"Image generation: Image Generation\",\"Activity recognition: Action Recognition\",\"Pose estimation: Pose Estimation\",\"Action localization: Temporal Action Localization\",\"Image classification: Image Classification\",\"Other vision process: Visual Question Answering\",\"Image classification: Image Classification\",\"Activity recognition: Multimodal Activity Recognition\",\"Object detection: Weakly Supervised Object Detection\",\"Facial recognition and modelling: Face Verification\",\"Image classification: Sequential Image Classification\",\"Facial recognition and modelling: Face Detection\",\"Pose estimation: Pose Estimation\",\"Semantic segmentation: Semantic Segmentation\",\"Other image process: Aesthetics Quality Assessment\",\"Other image process: Image Retrieval\",\"Activity recognition: Skeleton Based Action Recognition\",\"Other image process: Image Clustering\",\"Activity recognition: Action Recognition\",\"Other vision process: Object Counting\",\"Other vision process: Scene Text Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Image classification: Image Classification\",\"Other vision process: Visual Question Answering\",\"Other vision process: Domain Adaptation\",\"Other vision process: Visual Question Answering\",\"Semantic segmentation: Semantic Segmentation\",\"Image generation: Conditional Image Generation\",\"Object detection: RGB Salient Object Detection\",\"Activity recognition: Skeleton Based Action Recognition\",\"Object detection: 3D Object Detection\",\"Image generation: Image Generation\",\"Other vision process: Monocular Depth Estimation\",\"Activity recognition: Multimodal Activity Recognition\",\"Other vision process: Horizon Line Estimation\",\"Pose estimation: Keypoint Detection\",\"Object detection: Birds Eye View Object Detection\",\"Other vision process: Domain Adaptation\",\"Object recognition: Pedestrian Attribute Recognition\",\"Image classification: Image Classification\",\"Other vision process: Crowd Counting\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Image Retrieval\",\"Activity recognition: Action Recognition\",\"Object detection: Weakly Supervised Object Detection\",\"Activity recognition: Skeleton Based Action Recognition\",\"Other vision process: Object Counting\",\"Action localization: Temporal Action Localization\",\"Pose estimation: Pose Estimation\",\"Image classification: Image Classification\",\"Image generation: Conditional Image Generation\",\"Image classification: Image Classification\",\"Semantic segmentation: Nuclear Segmentation\",\"Action localization: Action Segmentation\",\"Object detection: RGB Salient Object Detection\",\"Other vision process: Metric Learning\",\"Other video process: Video Generation\",\"Other image process: Image Retrieval\",\"Object tracking: Visual Object Tracking\",\"Activity recognition: Skeleton Based Action Recognition\",\"Semantic segmentation: Instance Segmentation\",\"Object detection: Weakly Supervised Object Detection\",\"Other vision process: Visual Question Answering\",\"Object detection: Birds Eye View Object Detection\",\"Pose estimation: Keypoint Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Pose estimation: Keypoint Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Other video process: Video Retrieval\",\"Pose estimation: Pose Estimation\",\"Semantic segmentation: Video Semantic Segmentation\",\"Other image process: Image Retrieval\",\"Other 3D task: 3D Object Reconstruction\",\"Object detection: Dense Object Detection\",\"Semantic segmentation: 3D Part Segmentation\",\"Object detection: Object Detection\",\"Image generation: Conditional Image Generation\",\"Activity recognition: Action Classification\",\"Other 3D task: 3D Reconstruction\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Pose Estimation\",\"Image classification: Unsupervised Image Classification\",\"Pose estimation: Pose Estimation\",\"Image generation: Image Generation\",\"Object detection: Pedestrian Detection\",\"Other image process: Image Clustering\",\"Semantic segmentation: Multi-tissue Nucleus Segmentation\",\"Other vision process: Scene Text Detection\",\"Object detection: Object Detection\",\"Pose estimation: Keypoint Detection\",\"Image generation: Conditional Image Generation\",\"Semantic segmentation: Nuclear Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Image generation: Image Generation\",\"Action localization: Temporal Action Localization\",\"Activity recognition: Action Recognition\",\"Activity detection: Action Detection\",\"Facial recognition and modelling: Face Verification\",\"Activity recognition: Skeleton Based Action Recognition\",\"Other vision process: Scene Text Detection\",\"Pose estimation: Pose Estimation\",\"Other image process: Image Clustering\",\"Semantic segmentation: Semantic Segmentation\",\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"Semantic segmentation: Instance Segmentation\",\"Other vision process: Visual Question Answering\",\"Other vision process: Formation Energy\",\"Other vision process: Domain Adaptation\",\"Object tracking: Visual Object Tracking\",\"Other vision process: Monocular Depth Estimation\",\"Activity recognition: Multimodal Activity Recognition\",\"Object detection: Weakly Supervised Object Detection\",\"Facial recognition and modelling: Face Identification\",\"Other 3D task: 3D Point Cloud Classification\",\"Activity recognition: Skeleton Based Action Recognition\",\"Image classification: Document Image Classification\",\"Other image process: Color Image Denoising\",\"Other image process: Grayscale Image Denoising\",\"Facial recognition and modelling: Face Verification\",\"Activity recognition: Action Recognition\",\"Object detection: RGB Salient Object Detection\",\"Other image process: Aesthetics Quality Assessment\",\"Activity recognition: Action Classification\",\"Other vision process: Domain Adaptation\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Other vision process: Denoising\",\"Other vision process: Visual Question Answering\",\"Pose estimation: 3D Human Pose Estimation\",\"Gesture recognition: Hand Gesture Recognition\",\"Action localization: Temporal Action Localization\",\"Activity recognition: Action Recognition\",\"Pose estimation: Pose Estimation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Human Interaction Recognition\",\"Image generation: Image Generation\",\"Object tracking: Visual Object Tracking\",\"Other vision process: Metric Learning\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Other 3D task: 3D Point Cloud Classification\",\"Other vision process: Formation Energy\",\"Semantic segmentation: 3D Part Segmentation\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Other image process: Image Reconstruction\",\"Object detection: Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Pose estimation: Pose Estimation\",\"Object detection: RGB Salient Object Detection\",\"Other video process: Video Generation\",\"Other video process: Video Retrieval\",\"Other vision process: Visual Question Answering\",\"Other vision process: Object Counting\",\"Pose estimation: Hand Pose Estimation\",\"Other vision process: Scene Text Detection\",\"Facial recognition and modelling: Face Alignment\",\"Image classification: Image Classification\",\"Other vision process: Domain Generalization\",\"Facial recognition and modelling: Face Detection\",\"Activity recognition: Skeleton Based Action Recognition\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Hand Pose Estimation\",\"Image classification: Image Classification\",\"Object detection: Object Detection\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Other vision process: Visual Question Answering\",\"Object detection: Weakly Supervised Object Detection\",\"Other vision process: Scene Text Detection\",\"Semantic segmentation: Human Part Segmentation\",\"Object detection: Dense Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Other image process: Image Clustering\",\"Facial recognition and modelling: Face Detection\",\"Other vision process: Formation Energy\",\"Other vision process: Scene Text Detection\",\"Other vision process: Visual Dialog\",\"Image generation: Image Generation\",\"Pose estimation: 3D Human Pose Estimation\",\"Semantic segmentation: Pancreas Segmentation\",\"Image generation: Conditional Image Generation\",\"Image classification: Image Classification\",\"Other image process: Color Image Denoising\",\"Object tracking: Visual Object Tracking\",\"Object detection: Lane Detection\",\"Image classification: Sequential Image Classification\",\"Object detection: RGB Salient Object Detection\",\"Image classification: Image Classification\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Facial recognition and modelling: Face Verification\",\"Other image process: Image Clustering\",\"Semantic segmentation: Semantic Segmentation\",\"Other image process: Grayscale Image Denoising\",\"Other vision process: Domain Generalization\",\"Image generation: Image Generation\",\"Pose estimation: Head Pose Estimation\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Facial recognition and modelling: Face Alignment\",\"Activity recognition: Action Recognition\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: 3D Object Detection\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Object detection: Birds Eye View Object Detection\",\"Other 3D task: 3D Shape Classification\",\"Object detection: Object Detection\",\"Semantic segmentation: Skin Cancer Segmentation\",\"Activity recognition: Action Classification\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Semantic segmentation: 3D Part Segmentation\",\"Pose estimation: Pose Estimation\",\"Other vision process: Domain Adaptation\",\"Other vision process: Visual Dialog\",\"Pose estimation: Keypoint Detection\",\"Image-to-image translation: Fundus to Angiography Generation\",\"Semantic segmentation: Instance Segmentation\",\"Activity recognition: Action Classification\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Other image process: Image Retrieval\",\"Pose tracking: Pose Tracking\",\"Pose estimation: Pose Estimation\",\"Activity detection: Action Detection\",\"Pose estimation: Hand Pose Estimation\",\"Activity localization: Weakly Supervised Action Localization\",\"Image generation: Pose Transfer\",\"Object detection: 3D Object Detection\",\"Object detection: Lane Detection\",\"Image classification: Image Classification\",\"Other vision process: Domain Adaptation\",\"Object detection: Object Detection\",\"Activity recognition: Action Recognition\",\"Facial recognition and modelling: Face Verification\",\"Object detection: Birds Eye View Object Detection\",\"Other 3D task: 3D Point Cloud Classification\",\"Image classification: Document Image Classification\",\"Activity recognition: Skeleton Based Action Recognition\",\"Other image process: Image Retrieval\",\"Activity recognition: Action Recognition\",\"Image classification: Retinal OCT Disease Classification\",\"Other vision process: Scene Text Detection\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Face Identification\",\"Semantic segmentation: 3D Part Segmentation\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Object tracking: Multiple Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object detection: Weakly Supervised Object Detection\",\"Image classification: Image Classification\",\"Other 3D task: 3D Object Reconstruction\",\"Semantic segmentation: Skin Cancer Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Image generation: Conditional Image Generation\",\"Other vision process: Scene Text Detection\",\"Object detection: 3D Object Detection\",\"Image classification: Unsupervised Image Classification\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Other image process: Color Image Denoising\",\"Pose tracking: Pose Tracking\",\"Image generation: Image Generation\",\"Activity detection: Action Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Object tracking: Visual Object Tracking\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Other 3D task: 3D Point Cloud Classification\",\"Other image process: Image Retrieval\",\"Semantic segmentation: Instance Segmentation\",\"Other vision process: Monocular Depth Estimation\",\"Pose estimation: Pose Estimation\",\"Other vision process: Visual Question Answering\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Semantic segmentation: Scene Segmentation\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Image generation: Image Generation\",\"Object detection: Weakly Supervised Object Detection\",\"Facial recognition and modelling: Face Identification\",\"Object detection: Object Detection\",\"Image classification: Sequential Image Classification\",\"Other vision process: Scene Text Detection\",\"Activity recognition: Skeleton Based Action Recognition\",\"Pose estimation: Pose Estimation\",\"Semantic segmentation: Pancreas Segmentation\",\"Pose estimation: Keypoint Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Action localization: Temporal Action Localization\",\"Pose tracking: Pose Tracking\",\"Gesture recognition: Hand Gesture Recognition\",\"Other 3D task: 3D Shape Classification\",\"Other image process: Image Retrieval\",\"Semantic segmentation: Semantic Segmentation\",\"Facial recognition and modelling: Face Alignment\",\"Pose estimation: 3D Human Pose Estimation\",\"Other image process: Image Clustering\",\"Other vision process: Metric Learning\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Other video process: Video Retrieval\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Activity recognition: Skeleton Based Action Recognition\",\"Other vision process: Visual Question Answering\",\"Object detection: Object Detection\",\"Facial recognition and modelling: Face Alignment\",\"Semantic segmentation: Human Part Segmentation\",\"Image classification: Image Classification\",\"Pose estimation: Pose Estimation\",\"Image generation: Conditional Image Generation\",\"Other vision process: Monocular Depth Estimation\",\"Other image process: Grayscale Image Denoising\",\"Other vision process: Multivariate Time Series Imputation\",\"Object detection: Weakly Supervised Object Detection\",\"Other vision process: Scene Text Detection\",\"Object detection: Lane Detection\",\"Activity localization: Temporal Action Proposal Generation\",\"Activity recognition: Action Recognition\",\"Action localization: Temporal Action Localization\",\"Other image process: Grayscale Image Denoising\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Activity recognition: Skeleton Based Action Recognition\",\"Object tracking: Visual Object Tracking\",\"Pose estimation: 3D Human Pose Estimation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Activity recognition: Action Classification\",\"Other vision process: Monocular Depth Estimation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Other vision process: Formation Energy\",\"Object detection: RGB Salient Object Detection\",\"Other vision process: Scene Graph Generation\",\"Other video process: Video Retrieval\",\"Action localization: Action Segmentation\",\"Object recognition: Traffic Sign Recognition\",\"Other image process: Image Clustering\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity recognition: Action Classification\",\"Object detection: Pedestrian Detection\",\"Activity recognition: Action Recognition\",\"Object detection: Weakly Supervised Object Detection\",\"Other vision process: Scene Text Detection\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Image classification: Hyperspectral Image Classification\",\"Other image process: Color Image Denoising\",\"Image classification: Image Classification\",\"Other vision process: Denoising\",\"Image generation: Image Generation\",\"Other vision process: Domain Adaptation\",\"Semantic segmentation: Medical Image Segmentation\",\"Other vision process: Depth Completion\",\"Other video process: Video Retrieval\",\"Other vision process: Scene Graph Generation\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Other 3D task: 3D Reconstruction\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Nuclear Segmentation\",\"Image generation: Conditional Image Generation\",\"Image generation: Image Generation\",\"Facial recognition and modelling: Face Verification\",\"Semantic segmentation: Semantic Segmentation\",\"Facial recognition and modelling: Face Detection\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Other vision process: Visual Dialog\",\"Image classification: Sequential Image Classification\",\"Semantic segmentation: Lesion Segmentation\",\"Other image process: Aesthetics Quality Assessment\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Facial recognition and modelling: Face Detection\",\"Other video process: Video Frame Interpolation\",\"Other image process: Image Clustering\",\"Activity recognition: Action Classification\",\"Activity recognition: Action Recognition\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Other 3D task: 3D Reconstruction\",\"Other vision process: Unsupervised Domain Adaptation\",\"Semantic segmentation: Human Part Segmentation\",\"Object detection: Video Object Detection\",\"Object tracking: Visual Object Tracking\",\"Other vision process: Domain Adaptation\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Image classification: Image Classification\",\"Object tracking: Multiple Object Tracking\",\"Other image process: Image Retrieval\",\"Activity localization: Temporal Action Proposal Generation\",\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\",\"Other vision process: Depth Completion\",\"Activity recognition: Action Recognition\",\"Other image process: Image Clustering\",\"Object detection: Object Detection\",\"Pose estimation: Pose Estimation\",\"Activity recognition: Action Classification\",\"Image generation: Image Generation\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Human Interaction Recognition\",\"Activity recognition: Group Activity Recognition\",\"Object detection: Weakly Supervised Object Detection\",\"Other vision process: Scene Text Detection\",\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Other vision process: Video Prediction\",\"Other vision process: Scene Graph Generation\",\"Other vision process: Multivariate Time Series Imputation\",\"Other vision process: Monocular Depth Estimation\",\"Activity recognition: Action Recognition\",\"Semantic segmentation: 3D Semantic Instance Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Object tracking: Visual Object Tracking\",\"Other 3D task: 3D Point Cloud Classification\",\"Object detection: Object Detection\",\"Activity recognition: Action Classification\",\"Facial recognition and modelling: Face Verification\",\"Activity recognition: Skeleton Based Action Recognition\",\"Gesture recognition: Hand Gesture Recognition\",\"Semantic segmentation: Semantic Segmentation\",\"Pose estimation: Keypoint Detection\",\"Other vision process: Unsupervised Domain Adaptation\",\"Object detection: 3D Object Detection\",\"Other image process: Image Clustering\",\"Activity recognition: Egocentric Activity Recognition\",\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Image generation: Image Generation\",\"Object detection: Birds Eye View Object Detection\",\"Facial recognition and modelling: Face Identification\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Head Pose Estimation\",\"Activity recognition: Multimodal Activity Recognition\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation\",\"Other vision process: Domain Adaptation\",\"Other 3D task: 3D Object Reconstruction\",\"Activity recognition: Action Recognition\",\"Semantic segmentation: Panoptic Segmentation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Object detection: Object Detection\",\"Image classification: Image Classification\",\"Gesture recognition: Hand Gesture Recognition\",\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\",\"Semantic segmentation: Semantic Segmentation\",\"Other image process: Image Clustering\",\"Pose tracking: Pose Tracking\",\"Other image process: Image Retrieval\",\"Image classification: Hyperspectral Image Classification\",\"Facial recognition and modelling: Face Alignment\",\"Semantic segmentation: Instance Segmentation\",\"Pose estimation: Keypoint Detection\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Pose estimation: Pose Estimation\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Other vision process: Visual Question Answering\",\"Other vision process: Visual Dialog\",\"Other vision process: Depth Completion\",\"Semantic segmentation: Semantic Segmentation\",\"Object detection: 3D Object Detection\",\"Image generation: Image Generation\",\"Other vision process: Monocular Depth Estimation\",\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"Other vision process: Domain Generalization\",\"Other image process: Image Retrieval\",\"Action localization: Action Segmentation\",\"Other vision process: Scene Text Detection\",\"Facial recognition and modelling: Face Verification\",\"Other image process: Image Reconstruction\",\"Other vision process: Domain Adaptation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Action localization: Temporal Action Localization\",\"Activity recognition: Action Classification\",\"Other image process: Image Clustering\",\"Other 3D task: 3D Point Cloud Classification\",\"Image classification: Image Classification\",\"Activity recognition: Group Activity Recognition\",\"Object detection: Dense Object Detection\",\"Facial recognition and modelling: Face Verification\",\"Other video process: Video Frame Interpolation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Object recognition: Traffic Sign Recognition\",\"Other image process: Color Image Denoising\",\"Action localization: Temporal Action Localization\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Semantic segmentation: 3D Part Segmentation\",\"Other vision process: Visual Question Answering\",\"Image generation: Pose Transfer\",\"Other vision process: Visual Dialog\",\"Activity recognition: Action Recognition\",\"Activity detection: Action Detection\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Other vision process: Object Counting\",\"Other vision process: Curved Text Detection\",\"Object detection: 3D Object Detection\",\"Other vision process: Denoising\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Real-time Instance Segmentation\",\"Object detection: RGB Salient Object Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Image generation: Image Generation\",\"Other vision process: Scene Text Detection\",\"Facial recognition and modelling: Face Alignment\",\"Other vision process: Video Prediction\",\"Activity recognition: Action Recognition\",\"Other vision process: Visual Question Answering\",\"Semantic segmentation: Panoptic Segmentation\",\"Activity localization: Weakly Supervised Action Localization\",\"Image classification: Image Classification\",\"Other vision process: Depth Completion\",\"Pose tracking: Pose Tracking\",\"Other vision process: Domain Adaptation\",\"Activity recognition: Action Classification\",\"Other vision process: Horizon Line Estimation\",\"Activity recognition: Egocentric Activity Recognition\",\"Other vision process: Domain Generalization\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Other vision process: Formation Energy\",\"Semantic segmentation: Instance Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Activity localization: Weakly Supervised Action Localization\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"Image classification: Image Classification\",\"Facial recognition and modelling: Face Alignment\",\"Action localization: Temporal Action Localization\",\"Semantic segmentation: 3D Instance Segmentation\",\"Object tracking: Visual Object Tracking\",\"Other vision process: Visual Question Answering\",\"Other video process: Video Retrieval\",\"Other vision process: Domain Adaptation\",\"Pose estimation: Head Pose Estimation\",\"Activity recognition: Action Classification\",\"Activity recognition: Skeleton Based Action Recognition\",\"Object detection: RGB Salient Object Detection\",\"Other vision process: Crowd Counting\",\"Object detection: Object Detection\",\"Activity recognition: Action Recognition\",\"Object detection: 3D Object Detection\",\"Image generation: Image Generation\",\"Other vision process: Monocular Depth Estimation\",\"Object detection: Video Object Detection\",\"Activity recognition: Action Recognition\",\"Other vision process: Visual Question Answering\",\"Activity localization: Temporal Action Proposal Generation\",\"Image-to-image translation: Fundus to Angiography Generation\",\"Other video process: Video Retrieval\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Object tracking: Visual Object Tracking\",\"Action localization: Temporal Action Localization\",\"Object detection: Birds Eye View Object Detection\",\"Image classification: Image Classification\",\"Activity recognition: Action Recognition\",\"Other vision process: Metric Learning\",\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"Object detection: Object Detection\",\"Semantic segmentation: Semantic Segmentation\",\"Other vision process: Crowd Counting\",\"Facial recognition and modelling: Face Alignment\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Other vision process: Domain Adaptation\",\"Object detection: Lane Detection\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Semantic segmentation: Scene Segmentation\",\"Activity recognition: Egocentric Activity Recognition\",\"Semantic segmentation: Instance Segmentation\",\"Facial recognition and modelling: Face Verification\",\"Image generation: Image Generation\",\"Activity localization: Weakly Supervised Action Localization\",\"Image classification: Document Image Classification\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Other vision process: Visual Question Answering\",\"Activity recognition: Skeleton Based Action Recognition\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Other image process: Image Retrieval\",\"Action localization: Temporal Action Localization\",\"Semantic segmentation: 3D Part Segmentation\",\"Object detection: Object Detection\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Semantic segmentation: Instance Segmentation\",\"Pose estimation: 3D Human Pose Estimation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Other vision process: Visual Question Answering\",\"Other vision process: Domain Adaptation\",\"Object tracking: Multiple Object Tracking\",\"Semantic segmentation: Semantic Segmentation\",\"Other vision process: Scene Text Detection\",\"Object detection: Object Detection\",\"Pose estimation: Pose Estimation\",\"Semantic segmentation: Human Part Segmentation\",\"Image classification: Image Classification\",\"Facial recognition and modelling: Face Verification\",\"Other image process: Grayscale Image Denoising\",\"Object detection: Birds Eye View Object Detection\",\"Object recognition: Pedestrian Attribute Recognition\",\"Object detection: Weakly Supervised Object Detection\",\"Image classification: Satellite Image Classification\",\"Semantic segmentation: Panoptic Segmentation\",\"Object detection: Object Detection\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Action localization: Temporal Action Localization\",\"Other vision process: Scene Text Detection\",\"Image generation: Image Generation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Other vision process: Domain Adaptation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Semantic segmentation: Real-time Instance Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation\",\"Semantic segmentation: Semantic Segmentation\",\"Object detection: Weakly Supervised Object Detection\",\"Activity localization: Weakly Supervised Action Localization\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Pose estimation: 3D Human Pose Estimation\",\"Activity recognition: Action Recognition\",\"Image classification: Image Classification\",\"Semantic segmentation: 3D Instance Segmentation\",\"Image generation: Image Generation\",\"Other video process: Video Generation\",\"Image classification: Image Classification\",\"Semantic segmentation: Instance Segmentation\",\"Object detection: 3D Object Detection\",\"Pose estimation: Pose Estimation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Semantic segmentation: Real-time Instance Segmentation\",\"Other vision process: Visual Question Answering\",\"Activity recognition: Egocentric Activity Recognition\",\"Pose estimation: 3D Human Pose Estimation\",\"Other vision process: Scene Graph Generation\",\"Pose estimation: Pose Estimation\",\"Semantic segmentation: Lesion Segmentation\",\"Object detection: 3D Object Detection\",\"Action localization: Action Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Object detection: Video Object Detection\",\"Semantic segmentation: 3D Semantic Instance Segmentation\",\"Other video process: Video Frame Interpolation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: Video Semantic Segmentation\",\"Pose estimation: 3D Human Pose Estimation\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Year\"},\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"tickmode\":\"auto\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{},\"categoryorder\":\"array\",\"categoryarray\":[\"Semantic segmentation: Skin Cancer Segmentation\",\"Semantic segmentation: Real-time Instance Segmentation\",\"Semantic segmentation: Panoptic Segmentation\",\"Semantic segmentation: Pancreas Segmentation\",\"Semantic segmentation: Nuclear Segmentation\",\"Semantic segmentation: Multi-tissue Nucleus Segmentation\",\"Semantic segmentation: Medical Image Segmentation\",\"Semantic segmentation: Lung Nodule Segmentation\",\"Semantic segmentation: Retinal Vessel Segmentation\",\"Semantic segmentation: Scene Segmentation\",\"Semantic segmentation: Lesion Segmentation\",\"Semantic segmentation: Semantic Segmentation\",\"Semantic segmentation: 3D Part Segmentation\",\"Semantic segmentation: 3D Semantic Instance Segmentation\",\"Semantic segmentation: 3D Semantic Segmentation\",\"Semantic segmentation: Brain Tumor Segmentation\",\"Semantic segmentation: Human Part Segmentation\",\"Semantic segmentation: Electron Microscopy Image Segmentation\",\"Semantic segmentation: 3D Instance Segmentation\",\"Semantic segmentation: Video Semantic Segmentation\",\"Semantic segmentation: Instance Segmentation\",\"Pose tracking: Pose Tracking\",\"Pose estimation: 6D Pose Estimation using RGBD\",\"Pose estimation: 6D Pose Estimation using RGB\",\"Pose estimation: 6D Pose Estimation\",\"Pose estimation: 3D Human Pose Estimation\",\"Pose estimation: Hand Pose Estimation\",\"Pose estimation: Keypoint Detection\",\"Pose estimation: Head Pose Estimation\",\"Pose estimation: Pose Estimation\",\"Pose estimation: Weakly-supervised 3D Human Pose Estimation\",\"Other vision process: Denoising\",\"Other vision process: Object Counting\",\"Other vision process: Horizon Line Estimation\",\"Other vision process: Unsupervised Domain Adaptation\",\"Other vision process: Visual Dialog\",\"Other vision process: Video Prediction\",\"Other vision process: Metric Learning\",\"Other vision process: Monocular Depth Estimation\",\"Other vision process: Crowd Counting\",\"Other vision process: Curved Text Detection\",\"Other vision process: Multivariate Time Series Imputation\",\"Other vision process: Depth Completion\",\"Other vision process: Scene Graph Generation\",\"Other vision process: Domain Generalization\",\"Other vision process: Scene Text Detection\",\"Other vision process: Visual Question Answering\",\"Other vision process: Domain Adaptation\",\"Other vision process: Formation Energy\",\"Other video process: Video Retrieval\",\"Other video process: Video Frame Interpolation\",\"Other video process: Video Generation\",\"Other image process: Image Reconstruction\",\"Other image process: Image Retrieval\",\"Other image process: Grayscale Image Denoising\",\"Other image process: Aesthetics Quality Assessment\",\"Other image process: Image Clustering\",\"Other image process: Color Image Denoising\",\"Other 3D task: 3D Shape Classification\",\"Other 3D task: 3D Room Layouts From A Single RGB Panorama\",\"Other 3D task: 3D Reconstruction\",\"Other 3D task: 3D Object Reconstruction\",\"Other 3D task: 3D Point Cloud Classification\",\"Object tracking: Multiple Object Tracking\",\"Object tracking: Visual Object Tracking\",\"Object recognition: Pedestrian Attribute Recognition\",\"Object recognition: Traffic Sign Recognition\",\"Object detection: Dense Object Detection\",\"Object detection: Object Detection\",\"Object detection: Pedestrian Detection\",\"Object detection: Weakly Supervised Object Detection\",\"Object detection: Birds Eye View Object Detection\",\"Object detection: Lane Detection\",\"Object detection: 3D Object Detection\",\"Object detection: Video Object Detection\",\"Object detection: RGB Salient Object Detection\",\"Image-to-image translation: Fundus to Angiography Generation\",\"Image generation: Pose Transfer\",\"Image generation: Conditional Image Generation\",\"Image generation: Image Generation\",\"Image classification: Document Image Classification\",\"Image classification: Sequential Image Classification\",\"Image classification: Satellite Image Classification\",\"Image classification: Retinal OCT Disease Classification\",\"Image classification: Hyperspectral Image Classification\",\"Image classification: Unsupervised Image Classification\",\"Image classification: Image Classification\",\"Gesture recognition: Hand Gesture Recognition\",\"Facial recognition and modelling: Unsupervised Facial Landmark Detection\",\"Facial recognition and modelling: Face Identification\",\"Facial recognition and modelling: Face Alignment\",\"Facial recognition and modelling: Face Detection\",\"Facial recognition and modelling: Facial Expression Recognition\",\"Facial recognition and modelling: Face Verification\",\"Facial recognition and modelling: Facial Landmark Detection\",\"Emotion recognition: Emotion Recognition in Conversation\",\"Activity recognition: Skeleton Based Action Recognition\",\"Activity recognition: Multimodal Activity Recognition\",\"Activity recognition: Egocentric Activity Recognition\",\"Activity recognition: Human Interaction Recognition\",\"Activity recognition: Group Activity Recognition\",\"Activity recognition: Action Recognition\",\"Activity recognition: Action Classification\",\"Activity localization: Temporal Action Proposal Generation\",\"Activity localization: Weakly Supervised Action Localization\",\"Activity detection: Action Detection\",\"Action localization: Action Segmentation\",\"Action localization: Temporal Action Localization\"],\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"side\":\"left\"},\"legend\":{\"title\":{\"text\":\"task\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Vision process\",\"y\":0.995},\"font\":{\"size\":21},\"showlegend\":false,\"plot_bgcolor\":\"white\",\"height\":4023.0000000000005,\"width\":1500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a5ad99f9-6f07-4d14-bd02-557ff03a930a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative yearly distribution of state-of-the-art (SOTA) averaged gain ratio values - computer vision process - single arrows removed\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2012",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.5546,
          0.0189
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2013",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.1073,
          1,
          1,
          0.0397,
          0.0553
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2014",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.021,
          0.8122,
          0.4737,
          0.289,
          0.1446,
          0.9752,
          0.2834,
          1,
          0.2375,
          0.2513,
          0.2406,
          0.1392
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2015",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.0877,
          0.8858,
          0.0451,
          0.3958,
          0.0134,
          0.3753,
          0.2327,
          0.0741,
          0.426,
          0.1007,
          0.2878,
          0.8426,
          0.3991,
          0.2321,
          0.1675,
          0.1142,
          0.4459,
          0.2457,
          0.4321,
          0.5,
          0.117,
          0.4701,
          0.0854,
          1,
          0.1888,
          0.3222,
          0.0331,
          0.15,
          0.1785,
          0.4339,
          0.1637,
          0.0779,
          0.7477,
          0.4051,
          1,
          0.9509,
          0.5016
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2016",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          1,
          0.0707,
          0.6524,
          0.0478,
          0.056,
          0.2757,
          0.0258,
          1,
          0.3359,
          0.2119,
          0.4252,
          0.5954,
          0.7322,
          0.1471,
          0.4868,
          0.4832,
          0.4271,
          0.6898,
          0.0092,
          0.8889,
          0.3005,
          0.0418,
          0.1069,
          0.2762,
          0.072,
          0.2999,
          0.0741,
          0.572,
          0.3146,
          0.061,
          0.3235,
          0.3625,
          0.0241,
          0.8371,
          1,
          0.1233,
          0.2999,
          0.6547,
          1,
          0.0251,
          0.2523,
          0.5511,
          0.1736,
          0.0915,
          0.0144,
          0.3082,
          0.4115,
          0.2784,
          0.0093,
          0.0142,
          0.0606,
          0.0684,
          0.3326,
          0.3571,
          0,
          0.2824,
          0.2492,
          0.2441,
          0.096,
          0.3399,
          0.2329,
          0.2215,
          0.5084,
          0.6006,
          0.674,
          0.2304,
          0.6019,
          0.3137,
          0.1022,
          0.5259,
          0.4898,
          0.1001,
          0.5647,
          0.1125,
          0.1929,
          0.0388,
          0.1288,
          0.1429,
          0.9756
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2017",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.1328,
          0.2025,
          0.6995,
          0.0692,
          0.0541,
          0.9524,
          0.5645,
          0.1574,
          0.3612,
          0.1565,
          0.2235,
          0.0303,
          0.6528,
          0.5668,
          0.4081,
          0.2484,
          0.1096,
          0.2099,
          0.2205,
          0.1193,
          0.5877,
          0.2105,
          0.0331,
          0.4378,
          0.2112,
          1,
          0.2041,
          0.1026,
          0.3846,
          0.7172,
          0.103,
          0.1968,
          0.1629,
          0.2194,
          0.1332,
          0.8514,
          0.1767,
          0.4855,
          1,
          0.4565,
          0.1679,
          0.3172,
          0.0758,
          0.4474,
          0.4983,
          0.528,
          0.2366,
          0.7853,
          0.4284,
          1,
          0.8352,
          0.2561,
          0.0559,
          0.0607,
          0.2867,
          0.6277,
          0.2514,
          0.6175,
          0.3412,
          0.1244,
          0.4844,
          0.4142,
          0.5909,
          0.4898,
          0.6641,
          0.1161,
          0.7104,
          1,
          0.02,
          0.6614,
          0.0095,
          0.0387,
          0.4581,
          0.2874,
          0.1758,
          1,
          0.3268,
          0.6997,
          0.0363,
          0.0645,
          0.2688,
          0.5913,
          0.0083,
          1,
          0.219,
          0.0624,
          0.88,
          0.3456,
          0.0951,
          0.1108,
          0.4921,
          0.6932,
          0.5449,
          0.8514,
          0.0551,
          0.1598,
          0.474,
          0.3904,
          0.0103,
          1,
          1,
          0.0758,
          0.0809,
          1,
          0.6429,
          1,
          0.0909,
          0.9952,
          0.4958,
          0.2127,
          0.9847,
          0.4337,
          0.2492,
          0.2941,
          0.7097,
          0.5297,
          0.656,
          0.2656,
          0.6273,
          0.435,
          0.2438,
          0.619,
          0.5452,
          0.1491,
          0.6998,
          0.789,
          0.1976,
          0.5062,
          0.6503,
          0.3521,
          0.1702,
          0.3953,
          0.069,
          0.2578,
          0.383,
          0.2407,
          0.6857,
          0.0238,
          0.0546,
          0.0332,
          0.046,
          0.1898,
          1,
          0.5028,
          1,
          0.2406,
          0.5,
          0.1097,
          0.5417,
          0.0083,
          0.613,
          0.3524,
          0.3532,
          0.0247,
          0.7094
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2018",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.0583,
          0.5145,
          0.4342,
          0.3197,
          0.0837,
          0.3661,
          0.3059,
          0.4232,
          0.6486,
          0.2233,
          0.3333,
          0.9695,
          0.9199,
          0.6229,
          0.1321,
          0.0351,
          0.628,
          0.4938,
          0.2543,
          0.0865,
          0.3996,
          0.2075,
          0.3005,
          0.6875,
          1,
          0.2155,
          0.5545,
          0.387,
          0.1771,
          0.1202,
          0.6418,
          0.1351,
          0.1099,
          0.2131,
          0.2249,
          0.0083,
          0.1587,
          0.0929,
          1,
          0.5083,
          0.0203,
          0.4588,
          0.3293,
          1,
          1,
          0.2555,
          0.1184,
          0.0985,
          0.5245,
          0.0647,
          1,
          0.1925,
          0.0067,
          0.3804,
          0.6801,
          1,
          0.211,
          0.1429,
          0.3034,
          0.2694,
          0.5604,
          0.5045,
          0.6667,
          0.1608,
          0.6147,
          1,
          0.6543,
          0.0149,
          0.6951,
          0.714,
          0.2306,
          0.145,
          0.028,
          0.2865,
          0.4536,
          0.6039,
          1,
          0.1822,
          0.3698,
          1,
          0.7435,
          0.1992,
          0.5603,
          0.2047,
          0.1128,
          0.5515,
          0.2077,
          0.9967,
          0.1235,
          0.4686,
          0.297,
          0.7711,
          0.7567,
          0.5207,
          0.3576,
          1,
          0.3129,
          0.2185,
          1,
          0.3854,
          0.4436,
          1,
          0.5357,
          0.9856,
          0.4007,
          0.1059,
          0.1448,
          1,
          0.5,
          0.0631,
          0.1158,
          0.7291,
          0.5625,
          1,
          0.4543,
          0.3547,
          0.0321,
          0.6807,
          0.6698,
          0.0244,
          0.2687,
          0.1255,
          0.6183,
          0.1346,
          0.8995,
          0.1071,
          0.0378,
          0.086,
          0.1442,
          1,
          0.4091,
          0.0658,
          0.1139,
          0.0079,
          0.2609,
          0.2227,
          0.0225,
          0.0087,
          0.2577,
          1,
          0.6675,
          0.4542,
          0.6415,
          0.7361,
          0.34,
          0.5661,
          0.0808,
          0.0423,
          0.3352,
          0.1431,
          0.7527,
          0.0684,
          0.2788,
          0.5387,
          0.2295,
          1,
          0.7924,
          1,
          0.5789,
          0.3724,
          0.3967,
          0.0755,
          0.2526,
          1,
          0.0286,
          1,
          0.7779,
          1,
          0.1767,
          0.5,
          0.5107,
          0.5811,
          0.1675,
          0.027,
          0.4227,
          0.1343,
          0.2902,
          0.8571,
          0.6789,
          0.2451,
          0.2741,
          1,
          0.1735,
          0.4718,
          0.5095,
          0.221,
          0.0597,
          0.6087,
          0.3083,
          0.3195,
          0.549
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2019",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.0894,
          0.1641,
          0.3272,
          1,
          0.4,
          0.0323,
          0.3127,
          0.1212,
          1,
          0.2126,
          0.7353,
          0.0882,
          0.3141,
          0.564,
          0.371,
          0.1086,
          0.1323,
          0.0183,
          1,
          1,
          0.0018,
          0.1,
          0.036,
          1,
          0.068,
          0.6096,
          0.1324,
          0.4587,
          0.1426,
          0.069,
          0.098,
          0.2573,
          1,
          0.5464,
          1,
          1,
          0.6399,
          0.3213,
          0.3109,
          0.1256,
          0.2896,
          0.4466,
          1,
          0.9015,
          0.162,
          0.3733,
          0.1081,
          0.3059,
          0.6033,
          0.1943,
          0.6718,
          0.1348,
          0.2881,
          0.6667,
          1,
          0.155,
          1,
          0.1123,
          0.3147,
          1,
          0.4213,
          0.0663,
          1,
          0.4321,
          0.3247,
          0.7679,
          0.3574,
          0.0989,
          0.1341,
          0.6555,
          0.7784,
          0.6714,
          0.0519,
          0.339,
          0.0382,
          1,
          0.0144,
          0.4235,
          0.7854,
          0.7002,
          0.1623,
          1,
          0.0083,
          0.4063,
          0.0225,
          1,
          0.4905,
          0.2258,
          0.7067,
          1,
          0.2194,
          1,
          0.0589,
          0.0493,
          1,
          0.2305,
          0.0932,
          0.0985,
          0.2174,
          0.2823,
          0.066,
          0.6079,
          0.9629,
          0.1399,
          0.4736,
          0.0171,
          0.7183,
          0.9021,
          0.1952,
          0.0318,
          0.5126,
          0.0989,
          0.2932,
          0.1509,
          0.066,
          0.5481,
          0.2415,
          0.7593,
          0.4898,
          1,
          0.2647,
          0.835,
          0.3796,
          0.6173,
          0.3316,
          0.3395,
          0.3549,
          1,
          0.1793,
          0.0978,
          0.1469,
          0.0324,
          0.7344,
          1,
          0.4583,
          0.3309,
          0.6596,
          1,
          1,
          0.2284,
          0.3737,
          0.3,
          0.1982,
          1,
          1,
          0.4622,
          0.2029,
          0.046,
          0.1412,
          0.26,
          0.1818,
          0.1053,
          0.0137,
          0.071,
          0.9068,
          1,
          0.4198,
          0.1111,
          0.5071,
          0.0378,
          0.5032,
          0.0388,
          0.0466,
          0.0205,
          0.2772,
          0.1955,
          0.5483,
          0.9715,
          0.2791,
          1,
          0.0533,
          0.7974,
          0.471,
          0.0635,
          1,
          0.0838,
          0.3797,
          0.057,
          0.0114,
          0.0848,
          1,
          0.6248,
          0.2438,
          0.8357,
          0.9839,
          0.2369,
          0.0283,
          0.3356,
          0.172,
          0.04,
          0.0932,
          0.1636,
          0.2424,
          0.1413,
          0.8135,
          0.7122
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2020",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.25,
          0.0444,
          1,
          1,
          0.4159,
          0.2233,
          0.5266,
          0.4211,
          0.0098,
          0.19,
          0.0152,
          0.5909,
          0.3135,
          0.6966,
          0.8206,
          0.2075,
          0.4893,
          0.8021,
          0.4197,
          0.0204,
          0.4396
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 20
        },
        "height": 400,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1000,
        "xaxis": {
         "categoryarray": [
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020
         ],
         "categoryorder": "array",
         "tickmode": "linear"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"03882416-f0bb-47e5-9e4e-ac19ff26f40d\" class=\"plotly-graph-div\" style=\"height:400px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"03882416-f0bb-47e5-9e4e-ac19ff26f40d\")) {                    Plotly.newPlot(                        \"03882416-f0bb-47e5-9e4e-ac19ff26f40d\",                        [{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2012\",\"whiskerwidth\":0.1,\"y\":[0.5546,0.0189],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2013\",\"whiskerwidth\":0.1,\"y\":[0.1073,1.0,1.0,0.0397,0.0553],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2014\",\"whiskerwidth\":0.1,\"y\":[0.021,0.8122,0.4737,0.289,0.1446,0.9752,0.2834,1.0,0.2375,0.2513,0.2406,0.1392],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2015\",\"whiskerwidth\":0.1,\"y\":[0.0877,0.8858,0.0451,0.3958,0.0134,0.3753,0.2327,0.0741,0.426,0.1007,0.2878,0.8426,0.3991,0.2321,0.1675,0.1142,0.4459,0.2457,0.4321,0.5,0.117,0.4701,0.0854,1.0,0.1888,0.3222,0.0331,0.15,0.1785,0.4339,0.1637,0.0779,0.7477,0.4051,1.0,0.9509,0.5016],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2016\",\"whiskerwidth\":0.1,\"y\":[1.0,0.0707,0.6524,0.0478,0.056,0.2757,0.0258,1.0,0.3359,0.2119,0.4252,0.5954,0.7322,0.1471,0.4868,0.4832,0.4271,0.6898,0.0092,0.8889,0.3005,0.0418,0.1069,0.2762,0.072,0.2999,0.0741,0.572,0.3146,0.061,0.3235,0.3625,0.0241,0.8371,1.0,0.1233,0.2999,0.6547,1.0,0.0251,0.2523,0.5511,0.1736,0.0915,0.0144,0.3082,0.4115,0.2784,0.0093,0.0142,0.0606,0.0684,0.3326,0.3571,0.0,0.2824,0.2492,0.2441,0.096,0.3399,0.2329,0.2215,0.5084,0.6006,0.674,0.2304,0.6019,0.3137,0.1022,0.5259,0.4898,0.1001,0.5647,0.1125,0.1929,0.0388,0.1288,0.1429,0.9756],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2017\",\"whiskerwidth\":0.1,\"y\":[0.1328,0.2025,0.6995,0.0692,0.0541,0.9524,0.5645,0.1574,0.3612,0.1565,0.2235,0.0303,0.6528,0.5668,0.4081,0.2484,0.1096,0.2099,0.2205,0.1193,0.5877,0.2105,0.0331,0.4378,0.2112,1.0,0.2041,0.1026,0.3846,0.7172,0.103,0.1968,0.1629,0.2194,0.1332,0.8514,0.1767,0.4855,1.0,0.4565,0.1679,0.3172,0.0758,0.4474,0.4983,0.528,0.2366,0.7853,0.4284,1.0,0.8352,0.2561,0.0559,0.0607,0.2867,0.6277,0.2514,0.6175,0.3412,0.1244,0.4844,0.4142,0.5909,0.4898,0.6641,0.1161,0.7104,1.0,0.02,0.6614,0.0095,0.0387,0.4581,0.2874,0.1758,1.0,0.3268,0.6997,0.0363,0.0645,0.2688,0.5913,0.0083,1.0,0.219,0.0624,0.88,0.3456,0.0951,0.1108,0.4921,0.6932,0.5449,0.8514,0.0551,0.1598,0.474,0.3904,0.0103,1.0,1.0,0.0758,0.0809,1.0,0.6429,1.0,0.0909,0.9952,0.4958,0.2127,0.9847,0.4337,0.2492,0.2941,0.7097,0.5297,0.656,0.2656,0.6273,0.435,0.2438,0.619,0.5452,0.1491,0.6998,0.789,0.1976,0.5062,0.6503,0.3521,0.1702,0.3953,0.069,0.2578,0.383,0.2407,0.6857,0.0238,0.0546,0.0332,0.046,0.1898,1.0,0.5028,1.0,0.2406,0.5,0.1097,0.5417,0.0083,0.613,0.3524,0.3532,0.0247,0.7094],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2018\",\"whiskerwidth\":0.1,\"y\":[0.0583,0.5145,0.4342,0.3197,0.0837,0.3661,0.3059,0.4232,0.6486,0.2233,0.3333,0.9695,0.9199,0.6229,0.1321,0.0351,0.628,0.4938,0.2543,0.0865,0.3996,0.2075,0.3005,0.6875,1.0,0.2155,0.5545,0.387,0.1771,0.1202,0.6418,0.1351,0.1099,0.2131,0.2249,0.0083,0.1587,0.0929,1.0,0.5083,0.0203,0.4588,0.3293,1.0,1.0,0.2555,0.1184,0.0985,0.5245,0.0647,1.0,0.1925,0.0067,0.3804,0.6801,1.0,0.211,0.1429,0.3034,0.2694,0.5604,0.5045,0.6667,0.1608,0.6147,1.0,0.6543,0.0149,0.6951,0.714,0.2306,0.145,0.028,0.2865,0.4536,0.6039,1.0,0.1822,0.3698,1.0,0.7435,0.1992,0.5603,0.2047,0.1128,0.5515,0.2077,0.9967,0.1235,0.4686,0.297,0.7711,0.7567,0.5207,0.3576,1.0,0.3129,0.2185,1.0,0.3854,0.4436,1.0,0.5357,0.9856,0.4007,0.1059,0.1448,1.0,0.5,0.0631,0.1158,0.7291,0.5625,1.0,0.4543,0.3547,0.0321,0.6807,0.6698,0.0244,0.2687,0.1255,0.6183,0.1346,0.8995,0.1071,0.0378,0.086,0.1442,1.0,0.4091,0.0658,0.1139,0.0079,0.2609,0.2227,0.0225,0.0087,0.2577,1.0,0.6675,0.4542,0.6415,0.7361,0.34,0.5661,0.0808,0.0423,0.3352,0.1431,0.7527,0.0684,0.2788,0.5387,0.2295,1.0,0.7924,1.0,0.5789,0.3724,0.3967,0.0755,0.2526,1.0,0.0286,1.0,0.7779,1.0,0.1767,0.5,0.5107,0.5811,0.1675,0.027,0.4227,0.1343,0.2902,0.8571,0.6789,0.2451,0.2741,1.0,0.1735,0.4718,0.5095,0.221,0.0597,0.6087,0.3083,0.3195,0.549],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2019\",\"whiskerwidth\":0.1,\"y\":[0.0894,0.1641,0.3272,1.0,0.4,0.0323,0.3127,0.1212,1.0,0.2126,0.7353,0.0882,0.3141,0.564,0.371,0.1086,0.1323,0.0183,1.0,1.0,0.0018,0.1,0.036,1.0,0.068,0.6096,0.1324,0.4587,0.1426,0.069,0.098,0.2573,1.0,0.5464,1.0,1.0,0.6399,0.3213,0.3109,0.1256,0.2896,0.4466,1.0,0.9015,0.162,0.3733,0.1081,0.3059,0.6033,0.1943,0.6718,0.1348,0.2881,0.6667,1.0,0.155,1.0,0.1123,0.3147,1.0,0.4213,0.0663,1.0,0.4321,0.3247,0.7679,0.3574,0.0989,0.1341,0.6555,0.7784,0.6714,0.0519,0.339,0.0382,1.0,0.0144,0.4235,0.7854,0.7002,0.1623,1.0,0.0083,0.4063,0.0225,1.0,0.4905,0.2258,0.7067,1.0,0.2194,1.0,0.0589,0.0493,1.0,0.2305,0.0932,0.0985,0.2174,0.2823,0.066,0.6079,0.9629,0.1399,0.4736,0.0171,0.7183,0.9021,0.1952,0.0318,0.5126,0.0989,0.2932,0.1509,0.066,0.5481,0.2415,0.7593,0.4898,1.0,0.2647,0.835,0.3796,0.6173,0.3316,0.3395,0.3549,1.0,0.1793,0.0978,0.1469,0.0324,0.7344,1.0,0.4583,0.3309,0.6596,1.0,1.0,0.2284,0.3737,0.3,0.1982,1.0,1.0,0.4622,0.2029,0.046,0.1412,0.26,0.1818,0.1053,0.0137,0.071,0.9068,1.0,0.4198,0.1111,0.5071,0.0378,0.5032,0.0388,0.0466,0.0205,0.2772,0.1955,0.5483,0.9715,0.2791,1.0,0.0533,0.7974,0.471,0.0635,1.0,0.0838,0.3797,0.057,0.0114,0.0848,1.0,0.6248,0.2438,0.8357,0.9839,0.2369,0.0283,0.3356,0.172,0.04,0.0932,0.1636,0.2424,0.1413,0.8135,0.7122],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2020\",\"whiskerwidth\":0.1,\"y\":[0.25,0.0444,1.0,1.0,0.4159,0.2233,0.5266,0.4211,0.0098,0.19,0.0152,0.5909,0.3135,0.6966,0.8206,0.2075,0.4893,0.8021,0.4197,0.0204,0.4396],\"type\":\"box\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"font\":{\"size\":20},\"xaxis\":{\"tickmode\":\"linear\",\"categoryorder\":\"array\",\"categoryarray\":[2011,2012,2013,2014,2015,2016,2017,2018,2019,2020]},\"height\":400,\"width\":1000,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('03882416-f0bb-47e5-9e4e-ac19ff26f40d');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e1dfb_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >2012</th>\n",
       "      <th class=\"col_heading level0 col1\" >2013</th>\n",
       "      <th class=\"col_heading level0 col2\" >2014</th>\n",
       "      <th class=\"col_heading level0 col3\" >2015</th>\n",
       "      <th class=\"col_heading level0 col4\" >2016</th>\n",
       "      <th class=\"col_heading level0 col5\" >2017</th>\n",
       "      <th class=\"col_heading level0 col6\" >2018</th>\n",
       "      <th class=\"col_heading level0 col7\" >2019</th>\n",
       "      <th class=\"col_heading level0 col8\" >2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_e1dfb_row0_col0\" class=\"data row0 col0\" >2.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col1\" class=\"data row0 col1\" >5.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col2\" class=\"data row0 col2\" >12.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col3\" class=\"data row0 col3\" >37.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col4\" class=\"data row0 col4\" >79.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col5\" class=\"data row0 col5\" >155.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col6\" class=\"data row0 col6\" >191.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col7\" class=\"data row0 col7\" >196.000000</td>\n",
       "      <td id=\"T_e1dfb_row0_col8\" class=\"data row0 col8\" >21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_e1dfb_row1_col0\" class=\"data row1 col0\" >0.287000</td>\n",
       "      <td id=\"T_e1dfb_row1_col1\" class=\"data row1 col1\" >0.440000</td>\n",
       "      <td id=\"T_e1dfb_row1_col2\" class=\"data row1 col2\" >0.406000</td>\n",
       "      <td id=\"T_e1dfb_row1_col3\" class=\"data row1 col3\" >0.355000</td>\n",
       "      <td id=\"T_e1dfb_row1_col4\" class=\"data row1 col4\" >0.332000</td>\n",
       "      <td id=\"T_e1dfb_row1_col5\" class=\"data row1 col5\" >0.402000</td>\n",
       "      <td id=\"T_e1dfb_row1_col6\" class=\"data row1 col6\" >0.427000</td>\n",
       "      <td id=\"T_e1dfb_row1_col7\" class=\"data row1 col7\" >0.419000</td>\n",
       "      <td id=\"T_e1dfb_row1_col8\" class=\"data row1 col8\" >0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_e1dfb_row2_col0\" class=\"data row2 col0\" >0.379000</td>\n",
       "      <td id=\"T_e1dfb_row2_col1\" class=\"data row2 col1\" >0.511000</td>\n",
       "      <td id=\"T_e1dfb_row2_col2\" class=\"data row2 col2\" >0.336000</td>\n",
       "      <td id=\"T_e1dfb_row2_col3\" class=\"data row2 col3\" >0.287000</td>\n",
       "      <td id=\"T_e1dfb_row2_col4\" class=\"data row2 col4\" >0.279000</td>\n",
       "      <td id=\"T_e1dfb_row2_col5\" class=\"data row2 col5\" >0.299000</td>\n",
       "      <td id=\"T_e1dfb_row2_col6\" class=\"data row2 col6\" >0.311000</td>\n",
       "      <td id=\"T_e1dfb_row2_col7\" class=\"data row2 col7\" >0.342000</td>\n",
       "      <td id=\"T_e1dfb_row2_col8\" class=\"data row2 col8\" >0.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_e1dfb_row3_col0\" class=\"data row3 col0\" >0.019000</td>\n",
       "      <td id=\"T_e1dfb_row3_col1\" class=\"data row3 col1\" >0.040000</td>\n",
       "      <td id=\"T_e1dfb_row3_col2\" class=\"data row3 col2\" >0.021000</td>\n",
       "      <td id=\"T_e1dfb_row3_col3\" class=\"data row3 col3\" >0.013000</td>\n",
       "      <td id=\"T_e1dfb_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "      <td id=\"T_e1dfb_row3_col5\" class=\"data row3 col5\" >0.008000</td>\n",
       "      <td id=\"T_e1dfb_row3_col6\" class=\"data row3 col6\" >0.007000</td>\n",
       "      <td id=\"T_e1dfb_row3_col7\" class=\"data row3 col7\" >0.002000</td>\n",
       "      <td id=\"T_e1dfb_row3_col8\" class=\"data row3 col8\" >0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_e1dfb_row4_col0\" class=\"data row4 col0\" >0.153000</td>\n",
       "      <td id=\"T_e1dfb_row4_col1\" class=\"data row4 col1\" >0.055000</td>\n",
       "      <td id=\"T_e1dfb_row4_col2\" class=\"data row4 col2\" >0.214000</td>\n",
       "      <td id=\"T_e1dfb_row4_col3\" class=\"data row4 col3\" >0.117000</td>\n",
       "      <td id=\"T_e1dfb_row4_col4\" class=\"data row4 col4\" >0.098000</td>\n",
       "      <td id=\"T_e1dfb_row4_col5\" class=\"data row4 col5\" >0.157000</td>\n",
       "      <td id=\"T_e1dfb_row4_col6\" class=\"data row4 col6\" >0.160000</td>\n",
       "      <td id=\"T_e1dfb_row4_col7\" class=\"data row4 col7\" >0.119000</td>\n",
       "      <td id=\"T_e1dfb_row4_col8\" class=\"data row4 col8\" >0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_e1dfb_row5_col0\" class=\"data row5 col0\" >0.287000</td>\n",
       "      <td id=\"T_e1dfb_row5_col1\" class=\"data row5 col1\" >0.107000</td>\n",
       "      <td id=\"T_e1dfb_row5_col2\" class=\"data row5 col2\" >0.267000</td>\n",
       "      <td id=\"T_e1dfb_row5_col3\" class=\"data row5 col3\" >0.288000</td>\n",
       "      <td id=\"T_e1dfb_row5_col4\" class=\"data row5 col4\" >0.278000</td>\n",
       "      <td id=\"T_e1dfb_row5_col5\" class=\"data row5 col5\" >0.352000</td>\n",
       "      <td id=\"T_e1dfb_row5_col6\" class=\"data row5 col6\" >0.366000</td>\n",
       "      <td id=\"T_e1dfb_row5_col7\" class=\"data row5 col7\" >0.314000</td>\n",
       "      <td id=\"T_e1dfb_row5_col8\" class=\"data row5 col8\" >0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_e1dfb_row6_col0\" class=\"data row6 col0\" >0.421000</td>\n",
       "      <td id=\"T_e1dfb_row6_col1\" class=\"data row6 col1\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row6_col2\" class=\"data row6 col2\" >0.558000</td>\n",
       "      <td id=\"T_e1dfb_row6_col3\" class=\"data row6 col3\" >0.446000</td>\n",
       "      <td id=\"T_e1dfb_row6_col4\" class=\"data row6 col4\" >0.499000</td>\n",
       "      <td id=\"T_e1dfb_row6_col5\" class=\"data row6 col5\" >0.618000</td>\n",
       "      <td id=\"T_e1dfb_row6_col6\" class=\"data row6 col6\" >0.642000</td>\n",
       "      <td id=\"T_e1dfb_row6_col7\" class=\"data row6 col7\" >0.702000</td>\n",
       "      <td id=\"T_e1dfb_row6_col8\" class=\"data row6 col8\" >0.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e1dfb_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_e1dfb_row7_col0\" class=\"data row7 col0\" >0.555000</td>\n",
       "      <td id=\"T_e1dfb_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row7_col2\" class=\"data row7 col2\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row7_col3\" class=\"data row7 col3\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row7_col4\" class=\"data row7 col4\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row7_col5\" class=\"data row7 col5\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row7_col6\" class=\"data row7 col6\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_e1dfb_row7_col8\" class=\"data row7 col8\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25218291208>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computer Vision Process, showing anchors and complete trajectories (single arros removed).\n",
    "traj_df=plot_task_trajectory(\"ITO_00101\", \"Vision process\", anchor)\n",
    "\n",
    "boxplots=get_boxplot(traj_df)\n",
    "results=get_statistics(traj_df)\n",
    "\n",
    "#print statistics\n",
    "print(\"Comparative yearly distribution of state-of-the-art (SOTA) averaged gain ratio values - computer vision process - single arrows removed\")\n",
    "boxplots.show()\n",
    "results.T.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w6_X-zf8pdki",
    "outputId": "2d47a5c2-de0f-4862-b938-16741c14ee25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Computer code processing: Code Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Computer code processing: Code Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-08",
          "2018-03",
          "2018-04",
          "2018-10",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Dialog process: Visual Dialog",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Dialog process: Visual Dialog",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-09",
          "2017-11",
          "2017-09",
          "2019-02",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Dialog process: Visual Dialog",
          "Dialog process: Visual Dialog",
          "Dialog process: Visual Dialog",
          "Dialog process: Visual Dialog",
          "Dialog process: Visual Dialog"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Dialog process: Dialog State Tracking",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Dialog process: Dialog State Tracking",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-10",
          "2018-05"
         ],
         "xaxis": "x",
         "y": [
          "Dialog process: Dialog State Tracking",
          "Dialog process: Dialog State Tracking"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Dialog process: Dialog Act Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Dialog process: Dialog Act Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-11",
          "2017-09"
         ],
         "xaxis": "x",
         "y": [
          "Dialog process: Dialog Act Classification",
          "Dialog process: Dialog Act Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Inference and reasoning: Common Sense Reasoning",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Inference and reasoning: Common Sense Reasoning",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-10",
          "2018-05",
          "2019-02",
          "2019-06",
          "2019-07",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Inference and reasoning: Natural Language Inference",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Inference and reasoning: Natural Language Inference",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2019-07",
          "2019-06",
          "2019-01",
          "2018-10",
          "2018-06",
          "2018-05",
          "2018-04",
          "2017-12",
          "2018-09",
          "2017-09",
          "2017-11",
          "2014-08",
          "2016-09",
          "2017-02",
          "2015-08"
         ],
         "xaxis": "x",
         "y": [
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Information extraction: Named Entity Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Information extraction: Named Entity Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-06",
          "2019-11",
          "2019-03",
          "2019-01",
          "2019-08",
          "2018-10",
          "2018-08"
         ],
         "xaxis": "x",
         "y": [
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Information extraction: Joint Entity and Relation Extraction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Information extraction: Joint Entity and Relation Extraction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Information extraction: Joint Entity and Relation Extraction",
          "Information extraction: Joint Entity and Relation Extraction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Information extraction: Relation Extraction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Information extraction: Relation Extraction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2019-11",
          "2019-06",
          "2019-07",
          "2019-05",
          "2016-01",
          "2020-03",
          "2020-04",
          "2017-07",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2018-12",
          "2019-02",
          "2019-03",
          "2019-04",
          "2017-09"
         ],
         "xaxis": "x",
         "y": [
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Information extraction: Chinese Named Entity Recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Information extraction: Chinese Named Entity Recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-11",
          "2019-07",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Information extraction: Chinese Named Entity Recognition",
          "Information extraction: Chinese Named Entity Recognition",
          "Information extraction: Chinese Named Entity Recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Information retrieval: Conversational Response Selection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Information retrieval: Conversational Response Selection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-01",
          "2019-04",
          "2018-03",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Information retrieval: Conversational Response Selection",
          "Information retrieval: Conversational Response Selection",
          "Information retrieval: Conversational Response Selection",
          "Information retrieval: Conversational Response Selection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Machine translation: Unsupervised Machine Translation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Machine translation: Unsupervised Machine Translation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-10",
          "2019-01",
          "2019-05",
          "2019-02"
         ],
         "xaxis": "x",
         "y": [
          "Machine translation: Unsupervised Machine Translation",
          "Machine translation: Unsupervised Machine Translation",
          "Machine translation: Unsupervised Machine Translation",
          "Machine translation: Unsupervised Machine Translation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Natural language generation: Text Summarization",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Natural language generation: Text Summarization",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-05",
          "2019-10",
          "2019-04",
          "2018-08",
          "2018-07",
          "2017-06",
          "2016-06",
          "2016-02",
          "2015-09"
         ],
         "xaxis": "x",
         "y": [
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Summarization"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Natural language generation: Machine Translation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Natural language generation: Machine Translation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-10",
          "2019-09",
          "2019-06",
          "2019-05",
          "2019-01",
          "2018-10",
          "2018-09",
          "2018-08",
          "2018-06",
          "2017-06",
          "2018-02",
          "2018-03",
          "2014-09",
          "2017-11",
          "2016-03",
          "2016-06",
          "2017-05",
          "2016-07",
          "2017-01",
          "2016-11",
          "2016-10",
          "2016-09",
          "2016-08",
          "2014-10"
         ],
         "xaxis": "x",
         "y": [
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Natural language generation: Question Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Natural language generation: Question Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-05",
          "2018-06"
         ],
         "xaxis": "x",
         "y": [
          "Natural language generation: Question Generation",
          "Natural language generation: Question Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Natural language generation: Text Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Natural language generation: Text Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-05",
          "2017-09",
          "2018-02",
          "2019-01"
         ],
         "xaxis": "x",
         "y": [
          "Natural language generation: Text Generation",
          "Natural language generation: Text Generation",
          "Natural language generation: Text Generation",
          "Natural language generation: Text Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Natural language generation: Document Summarization",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Natural language generation: Document Summarization",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-08",
          "2019-05",
          "2019-03",
          "2018-08",
          "2017-05"
         ],
         "xaxis": "x",
         "y": [
          "Natural language generation: Document Summarization",
          "Natural language generation: Document Summarization",
          "Natural language generation: Document Summarization",
          "Natural language generation: Document Summarization",
          "Natural language generation: Document Summarization"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Natural language generation: Language Modelling",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Natural language generation: Language Modelling",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-09",
          "2016-12",
          "2018-03",
          "2018-09",
          "2014-12"
         ],
         "xaxis": "x",
         "y": [
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other NLP task: Text-to-Image Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other NLP task: Text-to-Image Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-12",
          "2019-09",
          "2019-03",
          "2019-01",
          "2017-11",
          "2017-10",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pragmatics analysis: Sentiment Analysis",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pragmatics analysis: Sentiment Analysis",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-01",
          "2017-12",
          "2017-08",
          "2017-07",
          "2018-02",
          "2018-04",
          "2017-04",
          "2018-05",
          "2017-02",
          "2016-02",
          "2015-11",
          "2015-06",
          "2015-02",
          "2014-08",
          "2014-06",
          "2013-10",
          "2018-10",
          "2016-07",
          "2019-01",
          "2019-06",
          "2019-05",
          "2019-09",
          "2019-07"
         ],
         "xaxis": "x",
         "y": [
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pragmatics analysis: Coreference Resolution",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pragmatics analysis: Coreference Resolution",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-02",
          "2019-08",
          "2019-07",
          "2018-04",
          "2017-07",
          "2016-09",
          "2016-06"
         ],
         "xaxis": "x",
         "y": [
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pragmatics analysis: Intent Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pragmatics analysis: Intent Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-12",
          "2019-06"
         ],
         "xaxis": "x",
         "y": [
          "Pragmatics analysis: Intent Detection",
          "Pragmatics analysis: Intent Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pragmatics analysis: Fake News Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pragmatics analysis: Fake News Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2017-12"
         ],
         "xaxis": "x",
         "y": [
          "Pragmatics analysis: Fake News Detection",
          "Pragmatics analysis: Fake News Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pragmatics analysis: Emotion Recognition in Conversation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pragmatics analysis: Emotion Recognition in Conversation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2019-04",
          "2018-11",
          "2018-10",
          "2019-08",
          "2018-06"
         ],
         "xaxis": "x",
         "y": [
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Emotion Recognition in Conversation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pragmatics analysis: Paraphrase Identification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pragmatics analysis: Paraphrase Identification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2013-10",
          "2017-04",
          "2018-07",
          "2019-01",
          "2019-06",
          "2017-09"
         ],
         "xaxis": "x",
         "y": [
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Paraphrase Identification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Question answering: Visual Question Answering",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Question answering: Visual Question Answering",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2020-02",
          "2019-09",
          "2019-08",
          "2019-07",
          "2019-06",
          "2019-05",
          "2019-04",
          "2019-02",
          "2017-07",
          "2018-03",
          "2017-08",
          "2016-12",
          "2016-11",
          "2016-06",
          "2016-03",
          "2017-05",
          "2017-04",
          "2018-05",
          "2016-05"
         ],
         "xaxis": "x",
         "y": [
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Question answering: Question Answering",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Question answering: Question Answering",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-08",
          "2019-09",
          "2018-08",
          "2018-07",
          "2018-06",
          "2018-05",
          "2018-04",
          "2018-01",
          "2017-12",
          "2017-10",
          "2017-08",
          "2017-07",
          "2017-06",
          "2017-05",
          "2017-04",
          "2017-03",
          "2016-11",
          "2016-10",
          "2016-09",
          "2016-02",
          "2016-03",
          "2018-09",
          "2018-10",
          "2018-11",
          "2019-01",
          "2019-02",
          "2019-05",
          "2019-06",
          "2015-11",
          "2015-06",
          "2014-12",
          "2014-06",
          "2019-08",
          "2019-07",
          "2016-06"
         ],
         "xaxis": "x",
         "y": [
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic analysis: Word Sense Disambiguation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic analysis: Word Sense Disambiguation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-11",
          "2018-05",
          "2018-02",
          "2016-06",
          "2016-03",
          "2017-09",
          "2019-05",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic analysis: Entity Disambiguation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic analysis: Entity Disambiguation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2017-05"
         ],
         "xaxis": "x",
         "y": [
          "Semantic analysis: Entity Disambiguation",
          "Semantic analysis: Entity Disambiguation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic analysis: Semantic Textual Similarity",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic analysis: Semantic Textual Similarity",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-05",
          "2018-03",
          "2019-06",
          "2019-07",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Semantic Textual Similarity"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic analysis: Semantic Parsing",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic analysis: Semantic Parsing",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-04",
          "2018-10"
         ],
         "xaxis": "x",
         "y": [
          "Semantic analysis: Semantic Parsing",
          "Semantic analysis: Semantic Parsing"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic analysis: Semantic Role Labeling",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic analysis: Semantic Role Labeling",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-12",
          "2018-02",
          "2018-05",
          "2018-10"
         ],
         "xaxis": "x",
         "y": [
          "Semantic analysis: Semantic Role Labeling",
          "Semantic analysis: Semantic Role Labeling",
          "Semantic analysis: Semantic Role Labeling",
          "Semantic analysis: Semantic Role Labeling"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Sentence embedding: Sentence Compression",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Sentence embedding: Sentence Compression",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-07",
          "2017-07"
         ],
         "xaxis": "x",
         "y": [
          "Sentence embedding: Sentence Compression",
          "Sentence embedding: Sentence Compression"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Syntactic analysis: Dependency Parsing",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Syntactic analysis: Dependency Parsing",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-03",
          "2018-07",
          "2016-11"
         ],
         "xaxis": "x",
         "y": [
          "Syntactic analysis: Dependency Parsing",
          "Syntactic analysis: Dependency Parsing",
          "Syntactic analysis: Dependency Parsing"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Syntactic analysis: Constituency Parsing",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Syntactic analysis: Constituency Parsing",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-07",
          "2016-11",
          "2019-03",
          "2018-05"
         ],
         "xaxis": "x",
         "y": [
          "Syntactic analysis: Constituency Parsing",
          "Syntactic analysis: Constituency Parsing",
          "Syntactic analysis: Constituency Parsing",
          "Syntactic analysis: Constituency Parsing"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Syntactic analysis: Chunking",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Syntactic analysis: Chunking",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-11",
          "2018-08"
         ],
         "xaxis": "x",
         "y": [
          "Syntactic analysis: Chunking",
          "Syntactic analysis: Chunking"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Syntactic analysis: Constituency Grammar Induction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Syntactic analysis: Constituency Grammar Induction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-06",
          "2019-04",
          "2018-10",
          "2018-08"
         ],
         "xaxis": "x",
         "y": [
          "Syntactic analysis: Constituency Grammar Induction",
          "Syntactic analysis: Constituency Grammar Induction",
          "Syntactic analysis: Constituency Grammar Induction",
          "Syntactic analysis: Constituency Grammar Induction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Syntactic analysis: Grammatical Error Detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Syntactic analysis: Grammatical Error Detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-07",
          "2016-11",
          "2018-11",
          "2017-07",
          "2017-04"
         ],
         "xaxis": "x",
         "y": [
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Grammatical Error Detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Syntactic analysis: Linguistic Acceptability Assessment",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Syntactic analysis: Linguistic Acceptability Assessment",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-09",
          "2019-06"
         ],
         "xaxis": "x",
         "y": [
          "Syntactic analysis: Linguistic Acceptability Assessment",
          "Syntactic analysis: Linguistic Acceptability Assessment"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Text classification: Citation Intent Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Text classification: Citation Intent Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-02",
          "2016-06",
          "2018-01"
         ],
         "xaxis": "x",
         "y": [
          "Text classification: Citation Intent Classification",
          "Text classification: Citation Intent Classification",
          "Text classification: Citation Intent Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Text classification: Text Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Text classification: Text Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-06",
          "2019-05",
          "2019-02",
          "2019-01",
          "2018-09",
          "2018-08",
          "2017-02",
          "2018-03",
          "2020-02",
          "2018-05",
          "2015-11",
          "2019-09",
          "2018-07"
         ],
         "xaxis": "x",
         "y": [
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Text classification: Document Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Text classification: Document Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-10",
          "2016-09",
          "2016-03",
          "2020-02",
          "2019-08",
          "2019-04",
          "2018-08",
          "2016-11"
         ],
         "xaxis": "x",
         "y": [
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Text classification: Sentence Classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Text classification: Sentence Classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2019-03",
          "2018-10"
         ],
         "xaxis": "x",
         "y": [
          "Text classification: Sentence Classification",
          "Text classification: Sentence Classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": [
          "<BR>task: Computer code processing: Code Generation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  CoNaLa - Code Generation benchmarking: BLEU<BR>  CoNaLa-Ext - Code Generation benchmarking: BLEU<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Android Repos - Code Generation benchmarking: Perplexity<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  100 sleep nights of 8 caregivers - Code Generation benchmarking: 14 gestures accuracy<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Django - Code Generation benchmarking: Accuracy<BR>",
          "<BR>task: Dialog process: Dialog Act Classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Switchboard corpus - Dialog Act Classification benchmarking: Accuracy<BR>",
          "<BR>task: Dialog process: Dialog State Tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Area<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Food<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Joint<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Price<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Request<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Joint<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Request<BR>",
          "<BR>task: Dialog process: Visual Dialog<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: Mean Rank<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Dialog process: Visual Dialog<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: NDCG (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  V-SNLI - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  XNLI French - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  XNLI Chinese - Natural Language Inference benchmarking: Accuracy<BR>  XNLI Chinese Dev - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ANLI test - Natural Language Inference benchmarking: A1<BR>  ANLI test - Natural Language Inference benchmarking: A2<BR>  ANLI test - Natural Language Inference benchmarking: A3<BR>  WNLI - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Train Accuracy<BR>  SNLI - Natural Language Inference benchmarking: Parameters<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog v0.9 - Common Sense Reasoning benchmarking: 1 in 10 R-at-5<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog  v0.9 - Common Sense Reasoning benchmarking: 1 in 10 R-at-5<BR>  Visual Dialog  v0.9 - Common Sense Reasoning benchmarking: Recall-at-10<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  ReCoRD - Common Sense Reasoning benchmarking: EM<BR>  ReCoRD - Common Sense Reasoning benchmarking: F1<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Event2Mind dev - Common Sense Reasoning benchmarking: Average Cross-Ent<BR>  Event2Mind test - Common Sense Reasoning benchmarking: Average Cross-Ent<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  SWAG - Common Sense Reasoning benchmarking: Dev<BR>  SWAG - Common Sense Reasoning benchmarking: Test<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Winograd Schema Challenge - Common Sense Reasoning benchmarking: Score<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2000 - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Joint Entity and Relation Extraction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Entity F1<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Relation F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  CoNLL04 - Relation Extraction benchmarking: NER Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Micro F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-10%<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-30%<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Long-tail emerging entities - Named Entity Recognition benchmarking: F1 (surface form)<BR>  Long-tail emerging entities - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  SciERC - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  JNLPBA - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  ACE 2004 - Named Entity Recognition benchmarking: F1<BR>  NCBI-disease - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  NYT - Relation Extraction benchmarking: F1<BR>  NYT-single - Relation Extraction benchmarking: F1<BR>  WebNLG - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  WLPC - Named Entity Recognition benchmarking: F1<BR>  WetLab - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  LINNAEUS - Named Entity Recognition benchmarking: F1<BR>  Species-800 - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Code-Switching English-Spanish NER - Named Entity Recognition benchmarking: F1<BR>  ontontoes chinese v5 - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  SoSciSoCi - Named Entity Recognition benchmarking: F1<BR>  SoSciSoCi - Named Entity Recognition benchmarking: Precision<BR>  SoSciSoCi - Named Entity Recognition benchmarking: Recall<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Re-TACRED - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>  Wikipedia-Wikidata relations - Relation Extraction benchmarking: Error rate<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>  GENIA - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  ChemProt - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ADE Corpus - Relation Extraction benchmarking: NER Macro F1<BR>  ADE Corpus - Relation Extraction benchmarking: RE+ Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: NER Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Macro F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  NYT24 - Relation Extraction benchmarking: F1<BR>  NYT29 - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  DocRED - Relation Extraction benchmarking: F1<BR>  DocRED - Relation Extraction benchmarking: Ign F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  FewRel - Relation Extraction benchmarking: F1<BR>  FewRel - Relation Extraction benchmarking: Precision<BR>  FewRel - Relation Extraction benchmarking: Recall<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  JNLPBA - Relation Extraction benchmarking: F1<BR>  SciERC - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  SighanNER - Chinese Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>  OntoNotes 4 - Chinese Named Entity Recognition benchmarking: F1<BR>  Resume NER - Chinese Named Entity Recognition benchmarking: F1<BR>  Weibo NER - Chinese Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  WLPC - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  MSRA Dev - Chinese Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Advising Corpus - Conversational Response Selection benchmarking: R-at-10<BR>  Advising Corpus - Conversational Response Selection benchmarking: R-at-1<BR>  Advising Corpus - Conversational Response Selection benchmarking: R@50<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  PolyAI AmazonQA - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI OpenSubtitles - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>",
          "<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 German-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  IWSLT2015 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  WMT 2017 English-Chinese - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 Thai-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Czech-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Czech - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Russian - Machine Translation benchmarking: BLEU score<BR>  WMT2016 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Romanian-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Russian-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Machine Translation benchmarking: 1-of-100 Accuracy<BR>  WMT 2017 Latvian-English - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  ACCURAT balanced test corpus for under resourced languages Estonian-Russian - Machine Translation benchmarking: BLEU<BR>  ACCURAT balanced test corpus for under resourced languages Russian-Estonian - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  WMT2016 Finnish-English - Machine Translation benchmarking: BLEU<BR>  WMT2017 Finnish-English - Machine Translation benchmarking: BLEU<BR>  WMT2019 Finnish-English - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 French-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  WMT 2017 English-Latvian - Machine Translation benchmarking: BLEU<BR>  WMT 2018 English-Estonian - Machine Translation benchmarking: BLEU<BR>  WMT 2018 English-Finnish - Machine Translation benchmarking: BLEU<BR>  WMT 2018 Estonian-English - Machine Translation benchmarking: BLEU<BR>  WMT 2018 Finnish-English - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-Czech - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Machine Translation benchmarking: Accuracy<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2015 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2015 English-Russian - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  WMT2019 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2019 English-German - Machine Translation benchmarking: SacreBLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2017 Arabic-English - Machine Translation benchmarking: Cased sacreBLEU<BR>  IWSLT2017 English-Arabic - Machine Translation benchmarking: Cased sacreBLEU<BR>  IWSLT2017 English-French - Machine Translation benchmarking: Cased sacreBLEU<BR>  IWSLT2017 French-English - Machine Translation benchmarking: Cased sacreBLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 Chinese-English - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: SacreBLEU<BR>  WMT2014 English-German - Machine Translation benchmarking: SacreBLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  LAMBADA - Language Modelling benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  X-Sum - Text Summarization benchmarking: ROUGE-1<BR>  X-Sum - Text Summarization benchmarking: ROUGE-2<BR>  X-Sum - Text Summarization benchmarking: ROUGE-3<BR>",
          "<BR>task: Natural language generation: Document Summarization<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: PPL<BR>",
          "<BR>task: Natural language generation: Document Summarization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Natural language generation: Question Generation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  SQuAD1.1 - Question Generation benchmarking: BLEU-4<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  COCO Captions - Text Generation benchmarking: BLEU-2<BR>  COCO Captions - Text Generation benchmarking: BLEU-3<BR>  COCO Captions - Text Generation benchmarking: BLEU-4<BR>  COCO Captions - Text Generation benchmarking: BLEU-5<BR>  Chinese Poems - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-3<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-4<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-5<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Yahoo Questions - Text Generation benchmarking: KL<BR>  Yahoo Questions - Text Generation benchmarking: NLL<BR>  Yahoo Questions - Text Generation benchmarking: Perplexity<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  LDC2016E25 - Text Generation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  DailyDialog - Text Generation benchmarking: BLEU-1<BR>  DailyDialog - Text Generation benchmarking: BLEU-2<BR>  DailyDialog - Text Generation benchmarking: BLEU-3<BR>  DailyDialog - Text Generation benchmarking: BLEU-4<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Pubmed - Text Summarization benchmarking: ROUGE-1<BR>  arXiv - Text Summarization benchmarking: ROUGE-1<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: PPL<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Text8 - Language Modelling benchmarking: Bit per Character (BPC)<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Hutter Prize - Language Modelling benchmarking: Bit per Character (BPC)<BR>  enwik8 - Language Modelling benchmarking: Bit per Character (BPC)<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  WikiText-2 - Language Modelling benchmarking: Test perplexity<BR>  WikiText-2 - Language Modelling benchmarking: Validation perplexity<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Test perplexity<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Validation perplexity<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: Validation perplexity<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  The Pile - Language Modelling benchmarking: Bits per byte<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  PTB - Language Modelling benchmarking: PPL<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>  GigaWord - Text Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Natural language generation: Question Generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Generation - Question Generation benchmarking: BLEU-1<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  CUB - Text-to-Image Generation benchmarking: FID<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  Oxford 102 Flowers - Text-to-Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: FID<BR>  Oxford 102 Flowers - Text-to-Image Generation benchmarking: FID<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: SOA-C<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: Acc<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: FID<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: LPIPS<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: Real<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  2017_test set - Paraphrase Identification benchmarking: 10 fold Cross validation<BR>",
          "<BR>task: Pragmatics analysis: Fake News Detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Agree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Disagree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Discuss)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Unrelated)<BR>  FNC-1 - Fake News Detection benchmarking: Weighted Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  MSRP - Paraphrase Identification benchmarking: Accuracy<BR>  MSRP - Paraphrase Identification benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Amazon Review Full - Sentiment Analysis benchmarking: Accuracy<BR>  Amazon Review Polarity - Sentiment Analysis benchmarking: Accuracy<BR>  Sogou News - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  GAP - Coreference Resolution benchmarking: Bias (F/M)<BR>  GAP - Coreference Resolution benchmarking: Feminine F1 (F)<BR>  GAP - Coreference Resolution benchmarking: Masculine F1 (M)<BR>  GAP - Coreference Resolution benchmarking: Overall F1<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  MR - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Yelp Binary classification - Sentiment Analysis benchmarking: Error<BR>  Yelp Fine-grained classification - Sentiment Analysis benchmarking: Error<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ASTD - Sentiment Analysis benchmarking: Average Recall<BR>  ArSAS - Sentiment Analysis benchmarking: Average Recall<BR>  FiQA - Sentiment Analysis benchmarking: MSE<BR>  FiQA - Sentiment Analysis benchmarking: R^2<BR>  Financial PhraseBank - Sentiment Analysis benchmarking: Accuracy<BR>  Financial PhraseBank - Sentiment Analysis benchmarking: F1 score<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Twitter - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  SemEval - Sentiment Analysis benchmarking: F1-score<BR>  SemEval 2017 Task 4-A - Sentiment Analysis benchmarking: Average Recall<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ChnSentiCorp - Sentiment Analysis benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Average<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Electronics<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  CR - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ATIS - Intent Detection benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  DailyDialog - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>  EmoryNLP - Emotion Recognition in Conversation benchmarking: Weighted Macro-F1<BR>",
          "<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  ASOS.com user intent - Intent Detection benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  MPQA - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  ATIS - Intent Detection benchmarking: Accuracy<BR>  SNIPS - Intent Detection benchmarking: Intent Accuracy<BR>  SNIPS - Intent Detection benchmarking: Slot F1 Score<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Arousal)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  RACE - Question Answering benchmarking: RACE-h<BR>  RACE - Question Answering benchmarking: RACE-m<BR>  RACE - Question Answering benchmarking: RACE<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  SCDE - Question Answering benchmarking: BA<BR>  SCDE - Question Answering benchmarking: DE<BR>  SCDE - Question Answering benchmarking: PA<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  NaturalQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  HotpotQA - Question Answering benchmarking: JOINT-F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  CODAH - Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  JD Product Question Answer - Question Answering benchmarking: BLEU<BR>  Natural Questions - Question Answering benchmarking: F1 (Long)<BR>  Natural Questions - Question Answering benchmarking: F1 (Short)<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  QuAC - Question Answering benchmarking: F1<BR>  QuAC - Question Answering benchmarking: HEQD<BR>  QuAC - Question Answering benchmarking: HEQQ<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  SQuAD2.0 dev - Question Answering benchmarking: EM<BR>  SQuAD2.0 dev - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  NewsQA - Question Answering benchmarking: EM<BR>  NewsQA - Question Answering benchmarking: F1<BR>  Quasart-T - Question Answering benchmarking: EM<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  AI2 Kaggle Dataset - Question Answering benchmarking: P-at-1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: number<BR>  VizWiz 2018 - Visual Question Answering benchmarking: other<BR>  VizWiz 2018 - Visual Question Answering benchmarking: unanswerable<BR>  VizWiz 2018 - Visual Question Answering benchmarking: yes/no<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  GQA test-dev - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  TDIUC - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  100 sleep nights of 8 caregivers - Visual Question Answering benchmarking: 14 gestures accuracy<BR>  HowmanyQA - Visual Question Answering benchmarking: Accuracy<BR>  TallyQA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CLEVR - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  WikiHop - Question Answering benchmarking: Test<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Reverb - Question Answering benchmarking: Accuracy<BR>  WebQuestions - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  QASent - Question Answering benchmarking: MAP<BR>  QASent - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  SemEvalCQA - Question Answering benchmarking: MAP<BR>  SemEvalCQA - Question Answering benchmarking: P-at-1<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 10k)<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 1k)<BR>  bAbi - Question Answering benchmarking: Mean Error Rate<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  SimpleQuestions - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  YahooCQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: P-at-1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Story Cloze Test - Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: BLEU-1<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  COMPLEXQUESTIONS - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-NE<BR>  MCTest-160 - Question Answering benchmarking: Accuracy<BR>  MCTest-500 - Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  STS Benchmark - Semantic Textual Similarity benchmarking: Pearson Correlation<BR>",
          "<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  WikiSQL - Semantic Parsing benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Task 1 - Word Sense Disambiguation benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Knowledge-based: - Word Sense Disambiguation benchmarking: All<BR>  Knowledge-based: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Knowledge-based: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-E<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-R<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>  MRPC - Semantic Textual Similarity benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2005 - Semantic Role Labeling benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  AQUAINT - Entity Disambiguation benchmarking: Micro-F1<BR>",
          "<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  ATIS - Semantic Parsing benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Geo - Semantic Parsing benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  spider - Semantic Parsing benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2007<BR>",
          "<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  AIDA-CoNLL - Entity Disambiguation benchmarking: In-KB Accuracy<BR>  TAC2010 - Entity Disambiguation benchmarking: Micro Precision<BR>",
          "<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  ACE2004 - Entity Disambiguation benchmarking: Micro-F1<BR>  MSNBC - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-CWEB - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-WIKI - Entity Disambiguation benchmarking: Micro-F1<BR>",
          "<BR>task: Sentence embedding: Sentence Compression<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Google Dataset - Sentence Compression benchmarking: CR<BR>  Google Dataset - Sentence Compression benchmarking: F1<BR>",
          "<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ10)<BR>",
          "<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>",
          "<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: LAS<BR>  Penn Treebank - Dependency Parsing benchmarking: POS<BR>  Penn Treebank - Dependency Parsing benchmarking: UAS<BR>",
          "<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  CoNLL-2009 - Dependency Parsing benchmarking: LAS<BR>  CoNLL-2009 - Dependency Parsing benchmarking: UAS<BR>",
          "<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  GENIA - LAS - Dependency Parsing benchmarking: F1<BR>  GENIA - UAS - Dependency Parsing benchmarking: F1<BR>",
          "<BR>task: Syntactic analysis: Chunking<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2000 - Chunking benchmarking: Exact Span F1<BR>",
          "<BR>task: Syntactic analysis: Chunking<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Penn Treebank - Chunking benchmarking: F1 score<BR>",
          "<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>",
          "<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  CoNLL-2014 A1 - Grammatical Error Detection benchmarking: F0.5<BR>  CoNLL-2014 A2 - Grammatical Error Detection benchmarking: F0.5<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>",
          "<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  JFLEG - Grammatical Error Detection benchmarking: F0.5<BR>",
          "<BR>task: Syntactic analysis: Linguistic Acceptability Assessment<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  CoLA - Linguistic Acceptability Assessment benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ10)<BR>",
          "<BR>task: Text classification: Sentence Classification<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  PubMed 20k RCT - Sentence Classification benchmarking: F1<BR>",
          "<BR>task: Text classification: Sentence Classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Paper Field - Sentence Classification benchmarking: F1<BR>  ScienceCite - Sentence Classification benchmarking: F1<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Reuters De-En - Document Classification benchmarking: Accuracy<BR>  Reuters En-De - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Citation Intent Classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>",
          "<BR>task: Text classification: Sentence Classification<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ACL-ARC - Sentence Classification benchmarking: F1<BR>  SciCite - Sentence Classification benchmarking: F1<BR>",
          "<BR>task: Text classification: Citation Intent Classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  SciCite - Citation Intent Classification benchmarking: F1<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  IMDb-M - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  WOS-11967 - Document Classification benchmarking: Accuracy<BR>  WOS-46985 - Document Classification benchmarking: Accuracy<BR>  WOS-5736 - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  AG News - Text Classification benchmarking: Error<BR>  DBpedia - Text Classification benchmarking: Error<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  RCV1 - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  TREC-50 - Text Classification benchmarking: Error<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Ohsumed - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: Accuracy<BR>  LOCAL DATASET - Text Classification benchmarking: Accuracy (%)<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: F-measure<BR>  R8 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: F-measure<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  MPQA - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  R52 - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Yelp-5 - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (10 classes)<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Yelp-2 - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  AAPD - Document Classification benchmarking: F1<BR>  Amazon - Document Classification benchmarking: Accuracy<BR>  BBCSport - Document Classification benchmarking: Accuracy<BR>  Classic - Document Classification benchmarking: Accuracy<BR>  Recipe - Document Classification benchmarking: Accuracy<BR>  Reuters-21578 - Document Classification benchmarking: Accuracy<BR>  Twitter - Document Classification benchmarking: Accuracy<BR>  Yelp-14 - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  RCV1 - Text Classification benchmarking: Macro F1<BR>  RCV1 - Text Classification benchmarking: Micro F1<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: Precision<BR>  20NEWS - Text Classification benchmarking: Recall<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Reuters-21578 - Document Classification benchmarking: F1<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Sogou News - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Amazon-2 - Text Classification benchmarking: Error<BR>  Amazon-5 - Text Classification benchmarking: Error<BR>  RCV1 - Text Classification benchmarking: P-at-1<BR>  RCV1 - Text Classification benchmarking: P-at-3<BR>  RCV1 - Text Classification benchmarking: P-at-5<BR>  RCV1 - Text Classification benchmarking: nDCG-at-1<BR>  RCV1 - Text Classification benchmarking: nDCG-at-3<BR>  RCV1 - Text Classification benchmarking: nDCG-at-5<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 20,
          "symbol": 42
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2017-08",
          "2018-10",
          "2018-05",
          "2017-05",
          "2016-03",
          "2016-03",
          "2016-06",
          "2016-05",
          "2017-04",
          "2018-06",
          "2018-09",
          "2019-05",
          "2019-04",
          "2018-03",
          "2019-06",
          "2018-12",
          "2017-12",
          "2015-08",
          "2018-09",
          "2014-04",
          "2019-09",
          "2018-11",
          "2018-10",
          "2018-05",
          "2018-08",
          "2018-06",
          "2018-05",
          "2018-08",
          "2014-06",
          "2014-10",
          "2016-08",
          "2017-09",
          "2014-09",
          "2019-01",
          "2018-10",
          "2017-06",
          "2019-04",
          "2019-08",
          "2019-09",
          "2020-03",
          "2018-09",
          "2017-09",
          "2018-06",
          "2019-01",
          "2018-04",
          "2019-11",
          "2019-06",
          "2019-05",
          "2019-03",
          "2018-10",
          "2018-05",
          "2019-04",
          "2019-04",
          "2018-12",
          "2018-02",
          "2019-01",
          "2019-04",
          "2019-01",
          "2018-04",
          "2017-11",
          "2016-07",
          "2018-03",
          "2016-06",
          "2015-12",
          "2017-09",
          "2018-05",
          "2019-06",
          "2018-09",
          "2018-10",
          "2019-01",
          "2015-08",
          "2019-07",
          "2019-10",
          "2019-11",
          "2018-08",
          "2014-09",
          "2018-07",
          "2019-08",
          "2017-09",
          "2017-05",
          "2017-04",
          "2016-09",
          "2017-02",
          "2018-05",
          "2014-06",
          "2015-09",
          "2018-08",
          "2017-04",
          "2013-12",
          "2016-02",
          "2016-07",
          "2016-11",
          "2016-12",
          "2018-03",
          "2018-09",
          "2019-02",
          "2019-11",
          "2016-02",
          "2018-08",
          "2016-10",
          "2016-12",
          "2017-10",
          "2017-11",
          "2017-02",
          "2018-06",
          "2017-07",
          "2013-10",
          "2016-07",
          "2016-04",
          "2019-08",
          "2017-07",
          "2017-02",
          "2015-09",
          "2019-08",
          "2019-01",
          "2019-02",
          "2017-04",
          "2019-06",
          "2014-12",
          "2015-05",
          "2017-12",
          "2013-10",
          "2019-06",
          "2019-09",
          "2019-12",
          "2019-03",
          "2018-03",
          "2018-12",
          "2017-07",
          "2018-03",
          "2020-04",
          "2019-07",
          "2019-05",
          "2019-04",
          "2019-01",
          "2018-10",
          "2018-08",
          "2018-05",
          "2017-11",
          "2017-03",
          "2017-08",
          "2019-08",
          "2019-07",
          "2019-04",
          "2019-02",
          "2018-10",
          "2018-08",
          "2017-07",
          "2017-04",
          "2016-12",
          "2016-06",
          "2017-10",
          "2014-04",
          "2014-05",
          "2014-12",
          "2015-03",
          "2015-06",
          "2016-02",
          "2016-06",
          "2016-08",
          "2016-11",
          "2017-05",
          "2017-07",
          "2015-11",
          "2016-03",
          "2018-03",
          "2019-10",
          "2015-05",
          "2016-03",
          "2017-09",
          "2018-01",
          "2017-05",
          "2013-10",
          "2018-05",
          "2018-04",
          "2019-09",
          "2014-11",
          "2018-05",
          "2018-09",
          "2018-02",
          "2017-07",
          "2016-01",
          "2017-04",
          "2015-09",
          "2018-08",
          "2017-11",
          "2015-06",
          "2016-11",
          "2018-08",
          "2017-11",
          "2016-08",
          "2014-12",
          "2016-07",
          "2018-11",
          "2019-01",
          "2018-10",
          "2018-08",
          "2019-03",
          "2014-10",
          "2013-06",
          "2018-01",
          "2019-03",
          "2019-06",
          "2017-09",
          "2014-05",
          "2015-04",
          "2015-09",
          "2016-02",
          "2016-07",
          "2016-12",
          "2017-07",
          "2018-05",
          "2018-06",
          "2019-08",
          "2018-09",
          "2019-01",
          "2019-04",
          "2019-05",
          "2019-04",
          "2019-08",
          "2019-11",
          "2014-03",
          "2019-02",
          "2018-10",
          "2019-06"
         ],
         "y": [
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation",
          "Computer code processing: Code Generation",
          "Dialog process: Dialog Act Classification",
          "Dialog process: Dialog State Tracking",
          "Dialog process: Visual Dialog",
          "Dialog process: Visual Dialog",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Inference and reasoning: Common Sense Reasoning",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Joint Entity and Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Relation Extraction",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Relation Extraction",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Chinese Named Entity Recognition",
          "Information extraction: Chinese Named Entity Recognition",
          "Information extraction: Relation Extraction",
          "Information extraction: Chinese Named Entity Recognition",
          "Information retrieval: Conversational Response Selection",
          "Information retrieval: Conversational Response Selection",
          "Information retrieval: Conversational Response Selection",
          "Information retrieval: Conversational Response Selection",
          "Machine translation: Unsupervised Machine Translation",
          "Machine translation: Unsupervised Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Language Modelling",
          "Natural language generation: Text Summarization",
          "Natural language generation: Document Summarization",
          "Natural language generation: Document Summarization",
          "Natural language generation: Question Generation",
          "Natural language generation: Text Generation",
          "Natural language generation: Text Generation",
          "Natural language generation: Text Generation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Text Summarization",
          "Natural language generation: Text Generation",
          "Natural language generation: Text Summarization",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Language Modelling",
          "Natural language generation: Text Summarization",
          "Natural language generation: Question Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Other NLP task: Text-to-Image Generation",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Fake News Detection",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Intent Detection",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Intent Detection",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Intent Detection",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Question Answering",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Semantic Parsing",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Semantic Role Labeling",
          "Semantic analysis: Entity Disambiguation",
          "Semantic analysis: Semantic Parsing",
          "Semantic analysis: Semantic Parsing",
          "Semantic analysis: Semantic Parsing",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Semantic Role Labeling",
          "Semantic analysis: Entity Disambiguation",
          "Semantic analysis: Entity Disambiguation",
          "Sentence embedding: Sentence Compression",
          "Syntactic analysis: Constituency Grammar Induction",
          "Syntactic analysis: Constituency Grammar Induction",
          "Syntactic analysis: Dependency Parsing",
          "Syntactic analysis: Dependency Parsing",
          "Syntactic analysis: Dependency Parsing",
          "Syntactic analysis: Chunking",
          "Syntactic analysis: Chunking",
          "Syntactic analysis: Constituency Parsing",
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Linguistic Acceptability Assessment",
          "Syntactic analysis: Constituency Grammar Induction",
          "Text classification: Sentence Classification",
          "Text classification: Sentence Classification",
          "Text classification: Document Classification",
          "Text classification: Citation Intent Classification",
          "Text classification: Sentence Classification",
          "Text classification: Citation Intent Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Document Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Document Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification",
          "Text classification: Document Classification",
          "Text classification: Document Classification",
          "Text classification: Text Classification",
          "Text classification: Text Classification"
         ]
        },
        {
         "hovertemplate": [
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2013-10<BR>ratio: 0.1102<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2013-10<BR>ratio: 1.0<BR>benchmarks:<BR>  MSRP - Paraphrase Identification benchmarking: Accuracy<BR>  MSRP - Paraphrase Identification benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2014-06<BR>ratio: 0.3305<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2014-06<BR>ratio: 0.76<BR>benchmarks:<BR>  WebQuestions - Question Answering benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2014-08<BR>ratio: 0.2308<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2014-08<BR>ratio: 0.2549<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2014-09<BR>ratio: 0.0886<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2014-10<BR>ratio: 0.0904<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2014-12<BR>ratio: 0.3032<BR>benchmarks:<BR>  QASent - Question Answering benchmarking: MAP<BR>  QASent - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: PPL<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-02<BR>ratio: 0.1186<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-06<BR>ratio: 0.0427<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2015-06<BR>ratio: 0.232<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  WebQuestions - Question Answering benchmarking: F1<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2015-08<BR>ratio: 0.5<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Train Accuracy<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2015-09<BR>ratio: 0.1435<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2015-11<BR>ratio: 0.2159<BR>benchmarks:<BR>  QASent - Question Answering benchmarking: MAP<BR>  QASent - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-11<BR>ratio: 0.4025<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Electronics<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2015-11<BR>ratio: 0.25<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2016-01<BR>ratio: 0.483<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2016-02<BR>ratio: 0.3491<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2016-02<BR>ratio: 0.3173<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-02<BR>ratio: 0.0718<BR>benchmarks:<BR>  SemEvalCQA - Question Answering benchmarking: P-at-1<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: P-at-1<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2016-03<BR>ratio: 0.1323<BR>benchmarks:<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Task 1 - Word Sense Disambiguation benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-03<BR>ratio: 0.5335<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  MCTest-500 - Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-03<BR>ratio: 0.2<BR>benchmarks:<BR>  WMT2015 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2016-03<BR>ratio: 0.5215<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2016-03<BR>ratio: 0.1966<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: LAS<BR>  Penn Treebank - Dependency Parsing benchmarking: POS<BR>  Penn Treebank - Dependency Parsing benchmarking: UAS<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-06<BR>ratio: 0.542<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2016 German-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Text classification: Citation Intent Classification<BR>date: 2016-06<BR>ratio: 0.7941<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2016-06<BR>ratio: 0.0702<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-06<BR>ratio: 0.3195<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-NE<BR>  SemEvalCQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 10k)<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 1k)<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2016-06<BR>ratio: 0.0771<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2016-06<BR>ratio: 0.0678<BR>benchmarks:<BR>  SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-07<BR>ratio: 0.218<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2016-07<BR>ratio: 0.094<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2016-07<BR>ratio: 0.9481<BR>benchmarks:<BR>  CoNLL-2014 A2 - Grammatical Error Detection benchmarking: F0.5<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-08<BR>ratio: 0.1882<BR>benchmarks:<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-08<BR>ratio: 0.3589<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2016-09<BR>ratio: 0.0286<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-09<BR>ratio: 0.101<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2016-09<BR>ratio: 0.098<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-09<BR>ratio: 0.2232<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2016-09<BR>ratio: 0.5<BR>benchmarks:<BR>  enwik8 - Language Modelling benchmarking: Bit per Character (BPC)<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2016-09<BR>ratio: 0.3558<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-10<BR>ratio: 0.6069<BR>benchmarks:<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>  bAbi - Question Answering benchmarking: Mean Error Rate<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-10<BR>ratio: 0.8<BR>benchmarks:<BR>  WMT2015 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2016-11<BR>ratio: 0.5084<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2016-11<BR>ratio: 0.0123<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2016-11<BR>ratio: 0.0632<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2016-11<BR>ratio: 0.0853<BR>benchmarks:<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>",
          "<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2016-11<BR>ratio: 0.8096<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: LAS<BR>  Penn Treebank - Dependency Parsing benchmarking: UAS<BR>",
          "<BR>task: Syntactic analysis: Chunking<BR>date: 2016-11<BR>ratio: 0.1739<BR>benchmarks:<BR>  Penn Treebank - Chunking benchmarking: F1 score<BR>",
          "<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2016-11<BR>ratio: 0.0711<BR>benchmarks:<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>",
          "<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2016-11<BR>ratio: 0.4857<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2016-12<BR>ratio: 0.7801<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2016-12<BR>ratio: 0.6667<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Test perplexity<BR>  WikiText-2 - Language Modelling benchmarking: Test perplexity<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2016-12<BR>ratio: 0.0708<BR>benchmarks:<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2017-01<BR>ratio: 0.0597<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-02<BR>ratio: 0.0392<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2017-02<BR>ratio: 0.2857<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-02<BR>ratio: 0.1937<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-03<BR>ratio: 0.033<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2017-04<BR>ratio: 0.1026<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-04<BR>ratio: 0.0174<BR>benchmarks:<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2017-04<BR>ratio: 0.108<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-04<BR>ratio: 0.1795<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2017-04<BR>ratio: 0.55<BR>benchmarks:<BR>  ATIS - Semantic Parsing benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2017-04<BR>ratio: 0.6016<BR>benchmarks:<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2017-05<BR>ratio: 0.2253<BR>benchmarks:<BR>  IWSLT2015 English-German - Machine Translation benchmarking: BLEU score<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-05<BR>ratio: 0.2429<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2017-05<BR>ratio: 0.2329<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2017-05<BR>ratio: 0.3586<BR>benchmarks:<BR>  COCO Captions - Text Generation benchmarking: BLEU-2<BR>  COCO Captions - Text Generation benchmarking: BLEU-3<BR>  COCO Captions - Text Generation benchmarking: BLEU-4<BR>  COCO Captions - Text Generation benchmarking: BLEU-5<BR>  Chinese Poems - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-5<BR>",
          "<BR>task: Natural language generation: Document Summarization<BR>date: 2017-05<BR>ratio: 0.2511<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2017-05<BR>ratio: 0.921<BR>benchmarks:<BR>  AIDA-CoNLL - Entity Disambiguation benchmarking: In-KB Accuracy<BR>  TAC2010 - Entity Disambiguation benchmarking: Micro Precision<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2017-06<BR>ratio: 0.3615<BR>benchmarks:<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>  GigaWord - Text Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-06<BR>ratio: 0.1524<BR>benchmarks:<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2017-06<BR>ratio: 0.4788<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  IWSLT2015 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2017-07<BR>ratio: 0.1465<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>",
          "<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2017-07<BR>ratio: 0.3698<BR>benchmarks:<BR>  CoNLL-2014 A1 - Grammatical Error Detection benchmarking: F0.5<BR>  CoNLL-2014 A2 - Grammatical Error Detection benchmarking: F0.5<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>",
          "<BR>task: Sentence embedding: Sentence Compression<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Google Dataset - Sentence Compression benchmarking: CR<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2017-07<BR>ratio: 0.0632<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Amazon Review Full - Sentiment Analysis benchmarking: Accuracy<BR>  Amazon Review Polarity - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-07<BR>ratio: 0.3933<BR>benchmarks:<BR>  SemEvalCQA - Question Answering benchmarking: MAP<BR>  SemEvalCQA - Question Answering benchmarking: P-at-1<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: P-at-1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2017-07<BR>ratio: 0.212<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>",
          "<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2017-07<BR>ratio: 0.2457<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2017-08<BR>ratio: 0.3456<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-08<BR>ratio: 0.5<BR>benchmarks:<BR>  AI2 Kaggle Dataset - Question Answering benchmarking: P-at-1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-08<BR>ratio: 0.2288<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2017-08<BR>ratio: 0.4269<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2017-09<BR>ratio: 0.4385<BR>benchmarks:<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: NER Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Micro F1<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2017-09<BR>ratio: 0.6226<BR>benchmarks:<BR>  COCO Captions - Text Generation benchmarking: BLEU-2<BR>  COCO Captions - Text Generation benchmarking: BLEU-3<BR>  COCO Captions - Text Generation benchmarking: BLEU-4<BR>  COCO Captions - Text Generation benchmarking: BLEU-5<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-3<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-4<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-5<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2017-09<BR>ratio: 0.3099<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>",
          "<BR>task: Dialog process: Dialog Act Classification<BR>date: 2017-09<BR>ratio: 0.7439<BR>benchmarks:<BR>  Switchboard corpus - Dialog Act Classification benchmarking: Accuracy<BR>",
          "<BR>task: Dialog process: Visual Dialog<BR>date: 2017-09<BR>ratio: 0.3904<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-09<BR>ratio: 0.0196<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2017-09<BR>ratio: 0.0407<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-10<BR>ratio: 0.7021<BR>benchmarks:<BR>  CUB - Text-to-Image Generation benchmarking: FID<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>  Oxford 102 Flowers - Text-to-Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2017-10<BR>ratio: 0.0798<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-10<BR>ratio: 0.6272<BR>benchmarks:<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-11<BR>ratio: 0.6346<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Dialog process: Visual Dialog<BR>date: 2017-11<BR>ratio: 0.2578<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Dialog process: Dialog Act Classification<BR>date: 2017-11<BR>ratio: 0.2561<BR>benchmarks:<BR>  Switchboard corpus - Dialog Act Classification benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2017-11<BR>ratio: 0.0459<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-11<BR>ratio: 0.0392<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Fake News Detection<BR>date: 2017-12<BR>ratio: 0.9439<BR>benchmarks:<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Agree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Disagree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Discuss)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Unrelated)<BR>  FNC-1 - Fake News Detection benchmarking: Weighted Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-12<BR>ratio: 0.1476<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2017-12<BR>ratio: 0.1887<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-12<BR>ratio: 0.0392<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2017-12<BR>ratio: 0.0571<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>",
          "<BR>task: Text classification: Citation Intent Classification<BR>date: 2018-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-01<BR>ratio: 0.4672<BR>benchmarks:<BR>  NewsQA - Question Answering benchmarking: EM<BR>  NewsQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-01<BR>ratio: 0.0809<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-02<BR>ratio: 0.3585<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2018-02<BR>ratio: 0.8333<BR>benchmarks:<BR>  Yahoo Questions - Text Generation benchmarking: Perplexity<BR>",
          "<BR>task: Text classification: Citation Intent Classification<BR>date: 2018-02<BR>ratio: 0.1176<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-02<BR>ratio: 0.0238<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2018-02<BR>ratio: 0.2079<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-02<BR>ratio: 0.4381<BR>benchmarks:<BR>  WMT2014 German-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-02<BR>ratio: 0.3667<BR>benchmarks:<BR>  MR - Sentiment Analysis benchmarking: Accuracy<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2018-03<BR>ratio: 0.541<BR>benchmarks:<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-03<BR>ratio: 0.0357<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2018-03<BR>ratio: 0.2009<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Validation perplexity<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-E<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-R<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-03<BR>ratio: 0.0152<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-04<BR>ratio: 0.5921<BR>benchmarks:<BR>  WikiHop - Question Answering benchmarking: Test<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-04<BR>ratio: 0.2234<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Electronics<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2018-04<BR>ratio: 0.245<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2018-04<BR>ratio: 0.137<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-04<BR>ratio: 0.0412<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>",
          "<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-05<BR>ratio: 0.1698<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>",
          "<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2018-05<BR>ratio: 0.1343<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-05<BR>ratio: 0.1569<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Dialog process: Dialog State Tracking<BR>date: 2018-05<BR>ratio: 0.8365<BR>benchmarks:<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Joint<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Request<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Joint<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Request<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-05<BR>ratio: 0.0536<BR>benchmarks:<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Event2Mind test - Common Sense Reasoning benchmarking: Average Cross-Ent<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-05<BR>ratio: 0.7182<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: BLEU-1<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NewsQA - Question Answering benchmarking: EM<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-05<BR>ratio: 0.2896<BR>benchmarks:<BR>  MPQA - Sentiment Analysis benchmarking: Accuracy<BR>  MR - Sentiment Analysis benchmarking: Accuracy<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-05<BR>ratio: 0.3299<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2018-05<BR>ratio: 0.0126<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-06<BR>ratio: 0.154<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-06<BR>ratio: 0.3862<BR>benchmarks:<BR>  RACE - Question Answering benchmarking: RACE-h<BR>  RACE - Question Answering benchmarking: RACE-m<BR>  RACE - Question Answering benchmarking: RACE<BR>  Story Cloze Test - Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-06<BR>ratio: 0.6132<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>  V-SNLI - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Question Generation<BR>date: 2018-06<BR>ratio: 0.0673<BR>benchmarks:<BR>  SQuAD1.1 - Question Generation benchmarking: BLEU-4<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Quasart-T - Question Answering benchmarking: EM<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2018-07<BR>ratio: 0.0282<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2018-07<BR>ratio: 0.3122<BR>benchmarks:<BR>  NYT - Relation Extraction benchmarking: F1<BR>  WebNLG - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Sentence embedding: Sentence Compression<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Google Dataset - Sentence Compression benchmarking: F1<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2018-07<BR>ratio: 0.0553<BR>benchmarks:<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>",
          "<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2018-07<BR>ratio: 0.791<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: POS<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-07<BR>ratio: 0.5132<BR>benchmarks:<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Chunking<BR>date: 2018-08<BR>ratio: 0.913<BR>benchmarks:<BR>  CoNLL 2000 - Chunking benchmarking: Exact Span F1<BR>  Penn Treebank - Chunking benchmarking: F1 score<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2018-08<BR>ratio: 0.4463<BR>benchmarks:<BR>  ADE Corpus - Relation Extraction benchmarking: NER Macro F1<BR>  ADE Corpus - Relation Extraction benchmarking: RE+ Macro F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Long-tail emerging entities - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-08<BR>ratio: 0.0714<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-08<BR>ratio: 0.2718<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2018-08<BR>ratio: 0.2449<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2018-08<BR>ratio: 0.0184<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Document Summarization<BR>date: 2018-08<BR>ratio: 0.5122<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: PPL<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-08<BR>ratio: 0.226<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-08<BR>ratio: 1.0<BR>benchmarks:<BR>  AG News - Text Classification benchmarking: Error<BR>",
          "<BR>task: Dialog process: Visual Dialog<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-09<BR>ratio: 0.0699<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Language Modelling<BR>date: 2018-09<BR>ratio: 1.0<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: Validation perplexity<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2018-09<BR>ratio: 0.3185<BR>benchmarks:<BR>  Re-TACRED - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2018-09<BR>ratio: 0.652<BR>benchmarks:<BR>  Ohsumed - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-09<BR>ratio: 0.2353<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-09<BR>ratio: 0.2419<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>  WikiHop - Question Answering benchmarking: Test<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-10<BR>ratio: 0.196<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>  SQuAD2.0 dev - Question Answering benchmarking: EM<BR>  SQuAD2.0 dev - Question Answering benchmarking: F1<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2018-10<BR>ratio: 0.5481<BR>benchmarks:<BR>  Django - Code Generation benchmarking: Accuracy<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>",
          "<BR>task: Dialog process: Dialog State Tracking<BR>date: 2018-10<BR>ratio: 0.327<BR>benchmarks:<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Joint<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Joint<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-10<BR>ratio: 0.2054<BR>benchmarks:<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>  GENIA - Named Entity Recognition benchmarking: F1<BR>  SciERC - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2018-10<BR>ratio: 0.15<BR>benchmarks:<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-10<BR>ratio: 0.3426<BR>benchmarks:<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-10<BR>ratio: 0.2123<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ10)<BR>",
          "<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2018-10<BR>ratio: 0.45<BR>benchmarks:<BR>  ATIS - Semantic Parsing benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-10<BR>ratio: 0.4277<BR>benchmarks:<BR>  CoNLL 2005 - Semantic Role Labeling benchmarking: F1<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-10<BR>ratio: 0.4178<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-10<BR>ratio: 0.6276<BR>benchmarks:<BR>  SWAG - Common Sense Reasoning benchmarking: Dev<BR>  SWAG - Common Sense Reasoning benchmarking: Test<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  WMT 2017 Latvian-English - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2018-10<BR>ratio: 0.1963<BR>benchmarks:<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>",
          "<BR>task: Text classification: Sentence Classification<BR>date: 2018-10<BR>ratio: 0.9057<BR>benchmarks:<BR>  SciCite - Sentence Classification benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Fake News Detection<BR>date: 2018-11<BR>ratio: 0.1404<BR>benchmarks:<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Agree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Disagree)<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-11<BR>ratio: 0.2719<BR>benchmarks:<BR>  SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2007<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>",
          "<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2018-11<BR>ratio: 0.2698<BR>benchmarks:<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2018-11<BR>ratio: 0.1449<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: BLEU-1<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NewsQA - Question Answering benchmarking: EM<BR>  NewsQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  CoNLL04 - Relation Extraction benchmarking: NER Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Macro F1<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-10%<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-30%<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-01<BR>ratio: 0.2041<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>  WikiHop - Question Answering benchmarking: Test<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  NCBI-disease - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-01<BR>ratio: 0.6137<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-01<BR>ratio: 0.3456<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>  XNLI French - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: FID<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-01<BR>ratio: 0.7765<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT 2017 English-Chinese - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-Czech - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Romanian-English - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-01<BR>ratio: 0.3558<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2019-01<BR>ratio: 0.2254<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-01<BR>ratio: 0.5299<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>  Yelp Fine-grained classification - Sentiment Analysis benchmarking: Error<BR>",
          "<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-01<BR>ratio: 0.4127<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Text Generation<BR>date: 2019-01<BR>ratio: 0.1667<BR>benchmarks:<BR>  Yahoo Questions - Text Generation benchmarking: Perplexity<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-02<BR>ratio: 0.4587<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Dialog process: Visual Dialog<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-02<BR>ratio: 0.3546<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: Accuracy<BR>  Ohsumed - Text Classification benchmarking: Accuracy<BR>  R52 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-02<BR>ratio: 0.8067<BR>benchmarks:<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-NE<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Winograd Schema Challenge - Common Sense Reasoning benchmarking: Score<BR>",
          "<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-02<BR>ratio: 0.397<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-02<BR>ratio: 0.3333<BR>benchmarks:<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-03<BR>ratio: 0.666<BR>benchmarks:<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>  SciERC - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2019-03<BR>ratio: 0.1343<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-03<BR>ratio: 0.1016<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>",
          "<BR>task: Text classification: Sentence Classification<BR>date: 2019-03<BR>ratio: 0.6981<BR>benchmarks:<BR>  ACL-ARC - Sentence Classification benchmarking: F1<BR>  SciCite - Sentence Classification benchmarking: F1<BR>  ScienceCite - Sentence Classification benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ChemProt - Relation Extraction benchmarking: F1<BR>  SciERC - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Natural language generation: Document Summarization<BR>date: 2019-03<BR>ratio: 0.3031<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2019-04<BR>ratio: 0.1967<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>",
          "<BR>task: Information extraction: Joint Entity and Relation Extraction<BR>date: 2019-04<BR>ratio: 0.2778<BR>benchmarks:<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Entity F1<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Relation F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-04<BR>ratio: 0.7252<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>",
          "<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-04<BR>ratio: 0.2995<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-04<BR>ratio: 0.2915<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-04<BR>ratio: 0.5876<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  COCO - Text-to-Image Generation benchmarking: SOA-C<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: Acc<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2019-04<BR>ratio: 0.5763<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2019-04<BR>ratio: 0.0123<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Dialog process: Visual Dialog<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>",
          "<BR>task: Natural language generation: Document Summarization<BR>date: 2019-05<BR>ratio: 0.0901<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-05<BR>ratio: 0.2987<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-German - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-05<BR>ratio: 0.3157<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-05<BR>ratio: 0.3432<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>  MPQA - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-05<BR>ratio: 0.4748<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>  STS Benchmark - Semantic Textual Similarity benchmarking: Pearson Correlation<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2019-05<BR>ratio: 0.6752<BR>benchmarks:<BR>  SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Task 1 - Word Sense Disambiguation benchmarking: F1<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2007<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>",
          "<BR>task: Natural language generation: Question Generation<BR>date: 2019-05<BR>ratio: 0.9327<BR>benchmarks:<BR>  SQuAD1.1 - Question Generation benchmarking: BLEU-4<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-05<BR>ratio: 0.4235<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-05<BR>ratio: 0.4549<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>  Sogou News - Text Classification benchmarking: Accuracy<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2019-05<BR>ratio: 0.2911<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>  GigaWord - Text Summarization benchmarking: ROUGE-L<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-05<BR>ratio: 0.4942<BR>benchmarks:<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-05<BR>ratio: 0.2659<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ADE Corpus - Relation Extraction benchmarking: NER Macro F1<BR>  ADE Corpus - Relation Extraction benchmarking: RE+ Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: NER Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Micro F1<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  ATIS - Intent Detection benchmarking: Accuracy<BR>  SNIPS - Intent Detection benchmarking: Slot F1 Score<BR>",
          "<BR>task: Syntactic analysis: Linguistic Acceptability Assessment<BR>date: 2019-06<BR>ratio: 0.8571<BR>benchmarks:<BR>  CoLA - Linguistic Acceptability Assessment benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-06<BR>ratio: 0.5<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-06<BR>ratio: 0.0675<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-06<BR>ratio: 0.5346<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>  R52 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: Accuracy<BR>  Yelp-2 - Text Classification benchmarking: Accuracy<BR>",
          "<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2019-06<BR>ratio: 0.5765<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ10)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ10)<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-06<BR>ratio: 0.5752<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT 2018 Finnish-English - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2019-06<BR>ratio: 0.6643<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-06<BR>ratio: 0.1145<BR>benchmarks:<BR>  DocRED - Relation Extraction benchmarking: F1<BR>  DocRED - Relation Extraction benchmarking: Ign F1<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-06<BR>ratio: 0.0469<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-06<BR>ratio: 0.4897<BR>benchmarks:<BR>  Quora Question Pairs - Question Answering benchmarking: Accuracy<BR>  RACE - Question Answering benchmarking: RACE-h<BR>  RACE - Question Answering benchmarking: RACE-m<BR>  RACE - Question Answering benchmarking: RACE<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>  SQuAD2.0 dev - Question Answering benchmarking: EM<BR>  SQuAD2.0 dev - Question Answering benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-06<BR>ratio: 0.4252<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>  STS Benchmark - Semantic Textual Similarity benchmarking: Pearson Correlation<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-06<BR>ratio: 0.4272<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-06<BR>ratio: 0.3025<BR>benchmarks:<BR>  ACE 2004 - Named Entity Recognition benchmarking: F1<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-07<BR>ratio: 0.5266<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>  MSRA Dev - Chinese Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-07<BR>ratio: 0.6501<BR>benchmarks:<BR>  NYT-single - Relation Extraction benchmarking: F1<BR>  Re-TACRED - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2019-07<BR>ratio: 0.2915<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-07<BR>ratio: 0.3767<BR>benchmarks:<BR>  NewsQA - Question Answering benchmarking: F1<BR>  TriviaQA - Question Answering benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-07<BR>ratio: 0.5481<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-07<BR>ratio: 0.4801<BR>benchmarks:<BR>  ANLI test - Natural Language Inference benchmarking: A1<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>  XNLI Chinese - Natural Language Inference benchmarking: Accuracy<BR>  XNLI Chinese Dev - Natural Language Inference benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-07<BR>ratio: 0.0577<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-07<BR>ratio: 0.2347<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-07<BR>ratio: 0.2383<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>  SWAG - Common Sense Reasoning benchmarking: Test<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  BBCSport - Document Classification benchmarking: Accuracy<BR>  Reuters-21578 - Document Classification benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Document Summarization<BR>date: 2019-08<BR>ratio: 0.1081<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-08<BR>ratio: 0.4554<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>",
          "<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2019-08<BR>ratio: 0.0358<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-08<BR>ratio: 0.4151<BR>benchmarks:<BR>  ACE 2004 - Named Entity Recognition benchmarking: F1<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>  GENIA - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-08<BR>ratio: 0.0067<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>",
          "<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: FID<BR>",
          "<BR>task: Question answering: Question Answering<BR>date: 2019-09<BR>ratio: 0.0784<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-09<BR>ratio: 0.0085<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2019-09<BR>ratio: 0.1111<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2019-09<BR>ratio: 0.8833<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: F-measure<BR>  Amazon-2 - Text Classification benchmarking: Error<BR>  Amazon-5 - Text Classification benchmarking: Error<BR>  R8 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: F-measure<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-09<BR>ratio: 0.4834<BR>benchmarks:<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>  DocRED - Relation Extraction benchmarking: F1<BR>  DocRED - Relation Extraction benchmarking: Ign F1<BR>  NYT - Relation Extraction benchmarking: F1<BR>  NYT-single - Relation Extraction benchmarking: F1<BR>  WebNLG - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-09<BR>ratio: 0.2136<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>",
          "<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2019-09<BR>ratio: 0.9661<BR>benchmarks:<BR>  SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>",
          "<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-09<BR>ratio: 0.0846<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>",
          "<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-09<BR>ratio: 0.0427<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>",
          "<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2019-09<BR>ratio: 0.6447<BR>benchmarks:<BR>  ACE2004 - Entity Disambiguation benchmarking: Micro-F1<BR>  AIDA-CoNLL - Entity Disambiguation benchmarking: In-KB Accuracy<BR>  AQUAINT - Entity Disambiguation benchmarking: Micro-F1<BR>  MSNBC - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-CWEB - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-WIKI - Entity Disambiguation benchmarking: Micro-F1<BR>",
          "<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>",
          "<BR>task: Syntactic analysis: Linguistic Acceptability Assessment<BR>date: 2019-09<BR>ratio: 0.1429<BR>benchmarks:<BR>  CoLA - Linguistic Acceptability Assessment benchmarking: Accuracy<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-09<BR>ratio: 0.2846<BR>benchmarks:<BR>  WMT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>",
          "<BR>task: Information extraction: Joint Entity and Relation Extraction<BR>date: 2019-09<BR>ratio: 0.7221<BR>benchmarks:<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Entity F1<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Relation F1<BR>",
          "<BR>task: Natural language generation: Machine Translation<BR>date: 2019-10<BR>ratio: 0.2671<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>",
          "<BR>task: Natural language generation: Text Summarization<BR>date: 2019-10<BR>ratio: 0.6089<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  X-Sum - Text Summarization benchmarking: ROUGE-1<BR>  X-Sum - Text Summarization benchmarking: ROUGE-2<BR>  X-Sum - Text Summarization benchmarking: ROUGE-3<BR>",
          "<BR>task: Computer code processing: Code Generation<BR>date: 2019-10<BR>ratio: 0.1871<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>",
          "<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-11<BR>ratio: 0.4096<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI AmazonQA - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>",
          "<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-11<BR>ratio: 0.1815<BR>benchmarks:<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2019-11<BR>ratio: 0.2333<BR>benchmarks:<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-11<BR>ratio: 0.5604<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>  Resume NER - Chinese Named Entity Recognition benchmarking: F1<BR>",
          "<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-12<BR>ratio: 1.0<BR>benchmarks:<BR>  ASOS.com user intent - Intent Detection benchmarking: F1<BR>",
          "<BR>task: Question answering: Visual Question Answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>",
          "<BR>task: Text classification: Document Classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Reuters-21578 - Document Classification benchmarking: F1<BR>",
          "<BR>task: Text classification: Text Classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  RCV1 - Text Classification benchmarking: Micro F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2020-03<BR>ratio: 0.3066<BR>benchmarks:<BR>  DocRED - Relation Extraction benchmarking: F1<BR>",
          "<BR>task: Information extraction: Relation Extraction<BR>date: 2020-04<BR>ratio: 0.2667<BR>benchmarks:<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "color": [
           0.1102,
           1,
           0.3305,
           0.76,
           0.2308,
           0.2549,
           0.0886,
           0.0904,
           0.3032,
           1,
           0.1186,
           0.0427,
           0.232,
           0.5,
           0.1435,
           0.2159,
           0.4025,
           0.25,
           0.483,
           0.3491,
           0.3173,
           0.0718,
           0.1323,
           0.2757,
           0.5335,
           0.2,
           0.5215,
           0.1966,
           0.2762,
           0.542,
           0.7941,
           0.2999,
           0.0702,
           0.3195,
           0.0771,
           0.0678,
           0.218,
           0.094,
           0.9481,
           0.1882,
           0.3589,
           0.0286,
           0.101,
           0.098,
           0.2232,
           0.5,
           0.3558,
           0.6069,
           0.8,
           0.5084,
           0.0123,
           0.0632,
           0.0853,
           0.8096,
           0.1739,
           0.0711,
           0.4857,
           0.7801,
           0.6667,
           0.0708,
           0.0597,
           0.0392,
           0.2857,
           0.1937,
           0.033,
           0.1026,
           0.0174,
           0.108,
           0.1795,
           0.55,
           0.6016,
           0.2253,
           0.2429,
           0.2329,
           0.3586,
           0.2511,
           0.921,
           0.3615,
           0.1524,
           0.4788,
           0.1465,
           0.3698,
           1,
           0.0632,
           1,
           0.3933,
           0.212,
           0.2457,
           0.3456,
           0.5,
           0.2288,
           0.4269,
           0.4385,
           0.6226,
           0.3099,
           0.7439,
           0.3904,
           0.0196,
           0.0407,
           0.7021,
           0.0798,
           0.6272,
           0.6346,
           0.2578,
           0.2561,
           0.0459,
           0.0392,
           0.9439,
           0.1476,
           0.1887,
           0.0392,
           0.0571,
           0.0882,
           0.4672,
           0.0809,
           0.3585,
           0.8333,
           0.1176,
           0.0238,
           0.2079,
           0.4381,
           0.3667,
           0.541,
           0.0357,
           0.2009,
           0.1587,
           1,
           1,
           0.0152,
           0.5921,
           0.2234,
           0.245,
           0.137,
           0.0412,
           0.1698,
           0.1343,
           0.1569,
           0.8365,
           0.0536,
           1,
           0.7182,
           0.2896,
           0.3299,
           0.0126,
           0.7567,
           0.154,
           0.3862,
           0.6132,
           0.0673,
           1,
           0.0282,
           0.3122,
           1,
           0.0553,
           0.791,
           0.5132,
           0.913,
           0.4463,
           1,
           0.0714,
           0.2718,
           0.2449,
           0.0184,
           0.5122,
           0.226,
           1,
           0.1442,
           0.0699,
           1,
           0.3185,
           0.652,
           0.2353,
           0.2419,
           0.196,
           0.5481,
           0.327,
           0.2054,
           0.15,
           0.3426,
           0.2123,
           0.45,
           0.2577,
           0.4277,
           0.4178,
           0.6276,
           1,
           0.1963,
           0.9057,
           0.1404,
           0.2719,
           0.2698,
           0.1449,
           0.5661,
           1,
           0.2041,
           1,
           0.6137,
           0.3456,
           1,
           0.7765,
           0.3558,
           0.2254,
           0.5299,
           0.4127,
           0.1667,
           0.4587,
           0.1426,
           0.3546,
           0.8067,
           1,
           0.397,
           0.3333,
           0.666,
           0.1343,
           0.1016,
           0.6981,
           1,
           0.3031,
           1,
           0.3147,
           0.1967,
           0.2778,
           0.7252,
           0.2995,
           0.2915,
           0.5876,
           0.5763,
           0.0123,
           0.4213,
           0.0901,
           0.2987,
           0.3157,
           0.3432,
           0.4748,
           0.6752,
           0.9327,
           0.4235,
           0.4549,
           0.2911,
           0.4942,
           0.2659,
           1,
           0.8571,
           0.5,
           0.0675,
           0.5346,
           0.5765,
           0.5752,
           0.6643,
           0.1145,
           0.0469,
           0.4897,
           0.4252,
           0.4272,
           0.3025,
           0.5266,
           0.6501,
           0.2915,
           0.3767,
           0.5481,
           0.4801,
           0.0577,
           0.2347,
           0.2383,
           0.3309,
           1,
           0.1081,
           0.4554,
           0.0358,
           0.4151,
           0.0067,
           1,
           0.0784,
           0.0085,
           0.1111,
           0.8833,
           0.4834,
           0.2136,
           0.9661,
           0.0846,
           0.0427,
           0.6447,
           0.046,
           0.1429,
           0.2846,
           0.7221,
           0.2671,
           0.6089,
           0.1871,
           0.4096,
           0.1815,
           0.2333,
           0.5604,
           1,
           0.5266,
           1,
           1,
           0.3066,
           0.2667
          ],
          "colorbar": {
           "len": 500,
           "lenmode": "pixels",
           "thickness": 10,
           "title": {
            "text": "ratio"
           }
          },
          "colorscale": [
           [
            0,
            "rgb(255,255,229)"
           ],
           [
            0.125,
            "rgb(247,252,185)"
           ],
           [
            0.25,
            "rgb(217,240,163)"
           ],
           [
            0.375,
            "rgb(173,221,142)"
           ],
           [
            0.5,
            "rgb(120,198,121)"
           ],
           [
            0.625,
            "rgb(65,171,93)"
           ],
           [
            0.75,
            "rgb(35,132,67)"
           ],
           [
            0.875,
            "rgb(0,104,55)"
           ],
           [
            1,
            "rgb(0,69,41)"
           ]
          ],
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "showscale": true,
          "size": 19,
          "symbol": "circle"
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2013-10",
          "2013-10",
          "2014-06",
          "2014-06",
          "2014-08",
          "2014-08",
          "2014-09",
          "2014-10",
          "2014-12",
          "2014-12",
          "2015-02",
          "2015-06",
          "2015-06",
          "2015-08",
          "2015-09",
          "2015-11",
          "2015-11",
          "2015-11",
          "2016-01",
          "2016-02",
          "2016-02",
          "2016-02",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-05",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-07",
          "2016-07",
          "2016-07",
          "2016-08",
          "2016-08",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-10",
          "2016-10",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-12",
          "2016-12",
          "2016-12",
          "2017-01",
          "2017-02",
          "2017-02",
          "2017-02",
          "2017-03",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-12",
          "2020-02",
          "2020-02",
          "2020-02",
          "2020-03",
          "2020-04"
         ],
         "y": [
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Sentiment Analysis",
          "Question answering: Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Inference and reasoning: Natural Language Inference",
          "Natural language generation: Machine Translation",
          "Natural language generation: Machine Translation",
          "Question answering: Question Answering",
          "Natural language generation: Language Modelling",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Sentiment Analysis",
          "Question answering: Question Answering",
          "Inference and reasoning: Natural Language Inference",
          "Natural language generation: Text Summarization",
          "Question answering: Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Text classification: Text Classification",
          "Information extraction: Relation Extraction",
          "Pragmatics analysis: Sentiment Analysis",
          "Natural language generation: Text Summarization",
          "Question answering: Question Answering",
          "Semantic analysis: Word Sense Disambiguation",
          "Question answering: Visual Question Answering",
          "Question answering: Question Answering",
          "Natural language generation: Machine Translation",
          "Text classification: Document Classification",
          "Syntactic analysis: Dependency Parsing",
          "Question answering: Visual Question Answering",
          "Natural language generation: Machine Translation",
          "Text classification: Citation Intent Classification",
          "Question answering: Visual Question Answering",
          "Pragmatics analysis: Coreference Resolution",
          "Question answering: Question Answering",
          "Natural language generation: Text Summarization",
          "Semantic analysis: Word Sense Disambiguation",
          "Natural language generation: Machine Translation",
          "Pragmatics analysis: Sentiment Analysis",
          "Syntactic analysis: Grammatical Error Detection",
          "Natural language generation: Machine Translation",
          "Question answering: Question Answering",
          "Pragmatics analysis: Coreference Resolution",
          "Question answering: Question Answering",
          "Inference and reasoning: Natural Language Inference",
          "Natural language generation: Machine Translation",
          "Natural language generation: Language Modelling",
          "Text classification: Document Classification",
          "Question answering: Question Answering",
          "Natural language generation: Machine Translation",
          "Question answering: Visual Question Answering",
          "Text classification: Document Classification",
          "Natural language generation: Machine Translation",
          "Question answering: Question Answering",
          "Syntactic analysis: Dependency Parsing",
          "Syntactic analysis: Chunking",
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Constituency Parsing",
          "Question answering: Visual Question Answering",
          "Natural language generation: Language Modelling",
          "Other NLP task: Text-to-Image Generation",
          "Natural language generation: Machine Translation",
          "Inference and reasoning: Natural Language Inference",
          "Text classification: Text Classification",
          "Pragmatics analysis: Sentiment Analysis",
          "Question answering: Question Answering",
          "Question answering: Visual Question Answering",
          "Question answering: Question Answering",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Sentiment Analysis",
          "Semantic analysis: Semantic Parsing",
          "Syntactic analysis: Grammatical Error Detection",
          "Natural language generation: Machine Translation",
          "Question answering: Question Answering",
          "Question answering: Visual Question Answering",
          "Natural language generation: Text Generation",
          "Natural language generation: Document Summarization",
          "Semantic analysis: Entity Disambiguation",
          "Natural language generation: Text Summarization",
          "Question answering: Question Answering",
          "Natural language generation: Machine Translation",
          "Pragmatics analysis: Coreference Resolution",
          "Syntactic analysis: Grammatical Error Detection",
          "Sentence embedding: Sentence Compression",
          "Question answering: Visual Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Question answering: Question Answering",
          "Information extraction: Relation Extraction",
          "Syntactic analysis: Constituency Parsing",
          "Question answering: Visual Question Answering",
          "Question answering: Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Computer code processing: Code Generation",
          "Information extraction: Relation Extraction",
          "Natural language generation: Text Generation",
          "Pragmatics analysis: Paraphrase Identification",
          "Dialog process: Dialog Act Classification",
          "Dialog process: Visual Dialog",
          "Inference and reasoning: Natural Language Inference",
          "Semantic analysis: Word Sense Disambiguation",
          "Other NLP task: Text-to-Image Generation",
          "Text classification: Document Classification",
          "Question answering: Question Answering",
          "Other NLP task: Text-to-Image Generation",
          "Dialog process: Visual Dialog",
          "Dialog process: Dialog Act Classification",
          "Natural language generation: Machine Translation",
          "Inference and reasoning: Natural Language Inference",
          "Pragmatics analysis: Fake News Detection",
          "Pragmatics analysis: Sentiment Analysis",
          "Semantic analysis: Semantic Role Labeling",
          "Inference and reasoning: Natural Language Inference",
          "Question answering: Question Answering",
          "Text classification: Citation Intent Classification",
          "Question answering: Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Semantic analysis: Semantic Role Labeling",
          "Natural language generation: Text Generation",
          "Text classification: Citation Intent Classification",
          "Semantic analysis: Word Sense Disambiguation",
          "Pragmatics analysis: Coreference Resolution",
          "Natural language generation: Machine Translation",
          "Pragmatics analysis: Sentiment Analysis",
          "Information retrieval: Conversational Response Selection",
          "Text classification: Text Classification",
          "Computer code processing: Code Generation",
          "Question answering: Visual Question Answering",
          "Natural language generation: Language Modelling",
          "Semantic analysis: Semantic Textual Similarity",
          "Natural language generation: Machine Translation",
          "Question answering: Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Pragmatics analysis: Coreference Resolution",
          "Computer code processing: Code Generation",
          "Inference and reasoning: Natural Language Inference",
          "Semantic analysis: Semantic Role Labeling",
          "Syntactic analysis: Constituency Parsing",
          "Inference and reasoning: Natural Language Inference",
          "Dialog process: Dialog State Tracking",
          "Semantic analysis: Word Sense Disambiguation",
          "Inference and reasoning: Common Sense Reasoning",
          "Question answering: Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Text classification: Text Classification",
          "Question answering: Visual Question Answering",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Natural language generation: Machine Translation",
          "Question answering: Question Answering",
          "Inference and reasoning: Natural Language Inference",
          "Natural language generation: Question Generation",
          "Question answering: Question Answering",
          "Pragmatics analysis: Paraphrase Identification",
          "Information extraction: Relation Extraction",
          "Sentence embedding: Sentence Compression",
          "Natural language generation: Text Summarization",
          "Syntactic analysis: Dependency Parsing",
          "Text classification: Text Classification",
          "Syntactic analysis: Chunking",
          "Information extraction: Relation Extraction",
          "Information extraction: Named Entity Recognition",
          "Syntactic analysis: Constituency Grammar Induction",
          "Question answering: Question Answering",
          "Natural language generation: Text Summarization",
          "Text classification: Document Classification",
          "Natural language generation: Document Summarization",
          "Natural language generation: Machine Translation",
          "Text classification: Text Classification",
          "Dialog process: Visual Dialog",
          "Natural language generation: Machine Translation",
          "Natural language generation: Language Modelling",
          "Information extraction: Relation Extraction",
          "Text classification: Text Classification",
          "Inference and reasoning: Natural Language Inference",
          "Question answering: Question Answering",
          "Question answering: Question Answering",
          "Computer code processing: Code Generation",
          "Dialog process: Dialog State Tracking",
          "Information extraction: Named Entity Recognition",
          "Machine translation: Unsupervised Machine Translation",
          "Inference and reasoning: Natural Language Inference",
          "Syntactic analysis: Constituency Grammar Induction",
          "Semantic analysis: Semantic Parsing",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Semantic analysis: Semantic Role Labeling",
          "Pragmatics analysis: Sentiment Analysis",
          "Inference and reasoning: Common Sense Reasoning",
          "Natural language generation: Machine Translation",
          "Information extraction: Relation Extraction",
          "Text classification: Sentence Classification",
          "Pragmatics analysis: Fake News Detection",
          "Semantic analysis: Word Sense Disambiguation",
          "Syntactic analysis: Grammatical Error Detection",
          "Question answering: Question Answering",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Information extraction: Relation Extraction",
          "Question answering: Question Answering",
          "Information extraction: Named Entity Recognition",
          "Text classification: Text Classification",
          "Inference and reasoning: Natural Language Inference",
          "Other NLP task: Text-to-Image Generation",
          "Natural language generation: Machine Translation",
          "Information retrieval: Conversational Response Selection",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Sentiment Analysis",
          "Machine translation: Unsupervised Machine Translation",
          "Natural language generation: Text Generation",
          "Question answering: Visual Question Answering",
          "Dialog process: Visual Dialog",
          "Text classification: Text Classification",
          "Question answering: Question Answering",
          "Inference and reasoning: Common Sense Reasoning",
          "Machine translation: Unsupervised Machine Translation",
          "Information extraction: Relation Extraction",
          "Information extraction: Named Entity Recognition",
          "Syntactic analysis: Constituency Parsing",
          "Other NLP task: Text-to-Image Generation",
          "Text classification: Sentence Classification",
          "Information extraction: Relation Extraction",
          "Natural language generation: Document Summarization",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Question answering: Visual Question Answering",
          "Syntactic analysis: Constituency Grammar Induction",
          "Information extraction: Joint Entity and Relation Extraction",
          "Information extraction: Relation Extraction",
          "Information extraction: Chinese Named Entity Recognition",
          "Information retrieval: Conversational Response Selection",
          "Other NLP task: Text-to-Image Generation",
          "Natural language generation: Text Summarization",
          "Text classification: Document Classification",
          "Dialog process: Visual Dialog",
          "Natural language generation: Document Summarization",
          "Natural language generation: Machine Translation",
          "Machine translation: Unsupervised Machine Translation",
          "Pragmatics analysis: Sentiment Analysis",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Word Sense Disambiguation",
          "Natural language generation: Question Generation",
          "Question answering: Visual Question Answering",
          "Text classification: Text Classification",
          "Natural language generation: Text Summarization",
          "Question answering: Question Answering",
          "Information extraction: Relation Extraction",
          "Pragmatics analysis: Intent Detection",
          "Syntactic analysis: Linguistic Acceptability Assessment",
          "Inference and reasoning: Natural Language Inference",
          "Pragmatics analysis: Sentiment Analysis",
          "Text classification: Text Classification",
          "Syntactic analysis: Constituency Grammar Induction",
          "Natural language generation: Machine Translation",
          "Pragmatics analysis: Paraphrase Identification",
          "Information extraction: Relation Extraction",
          "Question answering: Visual Question Answering",
          "Question answering: Question Answering",
          "Semantic analysis: Semantic Textual Similarity",
          "Inference and reasoning: Common Sense Reasoning",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Chinese Named Entity Recognition",
          "Information extraction: Relation Extraction",
          "Pragmatics analysis: Coreference Resolution",
          "Question answering: Question Answering",
          "Question answering: Visual Question Answering",
          "Inference and reasoning: Natural Language Inference",
          "Semantic analysis: Semantic Textual Similarity",
          "Pragmatics analysis: Sentiment Analysis",
          "Inference and reasoning: Common Sense Reasoning",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Text classification: Document Classification",
          "Natural language generation: Document Summarization",
          "Question answering: Visual Question Answering",
          "Pragmatics analysis: Coreference Resolution",
          "Information extraction: Named Entity Recognition",
          "Question answering: Question Answering",
          "Other NLP task: Text-to-Image Generation",
          "Question answering: Question Answering",
          "Pragmatics analysis: Sentiment Analysis",
          "Question answering: Visual Question Answering",
          "Text classification: Text Classification",
          "Information extraction: Relation Extraction",
          "Inference and reasoning: Common Sense Reasoning",
          "Semantic analysis: Word Sense Disambiguation",
          "Semantic analysis: Semantic Textual Similarity",
          "Inference and reasoning: Natural Language Inference",
          "Semantic analysis: Entity Disambiguation",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Syntactic analysis: Linguistic Acceptability Assessment",
          "Natural language generation: Machine Translation",
          "Information extraction: Joint Entity and Relation Extraction",
          "Natural language generation: Machine Translation",
          "Natural language generation: Text Summarization",
          "Computer code processing: Code Generation",
          "Information retrieval: Conversational Response Selection",
          "Information extraction: Named Entity Recognition",
          "Information extraction: Relation Extraction",
          "Information extraction: Chinese Named Entity Recognition",
          "Pragmatics analysis: Intent Detection",
          "Question answering: Visual Question Answering",
          "Text classification: Document Classification",
          "Text classification: Text Classification",
          "Information extraction: Relation Extraction",
          "Information extraction: Relation Extraction"
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 21
        },
        "height": 1782.0000000000002,
        "legend": {
         "title": {
          "text": "task"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Natural Language Processing",
         "y": 0.995
        },
        "width": 1500,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "tickmode": "auto",
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "categoryarray": [
          "Text classification: Sentence Classification",
          "Text classification: Document Classification",
          "Text classification: Text Classification",
          "Text classification: Citation Intent Classification",
          "Syntactic analysis: Linguistic Acceptability Assessment",
          "Syntactic analysis: Grammatical Error Detection",
          "Syntactic analysis: Constituency Grammar Induction",
          "Syntactic analysis: Chunking",
          "Syntactic analysis: Constituency Parsing",
          "Syntactic analysis: Dependency Parsing",
          "Sentence embedding: Sentence Compression",
          "Semantic analysis: Semantic Role Labeling",
          "Semantic analysis: Semantic Parsing",
          "Semantic analysis: Semantic Textual Similarity",
          "Semantic analysis: Entity Disambiguation",
          "Semantic analysis: Word Sense Disambiguation",
          "Question answering: Question Answering",
          "Question answering: Visual Question Answering",
          "Pragmatics analysis: Paraphrase Identification",
          "Pragmatics analysis: Emotion Recognition in Conversation",
          "Pragmatics analysis: Fake News Detection",
          "Pragmatics analysis: Intent Detection",
          "Pragmatics analysis: Coreference Resolution",
          "Pragmatics analysis: Sentiment Analysis",
          "Other NLP task: Text-to-Image Generation",
          "Natural language generation: Language Modelling",
          "Natural language generation: Document Summarization",
          "Natural language generation: Text Generation",
          "Natural language generation: Question Generation",
          "Natural language generation: Machine Translation",
          "Natural language generation: Text Summarization",
          "Machine translation: Unsupervised Machine Translation",
          "Information retrieval: Conversational Response Selection",
          "Information extraction: Chinese Named Entity Recognition",
          "Information extraction: Relation Extraction",
          "Information extraction: Joint Entity and Relation Extraction",
          "Information extraction: Named Entity Recognition",
          "Inference and reasoning: Natural Language Inference",
          "Inference and reasoning: Common Sense Reasoning",
          "Dialog process: Dialog Act Classification",
          "Dialog process: Dialog State Tracking",
          "Dialog process: Visual Dialog",
          "Computer code processing: Code Generation"
         ],
         "categoryorder": "array",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "side": "left",
         "title": {}
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"07ba0ae0-43f0-44bb-a9c0-b83dcd62b312\" class=\"plotly-graph-div\" style=\"height:1782.0000000000002px; width:1500px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"07ba0ae0-43f0-44bb-a9c0-b83dcd62b312\")) {                    Plotly.newPlot(                        \"07ba0ae0-43f0-44bb-a9c0-b83dcd62b312\",                        [{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Computer code processing: Code Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Computer code processing: Code Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-08\",\"2018-03\",\"2018-04\",\"2018-10\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Dialog process: Visual Dialog\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Dialog process: Visual Dialog\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-09\",\"2017-11\",\"2017-09\",\"2019-02\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Dialog process: Visual Dialog\",\"Dialog process: Visual Dialog\",\"Dialog process: Visual Dialog\",\"Dialog process: Visual Dialog\",\"Dialog process: Visual Dialog\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Dialog process: Dialog State Tracking\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Dialog process: Dialog State Tracking\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-10\",\"2018-05\"],\"xaxis\":\"x\",\"y\":[\"Dialog process: Dialog State Tracking\",\"Dialog process: Dialog State Tracking\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Dialog process: Dialog Act Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Dialog process: Dialog Act Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-11\",\"2017-09\"],\"xaxis\":\"x\",\"y\":[\"Dialog process: Dialog Act Classification\",\"Dialog process: Dialog Act Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Inference and reasoning: Common Sense Reasoning\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Inference and reasoning: Common Sense Reasoning\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-10\",\"2018-05\",\"2019-02\",\"2019-06\",\"2019-07\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Inference and reasoning: Natural Language Inference\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Inference and reasoning: Natural Language Inference\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2019-07\",\"2019-06\",\"2019-01\",\"2018-10\",\"2018-06\",\"2018-05\",\"2018-04\",\"2017-12\",\"2018-09\",\"2017-09\",\"2017-11\",\"2014-08\",\"2016-09\",\"2017-02\",\"2015-08\"],\"xaxis\":\"x\",\"y\":[\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Information extraction: Named Entity Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Information extraction: Named Entity Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-06\",\"2019-11\",\"2019-03\",\"2019-01\",\"2019-08\",\"2018-10\",\"2018-08\"],\"xaxis\":\"x\",\"y\":[\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Information extraction: Joint Entity and Relation Extraction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Information extraction: Joint Entity and Relation Extraction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Information extraction: Joint Entity and Relation Extraction\",\"Information extraction: Joint Entity and Relation Extraction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Information extraction: Relation Extraction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Information extraction: Relation Extraction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2019-11\",\"2019-06\",\"2019-07\",\"2019-05\",\"2016-01\",\"2020-03\",\"2020-04\",\"2017-07\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2018-12\",\"2019-02\",\"2019-03\",\"2019-04\",\"2017-09\"],\"xaxis\":\"x\",\"y\":[\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Information extraction: Chinese Named Entity Recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Information extraction: Chinese Named Entity Recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-11\",\"2019-07\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Information extraction: Chinese Named Entity Recognition\",\"Information extraction: Chinese Named Entity Recognition\",\"Information extraction: Chinese Named Entity Recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Information retrieval: Conversational Response Selection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Information retrieval: Conversational Response Selection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-01\",\"2019-04\",\"2018-03\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Information retrieval: Conversational Response Selection\",\"Information retrieval: Conversational Response Selection\",\"Information retrieval: Conversational Response Selection\",\"Information retrieval: Conversational Response Selection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Machine translation: Unsupervised Machine Translation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Machine translation: Unsupervised Machine Translation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-10\",\"2019-01\",\"2019-05\",\"2019-02\"],\"xaxis\":\"x\",\"y\":[\"Machine translation: Unsupervised Machine Translation\",\"Machine translation: Unsupervised Machine Translation\",\"Machine translation: Unsupervised Machine Translation\",\"Machine translation: Unsupervised Machine Translation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Natural language generation: Text Summarization\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Natural language generation: Text Summarization\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-05\",\"2019-10\",\"2019-04\",\"2018-08\",\"2018-07\",\"2017-06\",\"2016-06\",\"2016-02\",\"2015-09\"],\"xaxis\":\"x\",\"y\":[\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Summarization\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Natural language generation: Machine Translation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Natural language generation: Machine Translation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-10\",\"2019-09\",\"2019-06\",\"2019-05\",\"2019-01\",\"2018-10\",\"2018-09\",\"2018-08\",\"2018-06\",\"2017-06\",\"2018-02\",\"2018-03\",\"2014-09\",\"2017-11\",\"2016-03\",\"2016-06\",\"2017-05\",\"2016-07\",\"2017-01\",\"2016-11\",\"2016-10\",\"2016-09\",\"2016-08\",\"2014-10\"],\"xaxis\":\"x\",\"y\":[\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Natural language generation: Question Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Natural language generation: Question Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-05\",\"2018-06\"],\"xaxis\":\"x\",\"y\":[\"Natural language generation: Question Generation\",\"Natural language generation: Question Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Natural language generation: Text Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Natural language generation: Text Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-05\",\"2017-09\",\"2018-02\",\"2019-01\"],\"xaxis\":\"x\",\"y\":[\"Natural language generation: Text Generation\",\"Natural language generation: Text Generation\",\"Natural language generation: Text Generation\",\"Natural language generation: Text Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Natural language generation: Document Summarization\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Natural language generation: Document Summarization\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-08\",\"2019-05\",\"2019-03\",\"2018-08\",\"2017-05\"],\"xaxis\":\"x\",\"y\":[\"Natural language generation: Document Summarization\",\"Natural language generation: Document Summarization\",\"Natural language generation: Document Summarization\",\"Natural language generation: Document Summarization\",\"Natural language generation: Document Summarization\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Natural language generation: Language Modelling\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Natural language generation: Language Modelling\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-09\",\"2016-12\",\"2018-03\",\"2018-09\",\"2014-12\"],\"xaxis\":\"x\",\"y\":[\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other NLP task: Text-to-Image Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other NLP task: Text-to-Image Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-12\",\"2019-09\",\"2019-03\",\"2019-01\",\"2017-11\",\"2017-10\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pragmatics analysis: Sentiment Analysis\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pragmatics analysis: Sentiment Analysis\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-01\",\"2017-12\",\"2017-08\",\"2017-07\",\"2018-02\",\"2018-04\",\"2017-04\",\"2018-05\",\"2017-02\",\"2016-02\",\"2015-11\",\"2015-06\",\"2015-02\",\"2014-08\",\"2014-06\",\"2013-10\",\"2018-10\",\"2016-07\",\"2019-01\",\"2019-06\",\"2019-05\",\"2019-09\",\"2019-07\"],\"xaxis\":\"x\",\"y\":[\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pragmatics analysis: Coreference Resolution\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pragmatics analysis: Coreference Resolution\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-02\",\"2019-08\",\"2019-07\",\"2018-04\",\"2017-07\",\"2016-09\",\"2016-06\"],\"xaxis\":\"x\",\"y\":[\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pragmatics analysis: Intent Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pragmatics analysis: Intent Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-12\",\"2019-06\"],\"xaxis\":\"x\",\"y\":[\"Pragmatics analysis: Intent Detection\",\"Pragmatics analysis: Intent Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pragmatics analysis: Fake News Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pragmatics analysis: Fake News Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2017-12\"],\"xaxis\":\"x\",\"y\":[\"Pragmatics analysis: Fake News Detection\",\"Pragmatics analysis: Fake News Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pragmatics analysis: Emotion Recognition in Conversation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pragmatics analysis: Emotion Recognition in Conversation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2019-04\",\"2018-11\",\"2018-10\",\"2019-08\",\"2018-06\"],\"xaxis\":\"x\",\"y\":[\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Emotion Recognition in Conversation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pragmatics analysis: Paraphrase Identification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pragmatics analysis: Paraphrase Identification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2013-10\",\"2017-04\",\"2018-07\",\"2019-01\",\"2019-06\",\"2017-09\"],\"xaxis\":\"x\",\"y\":[\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Paraphrase Identification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Question answering: Visual Question Answering\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Question answering: Visual Question Answering\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2020-02\",\"2019-09\",\"2019-08\",\"2019-07\",\"2019-06\",\"2019-05\",\"2019-04\",\"2019-02\",\"2017-07\",\"2018-03\",\"2017-08\",\"2016-12\",\"2016-11\",\"2016-06\",\"2016-03\",\"2017-05\",\"2017-04\",\"2018-05\",\"2016-05\"],\"xaxis\":\"x\",\"y\":[\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Question answering: Question Answering\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Question answering: Question Answering\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-08\",\"2019-09\",\"2018-08\",\"2018-07\",\"2018-06\",\"2018-05\",\"2018-04\",\"2018-01\",\"2017-12\",\"2017-10\",\"2017-08\",\"2017-07\",\"2017-06\",\"2017-05\",\"2017-04\",\"2017-03\",\"2016-11\",\"2016-10\",\"2016-09\",\"2016-02\",\"2016-03\",\"2018-09\",\"2018-10\",\"2018-11\",\"2019-01\",\"2019-02\",\"2019-05\",\"2019-06\",\"2015-11\",\"2015-06\",\"2014-12\",\"2014-06\",\"2019-08\",\"2019-07\",\"2016-06\"],\"xaxis\":\"x\",\"y\":[\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic analysis: Word Sense Disambiguation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic analysis: Word Sense Disambiguation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-11\",\"2018-05\",\"2018-02\",\"2016-06\",\"2016-03\",\"2017-09\",\"2019-05\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic analysis: Entity Disambiguation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic analysis: Entity Disambiguation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2017-05\"],\"xaxis\":\"x\",\"y\":[\"Semantic analysis: Entity Disambiguation\",\"Semantic analysis: Entity Disambiguation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic analysis: Semantic Textual Similarity\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic analysis: Semantic Textual Similarity\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-05\",\"2018-03\",\"2019-06\",\"2019-07\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Semantic Textual Similarity\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic analysis: Semantic Parsing\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic analysis: Semantic Parsing\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-04\",\"2018-10\"],\"xaxis\":\"x\",\"y\":[\"Semantic analysis: Semantic Parsing\",\"Semantic analysis: Semantic Parsing\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic analysis: Semantic Role Labeling\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic analysis: Semantic Role Labeling\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-12\",\"2018-02\",\"2018-05\",\"2018-10\"],\"xaxis\":\"x\",\"y\":[\"Semantic analysis: Semantic Role Labeling\",\"Semantic analysis: Semantic Role Labeling\",\"Semantic analysis: Semantic Role Labeling\",\"Semantic analysis: Semantic Role Labeling\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Sentence embedding: Sentence Compression\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Sentence embedding: Sentence Compression\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-07\",\"2017-07\"],\"xaxis\":\"x\",\"y\":[\"Sentence embedding: Sentence Compression\",\"Sentence embedding: Sentence Compression\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Syntactic analysis: Dependency Parsing\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Syntactic analysis: Dependency Parsing\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-03\",\"2018-07\",\"2016-11\"],\"xaxis\":\"x\",\"y\":[\"Syntactic analysis: Dependency Parsing\",\"Syntactic analysis: Dependency Parsing\",\"Syntactic analysis: Dependency Parsing\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Syntactic analysis: Constituency Parsing\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Syntactic analysis: Constituency Parsing\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-07\",\"2016-11\",\"2019-03\",\"2018-05\"],\"xaxis\":\"x\",\"y\":[\"Syntactic analysis: Constituency Parsing\",\"Syntactic analysis: Constituency Parsing\",\"Syntactic analysis: Constituency Parsing\",\"Syntactic analysis: Constituency Parsing\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Syntactic analysis: Chunking\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Syntactic analysis: Chunking\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-11\",\"2018-08\"],\"xaxis\":\"x\",\"y\":[\"Syntactic analysis: Chunking\",\"Syntactic analysis: Chunking\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Syntactic analysis: Constituency Grammar Induction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Syntactic analysis: Constituency Grammar Induction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-06\",\"2019-04\",\"2018-10\",\"2018-08\"],\"xaxis\":\"x\",\"y\":[\"Syntactic analysis: Constituency Grammar Induction\",\"Syntactic analysis: Constituency Grammar Induction\",\"Syntactic analysis: Constituency Grammar Induction\",\"Syntactic analysis: Constituency Grammar Induction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Syntactic analysis: Grammatical Error Detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Syntactic analysis: Grammatical Error Detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-07\",\"2016-11\",\"2018-11\",\"2017-07\",\"2017-04\"],\"xaxis\":\"x\",\"y\":[\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Grammatical Error Detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Syntactic analysis: Linguistic Acceptability Assessment\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Syntactic analysis: Linguistic Acceptability Assessment\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-09\",\"2019-06\"],\"xaxis\":\"x\",\"y\":[\"Syntactic analysis: Linguistic Acceptability Assessment\",\"Syntactic analysis: Linguistic Acceptability Assessment\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Text classification: Citation Intent Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Text classification: Citation Intent Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-02\",\"2016-06\",\"2018-01\"],\"xaxis\":\"x\",\"y\":[\"Text classification: Citation Intent Classification\",\"Text classification: Citation Intent Classification\",\"Text classification: Citation Intent Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Text classification: Text Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Text classification: Text Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-06\",\"2019-05\",\"2019-02\",\"2019-01\",\"2018-09\",\"2018-08\",\"2017-02\",\"2018-03\",\"2020-02\",\"2018-05\",\"2015-11\",\"2019-09\",\"2018-07\"],\"xaxis\":\"x\",\"y\":[\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Text classification: Document Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Text classification: Document Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-10\",\"2016-09\",\"2016-03\",\"2020-02\",\"2019-08\",\"2019-04\",\"2018-08\",\"2016-11\"],\"xaxis\":\"x\",\"y\":[\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Text classification: Sentence Classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Text classification: Sentence Classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2019-03\",\"2018-10\"],\"xaxis\":\"x\",\"y\":[\"Text classification: Sentence Classification\",\"Text classification: Sentence Classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":[\"<BR>task: Computer code processing: Code Generation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  CoNaLa - Code Generation benchmarking: BLEU<BR>  CoNaLa-Ext - Code Generation benchmarking: BLEU<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Android Repos - Code Generation benchmarking: Perplexity<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  100 sleep nights of 8 caregivers - Code Generation benchmarking: 14 gestures accuracy<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Django - Code Generation benchmarking: Accuracy<BR>\",\"<BR>task: Dialog process: Dialog Act Classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Switchboard corpus - Dialog Act Classification benchmarking: Accuracy<BR>\",\"<BR>task: Dialog process: Dialog State Tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Area<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Food<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Joint<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Price<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Request<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Joint<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Request<BR>\",\"<BR>task: Dialog process: Visual Dialog<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: Mean Rank<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Dialog process: Visual Dialog<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: NDCG (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  V-SNLI - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  XNLI French - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  XNLI Chinese - Natural Language Inference benchmarking: Accuracy<BR>  XNLI Chinese Dev - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ANLI test - Natural Language Inference benchmarking: A1<BR>  ANLI test - Natural Language Inference benchmarking: A2<BR>  ANLI test - Natural Language Inference benchmarking: A3<BR>  WNLI - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Train Accuracy<BR>  SNLI - Natural Language Inference benchmarking: Parameters<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog v0.9 - Common Sense Reasoning benchmarking: 1 in 10 R-at-5<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog  v0.9 - Common Sense Reasoning benchmarking: 1 in 10 R-at-5<BR>  Visual Dialog  v0.9 - Common Sense Reasoning benchmarking: Recall-at-10<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  ReCoRD - Common Sense Reasoning benchmarking: EM<BR>  ReCoRD - Common Sense Reasoning benchmarking: F1<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Event2Mind dev - Common Sense Reasoning benchmarking: Average Cross-Ent<BR>  Event2Mind test - Common Sense Reasoning benchmarking: Average Cross-Ent<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  SWAG - Common Sense Reasoning benchmarking: Dev<BR>  SWAG - Common Sense Reasoning benchmarking: Test<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Winograd Schema Challenge - Common Sense Reasoning benchmarking: Score<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2000 - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Joint Entity and Relation Extraction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Entity F1<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Relation F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  CoNLL04 - Relation Extraction benchmarking: NER Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Micro F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-10%<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-30%<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Long-tail emerging entities - Named Entity Recognition benchmarking: F1 (surface form)<BR>  Long-tail emerging entities - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  SciERC - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  JNLPBA - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  ACE 2004 - Named Entity Recognition benchmarking: F1<BR>  NCBI-disease - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  NYT - Relation Extraction benchmarking: F1<BR>  NYT-single - Relation Extraction benchmarking: F1<BR>  WebNLG - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  WLPC - Named Entity Recognition benchmarking: F1<BR>  WetLab - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  LINNAEUS - Named Entity Recognition benchmarking: F1<BR>  Species-800 - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Code-Switching English-Spanish NER - Named Entity Recognition benchmarking: F1<BR>  ontontoes chinese v5 - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  SoSciSoCi - Named Entity Recognition benchmarking: F1<BR>  SoSciSoCi - Named Entity Recognition benchmarking: Precision<BR>  SoSciSoCi - Named Entity Recognition benchmarking: Recall<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Re-TACRED - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>  Wikipedia-Wikidata relations - Relation Extraction benchmarking: Error rate<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>  GENIA - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  ChemProt - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ADE Corpus - Relation Extraction benchmarking: NER Macro F1<BR>  ADE Corpus - Relation Extraction benchmarking: RE+ Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: NER Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Macro F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  NYT24 - Relation Extraction benchmarking: F1<BR>  NYT29 - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  DocRED - Relation Extraction benchmarking: F1<BR>  DocRED - Relation Extraction benchmarking: Ign F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  FewRel - Relation Extraction benchmarking: F1<BR>  FewRel - Relation Extraction benchmarking: Precision<BR>  FewRel - Relation Extraction benchmarking: Recall<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  JNLPBA - Relation Extraction benchmarking: F1<BR>  SciERC - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  SighanNER - Chinese Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>  OntoNotes 4 - Chinese Named Entity Recognition benchmarking: F1<BR>  Resume NER - Chinese Named Entity Recognition benchmarking: F1<BR>  Weibo NER - Chinese Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  WLPC - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  MSRA Dev - Chinese Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Advising Corpus - Conversational Response Selection benchmarking: R-at-10<BR>  Advising Corpus - Conversational Response Selection benchmarking: R-at-1<BR>  Advising Corpus - Conversational Response Selection benchmarking: R@50<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  PolyAI AmazonQA - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI OpenSubtitles - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>\",\"<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 German-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  IWSLT2015 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  WMT 2017 English-Chinese - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 Thai-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Czech-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Czech - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Russian - Machine Translation benchmarking: BLEU score<BR>  WMT2016 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Romanian-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Russian-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Machine Translation benchmarking: 1-of-100 Accuracy<BR>  WMT 2017 Latvian-English - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  ACCURAT balanced test corpus for under resourced languages Estonian-Russian - Machine Translation benchmarking: BLEU<BR>  ACCURAT balanced test corpus for under resourced languages Russian-Estonian - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  WMT2016 Finnish-English - Machine Translation benchmarking: BLEU<BR>  WMT2017 Finnish-English - Machine Translation benchmarking: BLEU<BR>  WMT2019 Finnish-English - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 French-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  WMT 2017 English-Latvian - Machine Translation benchmarking: BLEU<BR>  WMT 2018 English-Estonian - Machine Translation benchmarking: BLEU<BR>  WMT 2018 English-Finnish - Machine Translation benchmarking: BLEU<BR>  WMT 2018 Estonian-English - Machine Translation benchmarking: BLEU<BR>  WMT 2018 Finnish-English - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-Czech - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Machine Translation benchmarking: Accuracy<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2015 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2015 English-Russian - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  WMT2019 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2019 English-German - Machine Translation benchmarking: SacreBLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2017 Arabic-English - Machine Translation benchmarking: Cased sacreBLEU<BR>  IWSLT2017 English-Arabic - Machine Translation benchmarking: Cased sacreBLEU<BR>  IWSLT2017 English-French - Machine Translation benchmarking: Cased sacreBLEU<BR>  IWSLT2017 French-English - Machine Translation benchmarking: Cased sacreBLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 Chinese-English - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: SacreBLEU<BR>  WMT2014 English-German - Machine Translation benchmarking: SacreBLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  LAMBADA - Language Modelling benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  X-Sum - Text Summarization benchmarking: ROUGE-1<BR>  X-Sum - Text Summarization benchmarking: ROUGE-2<BR>  X-Sum - Text Summarization benchmarking: ROUGE-3<BR>\",\"<BR>task: Natural language generation: Document Summarization<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: PPL<BR>\",\"<BR>task: Natural language generation: Document Summarization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Natural language generation: Question Generation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  SQuAD1.1 - Question Generation benchmarking: BLEU-4<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  COCO Captions - Text Generation benchmarking: BLEU-2<BR>  COCO Captions - Text Generation benchmarking: BLEU-3<BR>  COCO Captions - Text Generation benchmarking: BLEU-4<BR>  COCO Captions - Text Generation benchmarking: BLEU-5<BR>  Chinese Poems - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-3<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-4<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-5<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Yahoo Questions - Text Generation benchmarking: KL<BR>  Yahoo Questions - Text Generation benchmarking: NLL<BR>  Yahoo Questions - Text Generation benchmarking: Perplexity<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  LDC2016E25 - Text Generation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  DailyDialog - Text Generation benchmarking: BLEU-1<BR>  DailyDialog - Text Generation benchmarking: BLEU-2<BR>  DailyDialog - Text Generation benchmarking: BLEU-3<BR>  DailyDialog - Text Generation benchmarking: BLEU-4<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Pubmed - Text Summarization benchmarking: ROUGE-1<BR>  arXiv - Text Summarization benchmarking: ROUGE-1<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: PPL<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Text8 - Language Modelling benchmarking: Bit per Character (BPC)<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Hutter Prize - Language Modelling benchmarking: Bit per Character (BPC)<BR>  enwik8 - Language Modelling benchmarking: Bit per Character (BPC)<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  WikiText-2 - Language Modelling benchmarking: Test perplexity<BR>  WikiText-2 - Language Modelling benchmarking: Validation perplexity<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Test perplexity<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Validation perplexity<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: Validation perplexity<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  The Pile - Language Modelling benchmarking: Bits per byte<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  PTB - Language Modelling benchmarking: PPL<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>  GigaWord - Text Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Natural language generation: Question Generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Generation - Question Generation benchmarking: BLEU-1<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  CUB - Text-to-Image Generation benchmarking: FID<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  Oxford 102 Flowers - Text-to-Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: FID<BR>  Oxford 102 Flowers - Text-to-Image Generation benchmarking: FID<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: SOA-C<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: Acc<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: FID<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: LPIPS<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: Real<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  2017_test set - Paraphrase Identification benchmarking: 10 fold Cross validation<BR>\",\"<BR>task: Pragmatics analysis: Fake News Detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Agree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Disagree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Discuss)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Unrelated)<BR>  FNC-1 - Fake News Detection benchmarking: Weighted Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  MSRP - Paraphrase Identification benchmarking: Accuracy<BR>  MSRP - Paraphrase Identification benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Amazon Review Full - Sentiment Analysis benchmarking: Accuracy<BR>  Amazon Review Polarity - Sentiment Analysis benchmarking: Accuracy<BR>  Sogou News - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  GAP - Coreference Resolution benchmarking: Bias (F/M)<BR>  GAP - Coreference Resolution benchmarking: Feminine F1 (F)<BR>  GAP - Coreference Resolution benchmarking: Masculine F1 (M)<BR>  GAP - Coreference Resolution benchmarking: Overall F1<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  MR - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Yelp Binary classification - Sentiment Analysis benchmarking: Error<BR>  Yelp Fine-grained classification - Sentiment Analysis benchmarking: Error<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ASTD - Sentiment Analysis benchmarking: Average Recall<BR>  ArSAS - Sentiment Analysis benchmarking: Average Recall<BR>  FiQA - Sentiment Analysis benchmarking: MSE<BR>  FiQA - Sentiment Analysis benchmarking: R^2<BR>  Financial PhraseBank - Sentiment Analysis benchmarking: Accuracy<BR>  Financial PhraseBank - Sentiment Analysis benchmarking: F1 score<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Twitter - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  SemEval - Sentiment Analysis benchmarking: F1-score<BR>  SemEval 2017 Task 4-A - Sentiment Analysis benchmarking: Average Recall<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ChnSentiCorp - Sentiment Analysis benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Average<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Electronics<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  CR - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ATIS - Intent Detection benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  DailyDialog - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>  EmoryNLP - Emotion Recognition in Conversation benchmarking: Weighted Macro-F1<BR>\",\"<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  ASOS.com user intent - Intent Detection benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  MPQA - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  ATIS - Intent Detection benchmarking: Accuracy<BR>  SNIPS - Intent Detection benchmarking: Intent Accuracy<BR>  SNIPS - Intent Detection benchmarking: Slot F1 Score<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Arousal)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  RACE - Question Answering benchmarking: RACE-h<BR>  RACE - Question Answering benchmarking: RACE-m<BR>  RACE - Question Answering benchmarking: RACE<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  SCDE - Question Answering benchmarking: BA<BR>  SCDE - Question Answering benchmarking: DE<BR>  SCDE - Question Answering benchmarking: PA<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  NaturalQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  HotpotQA - Question Answering benchmarking: JOINT-F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  CODAH - Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  JD Product Question Answer - Question Answering benchmarking: BLEU<BR>  Natural Questions - Question Answering benchmarking: F1 (Long)<BR>  Natural Questions - Question Answering benchmarking: F1 (Short)<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  QuAC - Question Answering benchmarking: F1<BR>  QuAC - Question Answering benchmarking: HEQD<BR>  QuAC - Question Answering benchmarking: HEQQ<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  SQuAD2.0 dev - Question Answering benchmarking: EM<BR>  SQuAD2.0 dev - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Quora Question Pairs - Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  NewsQA - Question Answering benchmarking: EM<BR>  NewsQA - Question Answering benchmarking: F1<BR>  Quasart-T - Question Answering benchmarking: EM<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  AI2 Kaggle Dataset - Question Answering benchmarking: P-at-1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: number<BR>  VizWiz 2018 - Visual Question Answering benchmarking: other<BR>  VizWiz 2018 - Visual Question Answering benchmarking: unanswerable<BR>  VizWiz 2018 - Visual Question Answering benchmarking: yes/no<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  GQA test-dev - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  TDIUC - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  100 sleep nights of 8 caregivers - Visual Question Answering benchmarking: 14 gestures accuracy<BR>  HowmanyQA - Visual Question Answering benchmarking: Accuracy<BR>  TallyQA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CLEVR - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  WikiHop - Question Answering benchmarking: Test<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Reverb - Question Answering benchmarking: Accuracy<BR>  WebQuestions - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  QASent - Question Answering benchmarking: MAP<BR>  QASent - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  SemEvalCQA - Question Answering benchmarking: MAP<BR>  SemEvalCQA - Question Answering benchmarking: P-at-1<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 10k)<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 1k)<BR>  bAbi - Question Answering benchmarking: Mean Error Rate<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  SimpleQuestions - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  YahooCQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: P-at-1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Story Cloze Test - Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: BLEU-1<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  COMPLEXQUESTIONS - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-NE<BR>  MCTest-160 - Question Answering benchmarking: Accuracy<BR>  MCTest-500 - Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  STS Benchmark - Semantic Textual Similarity benchmarking: Pearson Correlation<BR>\",\"<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  WikiSQL - Semantic Parsing benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Task 1 - Word Sense Disambiguation benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Knowledge-based: - Word Sense Disambiguation benchmarking: All<BR>  Knowledge-based: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Knowledge-based: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-E<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-R<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>  MRPC - Semantic Textual Similarity benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2005 - Semantic Role Labeling benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  AQUAINT - Entity Disambiguation benchmarking: Micro-F1<BR>\",\"<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  ATIS - Semantic Parsing benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Geo - Semantic Parsing benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  spider - Semantic Parsing benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2007<BR>\",\"<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  AIDA-CoNLL - Entity Disambiguation benchmarking: In-KB Accuracy<BR>  TAC2010 - Entity Disambiguation benchmarking: Micro Precision<BR>\",\"<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  ACE2004 - Entity Disambiguation benchmarking: Micro-F1<BR>  MSNBC - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-CWEB - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-WIKI - Entity Disambiguation benchmarking: Micro-F1<BR>\",\"<BR>task: Sentence embedding: Sentence Compression<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Google Dataset - Sentence Compression benchmarking: CR<BR>  Google Dataset - Sentence Compression benchmarking: F1<BR>\",\"<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ10)<BR>\",\"<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>\",\"<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: LAS<BR>  Penn Treebank - Dependency Parsing benchmarking: POS<BR>  Penn Treebank - Dependency Parsing benchmarking: UAS<BR>\",\"<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  CoNLL-2009 - Dependency Parsing benchmarking: LAS<BR>  CoNLL-2009 - Dependency Parsing benchmarking: UAS<BR>\",\"<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  GENIA - LAS - Dependency Parsing benchmarking: F1<BR>  GENIA - UAS - Dependency Parsing benchmarking: F1<BR>\",\"<BR>task: Syntactic analysis: Chunking<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  CoNLL 2000 - Chunking benchmarking: Exact Span F1<BR>\",\"<BR>task: Syntactic analysis: Chunking<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Penn Treebank - Chunking benchmarking: F1 score<BR>\",\"<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>\",\"<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  CoNLL-2014 A1 - Grammatical Error Detection benchmarking: F0.5<BR>  CoNLL-2014 A2 - Grammatical Error Detection benchmarking: F0.5<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>\",\"<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  JFLEG - Grammatical Error Detection benchmarking: F0.5<BR>\",\"<BR>task: Syntactic analysis: Linguistic Acceptability Assessment<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  CoLA - Linguistic Acceptability Assessment benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ10)<BR>\",\"<BR>task: Text classification: Sentence Classification<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  PubMed 20k RCT - Sentence Classification benchmarking: F1<BR>\",\"<BR>task: Text classification: Sentence Classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Paper Field - Sentence Classification benchmarking: F1<BR>  ScienceCite - Sentence Classification benchmarking: F1<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Reuters De-En - Document Classification benchmarking: Accuracy<BR>  Reuters En-De - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Citation Intent Classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>\",\"<BR>task: Text classification: Sentence Classification<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ACL-ARC - Sentence Classification benchmarking: F1<BR>  SciCite - Sentence Classification benchmarking: F1<BR>\",\"<BR>task: Text classification: Citation Intent Classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  SciCite - Citation Intent Classification benchmarking: F1<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  IMDb-M - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  WOS-11967 - Document Classification benchmarking: Accuracy<BR>  WOS-46985 - Document Classification benchmarking: Accuracy<BR>  WOS-5736 - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  AG News - Text Classification benchmarking: Error<BR>  DBpedia - Text Classification benchmarking: Error<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  RCV1 - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  TREC-50 - Text Classification benchmarking: Error<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Ohsumed - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: Accuracy<BR>  LOCAL DATASET - Text Classification benchmarking: Accuracy (%)<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: F-measure<BR>  R8 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: F-measure<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  MPQA - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  R52 - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Yelp-5 - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (10 classes)<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Yelp-2 - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  AAPD - Document Classification benchmarking: F1<BR>  Amazon - Document Classification benchmarking: Accuracy<BR>  BBCSport - Document Classification benchmarking: Accuracy<BR>  Classic - Document Classification benchmarking: Accuracy<BR>  Recipe - Document Classification benchmarking: Accuracy<BR>  Reuters-21578 - Document Classification benchmarking: Accuracy<BR>  Twitter - Document Classification benchmarking: Accuracy<BR>  Yelp-14 - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  RCV1 - Text Classification benchmarking: Macro F1<BR>  RCV1 - Text Classification benchmarking: Micro F1<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: Precision<BR>  20NEWS - Text Classification benchmarking: Recall<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Reuters-21578 - Document Classification benchmarking: F1<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Sogou News - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Amazon-2 - Text Classification benchmarking: Error<BR>  Amazon-5 - Text Classification benchmarking: Error<BR>  RCV1 - Text Classification benchmarking: P-at-1<BR>  RCV1 - Text Classification benchmarking: P-at-3<BR>  RCV1 - Text Classification benchmarking: P-at-5<BR>  RCV1 - Text Classification benchmarking: nDCG-at-1<BR>  RCV1 - Text Classification benchmarking: nDCG-at-3<BR>  RCV1 - Text Classification benchmarking: nDCG-at-5<BR>\"],\"marker\":{\"line\":{\"width\":1,\"color\":\"black\"},\"size\":20,\"symbol\":42},\"mode\":\"markers\",\"x\":[\"2017-08\",\"2018-10\",\"2018-05\",\"2017-05\",\"2016-03\",\"2016-03\",\"2016-06\",\"2016-05\",\"2017-04\",\"2018-06\",\"2018-09\",\"2019-05\",\"2019-04\",\"2018-03\",\"2019-06\",\"2018-12\",\"2017-12\",\"2015-08\",\"2018-09\",\"2014-04\",\"2019-09\",\"2018-11\",\"2018-10\",\"2018-05\",\"2018-08\",\"2018-06\",\"2018-05\",\"2018-08\",\"2014-06\",\"2014-10\",\"2016-08\",\"2017-09\",\"2014-09\",\"2019-01\",\"2018-10\",\"2017-06\",\"2019-04\",\"2019-08\",\"2019-09\",\"2020-03\",\"2018-09\",\"2017-09\",\"2018-06\",\"2019-01\",\"2018-04\",\"2019-11\",\"2019-06\",\"2019-05\",\"2019-03\",\"2018-10\",\"2018-05\",\"2019-04\",\"2019-04\",\"2018-12\",\"2018-02\",\"2019-01\",\"2019-04\",\"2019-01\",\"2018-04\",\"2017-11\",\"2016-07\",\"2018-03\",\"2016-06\",\"2015-12\",\"2017-09\",\"2018-05\",\"2019-06\",\"2018-09\",\"2018-10\",\"2019-01\",\"2015-08\",\"2019-07\",\"2019-10\",\"2019-11\",\"2018-08\",\"2014-09\",\"2018-07\",\"2019-08\",\"2017-09\",\"2017-05\",\"2017-04\",\"2016-09\",\"2017-02\",\"2018-05\",\"2014-06\",\"2015-09\",\"2018-08\",\"2017-04\",\"2013-12\",\"2016-02\",\"2016-07\",\"2016-11\",\"2016-12\",\"2018-03\",\"2018-09\",\"2019-02\",\"2019-11\",\"2016-02\",\"2018-08\",\"2016-10\",\"2016-12\",\"2017-10\",\"2017-11\",\"2017-02\",\"2018-06\",\"2017-07\",\"2013-10\",\"2016-07\",\"2016-04\",\"2019-08\",\"2017-07\",\"2017-02\",\"2015-09\",\"2019-08\",\"2019-01\",\"2019-02\",\"2017-04\",\"2019-06\",\"2014-12\",\"2015-05\",\"2017-12\",\"2013-10\",\"2019-06\",\"2019-09\",\"2019-12\",\"2019-03\",\"2018-03\",\"2018-12\",\"2017-07\",\"2018-03\",\"2020-04\",\"2019-07\",\"2019-05\",\"2019-04\",\"2019-01\",\"2018-10\",\"2018-08\",\"2018-05\",\"2017-11\",\"2017-03\",\"2017-08\",\"2019-08\",\"2019-07\",\"2019-04\",\"2019-02\",\"2018-10\",\"2018-08\",\"2017-07\",\"2017-04\",\"2016-12\",\"2016-06\",\"2017-10\",\"2014-04\",\"2014-05\",\"2014-12\",\"2015-03\",\"2015-06\",\"2016-02\",\"2016-06\",\"2016-08\",\"2016-11\",\"2017-05\",\"2017-07\",\"2015-11\",\"2016-03\",\"2018-03\",\"2019-10\",\"2015-05\",\"2016-03\",\"2017-09\",\"2018-01\",\"2017-05\",\"2013-10\",\"2018-05\",\"2018-04\",\"2019-09\",\"2014-11\",\"2018-05\",\"2018-09\",\"2018-02\",\"2017-07\",\"2016-01\",\"2017-04\",\"2015-09\",\"2018-08\",\"2017-11\",\"2015-06\",\"2016-11\",\"2018-08\",\"2017-11\",\"2016-08\",\"2014-12\",\"2016-07\",\"2018-11\",\"2019-01\",\"2018-10\",\"2018-08\",\"2019-03\",\"2014-10\",\"2013-06\",\"2018-01\",\"2019-03\",\"2019-06\",\"2017-09\",\"2014-05\",\"2015-04\",\"2015-09\",\"2016-02\",\"2016-07\",\"2016-12\",\"2017-07\",\"2018-05\",\"2018-06\",\"2019-08\",\"2018-09\",\"2019-01\",\"2019-04\",\"2019-05\",\"2019-04\",\"2019-08\",\"2019-11\",\"2014-03\",\"2019-02\",\"2018-10\",\"2019-06\"],\"y\":[\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\",\"Computer code processing: Code Generation\",\"Dialog process: Dialog Act Classification\",\"Dialog process: Dialog State Tracking\",\"Dialog process: Visual Dialog\",\"Dialog process: Visual Dialog\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Inference and reasoning: Common Sense Reasoning\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Joint Entity and Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Relation Extraction\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Relation Extraction\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Chinese Named Entity Recognition\",\"Information extraction: Chinese Named Entity Recognition\",\"Information extraction: Relation Extraction\",\"Information extraction: Chinese Named Entity Recognition\",\"Information retrieval: Conversational Response Selection\",\"Information retrieval: Conversational Response Selection\",\"Information retrieval: Conversational Response Selection\",\"Information retrieval: Conversational Response Selection\",\"Machine translation: Unsupervised Machine Translation\",\"Machine translation: Unsupervised Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Language Modelling\",\"Natural language generation: Text Summarization\",\"Natural language generation: Document Summarization\",\"Natural language generation: Document Summarization\",\"Natural language generation: Question Generation\",\"Natural language generation: Text Generation\",\"Natural language generation: Text Generation\",\"Natural language generation: Text Generation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Text Summarization\",\"Natural language generation: Text Generation\",\"Natural language generation: Text Summarization\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Language Modelling\",\"Natural language generation: Text Summarization\",\"Natural language generation: Question Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Other NLP task: Text-to-Image Generation\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Fake News Detection\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Intent Detection\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Intent Detection\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Intent Detection\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Question Answering\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Semantic Parsing\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Semantic Role Labeling\",\"Semantic analysis: Entity Disambiguation\",\"Semantic analysis: Semantic Parsing\",\"Semantic analysis: Semantic Parsing\",\"Semantic analysis: Semantic Parsing\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Semantic Role Labeling\",\"Semantic analysis: Entity Disambiguation\",\"Semantic analysis: Entity Disambiguation\",\"Sentence embedding: Sentence Compression\",\"Syntactic analysis: Constituency Grammar Induction\",\"Syntactic analysis: Constituency Grammar Induction\",\"Syntactic analysis: Dependency Parsing\",\"Syntactic analysis: Dependency Parsing\",\"Syntactic analysis: Dependency Parsing\",\"Syntactic analysis: Chunking\",\"Syntactic analysis: Chunking\",\"Syntactic analysis: Constituency Parsing\",\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Linguistic Acceptability Assessment\",\"Syntactic analysis: Constituency Grammar Induction\",\"Text classification: Sentence Classification\",\"Text classification: Sentence Classification\",\"Text classification: Document Classification\",\"Text classification: Citation Intent Classification\",\"Text classification: Sentence Classification\",\"Text classification: Citation Intent Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Document Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Document Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\",\"Text classification: Document Classification\",\"Text classification: Document Classification\",\"Text classification: Text Classification\",\"Text classification: Text Classification\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}},{\"hovertemplate\":[\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2013-10<BR>ratio: 0.1102<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2013-10<BR>ratio: 1.0<BR>benchmarks:<BR>  MSRP - Paraphrase Identification benchmarking: Accuracy<BR>  MSRP - Paraphrase Identification benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2014-06<BR>ratio: 0.3305<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2014-06<BR>ratio: 0.76<BR>benchmarks:<BR>  WebQuestions - Question Answering benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2014-08<BR>ratio: 0.2308<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2014-08<BR>ratio: 0.2549<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2014-09<BR>ratio: 0.0886<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2014-10<BR>ratio: 0.0904<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2014-12<BR>ratio: 0.3032<BR>benchmarks:<BR>  QASent - Question Answering benchmarking: MAP<BR>  QASent - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: PPL<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-02<BR>ratio: 0.1186<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-06<BR>ratio: 0.0427<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2015-06<BR>ratio: 0.232<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  WebQuestions - Question Answering benchmarking: F1<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2015-08<BR>ratio: 0.5<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Train Accuracy<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2015-09<BR>ratio: 0.1435<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2015-11<BR>ratio: 0.2159<BR>benchmarks:<BR>  QASent - Question Answering benchmarking: MAP<BR>  QASent - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2015-11<BR>ratio: 0.4025<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Electronics<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2015-11<BR>ratio: 0.25<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2016-01<BR>ratio: 0.483<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2016-02<BR>ratio: 0.3491<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2016-02<BR>ratio: 0.3173<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-02<BR>ratio: 0.0718<BR>benchmarks:<BR>  SemEvalCQA - Question Answering benchmarking: P-at-1<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: P-at-1<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2016-03<BR>ratio: 0.1323<BR>benchmarks:<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Task 1 - Word Sense Disambiguation benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-03<BR>ratio: 0.5335<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  MCTest-500 - Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-03<BR>ratio: 0.2<BR>benchmarks:<BR>  WMT2015 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2016-03<BR>ratio: 0.5215<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2016-03<BR>ratio: 0.1966<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: LAS<BR>  Penn Treebank - Dependency Parsing benchmarking: POS<BR>  Penn Treebank - Dependency Parsing benchmarking: UAS<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-06<BR>ratio: 0.542<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2016 German-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Text classification: Citation Intent Classification<BR>date: 2016-06<BR>ratio: 0.7941<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2016-06<BR>ratio: 0.0702<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-06<BR>ratio: 0.3195<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  CNN / Daily Mail - Question Answering benchmarking: Daily Mail<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-NE<BR>  SemEvalCQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 10k)<BR>  bAbi - Question Answering benchmarking: Accuracy (trained on 1k)<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2016-06<BR>ratio: 0.0771<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2016-06<BR>ratio: 0.0678<BR>benchmarks:<BR>  SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-07<BR>ratio: 0.218<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2016-07<BR>ratio: 0.094<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2016-07<BR>ratio: 0.9481<BR>benchmarks:<BR>  CoNLL-2014 A2 - Grammatical Error Detection benchmarking: F0.5<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-08<BR>ratio: 0.1882<BR>benchmarks:<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-08<BR>ratio: 0.3589<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2016-09<BR>ratio: 0.0286<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-09<BR>ratio: 0.101<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2016-09<BR>ratio: 0.098<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-09<BR>ratio: 0.2232<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2016-09<BR>ratio: 0.5<BR>benchmarks:<BR>  enwik8 - Language Modelling benchmarking: Bit per Character (BPC)<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2016-09<BR>ratio: 0.3558<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-10<BR>ratio: 0.6069<BR>benchmarks:<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>  bAbi - Question Answering benchmarking: Mean Error Rate<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-10<BR>ratio: 0.8<BR>benchmarks:<BR>  WMT2015 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2016-11<BR>ratio: 0.5084<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2016-11<BR>ratio: 0.0123<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2016-11<BR>ratio: 0.0632<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2016-11<BR>ratio: 0.0853<BR>benchmarks:<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>\",\"<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2016-11<BR>ratio: 0.8096<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: LAS<BR>  Penn Treebank - Dependency Parsing benchmarking: UAS<BR>\",\"<BR>task: Syntactic analysis: Chunking<BR>date: 2016-11<BR>ratio: 0.1739<BR>benchmarks:<BR>  Penn Treebank - Chunking benchmarking: F1 score<BR>\",\"<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2016-11<BR>ratio: 0.0711<BR>benchmarks:<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>\",\"<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2016-11<BR>ratio: 0.4857<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2016-12<BR>ratio: 0.7801<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2016-12<BR>ratio: 0.6667<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Test perplexity<BR>  WikiText-2 - Language Modelling benchmarking: Test perplexity<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2016-12<BR>ratio: 0.0708<BR>benchmarks:<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2017-01<BR>ratio: 0.0597<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-02<BR>ratio: 0.0392<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2017-02<BR>ratio: 0.2857<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-02<BR>ratio: 0.1937<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-03<BR>ratio: 0.033<BR>benchmarks:<BR>  CNN / Daily Mail - Question Answering benchmarking: CNN<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2017-04<BR>ratio: 0.1026<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-04<BR>ratio: 0.0174<BR>benchmarks:<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2017-04<BR>ratio: 0.108<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-04<BR>ratio: 0.1795<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2017-04<BR>ratio: 0.55<BR>benchmarks:<BR>  ATIS - Semantic Parsing benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2017-04<BR>ratio: 0.6016<BR>benchmarks:<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2017-05<BR>ratio: 0.2253<BR>benchmarks:<BR>  IWSLT2015 English-German - Machine Translation benchmarking: BLEU score<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-05<BR>ratio: 0.2429<BR>benchmarks:<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2017-05<BR>ratio: 0.2329<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2017-05<BR>ratio: 0.3586<BR>benchmarks:<BR>  COCO Captions - Text Generation benchmarking: BLEU-2<BR>  COCO Captions - Text Generation benchmarking: BLEU-3<BR>  COCO Captions - Text Generation benchmarking: BLEU-4<BR>  COCO Captions - Text Generation benchmarking: BLEU-5<BR>  Chinese Poems - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-5<BR>\",\"<BR>task: Natural language generation: Document Summarization<BR>date: 2017-05<BR>ratio: 0.2511<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2017-05<BR>ratio: 0.921<BR>benchmarks:<BR>  AIDA-CoNLL - Entity Disambiguation benchmarking: In-KB Accuracy<BR>  TAC2010 - Entity Disambiguation benchmarking: Micro Precision<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2017-06<BR>ratio: 0.3615<BR>benchmarks:<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>  GigaWord - Text Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-06<BR>ratio: 0.1524<BR>benchmarks:<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2017-06<BR>ratio: 0.4788<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  IWSLT2015 English-German - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2017-07<BR>ratio: 0.1465<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>\",\"<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2017-07<BR>ratio: 0.3698<BR>benchmarks:<BR>  CoNLL-2014 A1 - Grammatical Error Detection benchmarking: F0.5<BR>  CoNLL-2014 A2 - Grammatical Error Detection benchmarking: F0.5<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>\",\"<BR>task: Sentence embedding: Sentence Compression<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Google Dataset - Sentence Compression benchmarking: CR<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2017-07<BR>ratio: 0.0632<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Amazon Review Full - Sentiment Analysis benchmarking: Accuracy<BR>  Amazon Review Polarity - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-07<BR>ratio: 0.3933<BR>benchmarks:<BR>  SemEvalCQA - Question Answering benchmarking: MAP<BR>  SemEvalCQA - Question Answering benchmarking: P-at-1<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: MRR<BR>  YahooCQA - Question Answering benchmarking: P-at-1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2017-07<BR>ratio: 0.212<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>\",\"<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2017-07<BR>ratio: 0.2457<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2017-08<BR>ratio: 0.3456<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-08<BR>ratio: 0.5<BR>benchmarks:<BR>  AI2 Kaggle Dataset - Question Answering benchmarking: P-at-1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-08<BR>ratio: 0.2288<BR>benchmarks:<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2017-08<BR>ratio: 0.4269<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2017-09<BR>ratio: 0.4385<BR>benchmarks:<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: NER Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Micro F1<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2017-09<BR>ratio: 0.6226<BR>benchmarks:<BR>  COCO Captions - Text Generation benchmarking: BLEU-2<BR>  COCO Captions - Text Generation benchmarking: BLEU-3<BR>  COCO Captions - Text Generation benchmarking: BLEU-4<BR>  COCO Captions - Text Generation benchmarking: BLEU-5<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-2<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-3<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-4<BR>  EMNLP2017 WMT - Text Generation benchmarking: BLEU-5<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2017-09<BR>ratio: 0.3099<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>\",\"<BR>task: Dialog process: Dialog Act Classification<BR>date: 2017-09<BR>ratio: 0.7439<BR>benchmarks:<BR>  Switchboard corpus - Dialog Act Classification benchmarking: Accuracy<BR>\",\"<BR>task: Dialog process: Visual Dialog<BR>date: 2017-09<BR>ratio: 0.3904<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-09<BR>ratio: 0.0196<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2017-09<BR>ratio: 0.0407<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-10<BR>ratio: 0.7021<BR>benchmarks:<BR>  CUB - Text-to-Image Generation benchmarking: FID<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>  Oxford 102 Flowers - Text-to-Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2017-10<BR>ratio: 0.0798<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-10<BR>ratio: 0.6272<BR>benchmarks:<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2017-11<BR>ratio: 0.6346<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Dialog process: Visual Dialog<BR>date: 2017-11<BR>ratio: 0.2578<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Dialog process: Dialog Act Classification<BR>date: 2017-11<BR>ratio: 0.2561<BR>benchmarks:<BR>  Switchboard corpus - Dialog Act Classification benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2017-11<BR>ratio: 0.0459<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-11<BR>ratio: 0.0392<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Fake News Detection<BR>date: 2017-12<BR>ratio: 0.9439<BR>benchmarks:<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Agree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Disagree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Discuss)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Unrelated)<BR>  FNC-1 - Fake News Detection benchmarking: Weighted Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2017-12<BR>ratio: 0.1476<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2017-12<BR>ratio: 0.1887<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2017-12<BR>ratio: 0.0392<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2017-12<BR>ratio: 0.0571<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>\",\"<BR>task: Text classification: Citation Intent Classification<BR>date: 2018-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-01<BR>ratio: 0.4672<BR>benchmarks:<BR>  NewsQA - Question Answering benchmarking: EM<BR>  NewsQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-01<BR>ratio: 0.0809<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-02<BR>ratio: 0.3585<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2018-02<BR>ratio: 0.8333<BR>benchmarks:<BR>  Yahoo Questions - Text Generation benchmarking: Perplexity<BR>\",\"<BR>task: Text classification: Citation Intent Classification<BR>date: 2018-02<BR>ratio: 0.1176<BR>benchmarks:<BR>  ACL-ARC - Citation Intent Classification benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-02<BR>ratio: 0.0238<BR>benchmarks:<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2018-02<BR>ratio: 0.2079<BR>benchmarks:<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-02<BR>ratio: 0.4381<BR>benchmarks:<BR>  WMT2014 German-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-02<BR>ratio: 0.3667<BR>benchmarks:<BR>  MR - Sentiment Analysis benchmarking: Accuracy<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2018-03<BR>ratio: 0.541<BR>benchmarks:<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-03<BR>ratio: 0.0357<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2018-03<BR>ratio: 0.2009<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  WikiText-103 - Language Modelling benchmarking: Validation perplexity<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-E<BR>  SentEval - Semantic Textual Similarity benchmarking: SICK-R<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-03<BR>ratio: 0.0152<BR>benchmarks:<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-04<BR>ratio: 0.5921<BR>benchmarks:<BR>  WikiHop - Question Answering benchmarking: Test<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-04<BR>ratio: 0.2234<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Electronics<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2018-04<BR>ratio: 0.245<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2018-04<BR>ratio: 0.137<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-04<BR>ratio: 0.0412<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>\",\"<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-05<BR>ratio: 0.1698<BR>benchmarks:<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>\",\"<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2018-05<BR>ratio: 0.1343<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-05<BR>ratio: 0.1569<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Dialog process: Dialog State Tracking<BR>date: 2018-05<BR>ratio: 0.8365<BR>benchmarks:<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Joint<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Request<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Joint<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Request<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-05<BR>ratio: 0.0536<BR>benchmarks:<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Event2Mind test - Common Sense Reasoning benchmarking: Average Cross-Ent<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-05<BR>ratio: 0.7182<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: BLEU-1<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NewsQA - Question Answering benchmarking: EM<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-05<BR>ratio: 0.2896<BR>benchmarks:<BR>  MPQA - Sentiment Analysis benchmarking: Accuracy<BR>  MR - Sentiment Analysis benchmarking: Accuracy<BR>  SST-5 Fine-grained classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-05<BR>ratio: 0.3299<BR>benchmarks:<BR>  TREC-6 - Text Classification benchmarking: Error<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2018-05<BR>ratio: 0.0126<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-06<BR>ratio: 0.154<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-06<BR>ratio: 0.3862<BR>benchmarks:<BR>  RACE - Question Answering benchmarking: RACE-h<BR>  RACE - Question Answering benchmarking: RACE-m<BR>  RACE - Question Answering benchmarking: RACE<BR>  Story Cloze Test - Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-06<BR>ratio: 0.6132<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>  V-SNLI - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Question Generation<BR>date: 2018-06<BR>ratio: 0.0673<BR>benchmarks:<BR>  SQuAD1.1 - Question Generation benchmarking: BLEU-4<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Quasart-T - Question Answering benchmarking: EM<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2018-07<BR>ratio: 0.0282<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2018-07<BR>ratio: 0.3122<BR>benchmarks:<BR>  NYT - Relation Extraction benchmarking: F1<BR>  WebNLG - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Sentence embedding: Sentence Compression<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Google Dataset - Sentence Compression benchmarking: F1<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2018-07<BR>ratio: 0.0553<BR>benchmarks:<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>\",\"<BR>task: Syntactic analysis: Dependency Parsing<BR>date: 2018-07<BR>ratio: 0.791<BR>benchmarks:<BR>  Penn Treebank - Dependency Parsing benchmarking: POS<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-07<BR>ratio: 0.5132<BR>benchmarks:<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Chunking<BR>date: 2018-08<BR>ratio: 0.913<BR>benchmarks:<BR>  CoNLL 2000 - Chunking benchmarking: Exact Span F1<BR>  Penn Treebank - Chunking benchmarking: F1 score<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2018-08<BR>ratio: 0.4463<BR>benchmarks:<BR>  ADE Corpus - Relation Extraction benchmarking: NER Macro F1<BR>  ADE Corpus - Relation Extraction benchmarking: RE+ Macro F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Long-tail emerging entities - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-08<BR>ratio: 0.0714<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-08<BR>ratio: 0.2718<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2018-08<BR>ratio: 0.2449<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2018-08<BR>ratio: 0.0184<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Document Summarization<BR>date: 2018-08<BR>ratio: 0.5122<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: PPL<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-08<BR>ratio: 0.226<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-French - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-08<BR>ratio: 1.0<BR>benchmarks:<BR>  AG News - Text Classification benchmarking: Error<BR>\",\"<BR>task: Dialog process: Visual Dialog<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-09<BR>ratio: 0.0699<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Language Modelling<BR>date: 2018-09<BR>ratio: 1.0<BR>benchmarks:<BR>  One Billion Word - Language Modelling benchmarking: Validation perplexity<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2018-09<BR>ratio: 0.3185<BR>benchmarks:<BR>  Re-TACRED - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2018-09<BR>ratio: 0.652<BR>benchmarks:<BR>  Ohsumed - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-09<BR>ratio: 0.2353<BR>benchmarks:<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-09<BR>ratio: 0.2419<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>  WikiHop - Question Answering benchmarking: Test<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-10<BR>ratio: 0.196<BR>benchmarks:<BR>  CoQA - Question Answering benchmarking: In-domain<BR>  CoQA - Question Answering benchmarking: Out-of-domain<BR>  CoQA - Question Answering benchmarking: Overall<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>  SQuAD2.0 dev - Question Answering benchmarking: EM<BR>  SQuAD2.0 dev - Question Answering benchmarking: F1<BR>  TriviaQA - Question Answering benchmarking: EM<BR>  TriviaQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2018-10<BR>ratio: 0.5481<BR>benchmarks:<BR>  Django - Code Generation benchmarking: Accuracy<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>\",\"<BR>task: Dialog process: Dialog State Tracking<BR>date: 2018-10<BR>ratio: 0.327<BR>benchmarks:<BR>  Second dialogue state tracking challenge - Dialog State Tracking benchmarking: Joint<BR>  Wizard-of-Oz - Dialog State Tracking benchmarking: Joint<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2018-10<BR>ratio: 0.2054<BR>benchmarks:<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>  GENIA - Named Entity Recognition benchmarking: F1<BR>  SciERC - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2018-10<BR>ratio: 0.15<BR>benchmarks:<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2018-10<BR>ratio: 0.3426<BR>benchmarks:<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2018-10<BR>ratio: 0.2123<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ10)<BR>\",\"<BR>task: Semantic analysis: Semantic Parsing<BR>date: 2018-10<BR>ratio: 0.45<BR>benchmarks:<BR>  ATIS - Semantic Parsing benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Semantic analysis: Semantic Role Labeling<BR>date: 2018-10<BR>ratio: 0.4277<BR>benchmarks:<BR>  CoNLL 2005 - Semantic Role Labeling benchmarking: F1<BR>  OntoNotes - Semantic Role Labeling benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2018-10<BR>ratio: 0.4178<BR>benchmarks:<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Books<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: DVD<BR>  Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking: Kitchen<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2018-10<BR>ratio: 0.6276<BR>benchmarks:<BR>  SWAG - Common Sense Reasoning benchmarking: Dev<BR>  SWAG - Common Sense Reasoning benchmarking: Test<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  WMT 2017 Latvian-English - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2018-10<BR>ratio: 0.1963<BR>benchmarks:<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>\",\"<BR>task: Text classification: Sentence Classification<BR>date: 2018-10<BR>ratio: 0.9057<BR>benchmarks:<BR>  SciCite - Sentence Classification benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Fake News Detection<BR>date: 2018-11<BR>ratio: 0.1404<BR>benchmarks:<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Agree)<BR>  FNC-1 - Fake News Detection benchmarking: Per-class Accuracy (Disagree)<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2018-11<BR>ratio: 0.2719<BR>benchmarks:<BR>  SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2007<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>\",\"<BR>task: Syntactic analysis: Grammatical Error Detection<BR>date: 2018-11<BR>ratio: 0.2698<BR>benchmarks:<BR>  FCE - Grammatical Error Detection benchmarking: F0.5<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2018-11<BR>ratio: 0.1449<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: BLEU-1<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NewsQA - Question Answering benchmarking: EM<BR>  NewsQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  CoNLL04 - Relation Extraction benchmarking: NER Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Macro F1<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-10%<BR>  NYT Corpus - Relation Extraction benchmarking: P-at-30%<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-01<BR>ratio: 0.2041<BR>benchmarks:<BR>  MS MARCO - Question Answering benchmarking: Rouge-L<BR>  NarrativeQA - Question Answering benchmarking: BLEU-1<BR>  NarrativeQA - Question Answering benchmarking: BLEU-4<BR>  NarrativeQA - Question Answering benchmarking: Rouge-L<BR>  WikiHop - Question Answering benchmarking: Test<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  NCBI-disease - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-01<BR>ratio: 0.6137<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-01<BR>ratio: 0.3456<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>  SciTail - Natural Language Inference benchmarking: Accuracy<BR>  XNLI French - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: FID<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-01<BR>ratio: 0.7765<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT 2017 English-Chinese - Machine Translation benchmarking: BLEU score<BR>  WMT2014 English-Czech - Machine Translation benchmarking: BLEU score<BR>  WMT2016 Romanian-English - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-01<BR>ratio: 0.3558<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2019-01<BR>ratio: 0.2254<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-01<BR>ratio: 0.5299<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>  Yelp Fine-grained classification - Sentiment Analysis benchmarking: Error<BR>\",\"<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-01<BR>ratio: 0.4127<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Text Generation<BR>date: 2019-01<BR>ratio: 0.1667<BR>benchmarks:<BR>  Yahoo Questions - Text Generation benchmarking: Perplexity<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-02<BR>ratio: 0.4587<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Dialog process: Visual Dialog<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-02<BR>ratio: 0.3546<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: Accuracy<BR>  Ohsumed - Text Classification benchmarking: Accuracy<BR>  R52 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-02<BR>ratio: 0.8067<BR>benchmarks:<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-CN<BR>  Children's Book Test - Question Answering benchmarking: Accuracy-NE<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Winograd Schema Challenge - Common Sense Reasoning benchmarking: Score<BR>\",\"<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-02<BR>ratio: 0.397<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-02<BR>ratio: 0.3333<BR>benchmarks:<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-03<BR>ratio: 0.666<BR>benchmarks:<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>  SciERC - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Syntactic analysis: Constituency Parsing<BR>date: 2019-03<BR>ratio: 0.1343<BR>benchmarks:<BR>  Penn Treebank - Constituency Parsing benchmarking: F1 score<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-03<BR>ratio: 0.1016<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>\",\"<BR>task: Text classification: Sentence Classification<BR>date: 2019-03<BR>ratio: 0.6981<BR>benchmarks:<BR>  ACL-ARC - Sentence Classification benchmarking: F1<BR>  SciCite - Sentence Classification benchmarking: F1<BR>  ScienceCite - Sentence Classification benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ChemProt - Relation Extraction benchmarking: F1<BR>  SciERC - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Natural language generation: Document Summarization<BR>date: 2019-03<BR>ratio: 0.3031<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2019-04<BR>ratio: 0.1967<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>\",\"<BR>task: Information extraction: Joint Entity and Relation Extraction<BR>date: 2019-04<BR>ratio: 0.2778<BR>benchmarks:<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Entity F1<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Relation F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-04<BR>ratio: 0.7252<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2004 - Relation Extraction benchmarking: RE Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>\",\"<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-04<BR>ratio: 0.2995<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-04<BR>ratio: 0.2915<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-04<BR>ratio: 0.5876<BR>benchmarks:<BR>  COCO - Text-to-Image Generation benchmarking: Inception score<BR>  COCO - Text-to-Image Generation benchmarking: SOA-C<BR>  CUB - Text-to-Image Generation benchmarking: Inception score<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: Acc<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2019-04<BR>ratio: 0.5763<BR>benchmarks:<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-1<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-2<BR>  DUC 2004 Task 1 - Text Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2019-04<BR>ratio: 0.0123<BR>benchmarks:<BR>  Cora - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Dialog process: Visual Dialog<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>\",\"<BR>task: Natural language generation: Document Summarization<BR>date: 2019-05<BR>ratio: 0.0901<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-05<BR>ratio: 0.2987<BR>benchmarks:<BR>  IWSLT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-German - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Machine translation: Unsupervised Machine Translation<BR>date: 2019-05<BR>ratio: 0.3157<BR>benchmarks:<BR>  WMT2014 English-French - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2014 French-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 English-German - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 German-English - Unsupervised Machine Translation benchmarking: BLEU<BR>  WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-05<BR>ratio: 0.3432<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>  MPQA - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-05<BR>ratio: 0.4748<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>  STS Benchmark - Semantic Textual Similarity benchmarking: Pearson Correlation<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2019-05<BR>ratio: 0.6752<BR>benchmarks:<BR>  SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking: F1<BR>  SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 2 - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Task 1 - Word Sense Disambiguation benchmarking: F1<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2007<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2013<BR>  Supervised: - Word Sense Disambiguation benchmarking: SemEval 2015<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 2<BR>  Supervised: - Word Sense Disambiguation benchmarking: Senseval 3<BR>\",\"<BR>task: Natural language generation: Question Generation<BR>date: 2019-05<BR>ratio: 0.9327<BR>benchmarks:<BR>  SQuAD1.1 - Question Generation benchmarking: BLEU-4<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-05<BR>ratio: 0.4235<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-05<BR>ratio: 0.4549<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>  Sogou News - Text Classification benchmarking: Accuracy<BR>  Yahoo! Answers - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2019-05<BR>ratio: 0.2911<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  GigaWord - Text Summarization benchmarking: ROUGE-1<BR>  GigaWord - Text Summarization benchmarking: ROUGE-2<BR>  GigaWord - Text Summarization benchmarking: ROUGE-L<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-05<BR>ratio: 0.4942<BR>benchmarks:<BR>  TrecQA - Question Answering benchmarking: MAP<BR>  TrecQA - Question Answering benchmarking: MRR<BR>  WikiQA - Question Answering benchmarking: MAP<BR>  WikiQA - Question Answering benchmarking: MRR<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-05<BR>ratio: 0.2659<BR>benchmarks:<BR>  ACE 2004 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE+ Micro F1<BR>  ADE Corpus - Relation Extraction benchmarking: NER Macro F1<BR>  ADE Corpus - Relation Extraction benchmarking: RE+ Macro F1<BR>  CoNLL04 - Relation Extraction benchmarking: NER Micro F1<BR>  CoNLL04 - Relation Extraction benchmarking: RE+ Micro F1<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  ATIS - Intent Detection benchmarking: Accuracy<BR>  SNIPS - Intent Detection benchmarking: Slot F1 Score<BR>\",\"<BR>task: Syntactic analysis: Linguistic Acceptability Assessment<BR>date: 2019-06<BR>ratio: 0.8571<BR>benchmarks:<BR>  CoLA - Linguistic Acceptability Assessment benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-06<BR>ratio: 0.5<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-06<BR>ratio: 0.0675<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-06<BR>ratio: 0.5346<BR>benchmarks:<BR>  IMDb - Text Classification benchmarking: Accuracy (2 classes)<BR>  R52 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: Accuracy<BR>  Yelp-2 - Text Classification benchmarking: Accuracy<BR>\",\"<BR>task: Syntactic analysis: Constituency Grammar Induction<BR>date: 2019-06<BR>ratio: 0.5765<BR>benchmarks:<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Max F1 (WSJ10)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ)<BR>  PTB - Constituency Grammar Induction benchmarking: Mean F1 (WSJ10)<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-06<BR>ratio: 0.5752<BR>benchmarks:<BR>  IWSLT2015 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT 2018 Finnish-English - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Pragmatics analysis: Paraphrase Identification<BR>date: 2019-06<BR>ratio: 0.6643<BR>benchmarks:<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: Accuracy<BR>  Quora Question Pairs - Paraphrase Identification benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-06<BR>ratio: 0.1145<BR>benchmarks:<BR>  DocRED - Relation Extraction benchmarking: F1<BR>  DocRED - Relation Extraction benchmarking: Ign F1<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>  TACRED - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-06<BR>ratio: 0.0469<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-06<BR>ratio: 0.4897<BR>benchmarks:<BR>  Quora Question Pairs - Question Answering benchmarking: Accuracy<BR>  RACE - Question Answering benchmarking: RACE-h<BR>  RACE - Question Answering benchmarking: RACE-m<BR>  RACE - Question Answering benchmarking: RACE<BR>  SQuAD1.1 - Question Answering benchmarking: EM<BR>  SQuAD1.1 - Question Answering benchmarking: F1<BR>  SQuAD1.1 dev - Question Answering benchmarking: EM<BR>  SQuAD1.1 dev - Question Answering benchmarking: F1<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>  SQuAD2.0 dev - Question Answering benchmarking: EM<BR>  SQuAD2.0 dev - Question Answering benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-06<BR>ratio: 0.4252<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>  STS Benchmark - Semantic Textual Similarity benchmarking: Pearson Correlation<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-06<BR>ratio: 0.4272<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-06<BR>ratio: 0.3025<BR>benchmarks:<BR>  ACE 2004 - Named Entity Recognition benchmarking: F1<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-07<BR>ratio: 0.5266<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>  MSRA Dev - Chinese Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-07<BR>ratio: 0.6501<BR>benchmarks:<BR>  NYT-single - Relation Extraction benchmarking: F1<BR>  Re-TACRED - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2019-07<BR>ratio: 0.2915<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>  OntoNotes - Coreference Resolution benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-07<BR>ratio: 0.3767<BR>benchmarks:<BR>  NewsQA - Question Answering benchmarking: F1<BR>  TriviaQA - Question Answering benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-07<BR>ratio: 0.5481<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-07<BR>ratio: 0.4801<BR>benchmarks:<BR>  ANLI test - Natural Language Inference benchmarking: A1<BR>  MultiNLI - Natural Language Inference benchmarking: Mismatched<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>  XNLI Chinese - Natural Language Inference benchmarking: Accuracy<BR>  XNLI Chinese Dev - Natural Language Inference benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-07<BR>ratio: 0.0577<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-07<BR>ratio: 0.2347<BR>benchmarks:<BR>  IMDb - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-07<BR>ratio: 0.2383<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>  SWAG - Common Sense Reasoning benchmarking: Test<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  BBCSport - Document Classification benchmarking: Accuracy<BR>  Reuters-21578 - Document Classification benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Document Summarization<BR>date: 2019-08<BR>ratio: 0.1081<BR>benchmarks:<BR>  CNN / Daily Mail - Document Summarization benchmarking: ROUGE-1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-08<BR>ratio: 0.4554<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>\",\"<BR>task: Pragmatics analysis: Coreference Resolution<BR>date: 2019-08<BR>ratio: 0.0358<BR>benchmarks:<BR>  CoNLL 2012 - Coreference Resolution benchmarking: Avg F1<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-08<BR>ratio: 0.4151<BR>benchmarks:<BR>  ACE 2004 - Named Entity Recognition benchmarking: F1<BR>  ACE 2005 - Named Entity Recognition benchmarking: F1<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>  GENIA - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-08<BR>ratio: 0.0067<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>\",\"<BR>task: Other NLP task: Text-to-Image Generation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking: FID<BR>\",\"<BR>task: Question answering: Question Answering<BR>date: 2019-09<BR>ratio: 0.0784<BR>benchmarks:<BR>  SQuAD2.0 - Question Answering benchmarking: EM<BR>  SQuAD2.0 - Question Answering benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Sentiment Analysis<BR>date: 2019-09<BR>ratio: 0.0085<BR>benchmarks:<BR>  SST-2 Binary classification - Sentiment Analysis benchmarking: Accuracy<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2019-09<BR>ratio: 0.1111<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2019-09<BR>ratio: 0.8833<BR>benchmarks:<BR>  20NEWS - Text Classification benchmarking: F-measure<BR>  Amazon-2 - Text Classification benchmarking: Error<BR>  Amazon-5 - Text Classification benchmarking: Error<BR>  R8 - Text Classification benchmarking: Accuracy<BR>  R8 - Text Classification benchmarking: F-measure<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-09<BR>ratio: 0.4834<BR>benchmarks:<BR>  ACE 2005 - Relation Extraction benchmarking: NER Micro F1<BR>  ACE 2005 - Relation Extraction benchmarking: RE Micro F1<BR>  DocRED - Relation Extraction benchmarking: F1<BR>  DocRED - Relation Extraction benchmarking: Ign F1<BR>  NYT - Relation Extraction benchmarking: F1<BR>  NYT-single - Relation Extraction benchmarking: F1<BR>  WebNLG - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Inference and reasoning: Common Sense Reasoning<BR>date: 2019-09<BR>ratio: 0.2136<BR>benchmarks:<BR>  CommonsenseQA - Common Sense Reasoning benchmarking: Accuracy<BR>\",\"<BR>task: Semantic analysis: Word Sense Disambiguation<BR>date: 2019-09<BR>ratio: 0.9661<BR>benchmarks:<BR>  SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>  SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking: F1<BR>\",\"<BR>task: Semantic analysis: Semantic Textual Similarity<BR>date: 2019-09<BR>ratio: 0.0846<BR>benchmarks:<BR>  MRPC - Semantic Textual Similarity benchmarking: Accuracy<BR>\",\"<BR>task: Inference and reasoning: Natural Language Inference<BR>date: 2019-09<BR>ratio: 0.0427<BR>benchmarks:<BR>  MultiNLI - Natural Language Inference benchmarking: Matched<BR>  QNLI - Natural Language Inference benchmarking: Accuracy<BR>  RTE - Natural Language Inference benchmarking: Accuracy<BR>  SNLI - Natural Language Inference benchmarking: % Test Accuracy<BR>\",\"<BR>task: Semantic analysis: Entity Disambiguation<BR>date: 2019-09<BR>ratio: 0.6447<BR>benchmarks:<BR>  ACE2004 - Entity Disambiguation benchmarking: Micro-F1<BR>  AIDA-CoNLL - Entity Disambiguation benchmarking: In-KB Accuracy<BR>  AQUAINT - Entity Disambiguation benchmarking: Micro-F1<BR>  MSNBC - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-CWEB - Entity Disambiguation benchmarking: Micro-F1<BR>  WNED-WIKI - Entity Disambiguation benchmarking: Micro-F1<BR>\",\"<BR>task: Pragmatics analysis: Emotion Recognition in Conversation<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>\",\"<BR>task: Syntactic analysis: Linguistic Acceptability Assessment<BR>date: 2019-09<BR>ratio: 0.1429<BR>benchmarks:<BR>  CoLA - Linguistic Acceptability Assessment benchmarking: Accuracy<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-09<BR>ratio: 0.2846<BR>benchmarks:<BR>  WMT2014 German-English - Machine Translation benchmarking: BLEU score<BR>  WMT2016 English-Romanian - Machine Translation benchmarking: BLEU score<BR>\",\"<BR>task: Information extraction: Joint Entity and Relation Extraction<BR>date: 2019-09<BR>ratio: 0.7221<BR>benchmarks:<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Entity F1<BR>  SciERC - Joint Entity and Relation Extraction benchmarking: Relation F1<BR>\",\"<BR>task: Natural language generation: Machine Translation<BR>date: 2019-10<BR>ratio: 0.2671<BR>benchmarks:<BR>  IWSLT2015 English-Vietnamese - Machine Translation benchmarking: BLEU<BR>\",\"<BR>task: Natural language generation: Text Summarization<BR>date: 2019-10<BR>ratio: 0.6089<BR>benchmarks:<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-1<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-2<BR>  CNN / Daily Mail - Abstractive Text Summarization benchmarking: ROUGE-L<BR>  X-Sum - Text Summarization benchmarking: ROUGE-1<BR>  X-Sum - Text Summarization benchmarking: ROUGE-2<BR>  X-Sum - Text Summarization benchmarking: ROUGE-3<BR>\",\"<BR>task: Computer code processing: Code Generation<BR>date: 2019-10<BR>ratio: 0.1871<BR>benchmarks:<BR>  WikiSQL - Code Generation benchmarking: Exact Match Accuracy<BR>  WikiSQL - Code Generation benchmarking: Execution Accuracy<BR>\",\"<BR>task: Information retrieval: Conversational Response Selection<BR>date: 2019-11<BR>ratio: 0.4096<BR>benchmarks:<BR>  DSTC7 Ubuntu - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI AmazonQA - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>  PolyAI Reddit - Conversational Response Selection benchmarking: 1-of-100 Accuracy<BR>\",\"<BR>task: Information extraction: Named Entity Recognition<BR>date: 2019-11<BR>ratio: 0.1815<BR>benchmarks:<BR>  BC5CDR - Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2019-11<BR>ratio: 0.2333<BR>benchmarks:<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Chinese Named Entity Recognition<BR>date: 2019-11<BR>ratio: 0.5604<BR>benchmarks:<BR>  MSRA - Chinese Named Entity Recognition benchmarking: F1<BR>  Resume NER - Chinese Named Entity Recognition benchmarking: F1<BR>\",\"<BR>task: Pragmatics analysis: Intent Detection<BR>date: 2019-12<BR>ratio: 1.0<BR>benchmarks:<BR>  ASOS.com user intent - Intent Detection benchmarking: F1<BR>\",\"<BR>task: Question answering: Visual Question Answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>\",\"<BR>task: Text classification: Document Classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Reuters-21578 - Document Classification benchmarking: F1<BR>\",\"<BR>task: Text classification: Text Classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  RCV1 - Text Classification benchmarking: Micro F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2020-03<BR>ratio: 0.3066<BR>benchmarks:<BR>  DocRED - Relation Extraction benchmarking: F1<BR>\",\"<BR>task: Information extraction: Relation Extraction<BR>date: 2020-04<BR>ratio: 0.2667<BR>benchmarks:<BR>  SemEval-2010 Task 8 - Relation Extraction benchmarking: F1<BR>\"],\"marker\":{\"color\":[0.1102,1.0,0.3305,0.76,0.2308,0.2549,0.0886,0.0904,0.3032,1.0,0.1186,0.0427,0.232,0.5,0.1435,0.2159,0.4025,0.25,0.483,0.3491,0.3173,0.0718,0.1323,0.2757,0.5335,0.2,0.5215,0.1966,0.2762,0.542,0.7941,0.2999,0.0702,0.3195,0.0771,0.0678,0.218,0.094,0.9481,0.1882,0.3589,0.0286,0.101,0.098,0.2232,0.5,0.3558,0.6069,0.8,0.5084,0.0123,0.0632,0.0853,0.8096,0.1739,0.0711,0.4857,0.7801,0.6667,0.0708,0.0597,0.0392,0.2857,0.1937,0.033,0.1026,0.0174,0.108,0.1795,0.55,0.6016,0.2253,0.2429,0.2329,0.3586,0.2511,0.921,0.3615,0.1524,0.4788,0.1465,0.3698,1.0,0.0632,1.0,0.3933,0.212,0.2457,0.3456,0.5,0.2288,0.4269,0.4385,0.6226,0.3099,0.7439,0.3904,0.0196,0.0407,0.7021,0.0798,0.6272,0.6346,0.2578,0.2561,0.0459,0.0392,0.9439,0.1476,0.1887,0.0392,0.0571,0.0882,0.4672,0.0809,0.3585,0.8333,0.1176,0.0238,0.2079,0.4381,0.3667,0.541,0.0357,0.2009,0.1587,1.0,1.0,0.0152,0.5921,0.2234,0.245,0.137,0.0412,0.1698,0.1343,0.1569,0.8365,0.0536,1.0,0.7182,0.2896,0.3299,0.0126,0.7567,0.154,0.3862,0.6132,0.0673,1.0,0.0282,0.3122,1.0,0.0553,0.791,0.5132,0.913,0.4463,1.0,0.0714,0.2718,0.2449,0.0184,0.5122,0.226,1.0,0.1442,0.0699,1.0,0.3185,0.652,0.2353,0.2419,0.196,0.5481,0.327,0.2054,0.15,0.3426,0.2123,0.45,0.2577,0.4277,0.4178,0.6276,1.0,0.1963,0.9057,0.1404,0.2719,0.2698,0.1449,0.5661,1.0,0.2041,1.0,0.6137,0.3456,1.0,0.7765,0.3558,0.2254,0.5299,0.4127,0.1667,0.4587,0.1426,0.3546,0.8067,1.0,0.397,0.3333,0.666,0.1343,0.1016,0.6981,1.0,0.3031,1.0,0.3147,0.1967,0.2778,0.7252,0.2995,0.2915,0.5876,0.5763,0.0123,0.4213,0.0901,0.2987,0.3157,0.3432,0.4748,0.6752,0.9327,0.4235,0.4549,0.2911,0.4942,0.2659,1.0,0.8571,0.5,0.0675,0.5346,0.5765,0.5752,0.6643,0.1145,0.0469,0.4897,0.4252,0.4272,0.3025,0.5266,0.6501,0.2915,0.3767,0.5481,0.4801,0.0577,0.2347,0.2383,0.3309,1.0,0.1081,0.4554,0.0358,0.4151,0.0067,1.0,0.0784,0.0085,0.1111,0.8833,0.4834,0.2136,0.9661,0.0846,0.0427,0.6447,0.046,0.1429,0.2846,0.7221,0.2671,0.6089,0.1871,0.4096,0.1815,0.2333,0.5604,1.0,0.5266,1.0,1.0,0.3066,0.2667],\"colorbar\":{\"len\":500,\"lenmode\":\"pixels\",\"thickness\":10,\"title\":{\"text\":\"ratio\"}},\"colorscale\":[[0.0,\"rgb(255,255,229)\"],[0.125,\"rgb(247,252,185)\"],[0.25,\"rgb(217,240,163)\"],[0.375,\"rgb(173,221,142)\"],[0.5,\"rgb(120,198,121)\"],[0.625,\"rgb(65,171,93)\"],[0.75,\"rgb(35,132,67)\"],[0.875,\"rgb(0,104,55)\"],[1.0,\"rgb(0,69,41)\"]],\"opacity\":0.7,\"showscale\":true,\"size\":19,\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"markers\",\"x\":[\"2013-10\",\"2013-10\",\"2014-06\",\"2014-06\",\"2014-08\",\"2014-08\",\"2014-09\",\"2014-10\",\"2014-12\",\"2014-12\",\"2015-02\",\"2015-06\",\"2015-06\",\"2015-08\",\"2015-09\",\"2015-11\",\"2015-11\",\"2015-11\",\"2016-01\",\"2016-02\",\"2016-02\",\"2016-02\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-05\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-07\",\"2016-07\",\"2016-07\",\"2016-08\",\"2016-08\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-10\",\"2016-10\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-12\",\"2016-12\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-02\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-12\",\"2020-02\",\"2020-02\",\"2020-02\",\"2020-03\",\"2020-04\"],\"y\":[\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Sentiment Analysis\",\"Question answering: Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Inference and reasoning: Natural Language Inference\",\"Natural language generation: Machine Translation\",\"Natural language generation: Machine Translation\",\"Question answering: Question Answering\",\"Natural language generation: Language Modelling\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Sentiment Analysis\",\"Question answering: Question Answering\",\"Inference and reasoning: Natural Language Inference\",\"Natural language generation: Text Summarization\",\"Question answering: Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Text classification: Text Classification\",\"Information extraction: Relation Extraction\",\"Pragmatics analysis: Sentiment Analysis\",\"Natural language generation: Text Summarization\",\"Question answering: Question Answering\",\"Semantic analysis: Word Sense Disambiguation\",\"Question answering: Visual Question Answering\",\"Question answering: Question Answering\",\"Natural language generation: Machine Translation\",\"Text classification: Document Classification\",\"Syntactic analysis: Dependency Parsing\",\"Question answering: Visual Question Answering\",\"Natural language generation: Machine Translation\",\"Text classification: Citation Intent Classification\",\"Question answering: Visual Question Answering\",\"Pragmatics analysis: Coreference Resolution\",\"Question answering: Question Answering\",\"Natural language generation: Text Summarization\",\"Semantic analysis: Word Sense Disambiguation\",\"Natural language generation: Machine Translation\",\"Pragmatics analysis: Sentiment Analysis\",\"Syntactic analysis: Grammatical Error Detection\",\"Natural language generation: Machine Translation\",\"Question answering: Question Answering\",\"Pragmatics analysis: Coreference Resolution\",\"Question answering: Question Answering\",\"Inference and reasoning: Natural Language Inference\",\"Natural language generation: Machine Translation\",\"Natural language generation: Language Modelling\",\"Text classification: Document Classification\",\"Question answering: Question Answering\",\"Natural language generation: Machine Translation\",\"Question answering: Visual Question Answering\",\"Text classification: Document Classification\",\"Natural language generation: Machine Translation\",\"Question answering: Question Answering\",\"Syntactic analysis: Dependency Parsing\",\"Syntactic analysis: Chunking\",\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Constituency Parsing\",\"Question answering: Visual Question Answering\",\"Natural language generation: Language Modelling\",\"Other NLP task: Text-to-Image Generation\",\"Natural language generation: Machine Translation\",\"Inference and reasoning: Natural Language Inference\",\"Text classification: Text Classification\",\"Pragmatics analysis: Sentiment Analysis\",\"Question answering: Question Answering\",\"Question answering: Visual Question Answering\",\"Question answering: Question Answering\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Sentiment Analysis\",\"Semantic analysis: Semantic Parsing\",\"Syntactic analysis: Grammatical Error Detection\",\"Natural language generation: Machine Translation\",\"Question answering: Question Answering\",\"Question answering: Visual Question Answering\",\"Natural language generation: Text Generation\",\"Natural language generation: Document Summarization\",\"Semantic analysis: Entity Disambiguation\",\"Natural language generation: Text Summarization\",\"Question answering: Question Answering\",\"Natural language generation: Machine Translation\",\"Pragmatics analysis: Coreference Resolution\",\"Syntactic analysis: Grammatical Error Detection\",\"Sentence embedding: Sentence Compression\",\"Question answering: Visual Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Question answering: Question Answering\",\"Information extraction: Relation Extraction\",\"Syntactic analysis: Constituency Parsing\",\"Question answering: Visual Question Answering\",\"Question answering: Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Computer code processing: Code Generation\",\"Information extraction: Relation Extraction\",\"Natural language generation: Text Generation\",\"Pragmatics analysis: Paraphrase Identification\",\"Dialog process: Dialog Act Classification\",\"Dialog process: Visual Dialog\",\"Inference and reasoning: Natural Language Inference\",\"Semantic analysis: Word Sense Disambiguation\",\"Other NLP task: Text-to-Image Generation\",\"Text classification: Document Classification\",\"Question answering: Question Answering\",\"Other NLP task: Text-to-Image Generation\",\"Dialog process: Visual Dialog\",\"Dialog process: Dialog Act Classification\",\"Natural language generation: Machine Translation\",\"Inference and reasoning: Natural Language Inference\",\"Pragmatics analysis: Fake News Detection\",\"Pragmatics analysis: Sentiment Analysis\",\"Semantic analysis: Semantic Role Labeling\",\"Inference and reasoning: Natural Language Inference\",\"Question answering: Question Answering\",\"Text classification: Citation Intent Classification\",\"Question answering: Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Semantic analysis: Semantic Role Labeling\",\"Natural language generation: Text Generation\",\"Text classification: Citation Intent Classification\",\"Semantic analysis: Word Sense Disambiguation\",\"Pragmatics analysis: Coreference Resolution\",\"Natural language generation: Machine Translation\",\"Pragmatics analysis: Sentiment Analysis\",\"Information retrieval: Conversational Response Selection\",\"Text classification: Text Classification\",\"Computer code processing: Code Generation\",\"Question answering: Visual Question Answering\",\"Natural language generation: Language Modelling\",\"Semantic analysis: Semantic Textual Similarity\",\"Natural language generation: Machine Translation\",\"Question answering: Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Pragmatics analysis: Coreference Resolution\",\"Computer code processing: Code Generation\",\"Inference and reasoning: Natural Language Inference\",\"Semantic analysis: Semantic Role Labeling\",\"Syntactic analysis: Constituency Parsing\",\"Inference and reasoning: Natural Language Inference\",\"Dialog process: Dialog State Tracking\",\"Semantic analysis: Word Sense Disambiguation\",\"Inference and reasoning: Common Sense Reasoning\",\"Question answering: Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Text classification: Text Classification\",\"Question answering: Visual Question Answering\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Natural language generation: Machine Translation\",\"Question answering: Question Answering\",\"Inference and reasoning: Natural Language Inference\",\"Natural language generation: Question Generation\",\"Question answering: Question Answering\",\"Pragmatics analysis: Paraphrase Identification\",\"Information extraction: Relation Extraction\",\"Sentence embedding: Sentence Compression\",\"Natural language generation: Text Summarization\",\"Syntactic analysis: Dependency Parsing\",\"Text classification: Text Classification\",\"Syntactic analysis: Chunking\",\"Information extraction: Relation Extraction\",\"Information extraction: Named Entity Recognition\",\"Syntactic analysis: Constituency Grammar Induction\",\"Question answering: Question Answering\",\"Natural language generation: Text Summarization\",\"Text classification: Document Classification\",\"Natural language generation: Document Summarization\",\"Natural language generation: Machine Translation\",\"Text classification: Text Classification\",\"Dialog process: Visual Dialog\",\"Natural language generation: Machine Translation\",\"Natural language generation: Language Modelling\",\"Information extraction: Relation Extraction\",\"Text classification: Text Classification\",\"Inference and reasoning: Natural Language Inference\",\"Question answering: Question Answering\",\"Question answering: Question Answering\",\"Computer code processing: Code Generation\",\"Dialog process: Dialog State Tracking\",\"Information extraction: Named Entity Recognition\",\"Machine translation: Unsupervised Machine Translation\",\"Inference and reasoning: Natural Language Inference\",\"Syntactic analysis: Constituency Grammar Induction\",\"Semantic analysis: Semantic Parsing\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Semantic analysis: Semantic Role Labeling\",\"Pragmatics analysis: Sentiment Analysis\",\"Inference and reasoning: Common Sense Reasoning\",\"Natural language generation: Machine Translation\",\"Information extraction: Relation Extraction\",\"Text classification: Sentence Classification\",\"Pragmatics analysis: Fake News Detection\",\"Semantic analysis: Word Sense Disambiguation\",\"Syntactic analysis: Grammatical Error Detection\",\"Question answering: Question Answering\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Information extraction: Relation Extraction\",\"Question answering: Question Answering\",\"Information extraction: Named Entity Recognition\",\"Text classification: Text Classification\",\"Inference and reasoning: Natural Language Inference\",\"Other NLP task: Text-to-Image Generation\",\"Natural language generation: Machine Translation\",\"Information retrieval: Conversational Response Selection\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Sentiment Analysis\",\"Machine translation: Unsupervised Machine Translation\",\"Natural language generation: Text Generation\",\"Question answering: Visual Question Answering\",\"Dialog process: Visual Dialog\",\"Text classification: Text Classification\",\"Question answering: Question Answering\",\"Inference and reasoning: Common Sense Reasoning\",\"Machine translation: Unsupervised Machine Translation\",\"Information extraction: Relation Extraction\",\"Information extraction: Named Entity Recognition\",\"Syntactic analysis: Constituency Parsing\",\"Other NLP task: Text-to-Image Generation\",\"Text classification: Sentence Classification\",\"Information extraction: Relation Extraction\",\"Natural language generation: Document Summarization\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Question answering: Visual Question Answering\",\"Syntactic analysis: Constituency Grammar Induction\",\"Information extraction: Joint Entity and Relation Extraction\",\"Information extraction: Relation Extraction\",\"Information extraction: Chinese Named Entity Recognition\",\"Information retrieval: Conversational Response Selection\",\"Other NLP task: Text-to-Image Generation\",\"Natural language generation: Text Summarization\",\"Text classification: Document Classification\",\"Dialog process: Visual Dialog\",\"Natural language generation: Document Summarization\",\"Natural language generation: Machine Translation\",\"Machine translation: Unsupervised Machine Translation\",\"Pragmatics analysis: Sentiment Analysis\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Word Sense Disambiguation\",\"Natural language generation: Question Generation\",\"Question answering: Visual Question Answering\",\"Text classification: Text Classification\",\"Natural language generation: Text Summarization\",\"Question answering: Question Answering\",\"Information extraction: Relation Extraction\",\"Pragmatics analysis: Intent Detection\",\"Syntactic analysis: Linguistic Acceptability Assessment\",\"Inference and reasoning: Natural Language Inference\",\"Pragmatics analysis: Sentiment Analysis\",\"Text classification: Text Classification\",\"Syntactic analysis: Constituency Grammar Induction\",\"Natural language generation: Machine Translation\",\"Pragmatics analysis: Paraphrase Identification\",\"Information extraction: Relation Extraction\",\"Question answering: Visual Question Answering\",\"Question answering: Question Answering\",\"Semantic analysis: Semantic Textual Similarity\",\"Inference and reasoning: Common Sense Reasoning\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Chinese Named Entity Recognition\",\"Information extraction: Relation Extraction\",\"Pragmatics analysis: Coreference Resolution\",\"Question answering: Question Answering\",\"Question answering: Visual Question Answering\",\"Inference and reasoning: Natural Language Inference\",\"Semantic analysis: Semantic Textual Similarity\",\"Pragmatics analysis: Sentiment Analysis\",\"Inference and reasoning: Common Sense Reasoning\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Text classification: Document Classification\",\"Natural language generation: Document Summarization\",\"Question answering: Visual Question Answering\",\"Pragmatics analysis: Coreference Resolution\",\"Information extraction: Named Entity Recognition\",\"Question answering: Question Answering\",\"Other NLP task: Text-to-Image Generation\",\"Question answering: Question Answering\",\"Pragmatics analysis: Sentiment Analysis\",\"Question answering: Visual Question Answering\",\"Text classification: Text Classification\",\"Information extraction: Relation Extraction\",\"Inference and reasoning: Common Sense Reasoning\",\"Semantic analysis: Word Sense Disambiguation\",\"Semantic analysis: Semantic Textual Similarity\",\"Inference and reasoning: Natural Language Inference\",\"Semantic analysis: Entity Disambiguation\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Syntactic analysis: Linguistic Acceptability Assessment\",\"Natural language generation: Machine Translation\",\"Information extraction: Joint Entity and Relation Extraction\",\"Natural language generation: Machine Translation\",\"Natural language generation: Text Summarization\",\"Computer code processing: Code Generation\",\"Information retrieval: Conversational Response Selection\",\"Information extraction: Named Entity Recognition\",\"Information extraction: Relation Extraction\",\"Information extraction: Chinese Named Entity Recognition\",\"Pragmatics analysis: Intent Detection\",\"Question answering: Visual Question Answering\",\"Text classification: Document Classification\",\"Text classification: Text Classification\",\"Information extraction: Relation Extraction\",\"Information extraction: Relation Extraction\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Year\"},\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"tickmode\":\"auto\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{},\"categoryorder\":\"array\",\"categoryarray\":[\"Text classification: Sentence Classification\",\"Text classification: Document Classification\",\"Text classification: Text Classification\",\"Text classification: Citation Intent Classification\",\"Syntactic analysis: Linguistic Acceptability Assessment\",\"Syntactic analysis: Grammatical Error Detection\",\"Syntactic analysis: Constituency Grammar Induction\",\"Syntactic analysis: Chunking\",\"Syntactic analysis: Constituency Parsing\",\"Syntactic analysis: Dependency Parsing\",\"Sentence embedding: Sentence Compression\",\"Semantic analysis: Semantic Role Labeling\",\"Semantic analysis: Semantic Parsing\",\"Semantic analysis: Semantic Textual Similarity\",\"Semantic analysis: Entity Disambiguation\",\"Semantic analysis: Word Sense Disambiguation\",\"Question answering: Question Answering\",\"Question answering: Visual Question Answering\",\"Pragmatics analysis: Paraphrase Identification\",\"Pragmatics analysis: Emotion Recognition in Conversation\",\"Pragmatics analysis: Fake News Detection\",\"Pragmatics analysis: Intent Detection\",\"Pragmatics analysis: Coreference Resolution\",\"Pragmatics analysis: Sentiment Analysis\",\"Other NLP task: Text-to-Image Generation\",\"Natural language generation: Language Modelling\",\"Natural language generation: Document Summarization\",\"Natural language generation: Text Generation\",\"Natural language generation: Question Generation\",\"Natural language generation: Machine Translation\",\"Natural language generation: Text Summarization\",\"Machine translation: Unsupervised Machine Translation\",\"Information retrieval: Conversational Response Selection\",\"Information extraction: Chinese Named Entity Recognition\",\"Information extraction: Relation Extraction\",\"Information extraction: Joint Entity and Relation Extraction\",\"Information extraction: Named Entity Recognition\",\"Inference and reasoning: Natural Language Inference\",\"Inference and reasoning: Common Sense Reasoning\",\"Dialog process: Dialog Act Classification\",\"Dialog process: Dialog State Tracking\",\"Dialog process: Visual Dialog\",\"Computer code processing: Code Generation\"],\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"side\":\"left\"},\"legend\":{\"title\":{\"text\":\"task\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Natural Language Processing\",\"y\":0.995},\"font\":{\"size\":21},\"showlegend\":false,\"plot_bgcolor\":\"white\",\"height\":1782.0000000000002,\"width\":1500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('07ba0ae0-43f0-44bb-a9c0-b83dcd62b312');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative yearly distribution of state-of-the-art (SOTA) averaged gain ratio values - NLP - single arrows removed\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2013",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.1102,
          1
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2014",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.3305,
          0.76,
          0.2308,
          0.2549,
          0.0886,
          0.0904,
          0.3032,
          1
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2015",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.1186,
          0.0427,
          0.232,
          0.5,
          0.1435,
          0.2159,
          0.4025,
          0.25
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2016",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.483,
          0.3491,
          0.3173,
          0.0718,
          0.1323,
          0.2757,
          0.5335,
          0.2,
          0.5215,
          0.1966,
          0.2762,
          0.542,
          0.7941,
          0.2999,
          0.0702,
          0.3195,
          0.0771,
          0.0678,
          0.218,
          0.094,
          0.9481,
          0.1882,
          0.3589,
          0.0286,
          0.101,
          0.098,
          0.2232,
          0.5,
          0.3558,
          0.6069,
          0.8,
          0.5084,
          0.0123,
          0.0632,
          0.0853,
          0.8096,
          0.1739,
          0.0711,
          0.4857,
          0.7801,
          0.6667,
          0.0708
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2017",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.0597,
          0.0392,
          0.2857,
          0.1937,
          0.033,
          0.1026,
          0.0174,
          0.108,
          0.1795,
          0.55,
          0.6016,
          0.2253,
          0.2429,
          0.2329,
          0.3586,
          0.2511,
          0.921,
          0.3615,
          0.1524,
          0.4788,
          0.1465,
          0.3698,
          1,
          0.0632,
          1,
          0.3933,
          0.212,
          0.2457,
          0.3456,
          0.5,
          0.2288,
          0.4269,
          0.4385,
          0.6226,
          0.3099,
          0.7439,
          0.3904,
          0.0196,
          0.0407,
          0.7021,
          0.0798,
          0.6272,
          0.6346,
          0.2578,
          0.2561,
          0.0459,
          0.0392,
          0.9439,
          0.1476,
          0.1887,
          0.0392,
          0.0571
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2018",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.0882,
          0.4672,
          0.0809,
          0.3585,
          0.8333,
          0.1176,
          0.0238,
          0.2079,
          0.4381,
          0.3667,
          0.541,
          0.0357,
          0.2009,
          0.1587,
          1,
          1,
          0.0152,
          0.5921,
          0.2234,
          0.245,
          0.137,
          0.0412,
          0.1698,
          0.1343,
          0.1569,
          0.8365,
          0.0536,
          1,
          0.7182,
          0.2896,
          0.3299,
          0.0126,
          0.7567,
          0.154,
          0.3862,
          0.6132,
          0.0673,
          1,
          0.0282,
          0.3122,
          1,
          0.0553,
          0.791,
          0.5132,
          0.913,
          0.4463,
          1,
          0.0714,
          0.2718,
          0.2449,
          0.0184,
          0.5122,
          0.226,
          1,
          0.1442,
          0.0699,
          1,
          0.3185,
          0.652,
          0.2353,
          0.2419,
          0.196,
          0.5481,
          0.327,
          0.2054,
          0.15,
          0.3426,
          0.2123,
          0.45,
          0.2577,
          0.4277,
          0.4178,
          0.6276,
          1,
          0.1963,
          0.9057,
          0.1404,
          0.2719,
          0.2698,
          0.1449,
          0.5661,
          1
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2019",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.2041,
          1,
          0.6137,
          0.3456,
          1,
          0.7765,
          0.3558,
          0.2254,
          0.5299,
          0.4127,
          0.1667,
          0.4587,
          0.1426,
          0.3546,
          0.8067,
          1,
          0.397,
          0.3333,
          0.666,
          0.1343,
          0.1016,
          0.6981,
          1,
          0.3031,
          1,
          0.3147,
          0.1967,
          0.2778,
          0.7252,
          0.2995,
          0.2915,
          0.5876,
          0.5763,
          0.0123,
          0.4213,
          0.0901,
          0.2987,
          0.3157,
          0.3432,
          0.4748,
          0.6752,
          0.9327,
          0.4235,
          0.4549,
          0.2911,
          0.4942,
          0.2659,
          1,
          0.8571,
          0.5,
          0.0675,
          0.5346,
          0.5765,
          0.5752,
          0.6643,
          0.1145,
          0.0469,
          0.4897,
          0.4252,
          0.4272,
          0.3025,
          0.5266,
          0.6501,
          0.2915,
          0.3767,
          0.5481,
          0.4801,
          0.0577,
          0.2347,
          0.2383,
          0.3309,
          1,
          0.1081,
          0.4554,
          0.0358,
          0.4151,
          0.0067,
          1,
          0.0784,
          0.0085,
          0.1111,
          0.8833,
          0.4834,
          0.2136,
          0.9661,
          0.0846,
          0.0427,
          0.6447,
          0.046,
          0.1429,
          0.2846,
          0.7221,
          0.2671,
          0.6089,
          0.1871,
          0.4096,
          0.1815,
          0.2333,
          0.5604,
          1
         ]
        },
        {
         "boxpoints": "all",
         "jitter": 0.8,
         "line": {
          "width": 2
         },
         "marker": {
          "color": "Blue",
          "size": 3
         },
         "name": "2020",
         "type": "box",
         "whiskerwidth": 0.1,
         "y": [
          0.5266,
          1,
          1,
          0.3066,
          0.2667
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 20
        },
        "height": 400,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1000,
        "xaxis": {
         "categoryarray": [
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020
         ],
         "categoryorder": "array",
         "tickmode": "linear"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bb8c88d1-87b0-446a-9cd5-5d466fc1f498\" class=\"plotly-graph-div\" style=\"height:400px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bb8c88d1-87b0-446a-9cd5-5d466fc1f498\")) {                    Plotly.newPlot(                        \"bb8c88d1-87b0-446a-9cd5-5d466fc1f498\",                        [{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2013\",\"whiskerwidth\":0.1,\"y\":[0.1102,1.0],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2014\",\"whiskerwidth\":0.1,\"y\":[0.3305,0.76,0.2308,0.2549,0.0886,0.0904,0.3032,1.0],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2015\",\"whiskerwidth\":0.1,\"y\":[0.1186,0.0427,0.232,0.5,0.1435,0.2159,0.4025,0.25],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2016\",\"whiskerwidth\":0.1,\"y\":[0.483,0.3491,0.3173,0.0718,0.1323,0.2757,0.5335,0.2,0.5215,0.1966,0.2762,0.542,0.7941,0.2999,0.0702,0.3195,0.0771,0.0678,0.218,0.094,0.9481,0.1882,0.3589,0.0286,0.101,0.098,0.2232,0.5,0.3558,0.6069,0.8,0.5084,0.0123,0.0632,0.0853,0.8096,0.1739,0.0711,0.4857,0.7801,0.6667,0.0708],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2017\",\"whiskerwidth\":0.1,\"y\":[0.0597,0.0392,0.2857,0.1937,0.033,0.1026,0.0174,0.108,0.1795,0.55,0.6016,0.2253,0.2429,0.2329,0.3586,0.2511,0.921,0.3615,0.1524,0.4788,0.1465,0.3698,1.0,0.0632,1.0,0.3933,0.212,0.2457,0.3456,0.5,0.2288,0.4269,0.4385,0.6226,0.3099,0.7439,0.3904,0.0196,0.0407,0.7021,0.0798,0.6272,0.6346,0.2578,0.2561,0.0459,0.0392,0.9439,0.1476,0.1887,0.0392,0.0571],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2018\",\"whiskerwidth\":0.1,\"y\":[0.0882,0.4672,0.0809,0.3585,0.8333,0.1176,0.0238,0.2079,0.4381,0.3667,0.541,0.0357,0.2009,0.1587,1.0,1.0,0.0152,0.5921,0.2234,0.245,0.137,0.0412,0.1698,0.1343,0.1569,0.8365,0.0536,1.0,0.7182,0.2896,0.3299,0.0126,0.7567,0.154,0.3862,0.6132,0.0673,1.0,0.0282,0.3122,1.0,0.0553,0.791,0.5132,0.913,0.4463,1.0,0.0714,0.2718,0.2449,0.0184,0.5122,0.226,1.0,0.1442,0.0699,1.0,0.3185,0.652,0.2353,0.2419,0.196,0.5481,0.327,0.2054,0.15,0.3426,0.2123,0.45,0.2577,0.4277,0.4178,0.6276,1.0,0.1963,0.9057,0.1404,0.2719,0.2698,0.1449,0.5661,1.0],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2019\",\"whiskerwidth\":0.1,\"y\":[0.2041,1.0,0.6137,0.3456,1.0,0.7765,0.3558,0.2254,0.5299,0.4127,0.1667,0.4587,0.1426,0.3546,0.8067,1.0,0.397,0.3333,0.666,0.1343,0.1016,0.6981,1.0,0.3031,1.0,0.3147,0.1967,0.2778,0.7252,0.2995,0.2915,0.5876,0.5763,0.0123,0.4213,0.0901,0.2987,0.3157,0.3432,0.4748,0.6752,0.9327,0.4235,0.4549,0.2911,0.4942,0.2659,1.0,0.8571,0.5,0.0675,0.5346,0.5765,0.5752,0.6643,0.1145,0.0469,0.4897,0.4252,0.4272,0.3025,0.5266,0.6501,0.2915,0.3767,0.5481,0.4801,0.0577,0.2347,0.2383,0.3309,1.0,0.1081,0.4554,0.0358,0.4151,0.0067,1.0,0.0784,0.0085,0.1111,0.8833,0.4834,0.2136,0.9661,0.0846,0.0427,0.6447,0.046,0.1429,0.2846,0.7221,0.2671,0.6089,0.1871,0.4096,0.1815,0.2333,0.5604,1.0],\"type\":\"box\"},{\"boxpoints\":\"all\",\"jitter\":0.8,\"line\":{\"width\":2},\"marker\":{\"color\":\"Blue\",\"size\":3},\"name\":\"2020\",\"whiskerwidth\":0.1,\"y\":[0.5266,1.0,1.0,0.3066,0.2667],\"type\":\"box\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"font\":{\"size\":20},\"xaxis\":{\"tickmode\":\"linear\",\"categoryorder\":\"array\",\"categoryarray\":[2011,2012,2013,2014,2015,2016,2017,2018,2019,2020]},\"height\":400,\"width\":1000,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bb8c88d1-87b0-446a-9cd5-5d466fc1f498');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_921e1_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >2013</th>\n",
       "      <th class=\"col_heading level0 col1\" >2014</th>\n",
       "      <th class=\"col_heading level0 col2\" >2015</th>\n",
       "      <th class=\"col_heading level0 col3\" >2016</th>\n",
       "      <th class=\"col_heading level0 col4\" >2017</th>\n",
       "      <th class=\"col_heading level0 col5\" >2018</th>\n",
       "      <th class=\"col_heading level0 col6\" >2019</th>\n",
       "      <th class=\"col_heading level0 col7\" >2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_921e1_row0_col0\" class=\"data row0 col0\" >2.000000</td>\n",
       "      <td id=\"T_921e1_row0_col1\" class=\"data row0 col1\" >8.000000</td>\n",
       "      <td id=\"T_921e1_row0_col2\" class=\"data row0 col2\" >8.000000</td>\n",
       "      <td id=\"T_921e1_row0_col3\" class=\"data row0 col3\" >42.000000</td>\n",
       "      <td id=\"T_921e1_row0_col4\" class=\"data row0 col4\" >52.000000</td>\n",
       "      <td id=\"T_921e1_row0_col5\" class=\"data row0 col5\" >82.000000</td>\n",
       "      <td id=\"T_921e1_row0_col6\" class=\"data row0 col6\" >100.000000</td>\n",
       "      <td id=\"T_921e1_row0_col7\" class=\"data row0 col7\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_921e1_row1_col0\" class=\"data row1 col0\" >0.555000</td>\n",
       "      <td id=\"T_921e1_row1_col1\" class=\"data row1 col1\" >0.382000</td>\n",
       "      <td id=\"T_921e1_row1_col2\" class=\"data row1 col2\" >0.238000</td>\n",
       "      <td id=\"T_921e1_row1_col3\" class=\"data row1 col3\" >0.328000</td>\n",
       "      <td id=\"T_921e1_row1_col4\" class=\"data row1 col4\" >0.325000</td>\n",
       "      <td id=\"T_921e1_row1_col5\" class=\"data row1 col5\" >0.397000</td>\n",
       "      <td id=\"T_921e1_row1_col6\" class=\"data row1 col6\" >0.433000</td>\n",
       "      <td id=\"T_921e1_row1_col7\" class=\"data row1 col7\" >0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_921e1_row2_col0\" class=\"data row2 col0\" >0.629000</td>\n",
       "      <td id=\"T_921e1_row2_col1\" class=\"data row2 col1\" >0.326000</td>\n",
       "      <td id=\"T_921e1_row2_col2\" class=\"data row2 col2\" >0.150000</td>\n",
       "      <td id=\"T_921e1_row2_col3\" class=\"data row2 col3\" >0.257000</td>\n",
       "      <td id=\"T_921e1_row2_col4\" class=\"data row2 col4\" >0.270000</td>\n",
       "      <td id=\"T_921e1_row2_col5\" class=\"data row2 col5\" >0.317000</td>\n",
       "      <td id=\"T_921e1_row2_col6\" class=\"data row2 col6\" >0.286000</td>\n",
       "      <td id=\"T_921e1_row2_col7\" class=\"data row2 col7\" >0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_921e1_row3_col0\" class=\"data row3 col0\" >0.110000</td>\n",
       "      <td id=\"T_921e1_row3_col1\" class=\"data row3 col1\" >0.089000</td>\n",
       "      <td id=\"T_921e1_row3_col2\" class=\"data row3 col2\" >0.043000</td>\n",
       "      <td id=\"T_921e1_row3_col3\" class=\"data row3 col3\" >0.012000</td>\n",
       "      <td id=\"T_921e1_row3_col4\" class=\"data row3 col4\" >0.017000</td>\n",
       "      <td id=\"T_921e1_row3_col5\" class=\"data row3 col5\" >0.013000</td>\n",
       "      <td id=\"T_921e1_row3_col6\" class=\"data row3 col6\" >0.007000</td>\n",
       "      <td id=\"T_921e1_row3_col7\" class=\"data row3 col7\" >0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_921e1_row4_col0\" class=\"data row4 col0\" >0.333000</td>\n",
       "      <td id=\"T_921e1_row4_col1\" class=\"data row4 col1\" >0.196000</td>\n",
       "      <td id=\"T_921e1_row4_col2\" class=\"data row4 col2\" >0.137000</td>\n",
       "      <td id=\"T_921e1_row4_col3\" class=\"data row4 col3\" >0.095000</td>\n",
       "      <td id=\"T_921e1_row4_col4\" class=\"data row4 col4\" >0.107000</td>\n",
       "      <td id=\"T_921e1_row4_col5\" class=\"data row4 col5\" >0.151000</td>\n",
       "      <td id=\"T_921e1_row4_col6\" class=\"data row4 col6\" >0.222000</td>\n",
       "      <td id=\"T_921e1_row4_col7\" class=\"data row4 col7\" >0.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_921e1_row5_col0\" class=\"data row5 col0\" >0.555000</td>\n",
       "      <td id=\"T_921e1_row5_col1\" class=\"data row5 col1\" >0.279000</td>\n",
       "      <td id=\"T_921e1_row5_col2\" class=\"data row5 col2\" >0.224000</td>\n",
       "      <td id=\"T_921e1_row5_col3\" class=\"data row5 col3\" >0.276000</td>\n",
       "      <td id=\"T_921e1_row5_col4\" class=\"data row5 col4\" >0.248000</td>\n",
       "      <td id=\"T_921e1_row5_col5\" class=\"data row5 col5\" >0.281000</td>\n",
       "      <td id=\"T_921e1_row5_col6\" class=\"data row5 col6\" >0.403000</td>\n",
       "      <td id=\"T_921e1_row5_col7\" class=\"data row5 col7\" >0.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_921e1_row6_col0\" class=\"data row6 col0\" >0.778000</td>\n",
       "      <td id=\"T_921e1_row6_col1\" class=\"data row6 col1\" >0.438000</td>\n",
       "      <td id=\"T_921e1_row6_col2\" class=\"data row6 col2\" >0.288000</td>\n",
       "      <td id=\"T_921e1_row6_col3\" class=\"data row6 col3\" >0.506000</td>\n",
       "      <td id=\"T_921e1_row6_col4\" class=\"data row6 col4\" >0.449000</td>\n",
       "      <td id=\"T_921e1_row6_col5\" class=\"data row6 col5\" >0.586000</td>\n",
       "      <td id=\"T_921e1_row6_col6\" class=\"data row6 col6\" >0.593000</td>\n",
       "      <td id=\"T_921e1_row6_col7\" class=\"data row6 col7\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_921e1_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_921e1_row7_col0\" class=\"data row7 col0\" >1.000000</td>\n",
       "      <td id=\"T_921e1_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_921e1_row7_col2\" class=\"data row7 col2\" >0.500000</td>\n",
       "      <td id=\"T_921e1_row7_col3\" class=\"data row7 col3\" >0.948000</td>\n",
       "      <td id=\"T_921e1_row7_col4\" class=\"data row7 col4\" >1.000000</td>\n",
       "      <td id=\"T_921e1_row7_col5\" class=\"data row7 col5\" >1.000000</td>\n",
       "      <td id=\"T_921e1_row7_col6\" class=\"data row7 col6\" >1.000000</td>\n",
       "      <td id=\"T_921e1_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x252193a0cc8>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Natural Language Processing, showing anchors and complete trajectories (single arrows removed).\n",
    "traj_df=plot_task_trajectory(\"ITO_00141\", \"Natural Language Processing\", anchor)\n",
    "\n",
    "boxplots=get_boxplot(traj_df)\n",
    "results=get_statistics(traj_df)\n",
    "\n",
    "#print statistics\n",
    "print(\"Comparative yearly distribution of state-of-the-art (SOTA) averaged gain ratio values - NLP - single arrows removed\")\n",
    "boxplots.show()\n",
    "results.T.style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQxUvXbCnHnE"
   },
   "source": [
    "# Group level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "mMzVoz8qA1Dr"
   },
   "outputs": [],
   "source": [
    "#Defining Plotting Function\n",
    "def plot_group_trajectory(ito, class_label, anchor):\n",
    "    import pandas as pd\n",
    "    #Read input\n",
    "    input_file_name=\"data/get_ratio_df_all_per_global_\"+ito+\".csv\"\n",
    "    get_ratio_df_all_per_global = pd.read_csv(input_file_name)\n",
    "    #Drop first column.\n",
    "    get_ratio_df_all_per_global = get_ratio_df_all_per_global.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "\n",
    "    #Change the column to name the facets of the plot.\n",
    "    get_ratio_df_all_per_global = get_ratio_df_all_per_global.rename(\n",
    "        columns={\"merge\": \"metricName\"}\n",
    "    )\n",
    "    get_ratio_df_all_per_global[\"metricName\"] = get_ratio_df_all_per_global[\n",
    "        \"metricName\"\n",
    "    ].str.replace(\"\\\\\", \"\")\n",
    "\n",
    "    get_ratio_df_all_per_global[\"unique_ds\"] = (\n",
    "        get_ratio_df_all_per_global[\"ds\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"metricName\"]\n",
    "    )\n",
    "    get_ratio_df_all_per_global[\"unique_task\"] = (\n",
    "        get_ratio_df_all_per_global[\"task\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"metricName\"]\n",
    "    )\n",
    "\n",
    "    get_ratio_df_all_per_global[\"unique_task_ds_metric\"] = (\n",
    "        get_ratio_df_all_per_global[\"task\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"ds\"].astype(str)\n",
    "        + \", \"\n",
    "        + get_ratio_df_all_per_global[\"metricName\"]\n",
    "    )\n",
    "\n",
    "    def agg(ex):\n",
    "      ex[\"ds\"] = \"  \" + ex[\"subtask\"] + \": \" + ex[\"ds\"] + \" - \" + ex[\"metricName\"] + \"<BR>\"\n",
    "      return pd.Series({\"ds\": \"\".join(ex[\"ds\"].sort_values().unique()), \"ratio\": ex[\"ratio\"].mean()})\n",
    "    \n",
    "    get_ratio_df_all_per_global[\"subtask\"] = get_ratio_df_all_per_global[\"task\"]\n",
    "    \n",
    "    #fix dirty data\n",
    "    # get_ratio_df_all_per_global[\"task\"] = get_ratio_df_all_per_global[\"task\"].str.replace(\"Abstractive Text Summarization\", \"Text Summarization\")\n",
    "    # get_ratio_df_all_per_global[\"task\"] = get_ratio_df_all_per_global[\"task\"].str.replace(\"Document Text Summarization\", \"Text Summarization\")\n",
    "\n",
    "    #Calculate the average data frame using 'RATIO' as criteria per unique_task.\n",
    "    traj = get_ratio_df_all_per_global.copy()\n",
    "\n",
    "    #This copy is necessary to add the anchors in the plot, and avoid that they influence in the calculation of the ratio.\n",
    "    traj_complete = traj.copy()\n",
    "    #Select here only the anchors.\n",
    "    traj_anchors = traj_complete.drop(traj_complete[traj_complete[\"ratio\"]!=100].index)                 ######\n",
    "    \n",
    "    # traj_anchors.to_csv(\"artefacts/traj_anchors_\"+ito+\".csv\")\n",
    "    \n",
    "    #This is wrong here, this is averaging just the anchor values, check the effect\n",
    "\n",
    "    average_summary_anchors = pd.DataFrame(traj_anchors.groupby([\"task\", \"date\"])[\"subtask\", \"ds\", \"metricName\", \"ratio\"].apply(agg))\n",
    "    #\n",
    "    average_summary_anchors.sort_values(by=[\"date\"], ascending=True)\n",
    "    average_summary_anchors.reset_index(inplace=True)\n",
    "    average_summary_anchors[\"in_trajectory\"] = \"OUT\"\n",
    "   \n",
    "    #metricName causing some problems. Removed!\n",
    "    traj = traj.drop(traj[traj[\"metricName\"]==\"Parameters\"].index)\n",
    "\n",
    "    #This will delete from the traj data frame all the ds/tasks which the counts are equal to 1, \n",
    "    #not forming, therefore, a trajectory.\n",
    "    count_df = pd.DataFrame(traj[\"ds\"].value_counts())\n",
    "    count_df[count_df.ds == 1].index\n",
    "\n",
    "    #Use the symbol ~ to inverselly select here.\n",
    "    traj = traj[~traj[\"ds\"].isin(count_df[count_df.ds == 1].index)]\n",
    "    \n",
    "    count_df = pd.DataFrame(traj[\"task\"].value_counts())\n",
    "    count_df[count_df.task == 1].index\n",
    "\n",
    "    traj = traj[~traj[\"task\"].isin(count_df[count_df.task == 1].index)]\n",
    "\n",
    "    if(len(traj) == 0):\n",
    "        return(\"Not enough points to plot trajectory\")\n",
    "    \n",
    "    \n",
    "    #We have to drop the first anchors, because we want to calculate the ratio with only the remaining meaningful points\n",
    "    #traj = traj.drop(traj[traj[\"ratio\"]==100].index) # for the individual trajectories analysis, keep these points and make them equal to Zero in the relevant notebook.\n",
    "    traj = traj.drop(traj[traj[\"ratio\"]<0].index)\n",
    "    \n",
    "    \n",
    "    #Choose here which grouping variable to use.\n",
    "    #traj = traj.drop(traj[traj[\"ratio\"]>=100].index)                 ######  prepare average_summary , if this line is commented, then the anchors appear, but the ratio is then wrong\n",
    "    traj = traj[traj[\"ratio\"]<100].copy() \n",
    "    # traj.to_csv(\"artefacts/traj_\"+ito+\".csv\")\n",
    "\n",
    "    average_summary = pd.DataFrame(traj.groupby([\"task\", \"date\"])[\"subtask\", \"ds\", \"metricName\", \"ratio\"].apply(agg))\n",
    "    # average_summary.to_csv(\"artefacts/average_summary_\"+ito+\".csv\")\n",
    "    #Drop anchors points.\n",
    "    #average_summary = average_summary.drop(average_summary[average_summary[\"ratio\"]>0.5].index)\n",
    "\n",
    "    average_summary.sort_values(by=[\"date\"], ascending=True)\n",
    "    average_summary.reset_index(inplace=True)\n",
    "    # average_summary[\"date\"]=pd.to_datetime(average_summary['date'])\n",
    "    # average_summary[\"date\"]=average_summary[\"date\"].dt.year\n",
    "    \n",
    "    \n",
    "\n",
    "    average_summary[\"in_trajectory\"] = 1\n",
    "\n",
    "    i = 0\n",
    "    for t in average_summary.task.unique():\n",
    "        sota_per = 0\n",
    "        #Here \"date\" can't be unique, because we want to look for the best value obtained per year.\n",
    "        for v in average_summary[average_summary[\"task\"] == t].ratio:\n",
    "            per = average_summary[\n",
    "                (average_summary[\"task\"] == t) & (average_summary[\"ratio\"] == v)\n",
    "            ].ratio.astype(float)\n",
    "            per = per.iloc[0]\n",
    "            if per == 100:\n",
    "                average_summary.loc[i, \"in_trajectory\"] = \"OUT\"\n",
    "            elif per >= sota_per:\n",
    "                # print(per)\n",
    "                sota_per = per\n",
    "                average_summary.loc[i, \"in_trajectory\"] = \"IN\"\n",
    "            else:\n",
    "                average_summary.loc[i, \"in_trajectory\"] = \"IN\"  # change back to OUT, if you want only to display the ratio average state of the art\n",
    "                # sota_per = per\n",
    "            i = i + 1\n",
    "\n",
    "    average_summary[\"ratio\"] = average_summary[\"ratio\"].apply(lambda x: round(x, 4))\n",
    "    \n",
    "    #NOW PLOT IT\n",
    "    #try using plotly\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    from plotly.validators.scatter.marker import SymbolValidator\n",
    "\n",
    "    #PLACE HOLDER FOR THE ORDERING OF THE DATA FRAME #ISSUE_01\n",
    "    #Use one column of this variable to sort: average_summary (get input from Kathrin)\n",
    "    \n",
    "    #get grouping: this tries to add the annotations by Kathrin, we need a merge here...\n",
    "    grouping_table = pd.read_csv(\"data/trajectory_grouping_\"+ito+\".csv\")\n",
    "    \n",
    "    # fix dirty data\n",
    "    grouping_table[\"Suggested_label\"] = grouping_table[\"Suggested_label\"].str.replace(\"Semantic segmenation\", \"Semantic segmentation\")\n",
    "    # grouping_table[\"Suggested_label\"] = grouping_table[\"Suggested_label\"].str.replace(\"Text summarization\", \"Natural language generation\")\n",
    "    \n",
    "    grouping_table = grouping_table.rename(columns={'Class_Label': 'task'})\n",
    "    average_summary = pd.merge(average_summary, grouping_table, on = 'task', how = 'left')\n",
    "    average_summary = average_summary.drop(['id', 'Superclass_id','Superclass_label'], axis = 1)\n",
    "    average_summary = average_summary.dropna()\n",
    "    average_summary = average_summary.sort_values('Suggested_label')\n",
    "    average_summary[\"task\"] = average_summary[\"Suggested_label\"]+\": \"+average_summary[\"task\"]\n",
    "\n",
    "    average_summary_anchors = pd.merge(average_summary_anchors, grouping_table, on = 'task', how = 'left')\n",
    "    average_summary_anchors = average_summary_anchors.drop(['id', 'Superclass_id','Superclass_label'], axis = 1)\n",
    "    average_summary_anchors = average_summary_anchors.dropna()\n",
    "    average_summary_anchors = average_summary_anchors.sort_values('Suggested_label')\n",
    "    average_summary_anchors[\"task\"] = average_summary_anchors[\"Suggested_label\"]+\": \"+average_summary_anchors[\"task\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    #split the plot for the paper\n",
    "    #average_summary = average_summary.iloc[0:585]\n",
    "    #average_summary = average_summary.iloc[586:len(average_summary)]\n",
    "\n",
    "    #return(average_summary)\n",
    "    \n",
    "    average_summary_IN = average_summary[average_summary[\"in_trajectory\"] == \"IN\"]\n",
    "    \n",
    "    #anchors = average_summary[average_summary[\"in_trajectory\"] == \"OUT\"]\n",
    "    \n",
    "    #this is used to calculate the height of the plot.\n",
    "    if(anchor==1):\n",
    "        average_summary_OUT = average_summary_anchors.copy()\n",
    "        plot_title = 'Trajectory for average gain ratio (task per year). Trajectories with single arrow shown.'\n",
    "        img_file_title = \"top_classes_trajectory_plots/\"+ito+\"_with_anchors_and_single_arrows_shown.png\"\n",
    "    elif(anchor==0):\n",
    "        average_summary_OUT = average_summary_anchors.copy()\n",
    "        plot_title = 'Trajectory for average gain ratio (task per year). Trajectories with single arrow removed.'     \n",
    "        img_file_title = \"top_classes_trajectory_plots/\"+ito+\"_with_anchors_and_single_arrows_removed.png\"\n",
    "    \n",
    "    \n",
    "    #This block will take the values from average_summary_IN and delete those that have only one arrow per trajectory.\n",
    "    count_df = pd.DataFrame(average_summary_IN[\"task\"].value_counts())\n",
    "    count_df[count_df.task == 1].index\n",
    "    \n",
    "    ##~~~~~ uncomment this block to filter out the trajectories with one arrow only\n",
    "    if(anchor==0): average_summary_IN = average_summary_IN[~average_summary_IN[\"task\"].isin(count_df[count_df.task == 1].index)]\n",
    "    average_summary_IN_1_point = average_summary_IN[average_summary_IN[\"task\"].isin(count_df[count_df.task == 1].index)]\n",
    "    average_summary_IN_1_point[\"Single arrow\"] = \"Yes\"\n",
    "    # average_summary_IN_1_point.to_csv(\"artefacts/average_summary_IN_1_point_\"+ito+\".csv\", index=False)\n",
    "    ##~~~~~\n",
    "    #this will delete from the traj data frame, all the tasks which the average_summary_IN counts are equal to 1,\n",
    "    #meaning single arrows will be excluded from the plot.\n",
    "\n",
    "\n",
    "    #NEEDS to add a feature to goup based on the Suggested_label, detalhe, the ito file is fixed, change it to dynamic\n",
    "    #return(average_summary_IN)\n",
    "\n",
    "\n",
    "\n",
    "    average_summary_anchors=average_summary_anchors[average_summary_anchors[\"task\"].isin(average_summary_IN.task)].copy()\n",
    "    average_summary_anchors[[\"task\", \"subtask\"]] = average_summary_anchors[\"task\"].str.split(\": \", 1, expand=True)\n",
    "    average_summary_IN[[\"task\", \"subtask\"]] = average_summary_IN[\"task\"].str.split(\": \", 1, expand=True)\n",
    "\n",
    "    def agg_group(ex):\n",
    "      return pd.Series({\"ds\": \"\".join(ex[\"ds\"].sort_values().unique()), \"ratio\": ex[\"ratio\"].mean()})\n",
    "    average_summary_IN = average_summary_IN.groupby([\"task\", \"date\"])[\"ds\", \"ratio\"].apply(agg_group).reset_index()\n",
    "    average_summary_anchors = average_summary_anchors.groupby([\"task\", \"date\"])[\"ds\", \"ratio\"].apply(agg_group).reset_index()\n",
    "\n",
    "    if ito == \"ITO_00141\":\n",
    "      average_summary_IN.loc[average_summary_IN[\"task\"] == \"Other NLP task\", \"task\"] = \"Text-to-image Generation\"\n",
    "      average_summary_anchors.loc[average_summary_anchors[\"task\"] == \"Other NLP task\", \"task\"] = \"Text-to-image Generation\"\n",
    "\n",
    "    rows = len(average_summary_IN[\"task\"].unique())*20\n",
    "\n",
    "    fig_traj = px.line(average_summary_IN, \n",
    "                       x=\"date\", \n",
    "                       y=\"task\", \n",
    "                       color=\"task\", \n",
    "                       )\n",
    "    \n",
    "   \n",
    "                                                            \n",
    "    #adding anchors on both cases...\n",
    "    #This function is declared to add the anchor dots (white dots) to the trajectory.\n",
    "    def add_white(category,anchors_to_add,average_summary_IN,fig_traj):\n",
    "        \n",
    "        #select anchors that belong to selected trajectories\n",
    "        \n",
    "        fig_traj.add_trace(\n",
    "            go.Scatter(\n",
    "                x=anchors_to_add[\"date\"],\n",
    "                y=anchors_to_add[category],\n",
    "                #facet_row=\"task\",\n",
    "                #facet_row_spacing=0.009, \n",
    "                mode=\"markers\",\n",
    "                name=None,\n",
    "                marker=dict(\n",
    "                    symbol=42, \n",
    "                    size=21,\n",
    "                    line=dict(\n",
    "                        width=2\n",
    "                    ),\n",
    "                    \n",
    "                ),\n",
    "                \n",
    "                hovertemplate=\n",
    "                \"<BR>task: \"\n",
    "                + anchors_to_add[category]\n",
    "                + \"<BR>date: \"\n",
    "                + anchors_to_add[\"date\"].astype(\"string\")\n",
    "                + \"<BR>Anchor.\"\n",
    "                + \"<BR>benchmarks:<BR>\"\n",
    "                + anchors_to_add[\"ds\"].astype(\"string\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    average_summary_IN = average_summary_IN.sort_values(by=\"date\")\n",
    "    add_white(\"task\",average_summary_anchors,average_summary_IN,fig_traj) # it seems wrong with average_summary_IN being from where the anchors are taken...\n",
    "    fig_traj.add_trace(\n",
    "        go.Scatter(\n",
    "            x=average_summary_IN[\"date\"],\n",
    "            y=average_summary_IN[\"task\"],\n",
    "            #facet_row=\"task\",\n",
    "            #facet_row_spacing=0.009, \n",
    "            mode=\"markers\",\n",
    "            name=None,\n",
    "            hovertemplate=\n",
    "            \"<BR>task: \"\n",
    "            + average_summary_IN[\"task\"]\n",
    "            + \"<BR>date: \"\n",
    "            + average_summary_IN[\"date\"].astype(\"string\")\n",
    "            + \"<BR>ratio: \"\n",
    "            + average_summary_IN[\"ratio\"].astype(\"string\")\n",
    "            + \"<BR>benchmarks:<BR>\"\n",
    "            + average_summary_IN[\"ds\"].astype(\"string\"),\n",
    "            marker=dict(\n",
    "                size=20,  \n",
    "                symbol=\"circle\",  # https://plotly.com/python/marker-style/\n",
    "                opacity=0.7,  # alpha ratio\n",
    "                color=average_summary_IN[\"ratio\"],  # set color equal to a variable\n",
    "                colorscale=\"YlGn\",  # one of plotly colorscales\n",
    "                colorbar=dict(title=\"ratio\", lenmode=\"pixels\", len=500, thickness=10),\n",
    "                showscale=True,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "   \n",
    "    \n",
    "    fig_traj.update_traces(\n",
    "        marker=dict(line=dict(color=\"black\", width=1)),\n",
    "        line  =dict(width=0, color=\"black\")        \n",
    "    )\n",
    "\n",
    "    fig_traj.update_xaxes(showgrid=True, gridcolor=\"lightBlue\", title=\"Year\")\n",
    "    # title=ito+\": \"+class_label\n",
    "    fig_traj.update_yaxes(showgrid=True, gridcolor=\"lightBlue\", title=None)\n",
    "\n",
    "    \n",
    "    fig_traj.update_layout(\n",
    "        #title=\"Trajectory for ratio (task per year)\",\n",
    "        title_text=class_label,\n",
    "        showlegend=False,\n",
    "        font_size=21,\n",
    "        plot_bgcolor=\"white\",\n",
    "        height=rows*2.5,\n",
    "        width=1500,\n",
    "        xaxis=dict(\n",
    "            tickmode=\"auto\",\n",
    "        ),\n",
    "        yaxis={'side': 'left'}\n",
    "        \n",
    "    )  \n",
    "\n",
    "    \n",
    "    fig_traj.update_layout(\n",
    "        title={\n",
    "            'y':0.995,            \n",
    "            })\n",
    "\n",
    "    fig_traj.show()\n",
    "    fig_traj.write_html(f\"artefacts/{class_label.replace(' ', '_').lower()}_grp{('_single_arrow' if anchor else '')}.html\", include_plotlyjs=\"cdn\")\n",
    "    fig_traj.write_image(f\"artefacts/{class_label.replace(' ', '_').lower()}_grp{('_single_arrow' if anchor else '')}.png\", scale=2)\n",
    "    return(average_summary_IN)\n",
    "\n",
    "#plot_task_trajectory(\"ITO_00101\", \"Vision process\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 987
    },
    "id": "2NQ4c9-XW1ig",
    "outputId": "09c23855-00f0-4c8c-a85d-17d1d550712d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Computer code processing",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Computer code processing",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-08",
          "2018-03",
          "2018-04",
          "2018-10",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Computer code processing",
          "Computer code processing",
          "Computer code processing",
          "Computer code processing",
          "Computer code processing"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Dialog process",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Dialog process",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-09",
          "2017-11",
          "2018-05",
          "2018-09",
          "2018-10",
          "2019-02",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Dialog process",
          "Dialog process",
          "Dialog process",
          "Dialog process",
          "Dialog process",
          "Dialog process",
          "Dialog process"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Inference and reasoning",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Inference and reasoning",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-08",
          "2015-08",
          "2016-09",
          "2017-02",
          "2017-09",
          "2017-11",
          "2017-12",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-09",
          "2018-10",
          "2019-01",
          "2019-02",
          "2019-06",
          "2019-07",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Information extraction",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Information extraction",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-01",
          "2017-07",
          "2017-09",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-11",
          "2020-03",
          "2020-04"
         ],
         "xaxis": "x",
         "y": [
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Information retrieval",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Information retrieval",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-03",
          "2019-01",
          "2019-04",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Information retrieval",
          "Information retrieval",
          "Information retrieval",
          "Information retrieval"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Machine translation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Machine translation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-10",
          "2019-01",
          "2019-02",
          "2019-05"
         ],
         "xaxis": "x",
         "y": [
          "Machine translation",
          "Machine translation",
          "Machine translation",
          "Machine translation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Natural language generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Natural language generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-09",
          "2014-10",
          "2014-12",
          "2015-09",
          "2016-02",
          "2016-03",
          "2016-06",
          "2016-07",
          "2016-08",
          "2016-09",
          "2016-10",
          "2016-11",
          "2016-12",
          "2017-01",
          "2017-05",
          "2017-06",
          "2017-09",
          "2017-11",
          "2018-02",
          "2018-03",
          "2018-06",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2019-01",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-08",
          "2019-09",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Text-to-image Generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Text-to-image Generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-12",
          "2017-10",
          "2017-11",
          "2019-01",
          "2019-03",
          "2019-04",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Text-to-image Generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pragmatics analysis",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pragmatics analysis",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2013-10",
          "2014-06",
          "2014-08",
          "2015-02",
          "2015-06",
          "2015-11",
          "2016-02",
          "2016-06",
          "2016-07",
          "2016-09",
          "2017-02",
          "2017-04",
          "2017-07",
          "2017-08",
          "2017-09",
          "2017-12",
          "2018-01",
          "2018-02",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-10",
          "2018-11",
          "2019-01",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-12"
         ],
         "xaxis": "x",
         "y": [
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Question answering",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Question answering",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-06",
          "2014-12",
          "2015-06",
          "2015-11",
          "2016-02",
          "2016-03",
          "2016-05",
          "2016-06",
          "2016-08",
          "2016-09",
          "2016-10",
          "2016-11",
          "2016-12",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-06",
          "2017-07",
          "2017-08",
          "2017-10",
          "2017-12",
          "2018-01",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2018-11",
          "2019-01",
          "2019-02",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2020-02"
         ],
         "xaxis": "x",
         "y": [
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic analysis",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic analysis",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-03",
          "2016-06",
          "2017-04",
          "2017-05",
          "2017-09",
          "2017-12",
          "2018-02",
          "2018-03",
          "2018-05",
          "2018-10",
          "2018-11",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Sentence embedding",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Sentence embedding",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-07",
          "2018-07"
         ],
         "xaxis": "x",
         "y": [
          "Sentence embedding",
          "Sentence embedding"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Syntactic analysis",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Syntactic analysis",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-03",
          "2016-07",
          "2016-11",
          "2017-04",
          "2017-07",
          "2018-05",
          "2018-07",
          "2018-08",
          "2018-10",
          "2018-11",
          "2019-03",
          "2019-04",
          "2019-06",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Text classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Text classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-11",
          "2016-03",
          "2016-06",
          "2016-09",
          "2016-11",
          "2017-02",
          "2017-10",
          "2018-01",
          "2018-02",
          "2018-03",
          "2018-05",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-08",
          "2019-09",
          "2020-02"
         ],
         "xaxis": "x",
         "y": [
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": [
          "<BR>task: Computer code processing<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>",
          "<BR>task: Computer code processing<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: 100 sleep nights of 8 caregivers - Code Generation benchmarking - 14 gestures accuracy<BR>",
          "<BR>task: Computer code processing<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>",
          "<BR>task: Computer code processing<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Android Repos - Code Generation benchmarking - Perplexity<BR>",
          "<BR>task: Computer code processing<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: CoNaLa - Code Generation benchmarking - BLEU<BR>  Code Generation: CoNaLa-Ext - Code Generation benchmarking - BLEU<BR>",
          "<BR>task: Dialog process<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>",
          "<BR>task: Dialog process<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - Mean Rank<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Dialog process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Area<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Food<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Price<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>",
          "<BR>task: Dialog process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - NDCG (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - Parameters<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind dev - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - EM<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - F1<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: Quora Question Pairs - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A2<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A3<BR>  Natural Language Inference: WNLI - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - Recall-at-10<BR>",
          "<BR>task: Information extraction<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>",
          "<BR>task: Information extraction<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>",
          "<BR>task: Information extraction<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1 (surface form)<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Wikipedia-Wikidata relations - Relation Extraction benchmarking - Error rate<BR>",
          "<BR>task: Information extraction<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>",
          "<BR>task: Information extraction<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: OntoNotes 4 - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Weibo NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: CoNLL 2000 - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>",
          "<BR>task: Information extraction<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: SighanNER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: JNLPBA - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: JNLPBA - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WLPC - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WetLab - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: WLPC - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - F1<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Precision<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Recall<BR>",
          "<BR>task: Information extraction<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: LINNAEUS - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: Species-800 - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Code-Switching English-Spanish NER - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ontontoes chinese v5 - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT24 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT29 - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Precision<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Recall<BR>",
          "<BR>task: Information retrieval<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>",
          "<BR>task: Information retrieval<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>",
          "<BR>task: Information retrieval<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-10<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-1<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R@50<BR>",
          "<BR>task: Information retrieval<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI OpenSubtitles - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>",
          "<BR>task: Machine translation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Machine translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>",
          "<BR>task: Natural language generation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - Accuracy<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-Russian - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>",
          "<BR>task: Natural language generation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>  Language Modelling: Text8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 Thai-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Czech-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Russian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Russian-English - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: Hutter Prize - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Validation perplexity<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - KL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - NLL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>  Text Summarization: Pubmed - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: arXiv - Text Summarization benchmarking - ROUGE-1<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - 1-of-100 Accuracy<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Estonian-Russian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Russian-Estonian - Machine Translation benchmarking - BLEU<BR>  Text Generation: LDC2016E25 - Text Generation benchmarking - BLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: LAMBADA - Language Modelling benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - SacreBLEU<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - SacreBLEU<BR>  Question Generation: Visual Question Generation - Question Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-2<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-3<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-4<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 French-English - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT 2017 English-Latvian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Estonian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Finnish - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Estonian-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: The Pile - Language Modelling benchmarking - Bits per byte<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2016 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2017 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2019 Finnish-English - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - SacreBLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2017 Arabic-English - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-Arabic - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-French - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 French-English - Machine Translation benchmarking - Cased sacreBLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: PTB - Language Modelling benchmarking - PPL<BR>  Machine Translation: IWSLT2015 Chinese-English - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - FID<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - LPIPS<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Real<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Average<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Yelp Binary classification - Sentiment Analysis benchmarking - Error<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Sogou News - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: SemEval - Sentiment Analysis benchmarking - F1-score<BR>  Sentiment Analysis: SemEval 2017 Task 4-A - Sentiment Analysis benchmarking - Average Recall<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Arousal)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: CR - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: 2017_test set - Paraphrase Identification benchmarking - 10 fold Cross validation<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Intent Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Twitter - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - F1<BR>  Sentiment Analysis: ChnSentiCorp - Sentiment Analysis benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Bias (F/M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Feminine F1 (F)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Masculine F1 (M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Overall F1<BR>  Sentiment Analysis: ASTD - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: ArSAS - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - MSE<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - R^2<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - F1 score<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: DailyDialog - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>  Emotion Recognition in Conversation: EmoryNLP - Emotion Recognition in Conversation benchmarking - Weighted Macro-F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Reverb - Question Answering benchmarking - Accuracy<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>",
          "<BR>task: Question answering<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>",
          "<BR>task: Question answering<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>",
          "<BR>task: Question answering<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: SimpleQuestions - Question Answering benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>",
          "<BR>task: Question answering<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: MCTest-160 - Question Answering benchmarking - Accuracy<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>",
          "<BR>task: Question answering<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>",
          "<BR>task: Question answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Question answering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>",
          "<BR>task: Question answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: COMPLEXQUESTIONS - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>",
          "<BR>task: Question answering<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>",
          "<BR>task: Question answering<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>",
          "<BR>task: Question answering<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>",
          "<BR>task: Question answering<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: CLEVR - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Question answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QuAC - Question Answering benchmarking - F1<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQD<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQQ<BR>  Visual Question Answering: 100 sleep nights of 8 caregivers - Visual Question Answering benchmarking - 14 gestures accuracy<BR>  Visual Question Answering: HowmanyQA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TallyQA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: JD Product Question Answer - Question Answering benchmarking - BLEU<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Long)<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Short)<BR>",
          "<BR>task: Question answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TDIUC - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CODAH - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Question answering<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: HotpotQA - Question Answering benchmarking - JOINT-F1<BR>",
          "<BR>task: Question answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NaturalQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-dev - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - number<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - other<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - unanswerable<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - yes/no<BR>",
          "<BR>task: Question answering<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SCDE - Question Answering benchmarking - BA<BR>  Question Answering: SCDE - Question Answering benchmarking - DE<BR>  Question Answering: SCDE - Question Answering benchmarking - PA<BR>",
          "<BR>task: Semantic analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>",
          "<BR>task: Semantic analysis<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - All<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: Geo - Semantic Parsing benchmarking - Accuracy<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: spider - Semantic Parsing benchmarking - Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: WikiSQL - Semantic Parsing benchmarking - Accuracy<BR>",
          "<BR>task: Sentence embedding<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - UAS<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Dependency Parsing: GENIA - LAS - Dependency Parsing benchmarking - F1<BR>  Dependency Parsing: GENIA - UAS - Dependency Parsing benchmarking - F1<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: JFLEG - Grammatical Error Detection benchmarking - F0.5<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>",
          "<BR>task: Text classification<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters De-En - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters En-De - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>",
          "<BR>task: Text classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>  Text Classification: DBpedia - Text Classification benchmarking - Error<BR>",
          "<BR>task: Text classification<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: RCV1 - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-50 - Text Classification benchmarking - Error<BR>",
          "<BR>task: Text classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: WOS-11967 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-46985 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-5736 - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: LOCAL DATASET - Text Classification benchmarking - Accuracy (%)<BR>",
          "<BR>task: Text classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>",
          "<BR>task: Text classification<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: PubMed 20k RCT - Sentence Classification benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-5 - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: SciCite - Citation Intent Classification benchmarking - F1<BR>  Sentence Classification: Paper Field - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: AAPD - Document Classification benchmarking - F1<BR>  Document Classification: Amazon - Document Classification benchmarking - Accuracy<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Classic - Document Classification benchmarking - Accuracy<BR>  Document Classification: Recipe - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>  Document Classification: Twitter - Document Classification benchmarking - Accuracy<BR>  Document Classification: Yelp-14 - Document Classification benchmarking - Accuracy<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (10 classes)<BR>",
          "<BR>task: Text classification<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: IMDb-M - Document Classification benchmarking - Accuracy<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-5<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-5<BR>",
          "<BR>task: Text classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: MPQA - Document Classification benchmarking - Accuracy<BR>  Text Classification: RCV1 - Text Classification benchmarking - Macro F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>",
          "<BR>task: Text classification<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Precision<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Recall<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 21,
          "symbol": 42
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2016-03",
          "2017-05",
          "2017-08",
          "2018-05",
          "2018-10",
          "2016-03",
          "2016-05",
          "2016-06",
          "2017-04",
          "2014-04",
          "2015-08",
          "2017-12",
          "2018-03",
          "2018-05",
          "2018-06",
          "2018-08",
          "2018-09",
          "2018-10",
          "2018-11",
          "2018-12",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-09",
          "2014-06",
          "2014-09",
          "2014-10",
          "2016-08",
          "2017-06",
          "2017-09",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-08",
          "2018-09",
          "2018-10",
          "2019-01",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-08",
          "2019-09",
          "2019-11",
          "2020-03",
          "2018-02",
          "2018-12",
          "2019-01",
          "2019-04",
          "2018-04",
          "2019-01",
          "2013-12",
          "2014-06",
          "2014-09",
          "2015-08",
          "2015-09",
          "2015-12",
          "2016-02",
          "2016-06",
          "2016-07",
          "2016-09",
          "2016-11",
          "2016-12",
          "2017-02",
          "2017-04",
          "2017-05",
          "2017-09",
          "2017-11",
          "2018-03",
          "2018-05",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2019-01",
          "2019-02",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-10",
          "2019-11",
          "2016-10",
          "2016-12",
          "2017-10",
          "2017-11",
          "2013-10",
          "2014-12",
          "2015-05",
          "2015-09",
          "2016-04",
          "2016-07",
          "2017-02",
          "2017-04",
          "2017-07",
          "2017-12",
          "2018-03",
          "2018-06",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-06",
          "2019-08",
          "2019-09",
          "2019-12",
          "2014-04",
          "2014-05",
          "2014-12",
          "2015-03",
          "2015-06",
          "2015-11",
          "2016-02",
          "2016-03",
          "2016-06",
          "2016-08",
          "2016-11",
          "2016-12",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-07",
          "2017-08",
          "2017-10",
          "2017-11",
          "2018-03",
          "2018-05",
          "2018-08",
          "2018-10",
          "2019-01",
          "2019-02",
          "2019-04",
          "2019-05",
          "2019-07",
          "2019-08",
          "2020-04",
          "2013-10",
          "2014-11",
          "2015-05",
          "2016-01",
          "2016-03",
          "2017-04",
          "2017-05",
          "2017-07",
          "2017-09",
          "2018-01",
          "2018-02",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-09",
          "2019-09",
          "2019-10",
          "2015-09",
          "2014-12",
          "2015-06",
          "2016-07",
          "2016-08",
          "2016-11",
          "2017-11",
          "2018-08",
          "2018-10",
          "2018-11",
          "2019-01",
          "2013-06",
          "2014-03",
          "2014-05",
          "2014-10",
          "2015-04",
          "2015-09",
          "2016-02",
          "2016-07",
          "2016-12",
          "2017-07",
          "2017-09",
          "2018-01",
          "2018-05",
          "2018-06",
          "2018-08",
          "2018-09",
          "2018-10",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-08",
          "2019-11"
         ],
         "y": [
          "Computer code processing",
          "Computer code processing",
          "Computer code processing",
          "Computer code processing",
          "Computer code processing",
          "Dialog process",
          "Dialog process",
          "Dialog process",
          "Dialog process",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Inference and reasoning",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information extraction",
          "Information retrieval",
          "Information retrieval",
          "Information retrieval",
          "Information retrieval",
          "Machine translation",
          "Machine translation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Natural language generation",
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Text-to-image Generation",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Question answering",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Semantic analysis",
          "Sentence embedding",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Syntactic analysis",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification",
          "Text classification"
         ]
        },
        {
         "hovertemplate": [
          "<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>ratio: 0.5551<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2014-06<BR>ratio: 0.3305<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2014-06<BR>ratio: 0.76<BR>benchmarks:<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2014-08<BR>ratio: 0.2549<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2014-08<BR>ratio: 0.2308<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2014-09<BR>ratio: 0.0886<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2014-10<BR>ratio: 0.0904<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Question answering<BR>date: 2014-12<BR>ratio: 0.3032<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>",
          "<BR>task: Natural language generation<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2015-02<BR>ratio: 0.1186<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2015-06<BR>ratio: 0.0427<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2015-06<BR>ratio: 0.232<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2015-08<BR>ratio: 0.5<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2015-09<BR>ratio: 0.1435<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2015-11<BR>ratio: 0.4025<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>",
          "<BR>task: Text classification<BR>date: 2015-11<BR>ratio: 0.25<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>",
          "<BR>task: Question answering<BR>date: 2015-11<BR>ratio: 0.2159<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>",
          "<BR>task: Information extraction<BR>date: 2016-01<BR>ratio: 0.483<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2016-02<BR>ratio: 0.3491<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-02<BR>ratio: 0.3173<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>",
          "<BR>task: Question answering<BR>date: 2016-02<BR>ratio: 0.0718<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>",
          "<BR>task: Text classification<BR>date: 2016-03<BR>ratio: 0.5215<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2016-03<BR>ratio: 0.1966<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-03<BR>ratio: 0.2<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Semantic analysis<BR>date: 2016-03<BR>ratio: 0.1323<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2016-03<BR>ratio: 0.40459999999999996<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2016-06<BR>ratio: 0.0678<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-06<BR>ratio: 0.30955<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2016-06<BR>ratio: 0.0702<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2016-06<BR>ratio: 0.7941<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2016-06<BR>ratio: 0.3097<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2016-07<BR>ratio: 0.9481<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>ratio: 0.094<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-07<BR>ratio: 0.218<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Question answering<BR>date: 2016-08<BR>ratio: 0.3589<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-08<BR>ratio: 0.1882<BR>benchmarks:<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-09<BR>ratio: 0.36160000000000003<BR>benchmarks:<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2016-09<BR>ratio: 0.0286<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2016-09<BR>ratio: 0.3558<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2016-09<BR>ratio: 0.101<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2016-09<BR>ratio: 0.098<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2016-10<BR>ratio: 0.6069<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-10<BR>ratio: 0.8<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Question answering<BR>date: 2016-11<BR>ratio: 0.29685<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2016-11<BR>ratio: 0.38507499999999995<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-11<BR>ratio: 0.0632<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Text classification<BR>date: 2016-11<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2016-12<BR>ratio: 0.6667<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2016-12<BR>ratio: 0.0708<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Question answering<BR>date: 2016-12<BR>ratio: 0.7801<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-01<BR>ratio: 0.0597<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Text classification<BR>date: 2017-02<BR>ratio: 0.2857<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2017-02<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>ratio: 0.1937<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>",
          "<BR>task: Question answering<BR>date: 2017-03<BR>ratio: 0.033<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2017-04<BR>ratio: 0.6016<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-04<BR>ratio: 0.55<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>ratio: 0.14375<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2017-04<BR>ratio: 0.06<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-05<BR>ratio: 0.921<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-05<BR>ratio: 0.2783333333333333<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>",
          "<BR>task: Question answering<BR>date: 2017-05<BR>ratio: 0.2379<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-06<BR>ratio: 0.42015<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>",
          "<BR>task: Question answering<BR>date: 2017-06<BR>ratio: 0.1524<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2017-07<BR>ratio: 0.30775<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>",
          "<BR>task: Question answering<BR>date: 2017-07<BR>ratio: 0.22825<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Information extraction<BR>date: 2017-07<BR>ratio: 0.212<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>",
          "<BR>task: Sentence embedding<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>ratio: 0.57325<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2017-08<BR>ratio: 0.4228<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-08<BR>ratio: 0.2288<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Computer code processing<BR>date: 2017-08<BR>ratio: 0.4269<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-09<BR>ratio: 0.0407<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-09<BR>ratio: 0.3099<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>",
          "<BR>task: Dialog process<BR>date: 2017-09<BR>ratio: 0.56715<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2017-09<BR>ratio: 0.0196<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-09<BR>ratio: 0.6226<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>",
          "<BR>task: Information extraction<BR>date: 2017-09<BR>ratio: 0.4385<BR>benchmarks:<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2017-10<BR>ratio: 0.7021<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Text classification<BR>date: 2017-10<BR>ratio: 0.0798<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2017-10<BR>ratio: 0.6272<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2017-11<BR>ratio: 0.6346<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Dialog process<BR>date: 2017-11<BR>ratio: 0.25695<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2017-11<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2017-11<BR>ratio: 0.0459<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Question answering<BR>date: 2017-12<BR>ratio: 0.0571<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2017-12<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2017-12<BR>ratio: 0.1887<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>ratio: 0.54575<BR>benchmarks:<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-01<BR>ratio: 0.0809<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2018-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2018-01<BR>ratio: 0.4672<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-02<BR>ratio: 0.19115<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>",
          "<BR>task: Text classification<BR>date: 2018-02<BR>ratio: 0.1176<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-02<BR>ratio: 0.2873<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-02<BR>ratio: 0.6357<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>",
          "<BR>task: Question answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-03<BR>ratio: 0.5076<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>",
          "<BR>task: Information retrieval<BR>date: 2018-03<BR>ratio: 0.541<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2018-03<BR>ratio: 0.0357<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>",
          "<BR>task: Computer code processing<BR>date: 2018-03<BR>ratio: 0.2009<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2018-04<BR>ratio: 0.5921<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-04<BR>ratio: 0.0412<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-04<BR>ratio: 0.2342<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>",
          "<BR>task: Computer code processing<BR>date: 2018-04<BR>ratio: 0.137<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-05<BR>ratio: 0.2896<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-05<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>",
          "<BR>task: Dialog process<BR>date: 2018-05<BR>ratio: 0.8365<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-05<BR>ratio: 0.57845<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-05<BR>ratio: 0.11170000000000001<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>",
          "<BR>task: Question answering<BR>date: 2018-05<BR>ratio: 0.36539999999999995<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Text classification<BR>date: 2018-05<BR>ratio: 0.3299<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>",
          "<BR>task: Question answering<BR>date: 2018-06<BR>ratio: 0.3862<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-06<BR>ratio: 0.6132<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-06<BR>ratio: 0.11065<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-07<BR>ratio: 0.0553<BR>benchmarks:<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>",
          "<BR>task: Text classification<BR>date: 2018-07<BR>ratio: 0.5132<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Information extraction<BR>date: 2018-07<BR>ratio: 0.3122<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-07<BR>ratio: 0.0282<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>",
          "<BR>task: Sentence embedding<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-07<BR>ratio: 0.791<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>",
          "<BR>task: Question answering<BR>date: 2018-08<BR>ratio: 0.2718<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2018-08<BR>ratio: 0.72315<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-08<BR>ratio: 0.3277<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-08<BR>ratio: 0.4922<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>",
          "<BR>task: Text classification<BR>date: 2018-08<BR>ratio: 0.5092<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>",
          "<BR>task: Text classification<BR>date: 2018-09<BR>ratio: 0.652<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Dialog process<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-09<BR>ratio: 0.2353<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-09<BR>ratio: 0.53495<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Information extraction<BR>date: 2018-09<BR>ratio: 0.3185<BR>benchmarks:<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2018-09<BR>ratio: 0.2419<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>",
          "<BR>task: Question answering<BR>date: 2018-10<BR>ratio: 0.196<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-10<BR>ratio: 0.43885<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-10<BR>ratio: 0.2123<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2018-10<BR>ratio: 0.48510000000000003<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Dialog process<BR>date: 2018-10<BR>ratio: 0.327<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>",
          "<BR>task: Natural language generation<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Information extraction<BR>date: 2018-10<BR>ratio: 0.20085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>",
          "<BR>task: Machine translation<BR>date: 2018-10<BR>ratio: 0.15<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Text classification<BR>date: 2018-10<BR>ratio: 0.9057<BR>benchmarks:<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>",
          "<BR>task: Computer code processing<BR>date: 2018-10<BR>ratio: 0.5481<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-10<BR>ratio: 0.33775<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2018-11<BR>ratio: 0.2719<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2018-11<BR>ratio: 0.2698<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>",
          "<BR>task: Question answering<BR>date: 2018-11<BR>ratio: 0.1449<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2018-11<BR>ratio: 0.35325<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>",
          "<BR>task: Information extraction<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>",
          "<BR>task: Machine translation<BR>date: 2019-01<BR>ratio: 0.4127<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-01<BR>ratio: 0.47159999999999996<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>",
          "<BR>task: Information retrieval<BR>date: 2019-01<BR>ratio: 0.3558<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2019-01<BR>ratio: 0.6137<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>",
          "<BR>task: Question answering<BR>date: 2019-01<BR>ratio: 0.2041<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-01<BR>ratio: 0.3456<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Information extraction<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>ratio: 0.37765000000000004<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>",
          "<BR>task: Question answering<BR>date: 2019-02<BR>ratio: 0.6327<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Machine translation<BR>date: 2019-02<BR>ratio: 0.397<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>",
          "<BR>task: Information extraction<BR>date: 2019-02<BR>ratio: 0.3333<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Text classification<BR>date: 2019-02<BR>ratio: 0.3546<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Dialog process<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-03<BR>ratio: 0.3031<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2019-03<BR>ratio: 0.1016<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Text classification<BR>date: 2019-03<BR>ratio: 0.6981<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-03<BR>ratio: 0.833<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2019-03<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>",
          "<BR>task: Information retrieval<BR>date: 2019-04<BR>ratio: 0.2915<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>",
          "<BR>task: Dialog process<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-04<BR>ratio: 0.5763<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>",
          "<BR>task: Question answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Information extraction<BR>date: 2019-04<BR>ratio: 0.4341666666666666<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2019-04<BR>ratio: 0.5876<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2019-04<BR>ratio: 0.1967<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>",
          "<BR>task: Text classification<BR>date: 2019-04<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2019-05<BR>ratio: 0.4549<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Machine translation<BR>date: 2019-05<BR>ratio: 0.3157<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Question answering<BR>date: 2019-05<BR>ratio: 0.45885<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Semantic analysis<BR>date: 2019-05<BR>ratio: 0.575<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-05<BR>ratio: 0.40315<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-05<BR>ratio: 0.3432<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Information extraction<BR>date: 2019-05<BR>ratio: 0.2659<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-06<BR>ratio: 0.4636<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Information extraction<BR>date: 2019-06<BR>ratio: 0.2085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-06<BR>ratio: 0.5752<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2019-06<BR>ratio: 0.7168<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>ratio: 0.5772666666666666<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2019-06<BR>ratio: 0.4252<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>",
          "<BR>task: Question answering<BR>date: 2019-06<BR>ratio: 0.2683<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Text classification<BR>date: 2019-06<BR>ratio: 0.5346<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-07<BR>ratio: 0.3592<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>",
          "<BR>task: Question answering<BR>date: 2019-07<BR>ratio: 0.46240000000000003<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-07<BR>ratio: 0.2631<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Information extraction<BR>date: 2019-07<BR>ratio: 0.5883499999999999<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Semantic analysis<BR>date: 2019-07<BR>ratio: 0.0577<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>ratio: 0.18335<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>",
          "<BR>task: Information extraction<BR>date: 2019-08<BR>ratio: 0.4151<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-08<BR>ratio: 0.1081<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>",
          "<BR>task: Question answering<BR>date: 2019-08<BR>ratio: 0.23105<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Text classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>",
          "<BR>task: Syntactic analysis<BR>date: 2019-09<BR>ratio: 0.1429<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-09<BR>ratio: 0.2846<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>",
          "<BR>task: Information extraction<BR>date: 2019-09<BR>ratio: 0.60275<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>ratio: 0.02725<BR>benchmarks:<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>",
          "<BR>task: Text-to-image Generation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>",
          "<BR>task: Question answering<BR>date: 2019-09<BR>ratio: 0.09475<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Text classification<BR>date: 2019-09<BR>ratio: 0.8833<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>",
          "<BR>task: Inference and reasoning<BR>date: 2019-09<BR>ratio: 0.12815000000000001<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>",
          "<BR>task: Semantic analysis<BR>date: 2019-09<BR>ratio: 0.5651333333333334<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>",
          "<BR>task: Natural language generation<BR>date: 2019-10<BR>ratio: 0.438<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>",
          "<BR>task: Computer code processing<BR>date: 2019-10<BR>ratio: 0.1871<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>",
          "<BR>task: Information extraction<BR>date: 2019-11<BR>ratio: 0.32506666666666667<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information retrieval<BR>date: 2019-11<BR>ratio: 0.4096<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>",
          "<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>",
          "<BR>task: Question answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Text classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>",
          "<BR>task: Information extraction<BR>date: 2020-03<BR>ratio: 0.3066<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>",
          "<BR>task: Information extraction<BR>date: 2020-04<BR>ratio: 0.2667<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "color": [
           0.5551,
           0.3305,
           0.76,
           0.2549,
           0.2308,
           0.0886,
           0.0904,
           0.3032,
           1,
           0.1186,
           0.0427,
           0.232,
           0.5,
           0.1435,
           0.4025,
           0.25,
           0.2159,
           0.483,
           0.3491,
           0.3173,
           0.0718,
           0.5215,
           0.1966,
           0.2,
           0.1323,
           0.40459999999999996,
           0.2762,
           0.0678,
           0.30955,
           0.0702,
           0.7941,
           0.3097,
           0.9481,
           0.094,
           0.218,
           0.3589,
           0.1882,
           0.36160000000000003,
           0.0286,
           0.3558,
           0.101,
           0.098,
           0.6069,
           0.8,
           0.29685,
           0.38507499999999995,
           0.0632,
           0.0123,
           0.6667,
           0.0708,
           0.7801,
           0.0597,
           0.2857,
           0.0392,
           0.1937,
           0.033,
           0.6016,
           0.55,
           0.14375,
           0.06,
           0.921,
           0.2783333333333333,
           0.2379,
           0.42015,
           0.1524,
           0.30775,
           0.22825,
           0.212,
           1,
           0.57325,
           0.4228,
           0.2288,
           0.4269,
           0.0407,
           0.3099,
           0.56715,
           0.0196,
           0.6226,
           0.4385,
           0.7021,
           0.0798,
           0.6272,
           0.6346,
           0.25695,
           0.0392,
           0.0459,
           0.0571,
           0.0392,
           0.1887,
           0.54575,
           0.0809,
           0.0882,
           0.4672,
           0.19115,
           0.1176,
           0.2873,
           0.6357,
           0.1587,
           0.5076,
           1,
           0.541,
           0.0357,
           0.2009,
           0.5921,
           0.0412,
           0.2342,
           0.137,
           0.2896,
           0.1343,
           0.8365,
           0.57845,
           0.11170000000000001,
           0.36539999999999995,
           0.3299,
           0.7567,
           0.3862,
           0.6132,
           0.11065,
           0.0553,
           0.5132,
           0.3122,
           0.0282,
           1,
           1,
           0.791,
           0.2718,
           0.72315,
           0.3277,
           0.4922,
           0.5092,
           0.652,
           0.1442,
           0.2353,
           0.53495,
           0.3185,
           0.2419,
           0.196,
           0.43885,
           0.2123,
           0.48510000000000003,
           0.327,
           1,
           0.20085,
           0.15,
           0.9057,
           0.5481,
           0.33775,
           0.2719,
           0.2698,
           0.1449,
           0.35325,
           1,
           0.4127,
           0.47159999999999996,
           0.3558,
           0.6137,
           1,
           0.2041,
           0.3456,
           1,
           0.37765000000000004,
           0.6327,
           0.397,
           1,
           0.3333,
           0.3546,
           0.1426,
           0.3031,
           0.1016,
           0.6981,
           0.833,
           0.1343,
           0.2915,
           0.4213,
           0.5763,
           0.3147,
           0.4341666666666666,
           0.5876,
           1,
           0.1967,
           0.0123,
           0.4549,
           0.3157,
           0.45885,
           0.575,
           0.40315,
           0.3432,
           0.2659,
           0.4636,
           0.2085,
           0.5752,
           0.7168,
           0.5772666666666666,
           0.4252,
           0.2683,
           0.5346,
           0.3592,
           0.46240000000000003,
           0.2631,
           0.5883499999999999,
           0.0577,
           0.18335,
           0.4151,
           0.1081,
           0.23105,
           1,
           0.1429,
           0.2846,
           0.60275,
           0.02725,
           1,
           0.09475,
           0.8833,
           0.12815000000000001,
           0.5651333333333334,
           0.438,
           0.1871,
           0.32506666666666667,
           0.4096,
           1,
           0.5266,
           1,
           0.3066,
           0.2667
          ],
          "colorbar": {
           "len": 500,
           "lenmode": "pixels",
           "thickness": 10,
           "title": {
            "text": "ratio"
           }
          },
          "colorscale": [
           [
            0,
            "rgb(255,255,229)"
           ],
           [
            0.125,
            "rgb(247,252,185)"
           ],
           [
            0.25,
            "rgb(217,240,163)"
           ],
           [
            0.375,
            "rgb(173,221,142)"
           ],
           [
            0.5,
            "rgb(120,198,121)"
           ],
           [
            0.625,
            "rgb(65,171,93)"
           ],
           [
            0.75,
            "rgb(35,132,67)"
           ],
           [
            0.875,
            "rgb(0,104,55)"
           ],
           [
            1,
            "rgb(0,69,41)"
           ]
          ],
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "showscale": true,
          "size": 20,
          "symbol": "circle"
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2013-10",
          "2014-06",
          "2014-06",
          "2014-08",
          "2014-08",
          "2014-09",
          "2014-10",
          "2014-12",
          "2014-12",
          "2015-02",
          "2015-06",
          "2015-06",
          "2015-08",
          "2015-09",
          "2015-11",
          "2015-11",
          "2015-11",
          "2016-01",
          "2016-02",
          "2016-02",
          "2016-02",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-05",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-07",
          "2016-07",
          "2016-07",
          "2016-08",
          "2016-08",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-10",
          "2016-10",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-12",
          "2016-12",
          "2016-12",
          "2017-01",
          "2017-02",
          "2017-02",
          "2017-02",
          "2017-03",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-06",
          "2017-06",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-10",
          "2019-10",
          "2019-11",
          "2019-11",
          "2019-12",
          "2020-02",
          "2020-02",
          "2020-03",
          "2020-04"
         ],
         "y": [
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Question answering",
          "Inference and reasoning",
          "Pragmatics analysis",
          "Natural language generation",
          "Natural language generation",
          "Question answering",
          "Natural language generation",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Question answering",
          "Inference and reasoning",
          "Natural language generation",
          "Pragmatics analysis",
          "Text classification",
          "Question answering",
          "Information extraction",
          "Pragmatics analysis",
          "Natural language generation",
          "Question answering",
          "Text classification",
          "Syntactic analysis",
          "Natural language generation",
          "Semantic analysis",
          "Question answering",
          "Question answering",
          "Semantic analysis",
          "Natural language generation",
          "Pragmatics analysis",
          "Text classification",
          "Question answering",
          "Syntactic analysis",
          "Pragmatics analysis",
          "Natural language generation",
          "Question answering",
          "Natural language generation",
          "Natural language generation",
          "Pragmatics analysis",
          "Text classification",
          "Question answering",
          "Inference and reasoning",
          "Question answering",
          "Natural language generation",
          "Question answering",
          "Syntactic analysis",
          "Natural language generation",
          "Text classification",
          "Natural language generation",
          "Text-to-image Generation",
          "Question answering",
          "Natural language generation",
          "Text classification",
          "Inference and reasoning",
          "Pragmatics analysis",
          "Question answering",
          "Syntactic analysis",
          "Semantic analysis",
          "Pragmatics analysis",
          "Question answering",
          "Semantic analysis",
          "Natural language generation",
          "Question answering",
          "Natural language generation",
          "Question answering",
          "Syntactic analysis",
          "Question answering",
          "Information extraction",
          "Sentence embedding",
          "Pragmatics analysis",
          "Question answering",
          "Pragmatics analysis",
          "Computer code processing",
          "Semantic analysis",
          "Pragmatics analysis",
          "Dialog process",
          "Inference and reasoning",
          "Natural language generation",
          "Information extraction",
          "Text-to-image Generation",
          "Text classification",
          "Question answering",
          "Text-to-image Generation",
          "Dialog process",
          "Inference and reasoning",
          "Natural language generation",
          "Question answering",
          "Inference and reasoning",
          "Semantic analysis",
          "Pragmatics analysis",
          "Pragmatics analysis",
          "Text classification",
          "Question answering",
          "Semantic analysis",
          "Text classification",
          "Pragmatics analysis",
          "Natural language generation",
          "Question answering",
          "Natural language generation",
          "Semantic analysis",
          "Information retrieval",
          "Text classification",
          "Computer code processing",
          "Question answering",
          "Inference and reasoning",
          "Pragmatics analysis",
          "Computer code processing",
          "Pragmatics analysis",
          "Syntactic analysis",
          "Dialog process",
          "Inference and reasoning",
          "Semantic analysis",
          "Question answering",
          "Text classification",
          "Pragmatics analysis",
          "Question answering",
          "Inference and reasoning",
          "Natural language generation",
          "Natural language generation",
          "Text classification",
          "Information extraction",
          "Pragmatics analysis",
          "Sentence embedding",
          "Question answering",
          "Syntactic analysis",
          "Question answering",
          "Information extraction",
          "Natural language generation",
          "Syntactic analysis",
          "Text classification",
          "Text classification",
          "Dialog process",
          "Inference and reasoning",
          "Natural language generation",
          "Information extraction",
          "Question answering",
          "Question answering",
          "Semantic analysis",
          "Syntactic analysis",
          "Inference and reasoning",
          "Dialog process",
          "Natural language generation",
          "Information extraction",
          "Machine translation",
          "Text classification",
          "Computer code processing",
          "Pragmatics analysis",
          "Semantic analysis",
          "Syntactic analysis",
          "Question answering",
          "Pragmatics analysis",
          "Information extraction",
          "Machine translation",
          "Natural language generation",
          "Information retrieval",
          "Text classification",
          "Text-to-image Generation",
          "Question answering",
          "Inference and reasoning",
          "Information extraction",
          "Pragmatics analysis",
          "Question answering",
          "Machine translation",
          "Inference and reasoning",
          "Information extraction",
          "Text classification",
          "Dialog process",
          "Natural language generation",
          "Text-to-image Generation",
          "Text classification",
          "Information extraction",
          "Syntactic analysis",
          "Information retrieval",
          "Dialog process",
          "Natural language generation",
          "Question answering",
          "Information extraction",
          "Text-to-image Generation",
          "Pragmatics analysis",
          "Syntactic analysis",
          "Text classification",
          "Text classification",
          "Machine translation",
          "Question answering",
          "Semantic analysis",
          "Natural language generation",
          "Pragmatics analysis",
          "Information extraction",
          "Inference and reasoning",
          "Information extraction",
          "Natural language generation",
          "Syntactic analysis",
          "Pragmatics analysis",
          "Semantic analysis",
          "Question answering",
          "Text classification",
          "Inference and reasoning",
          "Question answering",
          "Pragmatics analysis",
          "Information extraction",
          "Semantic analysis",
          "Pragmatics analysis",
          "Information extraction",
          "Natural language generation",
          "Question answering",
          "Text classification",
          "Syntactic analysis",
          "Natural language generation",
          "Information extraction",
          "Pragmatics analysis",
          "Text-to-image Generation",
          "Question answering",
          "Text classification",
          "Inference and reasoning",
          "Semantic analysis",
          "Natural language generation",
          "Computer code processing",
          "Information extraction",
          "Information retrieval",
          "Pragmatics analysis",
          "Question answering",
          "Text classification",
          "Information extraction",
          "Information extraction"
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 21
        },
        "height": 700,
        "legend": {
         "title": {
          "text": "task"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Natural Language Processing",
         "y": 0.995
        },
        "width": 1500,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "tickmode": "auto",
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "categoryarray": [
          "Text classification",
          "Syntactic analysis",
          "Sentence embedding",
          "Semantic analysis",
          "Question answering",
          "Pragmatics analysis",
          "Text-to-image Generation",
          "Natural language generation",
          "Machine translation",
          "Information retrieval",
          "Information extraction",
          "Inference and reasoning",
          "Dialog process",
          "Computer code processing"
         ],
         "categoryorder": "array",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "side": "left",
         "title": {}
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ea0e8b58-d120-4141-9630-7800e8fcdf46\" class=\"plotly-graph-div\" style=\"height:700.0px; width:1500px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ea0e8b58-d120-4141-9630-7800e8fcdf46\")) {                    Plotly.newPlot(                        \"ea0e8b58-d120-4141-9630-7800e8fcdf46\",                        [{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Computer code processing\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Computer code processing\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-08\",\"2018-03\",\"2018-04\",\"2018-10\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Computer code processing\",\"Computer code processing\",\"Computer code processing\",\"Computer code processing\",\"Computer code processing\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Dialog process\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Dialog process\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-09\",\"2017-11\",\"2018-05\",\"2018-09\",\"2018-10\",\"2019-02\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Dialog process\",\"Dialog process\",\"Dialog process\",\"Dialog process\",\"Dialog process\",\"Dialog process\",\"Dialog process\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Inference and reasoning\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Inference and reasoning\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-08\",\"2015-08\",\"2016-09\",\"2017-02\",\"2017-09\",\"2017-11\",\"2017-12\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-09\",\"2018-10\",\"2019-01\",\"2019-02\",\"2019-06\",\"2019-07\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Information extraction\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Information extraction\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-01\",\"2017-07\",\"2017-09\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-11\",\"2020-03\",\"2020-04\"],\"xaxis\":\"x\",\"y\":[\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Information retrieval\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Information retrieval\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-03\",\"2019-01\",\"2019-04\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Information retrieval\",\"Information retrieval\",\"Information retrieval\",\"Information retrieval\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Machine translation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Machine translation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-10\",\"2019-01\",\"2019-02\",\"2019-05\"],\"xaxis\":\"x\",\"y\":[\"Machine translation\",\"Machine translation\",\"Machine translation\",\"Machine translation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Natural language generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Natural language generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-09\",\"2014-10\",\"2014-12\",\"2015-09\",\"2016-02\",\"2016-03\",\"2016-06\",\"2016-07\",\"2016-08\",\"2016-09\",\"2016-10\",\"2016-11\",\"2016-12\",\"2017-01\",\"2017-05\",\"2017-06\",\"2017-09\",\"2017-11\",\"2018-02\",\"2018-03\",\"2018-06\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2019-01\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-08\",\"2019-09\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Text-to-image Generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Text-to-image Generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-12\",\"2017-10\",\"2017-11\",\"2019-01\",\"2019-03\",\"2019-04\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Text-to-image Generation\",\"Text-to-image Generation\",\"Text-to-image Generation\",\"Text-to-image Generation\",\"Text-to-image Generation\",\"Text-to-image Generation\",\"Text-to-image Generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pragmatics analysis\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pragmatics analysis\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2013-10\",\"2014-06\",\"2014-08\",\"2015-02\",\"2015-06\",\"2015-11\",\"2016-02\",\"2016-06\",\"2016-07\",\"2016-09\",\"2017-02\",\"2017-04\",\"2017-07\",\"2017-08\",\"2017-09\",\"2017-12\",\"2018-01\",\"2018-02\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-10\",\"2018-11\",\"2019-01\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-12\"],\"xaxis\":\"x\",\"y\":[\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Question answering\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Question answering\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-06\",\"2014-12\",\"2015-06\",\"2015-11\",\"2016-02\",\"2016-03\",\"2016-05\",\"2016-06\",\"2016-08\",\"2016-09\",\"2016-10\",\"2016-11\",\"2016-12\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\",\"2017-07\",\"2017-08\",\"2017-10\",\"2017-12\",\"2018-01\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2018-11\",\"2019-01\",\"2019-02\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2020-02\"],\"xaxis\":\"x\",\"y\":[\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic analysis\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic analysis\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-03\",\"2016-06\",\"2017-04\",\"2017-05\",\"2017-09\",\"2017-12\",\"2018-02\",\"2018-03\",\"2018-05\",\"2018-10\",\"2018-11\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Sentence embedding\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Sentence embedding\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-07\",\"2018-07\"],\"xaxis\":\"x\",\"y\":[\"Sentence embedding\",\"Sentence embedding\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Syntactic analysis\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Syntactic analysis\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-03\",\"2016-07\",\"2016-11\",\"2017-04\",\"2017-07\",\"2018-05\",\"2018-07\",\"2018-08\",\"2018-10\",\"2018-11\",\"2019-03\",\"2019-04\",\"2019-06\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Text classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Text classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-11\",\"2016-03\",\"2016-06\",\"2016-09\",\"2016-11\",\"2017-02\",\"2017-10\",\"2018-01\",\"2018-02\",\"2018-03\",\"2018-05\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-08\",\"2019-09\",\"2020-02\"],\"xaxis\":\"x\",\"y\":[\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":[\"<BR>task: Computer code processing<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>\",\"<BR>task: Computer code processing<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: 100 sleep nights of 8 caregivers - Code Generation benchmarking - 14 gestures accuracy<BR>\",\"<BR>task: Computer code processing<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>\",\"<BR>task: Computer code processing<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Android Repos - Code Generation benchmarking - Perplexity<BR>\",\"<BR>task: Computer code processing<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: CoNaLa - Code Generation benchmarking - BLEU<BR>  Code Generation: CoNaLa-Ext - Code Generation benchmarking - BLEU<BR>\",\"<BR>task: Dialog process<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>\",\"<BR>task: Dialog process<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - Mean Rank<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Dialog process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Area<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Food<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Price<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>\",\"<BR>task: Dialog process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - NDCG (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - Parameters<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind dev - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - EM<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - F1<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: Quora Question Pairs - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A2<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A3<BR>  Natural Language Inference: WNLI - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - Recall-at-10<BR>\",\"<BR>task: Information extraction<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>\",\"<BR>task: Information extraction<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>\",\"<BR>task: Information extraction<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1 (surface form)<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Wikipedia-Wikidata relations - Relation Extraction benchmarking - Error rate<BR>\",\"<BR>task: Information extraction<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>\",\"<BR>task: Information extraction<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: OntoNotes 4 - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Weibo NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: CoNLL 2000 - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>\",\"<BR>task: Information extraction<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: SighanNER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: JNLPBA - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: JNLPBA - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WLPC - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WetLab - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: WLPC - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - F1<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Precision<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Recall<BR>\",\"<BR>task: Information extraction<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: LINNAEUS - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: Species-800 - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Code-Switching English-Spanish NER - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ontontoes chinese v5 - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT24 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT29 - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Precision<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Recall<BR>\",\"<BR>task: Information retrieval<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>\",\"<BR>task: Information retrieval<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>\",\"<BR>task: Information retrieval<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-10<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-1<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R@50<BR>\",\"<BR>task: Information retrieval<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI OpenSubtitles - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>\",\"<BR>task: Machine translation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Machine translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>\",\"<BR>task: Natural language generation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - Accuracy<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-Russian - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>\",\"<BR>task: Natural language generation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>  Language Modelling: Text8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 Thai-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Czech-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Russian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Russian-English - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: Hutter Prize - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Validation perplexity<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - KL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - NLL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>  Text Summarization: Pubmed - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: arXiv - Text Summarization benchmarking - ROUGE-1<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - 1-of-100 Accuracy<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Estonian-Russian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Russian-Estonian - Machine Translation benchmarking - BLEU<BR>  Text Generation: LDC2016E25 - Text Generation benchmarking - BLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: LAMBADA - Language Modelling benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - SacreBLEU<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - SacreBLEU<BR>  Question Generation: Visual Question Generation - Question Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-2<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-3<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-4<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 French-English - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT 2017 English-Latvian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Estonian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Finnish - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Estonian-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: The Pile - Language Modelling benchmarking - Bits per byte<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2016 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2017 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2019 Finnish-English - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - SacreBLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2017 Arabic-English - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-Arabic - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-French - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 French-English - Machine Translation benchmarking - Cased sacreBLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: PTB - Language Modelling benchmarking - PPL<BR>  Machine Translation: IWSLT2015 Chinese-English - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - FID<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - LPIPS<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Real<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Average<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Yelp Binary classification - Sentiment Analysis benchmarking - Error<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Sogou News - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: SemEval - Sentiment Analysis benchmarking - F1-score<BR>  Sentiment Analysis: SemEval 2017 Task 4-A - Sentiment Analysis benchmarking - Average Recall<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Arousal)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: CR - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: 2017_test set - Paraphrase Identification benchmarking - 10 fold Cross validation<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Intent Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Twitter - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - F1<BR>  Sentiment Analysis: ChnSentiCorp - Sentiment Analysis benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Bias (F/M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Feminine F1 (F)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Masculine F1 (M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Overall F1<BR>  Sentiment Analysis: ASTD - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: ArSAS - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - MSE<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - R^2<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - F1 score<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: DailyDialog - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>  Emotion Recognition in Conversation: EmoryNLP - Emotion Recognition in Conversation benchmarking - Weighted Macro-F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Reverb - Question Answering benchmarking - Accuracy<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>\",\"<BR>task: Question answering<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>\",\"<BR>task: Question answering<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>\",\"<BR>task: Question answering<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: SimpleQuestions - Question Answering benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>\",\"<BR>task: Question answering<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: MCTest-160 - Question Answering benchmarking - Accuracy<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>\",\"<BR>task: Question answering<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>\",\"<BR>task: Question answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Question answering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>\",\"<BR>task: Question answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: COMPLEXQUESTIONS - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>\",\"<BR>task: Question answering<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>\",\"<BR>task: Question answering<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>\",\"<BR>task: Question answering<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>\",\"<BR>task: Question answering<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: CLEVR - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Question answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QuAC - Question Answering benchmarking - F1<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQD<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQQ<BR>  Visual Question Answering: 100 sleep nights of 8 caregivers - Visual Question Answering benchmarking - 14 gestures accuracy<BR>  Visual Question Answering: HowmanyQA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TallyQA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: JD Product Question Answer - Question Answering benchmarking - BLEU<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Long)<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Short)<BR>\",\"<BR>task: Question answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TDIUC - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CODAH - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Question answering<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: HotpotQA - Question Answering benchmarking - JOINT-F1<BR>\",\"<BR>task: Question answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NaturalQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-dev - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - number<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - other<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - unanswerable<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - yes/no<BR>\",\"<BR>task: Question answering<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SCDE - Question Answering benchmarking - BA<BR>  Question Answering: SCDE - Question Answering benchmarking - DE<BR>  Question Answering: SCDE - Question Answering benchmarking - PA<BR>\",\"<BR>task: Semantic analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>\",\"<BR>task: Semantic analysis<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - All<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: Geo - Semantic Parsing benchmarking - Accuracy<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: spider - Semantic Parsing benchmarking - Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: WikiSQL - Semantic Parsing benchmarking - Accuracy<BR>\",\"<BR>task: Sentence embedding<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - UAS<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Dependency Parsing: GENIA - LAS - Dependency Parsing benchmarking - F1<BR>  Dependency Parsing: GENIA - UAS - Dependency Parsing benchmarking - F1<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: JFLEG - Grammatical Error Detection benchmarking - F0.5<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>\",\"<BR>task: Text classification<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters De-En - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters En-De - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>\",\"<BR>task: Text classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>  Text Classification: DBpedia - Text Classification benchmarking - Error<BR>\",\"<BR>task: Text classification<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: RCV1 - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-50 - Text Classification benchmarking - Error<BR>\",\"<BR>task: Text classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: WOS-11967 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-46985 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-5736 - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: LOCAL DATASET - Text Classification benchmarking - Accuracy (%)<BR>\",\"<BR>task: Text classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>\",\"<BR>task: Text classification<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: PubMed 20k RCT - Sentence Classification benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-5 - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: SciCite - Citation Intent Classification benchmarking - F1<BR>  Sentence Classification: Paper Field - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: AAPD - Document Classification benchmarking - F1<BR>  Document Classification: Amazon - Document Classification benchmarking - Accuracy<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Classic - Document Classification benchmarking - Accuracy<BR>  Document Classification: Recipe - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>  Document Classification: Twitter - Document Classification benchmarking - Accuracy<BR>  Document Classification: Yelp-14 - Document Classification benchmarking - Accuracy<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (10 classes)<BR>\",\"<BR>task: Text classification<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: IMDb-M - Document Classification benchmarking - Accuracy<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-5<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-5<BR>\",\"<BR>task: Text classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: MPQA - Document Classification benchmarking - Accuracy<BR>  Text Classification: RCV1 - Text Classification benchmarking - Macro F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>\",\"<BR>task: Text classification<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Precision<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Recall<BR>\"],\"marker\":{\"line\":{\"width\":1,\"color\":\"black\"},\"size\":21,\"symbol\":42},\"mode\":\"markers\",\"x\":[\"2016-03\",\"2017-05\",\"2017-08\",\"2018-05\",\"2018-10\",\"2016-03\",\"2016-05\",\"2016-06\",\"2017-04\",\"2014-04\",\"2015-08\",\"2017-12\",\"2018-03\",\"2018-05\",\"2018-06\",\"2018-08\",\"2018-09\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-09\",\"2014-06\",\"2014-09\",\"2014-10\",\"2016-08\",\"2017-06\",\"2017-09\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-08\",\"2018-09\",\"2018-10\",\"2019-01\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-08\",\"2019-09\",\"2019-11\",\"2020-03\",\"2018-02\",\"2018-12\",\"2019-01\",\"2019-04\",\"2018-04\",\"2019-01\",\"2013-12\",\"2014-06\",\"2014-09\",\"2015-08\",\"2015-09\",\"2015-12\",\"2016-02\",\"2016-06\",\"2016-07\",\"2016-09\",\"2016-11\",\"2016-12\",\"2017-02\",\"2017-04\",\"2017-05\",\"2017-09\",\"2017-11\",\"2018-03\",\"2018-05\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2019-01\",\"2019-02\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-10\",\"2019-11\",\"2016-10\",\"2016-12\",\"2017-10\",\"2017-11\",\"2013-10\",\"2014-12\",\"2015-05\",\"2015-09\",\"2016-04\",\"2016-07\",\"2017-02\",\"2017-04\",\"2017-07\",\"2017-12\",\"2018-03\",\"2018-06\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-06\",\"2019-08\",\"2019-09\",\"2019-12\",\"2014-04\",\"2014-05\",\"2014-12\",\"2015-03\",\"2015-06\",\"2015-11\",\"2016-02\",\"2016-03\",\"2016-06\",\"2016-08\",\"2016-11\",\"2016-12\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-07\",\"2017-08\",\"2017-10\",\"2017-11\",\"2018-03\",\"2018-05\",\"2018-08\",\"2018-10\",\"2019-01\",\"2019-02\",\"2019-04\",\"2019-05\",\"2019-07\",\"2019-08\",\"2020-04\",\"2013-10\",\"2014-11\",\"2015-05\",\"2016-01\",\"2016-03\",\"2017-04\",\"2017-05\",\"2017-07\",\"2017-09\",\"2018-01\",\"2018-02\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-09\",\"2019-09\",\"2019-10\",\"2015-09\",\"2014-12\",\"2015-06\",\"2016-07\",\"2016-08\",\"2016-11\",\"2017-11\",\"2018-08\",\"2018-10\",\"2018-11\",\"2019-01\",\"2013-06\",\"2014-03\",\"2014-05\",\"2014-10\",\"2015-04\",\"2015-09\",\"2016-02\",\"2016-07\",\"2016-12\",\"2017-07\",\"2017-09\",\"2018-01\",\"2018-05\",\"2018-06\",\"2018-08\",\"2018-09\",\"2018-10\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-08\",\"2019-11\"],\"y\":[\"Computer code processing\",\"Computer code processing\",\"Computer code processing\",\"Computer code processing\",\"Computer code processing\",\"Dialog process\",\"Dialog process\",\"Dialog process\",\"Dialog process\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Inference and reasoning\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information extraction\",\"Information retrieval\",\"Information retrieval\",\"Information retrieval\",\"Information retrieval\",\"Machine translation\",\"Machine translation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Natural language generation\",\"Text-to-image Generation\",\"Text-to-image Generation\",\"Text-to-image Generation\",\"Text-to-image Generation\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Question answering\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Semantic analysis\",\"Sentence embedding\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Syntactic analysis\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\",\"Text classification\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}},{\"hovertemplate\":[\"<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>ratio: 0.5551<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2014-06<BR>ratio: 0.3305<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2014-06<BR>ratio: 0.76<BR>benchmarks:<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2014-08<BR>ratio: 0.2549<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2014-08<BR>ratio: 0.2308<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2014-09<BR>ratio: 0.0886<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2014-10<BR>ratio: 0.0904<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Question answering<BR>date: 2014-12<BR>ratio: 0.3032<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>\",\"<BR>task: Natural language generation<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2015-02<BR>ratio: 0.1186<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2015-06<BR>ratio: 0.0427<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2015-06<BR>ratio: 0.232<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2015-08<BR>ratio: 0.5<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2015-09<BR>ratio: 0.1435<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2015-11<BR>ratio: 0.4025<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>\",\"<BR>task: Text classification<BR>date: 2015-11<BR>ratio: 0.25<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>\",\"<BR>task: Question answering<BR>date: 2015-11<BR>ratio: 0.2159<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>\",\"<BR>task: Information extraction<BR>date: 2016-01<BR>ratio: 0.483<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2016-02<BR>ratio: 0.3491<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-02<BR>ratio: 0.3173<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>\",\"<BR>task: Question answering<BR>date: 2016-02<BR>ratio: 0.0718<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>\",\"<BR>task: Text classification<BR>date: 2016-03<BR>ratio: 0.5215<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2016-03<BR>ratio: 0.1966<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-03<BR>ratio: 0.2<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Semantic analysis<BR>date: 2016-03<BR>ratio: 0.1323<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2016-03<BR>ratio: 0.40459999999999996<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2016-06<BR>ratio: 0.0678<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-06<BR>ratio: 0.30955<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2016-06<BR>ratio: 0.0702<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2016-06<BR>ratio: 0.7941<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2016-06<BR>ratio: 0.3097<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2016-07<BR>ratio: 0.9481<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>ratio: 0.094<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-07<BR>ratio: 0.218<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Question answering<BR>date: 2016-08<BR>ratio: 0.3589<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-08<BR>ratio: 0.1882<BR>benchmarks:<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-09<BR>ratio: 0.36160000000000003<BR>benchmarks:<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2016-09<BR>ratio: 0.0286<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2016-09<BR>ratio: 0.3558<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2016-09<BR>ratio: 0.101<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2016-09<BR>ratio: 0.098<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2016-10<BR>ratio: 0.6069<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-10<BR>ratio: 0.8<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Question answering<BR>date: 2016-11<BR>ratio: 0.29685<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2016-11<BR>ratio: 0.38507499999999995<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-11<BR>ratio: 0.0632<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Text classification<BR>date: 2016-11<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2016-12<BR>ratio: 0.6667<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2016-12<BR>ratio: 0.0708<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Question answering<BR>date: 2016-12<BR>ratio: 0.7801<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-01<BR>ratio: 0.0597<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Text classification<BR>date: 2017-02<BR>ratio: 0.2857<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2017-02<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>ratio: 0.1937<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>\",\"<BR>task: Question answering<BR>date: 2017-03<BR>ratio: 0.033<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2017-04<BR>ratio: 0.6016<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-04<BR>ratio: 0.55<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>ratio: 0.14375<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2017-04<BR>ratio: 0.06<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-05<BR>ratio: 0.921<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-05<BR>ratio: 0.2783333333333333<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>\",\"<BR>task: Question answering<BR>date: 2017-05<BR>ratio: 0.2379<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-06<BR>ratio: 0.42015<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>\",\"<BR>task: Question answering<BR>date: 2017-06<BR>ratio: 0.1524<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2017-07<BR>ratio: 0.30775<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>\",\"<BR>task: Question answering<BR>date: 2017-07<BR>ratio: 0.22825<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Information extraction<BR>date: 2017-07<BR>ratio: 0.212<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>\",\"<BR>task: Sentence embedding<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>ratio: 0.57325<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2017-08<BR>ratio: 0.4228<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-08<BR>ratio: 0.2288<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Computer code processing<BR>date: 2017-08<BR>ratio: 0.4269<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-09<BR>ratio: 0.0407<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-09<BR>ratio: 0.3099<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>\",\"<BR>task: Dialog process<BR>date: 2017-09<BR>ratio: 0.56715<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2017-09<BR>ratio: 0.0196<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-09<BR>ratio: 0.6226<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>\",\"<BR>task: Information extraction<BR>date: 2017-09<BR>ratio: 0.4385<BR>benchmarks:<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2017-10<BR>ratio: 0.7021<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Text classification<BR>date: 2017-10<BR>ratio: 0.0798<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2017-10<BR>ratio: 0.6272<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2017-11<BR>ratio: 0.6346<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Dialog process<BR>date: 2017-11<BR>ratio: 0.25695<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2017-11<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2017-11<BR>ratio: 0.0459<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Question answering<BR>date: 2017-12<BR>ratio: 0.0571<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2017-12<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2017-12<BR>ratio: 0.1887<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>ratio: 0.54575<BR>benchmarks:<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-01<BR>ratio: 0.0809<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2018-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2018-01<BR>ratio: 0.4672<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-02<BR>ratio: 0.19115<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>\",\"<BR>task: Text classification<BR>date: 2018-02<BR>ratio: 0.1176<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-02<BR>ratio: 0.2873<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-02<BR>ratio: 0.6357<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>\",\"<BR>task: Question answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-03<BR>ratio: 0.5076<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>\",\"<BR>task: Information retrieval<BR>date: 2018-03<BR>ratio: 0.541<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2018-03<BR>ratio: 0.0357<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>\",\"<BR>task: Computer code processing<BR>date: 2018-03<BR>ratio: 0.2009<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2018-04<BR>ratio: 0.5921<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-04<BR>ratio: 0.0412<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-04<BR>ratio: 0.2342<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>\",\"<BR>task: Computer code processing<BR>date: 2018-04<BR>ratio: 0.137<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-05<BR>ratio: 0.2896<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-05<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>\",\"<BR>task: Dialog process<BR>date: 2018-05<BR>ratio: 0.8365<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-05<BR>ratio: 0.57845<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-05<BR>ratio: 0.11170000000000001<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>\",\"<BR>task: Question answering<BR>date: 2018-05<BR>ratio: 0.36539999999999995<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Text classification<BR>date: 2018-05<BR>ratio: 0.3299<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>\",\"<BR>task: Question answering<BR>date: 2018-06<BR>ratio: 0.3862<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-06<BR>ratio: 0.6132<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-06<BR>ratio: 0.11065<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-07<BR>ratio: 0.0553<BR>benchmarks:<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>\",\"<BR>task: Text classification<BR>date: 2018-07<BR>ratio: 0.5132<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Information extraction<BR>date: 2018-07<BR>ratio: 0.3122<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-07<BR>ratio: 0.0282<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>\",\"<BR>task: Sentence embedding<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-07<BR>ratio: 0.791<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>\",\"<BR>task: Question answering<BR>date: 2018-08<BR>ratio: 0.2718<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2018-08<BR>ratio: 0.72315<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-08<BR>ratio: 0.3277<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-08<BR>ratio: 0.4922<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>\",\"<BR>task: Text classification<BR>date: 2018-08<BR>ratio: 0.5092<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>\",\"<BR>task: Text classification<BR>date: 2018-09<BR>ratio: 0.652<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Dialog process<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-09<BR>ratio: 0.2353<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-09<BR>ratio: 0.53495<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Information extraction<BR>date: 2018-09<BR>ratio: 0.3185<BR>benchmarks:<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2018-09<BR>ratio: 0.2419<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>\",\"<BR>task: Question answering<BR>date: 2018-10<BR>ratio: 0.196<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-10<BR>ratio: 0.43885<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-10<BR>ratio: 0.2123<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2018-10<BR>ratio: 0.48510000000000003<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Dialog process<BR>date: 2018-10<BR>ratio: 0.327<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>\",\"<BR>task: Natural language generation<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Information extraction<BR>date: 2018-10<BR>ratio: 0.20085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>\",\"<BR>task: Machine translation<BR>date: 2018-10<BR>ratio: 0.15<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Text classification<BR>date: 2018-10<BR>ratio: 0.9057<BR>benchmarks:<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>\",\"<BR>task: Computer code processing<BR>date: 2018-10<BR>ratio: 0.5481<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-10<BR>ratio: 0.33775<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2018-11<BR>ratio: 0.2719<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2018-11<BR>ratio: 0.2698<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>\",\"<BR>task: Question answering<BR>date: 2018-11<BR>ratio: 0.1449<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2018-11<BR>ratio: 0.35325<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>\",\"<BR>task: Information extraction<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>\",\"<BR>task: Machine translation<BR>date: 2019-01<BR>ratio: 0.4127<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-01<BR>ratio: 0.47159999999999996<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>\",\"<BR>task: Information retrieval<BR>date: 2019-01<BR>ratio: 0.3558<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2019-01<BR>ratio: 0.6137<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>\",\"<BR>task: Question answering<BR>date: 2019-01<BR>ratio: 0.2041<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-01<BR>ratio: 0.3456<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Information extraction<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>ratio: 0.37765000000000004<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>\",\"<BR>task: Question answering<BR>date: 2019-02<BR>ratio: 0.6327<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Machine translation<BR>date: 2019-02<BR>ratio: 0.397<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>\",\"<BR>task: Information extraction<BR>date: 2019-02<BR>ratio: 0.3333<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Text classification<BR>date: 2019-02<BR>ratio: 0.3546<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Dialog process<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-03<BR>ratio: 0.3031<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2019-03<BR>ratio: 0.1016<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Text classification<BR>date: 2019-03<BR>ratio: 0.6981<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-03<BR>ratio: 0.833<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2019-03<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>\",\"<BR>task: Information retrieval<BR>date: 2019-04<BR>ratio: 0.2915<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>\",\"<BR>task: Dialog process<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-04<BR>ratio: 0.5763<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>\",\"<BR>task: Question answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Information extraction<BR>date: 2019-04<BR>ratio: 0.4341666666666666<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2019-04<BR>ratio: 0.5876<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2019-04<BR>ratio: 0.1967<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>\",\"<BR>task: Text classification<BR>date: 2019-04<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2019-05<BR>ratio: 0.4549<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Machine translation<BR>date: 2019-05<BR>ratio: 0.3157<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Question answering<BR>date: 2019-05<BR>ratio: 0.45885<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Semantic analysis<BR>date: 2019-05<BR>ratio: 0.575<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-05<BR>ratio: 0.40315<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-05<BR>ratio: 0.3432<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Information extraction<BR>date: 2019-05<BR>ratio: 0.2659<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-06<BR>ratio: 0.4636<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Information extraction<BR>date: 2019-06<BR>ratio: 0.2085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-06<BR>ratio: 0.5752<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2019-06<BR>ratio: 0.7168<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>ratio: 0.5772666666666666<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2019-06<BR>ratio: 0.4252<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>\",\"<BR>task: Question answering<BR>date: 2019-06<BR>ratio: 0.2683<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Text classification<BR>date: 2019-06<BR>ratio: 0.5346<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-07<BR>ratio: 0.3592<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>\",\"<BR>task: Question answering<BR>date: 2019-07<BR>ratio: 0.46240000000000003<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-07<BR>ratio: 0.2631<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Information extraction<BR>date: 2019-07<BR>ratio: 0.5883499999999999<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Semantic analysis<BR>date: 2019-07<BR>ratio: 0.0577<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>ratio: 0.18335<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>\",\"<BR>task: Information extraction<BR>date: 2019-08<BR>ratio: 0.4151<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-08<BR>ratio: 0.1081<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>\",\"<BR>task: Question answering<BR>date: 2019-08<BR>ratio: 0.23105<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Text classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>\",\"<BR>task: Syntactic analysis<BR>date: 2019-09<BR>ratio: 0.1429<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-09<BR>ratio: 0.2846<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>\",\"<BR>task: Information extraction<BR>date: 2019-09<BR>ratio: 0.60275<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>ratio: 0.02725<BR>benchmarks:<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>\",\"<BR>task: Text-to-image Generation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>\",\"<BR>task: Question answering<BR>date: 2019-09<BR>ratio: 0.09475<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Text classification<BR>date: 2019-09<BR>ratio: 0.8833<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>\",\"<BR>task: Inference and reasoning<BR>date: 2019-09<BR>ratio: 0.12815000000000001<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>\",\"<BR>task: Semantic analysis<BR>date: 2019-09<BR>ratio: 0.5651333333333334<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>\",\"<BR>task: Natural language generation<BR>date: 2019-10<BR>ratio: 0.438<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>\",\"<BR>task: Computer code processing<BR>date: 2019-10<BR>ratio: 0.1871<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>\",\"<BR>task: Information extraction<BR>date: 2019-11<BR>ratio: 0.32506666666666667<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information retrieval<BR>date: 2019-11<BR>ratio: 0.4096<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>\",\"<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>\",\"<BR>task: Question answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Text classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>\",\"<BR>task: Information extraction<BR>date: 2020-03<BR>ratio: 0.3066<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>\",\"<BR>task: Information extraction<BR>date: 2020-04<BR>ratio: 0.2667<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>\"],\"marker\":{\"color\":[0.5551,0.3305,0.76,0.2549,0.2308,0.0886,0.0904,0.3032,1.0,0.1186,0.0427,0.232,0.5,0.1435,0.4025,0.25,0.2159,0.483,0.3491,0.3173,0.0718,0.5215,0.1966,0.2,0.1323,0.40459999999999996,0.2762,0.0678,0.30955,0.0702,0.7941,0.3097,0.9481,0.094,0.218,0.3589,0.1882,0.36160000000000003,0.0286,0.3558,0.101,0.098,0.6069,0.8,0.29685,0.38507499999999995,0.0632,0.0123,0.6667,0.0708,0.7801,0.0597,0.2857,0.0392,0.1937,0.033,0.6016,0.55,0.14375,0.06,0.921,0.2783333333333333,0.2379,0.42015,0.1524,0.30775,0.22825,0.212,1.0,0.57325,0.4228,0.2288,0.4269,0.0407,0.3099,0.56715,0.0196,0.6226,0.4385,0.7021,0.0798,0.6272,0.6346,0.25695,0.0392,0.0459,0.0571,0.0392,0.1887,0.54575,0.0809,0.0882,0.4672,0.19115,0.1176,0.2873,0.6357,0.1587,0.5076,1.0,0.541,0.0357,0.2009,0.5921,0.0412,0.2342,0.137,0.2896,0.1343,0.8365,0.57845,0.11170000000000001,0.36539999999999995,0.3299,0.7567,0.3862,0.6132,0.11065,0.0553,0.5132,0.3122,0.0282,1.0,1.0,0.791,0.2718,0.72315,0.3277,0.4922,0.5092,0.652,0.1442,0.2353,0.53495,0.3185,0.2419,0.196,0.43885,0.2123,0.48510000000000003,0.327,1.0,0.20085,0.15,0.9057,0.5481,0.33775,0.2719,0.2698,0.1449,0.35325,1.0,0.4127,0.47159999999999996,0.3558,0.6137,1.0,0.2041,0.3456,1.0,0.37765000000000004,0.6327,0.397,1.0,0.3333,0.3546,0.1426,0.3031,0.1016,0.6981,0.833,0.1343,0.2915,0.4213,0.5763,0.3147,0.4341666666666666,0.5876,1.0,0.1967,0.0123,0.4549,0.3157,0.45885,0.575,0.40315,0.3432,0.2659,0.4636,0.2085,0.5752,0.7168,0.5772666666666666,0.4252,0.2683,0.5346,0.3592,0.46240000000000003,0.2631,0.5883499999999999,0.0577,0.18335,0.4151,0.1081,0.23105,1.0,0.1429,0.2846,0.60275,0.02725,1.0,0.09475,0.8833,0.12815000000000001,0.5651333333333334,0.438,0.1871,0.32506666666666667,0.4096,1.0,0.5266,1.0,0.3066,0.2667],\"colorbar\":{\"len\":500,\"lenmode\":\"pixels\",\"thickness\":10,\"title\":{\"text\":\"ratio\"}},\"colorscale\":[[0.0,\"rgb(255,255,229)\"],[0.125,\"rgb(247,252,185)\"],[0.25,\"rgb(217,240,163)\"],[0.375,\"rgb(173,221,142)\"],[0.5,\"rgb(120,198,121)\"],[0.625,\"rgb(65,171,93)\"],[0.75,\"rgb(35,132,67)\"],[0.875,\"rgb(0,104,55)\"],[1.0,\"rgb(0,69,41)\"]],\"opacity\":0.7,\"showscale\":true,\"size\":20,\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"markers\",\"x\":[\"2013-10\",\"2014-06\",\"2014-06\",\"2014-08\",\"2014-08\",\"2014-09\",\"2014-10\",\"2014-12\",\"2014-12\",\"2015-02\",\"2015-06\",\"2015-06\",\"2015-08\",\"2015-09\",\"2015-11\",\"2015-11\",\"2015-11\",\"2016-01\",\"2016-02\",\"2016-02\",\"2016-02\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-05\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-07\",\"2016-07\",\"2016-07\",\"2016-08\",\"2016-08\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-10\",\"2016-10\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-12\",\"2016-12\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-02\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-06\",\"2017-06\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-10\",\"2019-10\",\"2019-11\",\"2019-11\",\"2019-12\",\"2020-02\",\"2020-02\",\"2020-03\",\"2020-04\"],\"y\":[\"Pragmatics analysis\",\"Pragmatics analysis\",\"Question answering\",\"Inference and reasoning\",\"Pragmatics analysis\",\"Natural language generation\",\"Natural language generation\",\"Question answering\",\"Natural language generation\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Question answering\",\"Inference and reasoning\",\"Natural language generation\",\"Pragmatics analysis\",\"Text classification\",\"Question answering\",\"Information extraction\",\"Pragmatics analysis\",\"Natural language generation\",\"Question answering\",\"Text classification\",\"Syntactic analysis\",\"Natural language generation\",\"Semantic analysis\",\"Question answering\",\"Question answering\",\"Semantic analysis\",\"Natural language generation\",\"Pragmatics analysis\",\"Text classification\",\"Question answering\",\"Syntactic analysis\",\"Pragmatics analysis\",\"Natural language generation\",\"Question answering\",\"Natural language generation\",\"Natural language generation\",\"Pragmatics analysis\",\"Text classification\",\"Question answering\",\"Inference and reasoning\",\"Question answering\",\"Natural language generation\",\"Question answering\",\"Syntactic analysis\",\"Natural language generation\",\"Text classification\",\"Natural language generation\",\"Text-to-image Generation\",\"Question answering\",\"Natural language generation\",\"Text classification\",\"Inference and reasoning\",\"Pragmatics analysis\",\"Question answering\",\"Syntactic analysis\",\"Semantic analysis\",\"Pragmatics analysis\",\"Question answering\",\"Semantic analysis\",\"Natural language generation\",\"Question answering\",\"Natural language generation\",\"Question answering\",\"Syntactic analysis\",\"Question answering\",\"Information extraction\",\"Sentence embedding\",\"Pragmatics analysis\",\"Question answering\",\"Pragmatics analysis\",\"Computer code processing\",\"Semantic analysis\",\"Pragmatics analysis\",\"Dialog process\",\"Inference and reasoning\",\"Natural language generation\",\"Information extraction\",\"Text-to-image Generation\",\"Text classification\",\"Question answering\",\"Text-to-image Generation\",\"Dialog process\",\"Inference and reasoning\",\"Natural language generation\",\"Question answering\",\"Inference and reasoning\",\"Semantic analysis\",\"Pragmatics analysis\",\"Pragmatics analysis\",\"Text classification\",\"Question answering\",\"Semantic analysis\",\"Text classification\",\"Pragmatics analysis\",\"Natural language generation\",\"Question answering\",\"Natural language generation\",\"Semantic analysis\",\"Information retrieval\",\"Text classification\",\"Computer code processing\",\"Question answering\",\"Inference and reasoning\",\"Pragmatics analysis\",\"Computer code processing\",\"Pragmatics analysis\",\"Syntactic analysis\",\"Dialog process\",\"Inference and reasoning\",\"Semantic analysis\",\"Question answering\",\"Text classification\",\"Pragmatics analysis\",\"Question answering\",\"Inference and reasoning\",\"Natural language generation\",\"Natural language generation\",\"Text classification\",\"Information extraction\",\"Pragmatics analysis\",\"Sentence embedding\",\"Question answering\",\"Syntactic analysis\",\"Question answering\",\"Information extraction\",\"Natural language generation\",\"Syntactic analysis\",\"Text classification\",\"Text classification\",\"Dialog process\",\"Inference and reasoning\",\"Natural language generation\",\"Information extraction\",\"Question answering\",\"Question answering\",\"Semantic analysis\",\"Syntactic analysis\",\"Inference and reasoning\",\"Dialog process\",\"Natural language generation\",\"Information extraction\",\"Machine translation\",\"Text classification\",\"Computer code processing\",\"Pragmatics analysis\",\"Semantic analysis\",\"Syntactic analysis\",\"Question answering\",\"Pragmatics analysis\",\"Information extraction\",\"Machine translation\",\"Natural language generation\",\"Information retrieval\",\"Text classification\",\"Text-to-image Generation\",\"Question answering\",\"Inference and reasoning\",\"Information extraction\",\"Pragmatics analysis\",\"Question answering\",\"Machine translation\",\"Inference and reasoning\",\"Information extraction\",\"Text classification\",\"Dialog process\",\"Natural language generation\",\"Text-to-image Generation\",\"Text classification\",\"Information extraction\",\"Syntactic analysis\",\"Information retrieval\",\"Dialog process\",\"Natural language generation\",\"Question answering\",\"Information extraction\",\"Text-to-image Generation\",\"Pragmatics analysis\",\"Syntactic analysis\",\"Text classification\",\"Text classification\",\"Machine translation\",\"Question answering\",\"Semantic analysis\",\"Natural language generation\",\"Pragmatics analysis\",\"Information extraction\",\"Inference and reasoning\",\"Information extraction\",\"Natural language generation\",\"Syntactic analysis\",\"Pragmatics analysis\",\"Semantic analysis\",\"Question answering\",\"Text classification\",\"Inference and reasoning\",\"Question answering\",\"Pragmatics analysis\",\"Information extraction\",\"Semantic analysis\",\"Pragmatics analysis\",\"Information extraction\",\"Natural language generation\",\"Question answering\",\"Text classification\",\"Syntactic analysis\",\"Natural language generation\",\"Information extraction\",\"Pragmatics analysis\",\"Text-to-image Generation\",\"Question answering\",\"Text classification\",\"Inference and reasoning\",\"Semantic analysis\",\"Natural language generation\",\"Computer code processing\",\"Information extraction\",\"Information retrieval\",\"Pragmatics analysis\",\"Question answering\",\"Text classification\",\"Information extraction\",\"Information extraction\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Year\"},\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"tickmode\":\"auto\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{},\"categoryorder\":\"array\",\"categoryarray\":[\"Text classification\",\"Syntactic analysis\",\"Sentence embedding\",\"Semantic analysis\",\"Question answering\",\"Pragmatics analysis\",\"Text-to-image Generation\",\"Natural language generation\",\"Machine translation\",\"Information retrieval\",\"Information extraction\",\"Inference and reasoning\",\"Dialog process\",\"Computer code processing\"],\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"side\":\"left\"},\"legend\":{\"title\":{\"text\":\"task\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Natural Language Processing\",\"y\":0.995},\"font\":{\"size\":21},\"showlegend\":false,\"plot_bgcolor\":\"white\",\"height\":700.0,\"width\":1500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ea0e8b58-d120-4141-9630-7800e8fcdf46');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "traj_df=plot_group_trajectory(\"ITO_00141\", \"Natural Language Processing\", anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C2AnSdJGR6ZF",
    "outputId": "0c0b6be2-5dd6-4f26-a633-180b4ac83214"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Action localization",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Action localization",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-01",
          "2016-09",
          "2016-11",
          "2017-03",
          "2017-05",
          "2018-04",
          "2018-06",
          "2019-03",
          "2019-04",
          "2019-06",
          "2019-07",
          "2019-09",
          "2019-11",
          "2020-03"
         ],
         "xaxis": "x",
         "y": [
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-03",
          "2017-12",
          "2018-03",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Activity detection",
          "Activity detection",
          "Activity detection",
          "Activity detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity localization",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity localization",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-12",
          "2018-06",
          "2018-07",
          "2018-11",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Activity recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Activity recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2013-02",
          "2014-06",
          "2014-12",
          "2015-03",
          "2015-05",
          "2015-12",
          "2016-01",
          "2016-03",
          "2016-04",
          "2016-06",
          "2016-08",
          "2016-09",
          "2016-11",
          "2016-12",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-06",
          "2017-08",
          "2017-11",
          "2017-12",
          "2018-01",
          "2018-02",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-10",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-11",
          "2019-12",
          "2020-02"
         ],
         "xaxis": "x",
         "y": [
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Emotion recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Emotion recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2018-06",
          "2018-10",
          "2018-11",
          "2019-04",
          "2019-08",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Emotion recognition",
          "Emotion recognition",
          "Emotion recognition",
          "Emotion recognition",
          "Emotion recognition",
          "Emotion recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Facial recognition and modelling",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Facial recognition and modelling",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2013-07",
          "2014-06",
          "2014-12",
          "2015-02",
          "2015-03",
          "2015-08",
          "2015-11",
          "2016-03",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-06",
          "2017-07",
          "2017-08",
          "2017-09",
          "2017-10",
          "2017-11",
          "2017-12",
          "2018-01",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-08",
          "2018-09",
          "2018-10",
          "2018-12",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-08",
          "2019-10",
          "2019-11"
         ],
         "xaxis": "x",
         "y": [
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Gesture recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Gesture recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-05",
          "2018-04",
          "2018-12",
          "2019-01"
         ],
         "xaxis": "x",
         "y": [
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image classification",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image classification",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2012-12",
          "2013-02",
          "2013-11",
          "2013-12",
          "2014-04",
          "2014-06",
          "2014-09",
          "2014-12",
          "2015-02",
          "2015-06",
          "2015-11",
          "2015-12",
          "2016-02",
          "2016-03",
          "2016-05",
          "2016-08",
          "2016-10",
          "2016-11",
          "2017-02",
          "2017-04",
          "2017-07",
          "2017-08",
          "2017-09",
          "2017-10",
          "2017-12",
          "2018-01",
          "2018-02",
          "2018-03",
          "2018-05",
          "2018-07",
          "2018-10",
          "2018-11",
          "2019-01",
          "2019-02",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-08",
          "2019-10",
          "2019-11",
          "2019-12",
          "2020-01"
         ],
         "xaxis": "x",
         "y": [
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image generation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image generation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-01",
          "2016-06",
          "2016-10",
          "2016-12",
          "2017-02",
          "2017-03",
          "2017-06",
          "2017-09",
          "2017-10",
          "2017-12",
          "2018-02",
          "2018-03",
          "2018-05",
          "2018-07",
          "2018-09",
          "2018-11",
          "2018-12",
          "2019-03",
          "2019-04",
          "2019-07",
          "2019-08",
          "2019-11",
          "2019-12"
         ],
         "xaxis": "x",
         "y": [
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Image-to-image translation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Image-to-image translation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-11",
          "2019-07"
         ],
         "xaxis": "x",
         "y": [
          "Image-to-image translation",
          "Image-to-image translation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object detection",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object detection",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-12",
          "2015-06",
          "2015-11",
          "2015-12",
          "2016-03",
          "2016-06",
          "2016-08",
          "2016-09",
          "2016-11",
          "2016-12",
          "2017-02",
          "2017-03",
          "2017-04",
          "2017-06",
          "2017-07",
          "2017-08",
          "2017-10",
          "2017-11",
          "2017-12",
          "2018-02",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-03",
          "2019-04",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-10",
          "2019-11",
          "2020-01",
          "2020-03"
         ],
         "xaxis": "x",
         "y": [
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object recognition",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object recognition",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-08",
          "2018-06",
          "2019-04",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Object recognition",
          "Object recognition",
          "Object recognition",
          "Object recognition"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Object tracking",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Object tracking",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-11",
          "2017-04",
          "2017-06",
          "2017-10",
          "2018-02",
          "2018-03",
          "2018-06",
          "2018-11",
          "2018-12",
          "2019-06",
          "2019-07",
          "2019-09"
         ],
         "xaxis": "x",
         "y": [
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other 3D task",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other 3D task",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-12",
          "2017-04",
          "2017-06",
          "2017-11",
          "2018-01",
          "2018-02",
          "2018-03",
          "2018-04",
          "2018-08",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-04"
         ],
         "xaxis": "x",
         "y": [
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other image process",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other image process",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2012-08",
          "2015-04",
          "2015-11",
          "2016-04",
          "2016-08",
          "2016-11",
          "2016-12",
          "2017-03",
          "2017-04",
          "2017-06",
          "2017-09",
          "2017-10",
          "2017-12",
          "2018-01",
          "2018-02",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-10",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-09",
          "2019-10"
         ],
         "xaxis": "x",
         "y": [
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other video process",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other video process",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2016-11",
          "2016-12",
          "2017-07",
          "2018-04",
          "2018-06",
          "2018-08",
          "2018-10",
          "2019-04",
          "2019-06",
          "2019-07",
          "2019-12",
          "2020-03"
         ],
         "xaxis": "x",
         "y": [
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Other vision process",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Other vision process",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-09",
          "2015-02",
          "2015-04",
          "2015-05",
          "2015-12",
          "2016-03",
          "2016-04",
          "2016-05",
          "2016-06",
          "2016-07",
          "2016-08",
          "2016-09",
          "2016-11",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-06",
          "2017-07",
          "2017-08",
          "2017-09",
          "2017-10",
          "2017-11",
          "2017-12",
          "2018-01",
          "2018-02",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-10",
          "2019-11",
          "2020-01",
          "2020-02"
         ],
         "xaxis": "x",
         "y": [
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose estimation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose estimation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2015-11",
          "2016-01",
          "2016-03",
          "2016-08",
          "2016-09",
          "2016-11",
          "2016-12",
          "2017-01",
          "2017-02",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-07",
          "2017-08",
          "2017-09",
          "2017-10",
          "2017-11",
          "2017-12",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-06",
          "2019-09",
          "2019-10",
          "2019-11",
          "2019-12",
          "2020-01",
          "2020-02",
          "2020-04"
         ],
         "xaxis": "x",
         "y": [
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Pose tracking",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Pose tracking",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2017-12",
          "2018-02",
          "2018-04",
          "2019-02",
          "2019-05"
         ],
         "xaxis": "x",
         "y": [
          "Pose tracking",
          "Pose tracking",
          "Pose tracking",
          "Pose tracking",
          "Pose tracking"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "task=%{y}<br>date=%{x}<extra></extra>",
         "legendgroup": "Semantic segmentation",
         "line": {
          "color": "black",
          "dash": "solid",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Semantic segmentation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "2014-11",
          "2014-12",
          "2015-02",
          "2015-03",
          "2015-04",
          "2015-05",
          "2015-09",
          "2015-11",
          "2016-03",
          "2016-05",
          "2016-06",
          "2016-11",
          "2016-12",
          "2017-03",
          "2017-04",
          "2017-06",
          "2017-08",
          "2017-09",
          "2017-10",
          "2017-11",
          "2017-12",
          "2018-01",
          "2018-02",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-10",
          "2019-11",
          "2019-12",
          "2020-01",
          "2020-03",
          "2020-04"
         ],
         "xaxis": "x",
         "y": [
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation"
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": [
          "<BR>task: Action localization<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>",
          "<BR>task: Action localization<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>",
          "<BR>task: Action localization<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: MEXaction2 - Temporal Action Localization benchmarking - mAP<BR>",
          "<BR>task: Action localization<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>",
          "<BR>task: Action localization<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>",
          "<BR>task: Action localization<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Frame-mAP<BR>",
          "<BR>task: Action localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>",
          "<BR>task: Action localization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Video-mAP 0.5<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Video-mAP 0.5<BR>",
          "<BR>task: Action localization<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>",
          "<BR>task: Action localization<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>",
          "<BR>task: Action localization<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@50%<BR>",
          "<BR>task: Action localization<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>",
          "<BR>task: Activity detection<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>",
          "<BR>task: Activity detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>",
          "<BR>task: Activity detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.1<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.2<BR>",
          "<BR>task: Activity detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - mAP<BR>",
          "<BR>task: Activity localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Activity localization<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (test)<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>",
          "<BR>task: Activity localization<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>",
          "<BR>task: Activity localization<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@1000<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@200<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@500<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@50<BR>",
          "<BR>task: Activity localization<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Activity localization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - Mean mAP<BR>",
          "<BR>task: Activity localization<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.5<BR>  Weakly Supervised Action Localization: THUMOS’14 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Activity recognition<BR>date: 2012-07<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2012-10<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: CAD-120 - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Action Recognition: VIRAT Ground 2.0 - Action Recognition benchmarking - Average Accuracy<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UT-Kinect - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>",
          "<BR>task: Activity recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.5<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ActivityNet - Action Recognition benchmarking - mAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: SBU - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Volleyball - Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.1<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: ActivityNet-1.2 - Action Classification benchmarking - mAP<BR>  Action Classification: THUMOS’14 - Action Classification benchmarking - mAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CS<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CV1<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CV2<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - mAP (Val)<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ActionNet-VE - Action Recognition benchmarking - F-measure (%)<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: J-HMBD Early Action - Skeleton Based Action Recognition benchmarking - 10%<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 5 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top-1 Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ICVL-4 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: IRD - Action Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-1 (%)<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-5 (%)<BR>  Skeleton Based Action Recognition: UAV-Human - Skeleton Based Action Recognition benchmarking - Average Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: UTD-MHAD - Multimodal Activity Recognition benchmarking - Accuracy (CS)<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-5 Accuracy<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - GFlops<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - Params (M)<BR>  Action Recognition: AVA v2.2 - Action Recognition benchmarking - mAP<BR>  Action Recognition: Diving-48 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: UTD-MHAD - Action Recognition benchmarking - Accuracy<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S1)<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: LboroHAR - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - Speed  (FPS)<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Object Top 5 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Object Top-1 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Verb Top-1 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Verb Top-5 Accuracy<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Video hit-at-5<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: MiniKinetics - Action Classification benchmarking - Top-1 Accuracy<BR>  Skeleton Based Action Recognition: MSR Action3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UPenn Action - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - No. parameters<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: THUMOS'14 - Action Classification benchmarking - mAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking - Train F-measure<BR>",
          "<BR>task: Activity recognition<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: EPIC-KITCHENS-55 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: EgoGesture - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: EgoGesture - Action Recognition benchmarking - Top-5 Accuracy<BR>",
          "<BR>task: Emotion recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Arousal)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>",
          "<BR>task: Emotion recognition<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>",
          "<BR>task: Emotion recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: DailyDialog - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>  Emotion Recognition in Conversation: EmoryNLP - Emotion Recognition in Conversation benchmarking - Weighted Macro-F1<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2013-07<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FER2013 - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW2000 - Face Alignment benchmarking - Error rate<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: Oulu-CASIA - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: JAFFE - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - FR-at-0.1(%, all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - ME (%, all)<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: MMI - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW2000-3D - Face Alignment benchmarking - Mean NME<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Oulu-CASIA - Facial Expression Recognition benchmarking - Accuracy (10-fold)<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: 3DFAW - Face Alignment benchmarking - CVGTCE<BR>  Face Alignment: 3DFAW - Face Alignment benchmarking - GTE<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Cohn-Kanade - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW-Full - Face Alignment benchmarking - Mean NME<BR>  Face Alignment: LS3D-W Balanced - Face Alignment benchmarking - AUC0.07<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>  Unsupervised Facial Landmark Detection: AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Failure private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Face Alignment: 300W - Face Alignment benchmarking - Mean Error Rate private<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: IJB-A - Face Identification benchmarking - Accuracy<BR>  Face Identification: IJB-B - Face Identification benchmarking - Accuracy<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-B - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - Mean Error Rate<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW-LFPA - Face Alignment benchmarking - Mean NME<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.1<BR>  Face Verification: IJB-B - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.001<BR>  Facial Expression Recognition: SFEW - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: COFW - Face Alignment benchmarking - Mean Error Rate<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Facial Landmark Detection: AFLW-Front - Facial Landmark Detection benchmarking - Mean NME<BR>  Facial Landmark Detection: AFLW-Full - Facial Landmark Detection benchmarking - Mean NME<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW - Face Alignment benchmarking - Mean NME<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (7 emotion)<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Real-World Affective Faces - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FERPlus - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: IBUG - Face Alignment benchmarking - Mean Error Rate<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FERG - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IIIT-D Viewed Sketch - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: RAF-DB - Facial Expression Recognition benchmarking - Overall Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: AgeDB-30 - Face Verification benchmarking - Accuracy<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - MOS<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - MS-SSIM<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - PSNR<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - SSIM<BR>",
          "<BR>task: Gesture recognition<BR>date: 2013-03<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Cambridge - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLearn val - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: BUAA - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: MGB - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: SmartWatch - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Jester test - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLean test - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: Jester val - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>  Hand Gesture Recognition: Jester val - Hand Gesture Recognition benchmarking - Top 5 Accuracy<BR>  Hand Gesture Recognition: NVGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Northwestern University - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Gesture recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: DHG-14 - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: DHG-28 - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: SHREC 2017 - Hand Gesture Recognition benchmarking - 14 gestures accuracy<BR>  Hand Gesture Recognition: SHREC 2017 - Hand Gesture Recognition benchmarking - 28 gestures accuracy<BR>  Hand Gesture Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Hand Gesture Recognition benchmarking - 14 gestures accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>",
          "<BR>task: Image classification<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2013-01<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>",
          "<BR>task: Image classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Image classification<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>  Satellite Image Classification: SAT-6 - Satellite Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - # of clusters (k)<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>",
          "<BR>task: Image classification<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Retinal OCT Disease Classification: Srinivasan2014 - Retinal OCT Disease Classification benchmarking - Acc<BR>",
          "<BR>task: Image classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Hyperspectral Image Classification: Indian Pines - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>  Hyperspectral Image Classification: Pavia University - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: iNaturalist 2018 - Image Classification benchmarking - Top-1 Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>",
          "<BR>task: Image classification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: EMNIST-Balanced - Image Classification benchmarking - Accuracy<BR>  Image Classification: MultiMNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: smallNORB - Image Classification benchmarking - Classification Error<BR>",
          "<BR>task: Image classification<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Clothing1M - Image Classification benchmarking - Accuracy<BR>  Image Classification: Food-101N - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: MNIST - Unsupervised Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sequential Image Classification: Sequential CIFAR-10 - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Document Image Classification: Noisy Bangla Characters - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: Noisy Bangla Numeral - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: n-MNIST - Document Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: CIFAR-10 - Unsupervised Image Classification benchmarking - Accuracy<BR>  Unsupervised Image Classification: CIFAR-20 - Unsupervised Image Classification benchmarking - Accuracy<BR>  Unsupervised Image Classification: STL-10 - Unsupervised Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CINIC-10 - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Error<BR>",
          "<BR>task: Image classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Hyperspectral Image Classification: Salinas Scene - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: EMNIST-Letters - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Params<BR>",
          "<BR>task: Image classification<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Flowers-102 - Image Classification benchmarking - Accuracy<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top-1 Accuracy<BR>",
          "<BR>task: Image generation<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - bits/dimension<BR>",
          "<BR>task: Image generation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Image generation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: Binarized MNIST - Image Generation benchmarking - nats<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>",
          "<BR>task: Image generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Image generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Image generation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CAT 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - IS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - SSIM<BR>",
          "<BR>task: Image generation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Bedroom 64 x 64 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Image generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 1024x1024 - Image Generation benchmarking - FID<BR>  Image Generation: CelebA-HQ 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Cat 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Churches 256 x 256 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - LPIPS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - Retrieval Top10 Recall<BR>",
          "<BR>task: Image generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - bpd<BR>",
          "<BR>task: Image generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: ImageNet 128x128 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 128x128 - Image Generation benchmarking - IS<BR>  Image Generation: ImageNet 256x256 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 128x128 - Image Generation benchmarking - FID<BR>  Image Generation: MNIST - Image Generation benchmarking - bits/dimension<BR>",
          "<BR>task: Image generation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Bedroom - Image Generation benchmarking - FID-50k<BR>",
          "<BR>task: Image generation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 64x64 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - DS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - PCKh<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - DS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - IS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - PCKh<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - SSIM<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - mask-IS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - mask-SSIM<BR>",
          "<BR>task: Image generation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: Fashion-MNIST - Image Generation benchmarking - FID<BR>  Image Generation: MNIST - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: Stacked MNIST - Image Generation benchmarking - FID<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: ADE-Indoor - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-100 - Image Generation benchmarking - FID<BR>  Image Generation: Cityscapes-25K 256x512 - Image Generation benchmarking - FID<BR>  Image Generation: Cityscapes-5K 256x512 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Car 512 x 384 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Horse 256 x 256 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image-to-image translation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>",
          "<BR>task: Image-to-image translation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - Kernel Inception Distance<BR>",
          "<BR>task: Object detection<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Detection: Caltech - Pedestrian Detection benchmarking - Reasonable Miss Rate<BR>",
          "<BR>task: Object detection<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Lane Detection: Caltech Lanes Cordova - Lane Detection benchmarking - F1<BR>  Lane Detection: Caltech Lanes Washington - Lane Detection benchmarking - F1<BR>",
          "<BR>task: Object detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>",
          "<BR>task: Object detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Object Detection: Visual Genome - Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>  Weakly Supervised Object Detection: Watercolor2k - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>",
          "<BR>task: Object detection<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>",
          "<BR>task: Object detection<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>",
          "<BR>task: Object detection<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PeopleArt - Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: ISTD - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: UCF - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  Weakly Supervised Object Detection: ImageNet - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP50<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>",
          "<BR>task: Object detection<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Large MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Medium MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Small MR^-2<BR>",
          "<BR>task: Object detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: iSAID - Object Detection benchmarking - Average Precision<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - runtime (ms)<BR>",
          "<BR>task: Object detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - Average MAE<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - mean E-Measure<BR>",
          "<BR>task: Object detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>",
          "<BR>task: Object detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: NYU Depth v2 - 3D Object Detection benchmarking - MAP<BR>  3D Object Detection: SUN-RGBD - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Bare MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Heavy MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Partial MR^-2<BR>",
          "<BR>task: Object detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Lane Detection: CULane - Lane Detection benchmarking - F1 score<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - F1 score<BR>",
          "<BR>task: Object detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Clipart1k - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: Comic2k - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - MAE<BR>",
          "<BR>task: Object detection<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: IconArt - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PeopleArt - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Object detection<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - NDS<BR>",
          "<BR>task: Object detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.5<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-test - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-test - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - F-measure<BR>",
          "<BR>task: Object detection<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP50<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP75<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AR<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARI<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARm<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARs<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP50<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP75<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AR<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARI<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARm<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARs<BR>",
          "<BR>task: Object detection<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cyclists Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Hard - Birds Eye View Object Detection benchmarking - AP<BR>",
          "<BR>task: Object detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - mAP<BR>  Lane Detection: BDD100K - Lane Detection benchmarking - Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: BDD100K - Object Detection benchmarking - mAP-at-0.5<BR>  Object Detection: India Driving Dataset - Object Detection benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Object detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO 2017 - Object Detection benchmarking - Mean mAP<BR>",
          "<BR>task: Object recognition<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: GTSRB - Traffic Sign Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Object recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Backpack<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Gender<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Hat<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCS<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCS<BR>",
          "<BR>task: Object recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: PA-100K - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: PETA - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: RAP - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Object recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking - MAP<BR>  Traffic Sign Recognition: Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking - MAP<BR>",
          "<BR>task: Object recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP at-0.5:0.95<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP-at-0.50<BR>",
          "<BR>task: Object tracking<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>",
          "<BR>task: Object tracking<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>",
          "<BR>task: Object tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>",
          "<BR>task: Object tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Unseen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - O (Average of Measures)<BR>",
          "<BR>task: Object tracking<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: VOT2016 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>",
          "<BR>task: Object tracking<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Unseen)<BR>",
          "<BR>task: Object tracking<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: GOT-10k - Visual Object Tracking benchmarking - Average Overlap<BR>  Visual Object Tracking: GOT-10k - Visual Object Tracking benchmarking - Success Rate 0.5<BR>  Visual Object Tracking: LaSOT - Visual Object Tracking benchmarking - AUC<BR>",
          "<BR>task: Object tracking<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>",
          "<BR>task: Object tracking<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2019 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>",
          "<BR>task: Object tracking<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - Precision<BR>",
          "<BR>task: Other 3D task<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  3D Reconstruction: Scan2CAD - 3D Reconstruction benchmarking - Average Accuracy<BR>",
          "<BR>task: Other 3D task<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D−R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D−R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: Sydney Urban Objects - 3D Point Cloud Classification benchmarking - F1<BR>  3D Reconstruction: Data3D−R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>",
          "<BR>task: Other 3D task<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>",
          "<BR>task: Other 3D task<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Mean Accuracy<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>",
          "<BR>task: Other image process<BR>date: 2012-03<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2012-08<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Other image process<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Other image process<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Other image process<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>",
          "<BR>task: Other image process<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - PSNR (sRGB)<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - SSIM (sRGB)<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Other image process<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>",
          "<BR>task: Other image process<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: FRGC - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Clustering: UMist - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>",
          "<BR>task: Other image process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - SSIM<BR>",
          "<BR>task: Other image process<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: BSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Other image process<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Other image process<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: BSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma5 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: FRGC - Image Clustering benchmarking - Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Other image process<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma35 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma75 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma35 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma60 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Other image process<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: ARL Polarimetric Thermal Face Dataset - Image Clustering benchmarking - Accuracy<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Other image process<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: Set12 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Other image process<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: Urban100 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: CUB-200-2011 - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Other image process<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: LetterA-J - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: LetterA-J - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Other image process<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: street2shop - topwear - Image Retrieval benchmarking - Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: INRIA Holidays - Image Retrieval benchmarking - Mean mAP<BR>  Image Retrieval: NUS-WIDE - Image Retrieval benchmarking - MAP<BR>",
          "<BR>task: Other image process<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - HP<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - MMD<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - HP<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - MMD<BR>",
          "<BR>task: Other image process<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: Set12 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Retrieval: DeepFashion - Image Retrieval benchmarking - Recall-at-20<BR>",
          "<BR>task: Other image process<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Clothes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Clothes - Image Reconstruction benchmarking - LPIPS<BR>",
          "<BR>task: Other video process<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Other video process<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>",
          "<BR>task: Other video process<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Other video process<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - Interpolation Error<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>",
          "<BR>task: Other video process<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text Mean Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text Median Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-1<BR>",
          "<BR>task: Other video process<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Other video process<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - tOF<BR>  Video Generation: TrailerFaces - Video Generation benchmarking - FID<BR>",
          "<BR>task: Other video process<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: BAIR Robot Pushing - Video Generation benchmarking - FVD score<BR>  Video Generation: Kinetics-600 12 frames, 128x128 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 12 frames, 64x64 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 12 frames, 64x64 - Video Generation benchmarking - Inception Score<BR>  Video Generation: Kinetics-600 48 frames, 64x64 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 48 frames, 64x64 - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Other video process<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, 64x64, Unconditional - Video Generation benchmarking - Inception Score<BR>",
          "<BR>task: Other video process<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - SSIM<BR>",
          "<BR>task: Other vision process<BR>date: 2010-11<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: Beijing Air Quality - Multivariate Time Series Imputation benchmarking - MAE (PM2.5)<BR>  Multivariate Time Series Imputation: KDD CUP Challenge 2018 - Multivariate Time Series Imputation benchmarking - MSE (10% missing)<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - MAE (10% of data as GT)<BR>  Multivariate Time Series Imputation: UCI localization data - Multivariate Time Series Imputation benchmarking - MAE (10% missing)<BR>",
          "<BR>task: Other vision process<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-Olympic - Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>",
          "<BR>task: Other vision process<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Other vision process<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SYNSIG-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Signs-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>",
          "<BR>task: Other vision process<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>",
          "<BR>task: Other vision process<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>",
          "<BR>task: Other vision process<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Domain Generalization: ImageNet-R - Domain Generalization benchmarking - Top-1 Error Rate<BR>",
          "<BR>task: Other vision process<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Horizon Line Estimation: Eurasian Cities Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: Horizon Lines in the Wild - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: York Urban Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-relRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Other vision process<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: HMDBfull-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - Mean Rank<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Other vision process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: MuJoCo - Multivariate Time Series Imputation benchmarking - MSE (10^2, 50% missing)<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>",
          "<BR>task: Other vision process<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Scene Graph Generation: VRD - Scene Graph Generation benchmarking - Recall-at-50<BR>",
          "<BR>task: Other vision process<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Synth Objects-to-LINEMOD - Domain Adaptation benchmarking - Classification Accuracy<BR>  Domain Adaptation: Synth Objects-to-LINEMOD - Domain Adaptation benchmarking - Mean Angle Error<BR>",
          "<BR>task: Other vision process<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split unsupervised - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE log<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - SQ Rel<BR>",
          "<BR>task: Other vision process<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>",
          "<BR>task: Other vision process<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Other vision process<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>",
          "<BR>task: Other vision process<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Other vision process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - NDCG (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: USPS-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>",
          "<BR>task: Other vision process<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MSE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: UCF CC 50 - Crowd Counting benchmarking - MAE<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>",
          "<BR>task: Other vision process<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - Runtime [ms]<BR>",
          "<BR>task: Other vision process<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - H-Mean<BR>",
          "<BR>task: Other vision process<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>",
          "<BR>task: Other vision process<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MAE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MSE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - SSIM<BR>",
          "<BR>task: Other vision process<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - OOB Rate (10^−3)<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Difference<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Length<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Player Distance<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Step Change (10^−3)<BR>  Multivariate Time Series Imputation: PEMS-SF - Multivariate Time Series Imputation benchmarking - L2 Loss (10^-4)<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Other vision process<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Unsupervised Domain Adaptation: Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Other vision process<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Formation Energy: Materials Project - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - RMSE<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - Sq Rel<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - mse (10^-3)<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - F-Measure<BR>",
          "<BR>task: Other vision process<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech-10 - Domain Adaptation benchmarking - Accuracy (%)<BR>",
          "<BR>task: Other vision process<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: CLEVR - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Other vision process<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: 100 sleep nights of 8 caregivers - Visual Question Answering benchmarking - 14 gestures accuracy<BR>  Visual Question Answering: HowmanyQA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TallyQA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking - mIoU<BR>  Domain Generalization: ImageNet-C - Domain Generalization benchmarking - mean Corruption Error (mCE)<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - TIoU<BR>",
          "<BR>task: Other vision process<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: SIM10K to BDD100K - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Other vision process<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iMAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iRMSE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TDIUC - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - H-Mean<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Other vision process<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: VOID - Depth Completion benchmarking - MAE<BR>  Depth Completion: VOID - Depth Completion benchmarking - RMSE<BR>  Depth Completion: VOID - Depth Completion benchmarking - iMAE<BR>  Depth Completion: VOID - Depth Completion benchmarking - iRMSE<BR>  Formation Energy: OQMD v1.2 - Formation Energy benchmarking - MAE<BR>  Unsupervised Domain Adaptation: PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking - AP-at-0.7<BR>  Video Prediction: CMU Mocap-1 - Video Prediction benchmarking - Test Error<BR>  Video Prediction: CMU Mocap-2 - Video Prediction benchmarking - Test Error<BR>",
          "<BR>task: Other vision process<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: IC19-ReCTs - Scene Text Detection benchmarking - F-Measure<BR>",
          "<BR>task: Other vision process<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - ATV<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - AUC<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - MSE<BR>  Visual Question Answering: GQA test-dev - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - number<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - other<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - unanswerable<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - yes/no<BR>",
          "<BR>task: Other vision process<BR>date: 2020-02<BR>Anchor.<BR>benchmarks:<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - mean Recall @20<BR>",
          "<BR>task: Pose estimation<BR>date: 2010-03<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: HumanEva-I - 3D Human Pose Estimation benchmarking - Mean Reconstruction Error (mm)<BR>",
          "<BR>task: Pose estimation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Human3.6M - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>",
          "<BR>task: Pose estimation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>",
          "<BR>task: Pose estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Pose estimation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: BIWI - Head Pose Estimation benchmarking - MAE (trained with other data)<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Total Capture - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>  Pose Estimation: FLIC Elbows - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: FLIC Wrists - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: J-HMDB - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>  Pose Estimation: ITOP top-view - Pose Estimation benchmarking - Mean mAP<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: BJUT-3D - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: Pointing'04 - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - FPS<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: UAV-Human - Pose Estimation benchmarking - mAP<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Frames Per View<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Views<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Hand Pose Estimation: MSRA Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - MPJPE (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - MPJPE (CS)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CS)<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - 3DPCK<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - MJPE<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2017 - Hand Pose Estimation benchmarking - Average 3D Error<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: BIWI - Head Pose Estimation benchmarking - MAE (trained with BIWI data)<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean IoU<BR>  6D Pose Estimation using RGB: OCCLUSION - 6D Pose Estimation using RGB benchmarking - MAP<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean IoU<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - IoU-2D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - IoU-3D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - VSS-2D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - VSS-3D<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - PA-MPJPE<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - acceleration error<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>  Pose Estimation: UPenn Action - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: DensePose-COCO - Pose Estimation benchmarking - AP<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADI<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADI<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: CHALL H80K - 3D Human Pose Estimation benchmarking - MPJPE<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean AUC<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 10cm<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 5cm<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-25<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-50<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 5, 5cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 10cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 5cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-25<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-50<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 5, 5cm<BR>  6D Pose Estimation: LineMOD - 6D Pose Estimation benchmarking - Accuracy (ADD)<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP<BR>  Pose Estimation: COCO minival - Pose Estimation benchmarking - AP<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: T-LESS - 6D Pose Estimation using RGB benchmarking - Mean Recall<BR>  6D Pose Estimation using RGBD: T-LESS - 6D Pose Estimation using RGBD benchmarking - Mean Recall<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking - MPJAE<BR>  3D Human Pose Estimation: 3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking - MPJPE<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: T-LESS - 6D Pose Estimation using RGB benchmarking - Recall (VSD)<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - FPS<BR>  Hand Pose Estimation: K2HPD - Hand Pose Estimation benchmarking - PDJ@5mm<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - FPS<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPVPE<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - 5°5 cm<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - IOU25<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - Rerr<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - Terr<BR>",
          "<BR>task: Pose estimation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2019 - Hand Pose Estimation benchmarking - Average 3D Error<BR>",
          "<BR>task: Pose estimation<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - PCK3D<BR>",
          "<BR>task: Pose tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: Multi-Person PoseTrack - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: Multi-Person PoseTrack - Pose Tracking benchmarking - MOTP<BR>",
          "<BR>task: Pose tracking<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>",
          "<BR>task: Pose tracking<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2018 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2018 - Pose Tracking benchmarking - mAP<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Dice<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SkyScapes-Lane - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Scene Segmentation: SUN-RGBD - Scene Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2013 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2013 leaderboard - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Medical Image Segmentation: CVC-ClinicDB - Medical Image Segmentation benchmarking - mean Dice<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - Warping Error<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - Average MAE<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - S-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - max E-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - mean Dice<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Dice<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Jaccard Index<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Dice Score<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Precision<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Recall<BR>  Pancreas Segmentation: TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking - Dice Score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: Kvasir-Instrument - Semantic Segmentation benchmarking - DSC<BR>  Semantic Segmentation: Kvasir-Instrument - Semantic Segmentation benchmarking - mIoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Global Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2015 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: ISLES-2015 - Lesion Segmentation benchmarking - Dice Score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: NYU Depth v2 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2011 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - Average Accuracy<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Lesion Segmentation: ISIC 2017 - Lesion Segmentation benchmarking - Mean IoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: Cityscapes test - Panoptic Segmentation benchmarking - PQ<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: ShapeNet - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2007 - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2014 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2017 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Instance Segmentation: ScanNetV1 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.25<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Instance Segmentation: NYU Depth v2 - Instance Segmentation benchmarking - mAP-at-0.5<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - oAcc<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mAcc<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mIoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQst<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: iSAID - Instance Segmentation benchmarking - Average Precision<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Medical Image Segmentation: iSEG 2017 Challenge - Medical Image Segmentation benchmarking - Dice Score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Retinal Vessel Segmentation: HRF - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: HRF - Retinal Vessel Segmentation benchmarking - F1 score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Medical Image Segmentation: 2018 Data Science Bowl - Medical Image Segmentation benchmarking - Dice<BR>  Medical Image Segmentation: 2018 Data Science Bowl - Medical Image Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Freiburg Forest - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SUN-RGBD - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SYNTHIA-CVPR’16 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ScanNetV2 - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: BUS 2017 Dataset B - Lesion Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: MHP v2.0 - Human Part Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: KITTI Semantic Segmentation - Semantic Segmentation benchmarking - Mean IoU (class)<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: COCO panoptic - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - Pixel Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mRec<BR>  3D Instance Segmentation: ScanNet - 3D Instance Segmentation benchmarking - mAP<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - Accuracy<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - VInfo<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - VRand<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: SceneNN - 3D Instance Segmentation benchmarking - mAP-at-0.5<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - AVD<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Dice Score<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Precision<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Recall<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - VS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP50<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP75<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - 3DIoU<BR>  Semantic Segmentation: ParisLille3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - MSD<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - VS<BR>  Brain Tumor Segmentation: BRATS 2018 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - Total Variation of Information<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Merge<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Split<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - MSD<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - VS<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - MSD<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - VS<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Segmentation: S3DIS - 3D Semantic Segmentation benchmarking - mAcc<BR>  3D Semantic Segmentation: S3DIS - 3D Semantic Segmentation benchmarking - mIoU<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - mIoU<BR>  Lung Nodule Segmentation: Montgomery County - Lung Nodule Segmentation benchmarking - Accuracy<BR>  Lung Nodule Segmentation: Montgomery County - Lung Nodule Segmentation benchmarking - mIoU<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - F1-Score<BR>  Lung Nodule Segmentation: LIDC-IDRI - Lung Nodule Segmentation benchmarking - Dice<BR>  Lung Nodule Segmentation: LIDC-IDRI - Lung Nodule Segmentation benchmarking - IoU<BR>  Lung Nodule Segmentation: Lung Nodule  - Lung Nodule Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: DRIVE - Medical Image Segmentation benchmarking - F1 score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Skin Cancer Segmentation: PH2 - Skin Cancer Segmentation benchmarking - IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mCov<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mWCov<BR>  Instance Segmentation: LVIS v1.0 - Instance Segmentation benchmarking - mask AP<BR>  Medical Image Segmentation: Cell - Medical Image Segmentation benchmarking - IoU<BR>  Medical Image Segmentation: EM - Medical Image Segmentation benchmarking - IoU<BR>  Semantic Segmentation: BDD - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Category mIoU<BR>  Semantic Segmentation: GTAV-to-Cityscapes Labels - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - Frame (fps)<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "line": {
           "color": "black",
           "width": 1
          },
          "size": 21,
          "symbol": 42
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2015-06",
          "2015-11",
          "2016-01",
          "2016-02",
          "2016-04",
          "2016-09",
          "2017-03",
          "2017-05",
          "2018-04",
          "2018-06",
          "2019-03",
          "2020-01",
          "2015-07",
          "2016-12",
          "2017-03",
          "2019-04",
          "2017-03",
          "2017-07",
          "2017-12",
          "2018-06",
          "2018-07",
          "2019-08",
          "2019-11",
          "2012-07",
          "2012-10",
          "2012-12",
          "2013-06",
          "2014-03",
          "2014-06",
          "2014-11",
          "2015-11",
          "2015-12",
          "2016-04",
          "2016-06",
          "2016-08",
          "2016-11",
          "2016-12",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-06",
          "2017-08",
          "2017-10",
          "2017-11",
          "2017-12",
          "2018-01",
          "2018-02",
          "2018-06",
          "2018-07",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2020-04",
          "2017-07",
          "2019-03",
          "2019-09",
          "2013-07",
          "2014-04",
          "2014-06",
          "2014-08",
          "2014-12",
          "2015-03",
          "2015-05",
          "2015-06",
          "2015-07",
          "2015-09",
          "2015-11",
          "2016-04",
          "2016-07",
          "2016-09",
          "2016-10",
          "2017-01",
          "2017-03",
          "2017-05",
          "2017-06",
          "2017-08",
          "2017-09",
          "2017-10",
          "2018-02",
          "2018-03",
          "2018-04",
          "2018-05",
          "2018-08",
          "2018-12",
          "2019-02",
          "2019-03",
          "2019-05",
          "2019-07",
          "2019-08",
          "2013-03",
          "2014-06",
          "2017-01",
          "2017-05",
          "2017-07",
          "2017-11",
          "2018-04",
          "2019-01",
          "2019-07",
          "2012-02",
          "2012-12",
          "2013-01",
          "2013-06",
          "2013-12",
          "2015-02",
          "2015-04",
          "2015-09",
          "2015-11",
          "2015-12",
          "2016-03",
          "2016-12",
          "2017-07",
          "2017-08",
          "2017-10",
          "2017-11",
          "2018-02",
          "2018-03",
          "2018-05",
          "2018-06",
          "2018-07",
          "2018-10",
          "2019-01",
          "2019-02",
          "2019-04",
          "2019-06",
          "2019-10",
          "2019-12",
          "2014-10",
          "2015-11",
          "2016-01",
          "2016-06",
          "2016-10",
          "2017-03",
          "2017-05",
          "2017-06",
          "2017-09",
          "2017-10",
          "2017-12",
          "2018-02",
          "2018-07",
          "2018-09",
          "2018-11",
          "2018-12",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-10",
          "2019-11",
          "2019-12",
          "2016-11",
          "2017-11",
          "2014-03",
          "2014-06",
          "2014-07",
          "2014-11",
          "2015-04",
          "2015-05",
          "2015-06",
          "2015-11",
          "2015-12",
          "2016-03",
          "2016-08",
          "2016-09",
          "2016-10",
          "2016-11",
          "2016-12",
          "2017-02",
          "2017-03",
          "2017-07",
          "2017-08",
          "2017-11",
          "2017-12",
          "2018-03",
          "2018-06",
          "2018-10",
          "2018-12",
          "2019-03",
          "2019-04",
          "2019-05",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-12",
          "2012-02",
          "2015-12",
          "2017-09",
          "2018-06",
          "2019-04",
          "2015-04",
          "2015-12",
          "2016-06",
          "2016-11",
          "2017-04",
          "2017-06",
          "2018-11",
          "2019-01",
          "2019-07",
          "2019-09",
          "2016-03",
          "2016-04",
          "2016-10",
          "2016-12",
          "2018-03",
          "2012-03",
          "2012-08",
          "2013-12",
          "2014-12",
          "2015-04",
          "2015-08",
          "2015-11",
          "2015-12",
          "2016-04",
          "2016-06",
          "2016-08",
          "2016-11",
          "2017-03",
          "2017-04",
          "2017-06",
          "2017-10",
          "2018-04",
          "2018-05",
          "2018-10",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-08",
          "2019-10",
          "2015-06",
          "2016-09",
          "2016-10",
          "2017-08",
          "2018-06",
          "2018-08",
          "2019-04",
          "2019-07",
          "2019-12",
          "2020-03",
          "2010-11",
          "2012-12",
          "2014-06",
          "2014-09",
          "2014-11",
          "2014-12",
          "2015-02",
          "2015-05",
          "2015-06",
          "2015-08",
          "2015-11",
          "2015-12",
          "2016-04",
          "2016-05",
          "2016-06",
          "2016-07",
          "2016-08",
          "2016-09",
          "2016-10",
          "2016-12",
          "2017-02",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-06",
          "2017-07",
          "2017-08",
          "2017-09",
          "2017-11",
          "2017-12",
          "2018-01",
          "2018-03",
          "2018-06",
          "2018-07",
          "2018-08",
          "2018-10",
          "2018-11",
          "2018-12",
          "2019-02",
          "2019-04",
          "2019-05",
          "2019-06",
          "2019-07",
          "2019-08",
          "2020-02",
          "2010-03",
          "2013-12",
          "2014-07",
          "2014-11",
          "2015-11",
          "2016-01",
          "2016-03",
          "2016-05",
          "2016-11",
          "2016-12",
          "2017-01",
          "2017-02",
          "2017-03",
          "2017-04",
          "2017-05",
          "2017-08",
          "2017-10",
          "2017-11",
          "2017-12",
          "2018-02",
          "2018-03",
          "2018-09",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-10",
          "2020-01",
          "2020-04",
          "2016-11",
          "2017-10",
          "2018-04",
          "2014-07",
          "2014-11",
          "2014-12",
          "2015-05",
          "2015-11",
          "2015-12",
          "2016-03",
          "2016-04",
          "2016-05",
          "2016-06",
          "2016-11",
          "2016-12",
          "2017-02",
          "2017-03",
          "2017-04",
          "2017-06",
          "2017-07",
          "2017-09",
          "2017-11",
          "2018-01",
          "2018-03",
          "2018-04",
          "2018-06",
          "2018-07",
          "2018-08",
          "2018-09",
          "2018-10",
          "2018-11",
          "2018-12",
          "2019-01",
          "2019-02",
          "2019-03",
          "2019-04",
          "2019-06",
          "2019-07",
          "2019-08",
          "2019-09",
          "2019-11",
          "2019-12",
          "2020-01"
         ],
         "y": [
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Action localization",
          "Activity detection",
          "Activity detection",
          "Activity detection",
          "Activity detection",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity localization",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Activity recognition",
          "Emotion recognition",
          "Emotion recognition",
          "Emotion recognition",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Gesture recognition",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image classification",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image generation",
          "Image-to-image translation",
          "Image-to-image translation",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object detection",
          "Object recognition",
          "Object recognition",
          "Object recognition",
          "Object recognition",
          "Object recognition",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Object tracking",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other 3D task",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other image process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other video process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Other vision process",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose estimation",
          "Pose tracking",
          "Pose tracking",
          "Pose tracking",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation",
          "Semantic segmentation"
         ]
        },
        {
         "hovertemplate": [
          "<BR>task: Other image process<BR>date: 2012-08<BR>ratio: 0.5546<BR>benchmarks:<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Image classification<BR>date: 2012-12<BR>ratio: 0.0189<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Activity recognition<BR>date: 2013-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Skeleton Based Action Recognition: CAD-120 - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2013-02<BR>ratio: 0.1073<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2013-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Facial Expression Recognition: FER2013 - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2013-11<BR>ratio: 0.0397<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2013-12<BR>ratio: 0.0553<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2014-04<BR>ratio: 0.021<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>",
          "<BR>task: Activity recognition<BR>date: 2014-06<BR>ratio: 0.8122<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2014-06<BR>ratio: 0.289<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2014-06<BR>ratio: 0.4737<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2014-09<BR>ratio: 0.9752<BR>benchmarks:<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-Olympic - Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2014-09<BR>ratio: 0.1446<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2014-11<BR>ratio: 0.2834<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Object detection<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Detection: Caltech - Pedestrian Detection benchmarking - Reasonable Miss Rate<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2014-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2014-12<BR>ratio: 0.2375<BR>benchmarks:<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>",
          "<BR>task: Image classification<BR>date: 2014-12<BR>ratio: 0.1392<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2014-12<BR>ratio: 0.2513<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-02<BR>ratio: 0.0877<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-02<BR>ratio: 0.0451<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2015-02<BR>ratio: 0.0134<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2015-02<BR>ratio: 0.6408<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2015-03<BR>ratio: 0.3753<BR>benchmarks:<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-03<BR>ratio: 0.2327<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-03<BR>ratio: 0.0741<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Other vision process<BR>date: 2015-04<BR>ratio: 0.1007<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Other image process<BR>date: 2015-04<BR>ratio: 0.426<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-04<BR>ratio: 0.2878<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Other vision process<BR>date: 2015-05<BR>ratio: 0.24846666666666664<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2015-05<BR>ratio: 0.1675<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-05<BR>ratio: 0.8426<BR>benchmarks:<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>",
          "<BR>task: Image classification<BR>date: 2015-06<BR>ratio: 0.4459<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Object detection<BR>date: 2015-06<BR>ratio: 0.2457<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-08<BR>ratio: 0.4321<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-09<BR>ratio: 0.5<BR>benchmarks:<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2015-11<BR>ratio: 0.17486666666666664<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Image classification<BR>date: 2015-11<BR>ratio: 0.3169<BR>benchmarks:<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2015-11<BR>ratio: 0.58925<BR>benchmarks:<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Jaccard Index<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Pose estimation<BR>date: 2015-11<BR>ratio: 0.0331<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Other image process<BR>date: 2015-11<BR>ratio: 0.1694<BR>benchmarks:<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>",
          "<BR>task: Object detection<BR>date: 2015-11<BR>ratio: 0.4339<BR>benchmarks:<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Object detection<BR>date: 2015-12<BR>ratio: 0.5016<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>",
          "<BR>task: Image classification<BR>date: 2015-12<BR>ratio: 0.47796666666666665<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2015-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Action Recognition: ActivityNet - Action Recognition benchmarking - mAP<BR>",
          "<BR>task: Other vision process<BR>date: 2015-12<BR>ratio: 0.7477<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-01<BR>ratio: 0.6524<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Image generation<BR>date: 2016-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: Binarized MNIST - Image Generation benchmarking - nats<BR>",
          "<BR>task: Action localization<BR>date: 2016-01<BR>ratio: 0.0478<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-01<BR>ratio: 0.0707<BR>benchmarks:<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Image classification<BR>date: 2016-02<BR>ratio: 0.056<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Multimodal Activity Recognition: MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2016-03<BR>ratio: 0.2255<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-03<BR>ratio: 0.1471<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2016-03<BR>ratio: 0.40365<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Object detection<BR>date: 2016-03<BR>ratio: 0.3359<BR>benchmarks:<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other vision process<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-03<BR>ratio: 0.7322<BR>benchmarks:<BR>  Pose Estimation: FLIC Elbows - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: FLIC Wrists - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-04<BR>ratio: 0.21814999999999998<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>",
          "<BR>task: Other image process<BR>date: 2016-04<BR>ratio: 0.5532666666666667<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - NMI<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>",
          "<BR>task: Other vision process<BR>date: 2016-04<BR>ratio: 0.5947<BR>benchmarks:<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-relRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Image classification<BR>date: 2016-05<BR>ratio: 0.1069<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Other vision process<BR>date: 2016-05<BR>ratio: 0.1741<BR>benchmarks:<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-05<BR>ratio: 0.0418<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Image generation<BR>date: 2016-06<BR>ratio: 0.46724999999999994<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-06<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-06<BR>ratio: 0.0741<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Object detection<BR>date: 2016-06<BR>ratio: 0.31905<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>",
          "<BR>task: Other vision process<BR>date: 2016-07<BR>ratio: 0.0241<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>",
          "<BR>task: Other vision process<BR>date: 2016-08<BR>ratio: 0.6356666666666667<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Signs-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>  Horizon Line Estimation: Eurasian Cities Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: York Urban Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>",
          "<BR>task: Object detection<BR>date: 2016-08<BR>ratio: 0.2999<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-08<BR>ratio: 0.4643<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-08<BR>ratio: 0.1233<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Other image process<BR>date: 2016-08<BR>ratio: 0.36235<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Image classification<BR>date: 2016-08<BR>ratio: 0.0251<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Object recognition<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Backpack<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Gender<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Hat<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCS<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCS<BR>",
          "<BR>task: Action localization<BR>date: 2016-09<BR>ratio: 0.2784<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-09<BR>ratio: 0.0093<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>",
          "<BR>task: Object detection<BR>date: 2016-09<BR>ratio: 0.0144<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other vision process<BR>date: 2016-09<BR>ratio: 0.4115<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-09<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>",
          "<BR>task: Image generation<BR>date: 2016-10<BR>ratio: 0.0606<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Image classification<BR>date: 2016-10<BR>ratio: 0.0142<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Action localization<BR>date: 2016-11<BR>ratio: 0.3571<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-11<BR>ratio: 0.674<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-11<BR>ratio: 0.3399<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>",
          "<BR>task: Other image process<BR>date: 2016-11<BR>ratio: 0.2441<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>",
          "<BR>task: Other vision process<BR>date: 2016-11<BR>ratio: 0.3954<BR>benchmarks:<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>",
          "<BR>task: Object tracking<BR>date: 2016-11<BR>ratio: 0.096<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>",
          "<BR>task: Object detection<BR>date: 2016-11<BR>ratio: 0.27403333333333335<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other video process<BR>date: 2016-11<BR>ratio: 0.2492<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>",
          "<BR>task: Image classification<BR>date: 2016-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-11<BR>ratio: 0.26530000000000004<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Global Accuracy<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Other 3D task<BR>date: 2016-12<BR>ratio: 0.77015<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D−R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D−R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Reconstruction: Data3D−R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>",
          "<BR>task: Object detection<BR>date: 2016-12<BR>ratio: 0.07565<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>",
          "<BR>task: Other image process<BR>date: 2016-12<BR>ratio: 0.1001<BR>benchmarks:<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>",
          "<BR>task: Pose estimation<BR>date: 2016-12<BR>ratio: 0.5639000000000001<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>",
          "<BR>task: Image generation<BR>date: 2016-12<BR>ratio: 0.1288<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Other video process<BR>date: 2016-12<BR>ratio: 0.1022<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2016-12<BR>ratio: 0.33213333333333334<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Activity recognition<BR>date: 2016-12<BR>ratio: 0.1429<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-01<BR>ratio: 0.16765000000000002<BR>benchmarks:<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-02<BR>ratio: 0.0692<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Image generation<BR>date: 2017-02<BR>ratio: 0.0541<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Image classification<BR>date: 2017-02<BR>ratio: 0.6995<BR>benchmarks:<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>",
          "<BR>task: Object detection<BR>date: 2017-02<BR>ratio: 0.9524<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Medium MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Small MR^-2<BR>",
          "<BR>task: Object detection<BR>date: 2017-03<BR>ratio: 0.1565<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-03<BR>ratio: 0.2235<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-03<BR>ratio: 0.1193<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Activity detection<BR>date: 2017-03<BR>ratio: 0.2205<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>",
          "<BR>task: Image generation<BR>date: 2017-03<BR>ratio: 0.13935<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>",
          "<BR>task: Action localization<BR>date: 2017-03<BR>ratio: 0.1096<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-03<BR>ratio: 0.44627500000000003<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Other image process<BR>date: 2017-03<BR>ratio: 0.5645<BR>benchmarks:<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-03<BR>ratio: 0.3988<BR>benchmarks:<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: SBU - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2017-03<BR>ratio: 0.3612<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Other image process<BR>date: 2017-04<BR>ratio: 0.5854250000000001<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Color Image Denoising: BSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: FRGC - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-04<BR>ratio: 0.20765<BR>benchmarks:<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-04<BR>ratio: 0.2189333333333333<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.5<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>",
          "<BR>task: Other vision process<BR>date: 2017-04<BR>ratio: 0.32234<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Object tracking<BR>date: 2017-04<BR>ratio: 0.103<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>",
          "<BR>task: Other 3D task<BR>date: 2017-04<BR>ratio: 0.8514<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: Sydney Urban Objects - 3D Point Cloud Classification benchmarking - F1<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-04<BR>ratio: 0.51655<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>",
          "<BR>task: Image classification<BR>date: 2017-04<BR>ratio: 0.4855<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2017-04<BR>ratio: 0.1476<BR>benchmarks:<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-04<BR>ratio: 0.15055000000000002<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-05<BR>ratio: 0.2803<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>",
          "<BR>task: Other vision process<BR>date: 2017-05<BR>ratio: 0.5805666666666667<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-05<BR>ratio: 0.53035<BR>benchmarks:<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CS)<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>",
          "<BR>task: Action localization<BR>date: 2017-05<BR>ratio: 0.2561<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Frame-mAP<BR>",
          "<BR>task: Gesture recognition<BR>date: 2017-05<BR>ratio: 0.8352<BR>benchmarks:<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-05<BR>ratio: 0.2366<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-06<BR>ratio: 0.4898<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Other 3D task<BR>date: 2017-06<BR>ratio: 0.4844<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Object tracking<BR>date: 2017-06<BR>ratio: 0.6175<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Seen)<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-06<BR>ratio: 0.6277<BR>benchmarks:<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2017-06<BR>ratio: 0.6641<BR>benchmarks:<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other image process<BR>date: 2017-06<BR>ratio: 0.7104<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-06<BR>ratio: 0.2771333333333333<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>",
          "<BR>task: Image generation<BR>date: 2017-06<BR>ratio: 0.2514<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Other vision process<BR>date: 2017-06<BR>ratio: 0.37770000000000004<BR>benchmarks:<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>",
          "<BR>task: Image classification<BR>date: 2017-07<BR>ratio: 0.0363<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-07<BR>ratio: 0.8307<BR>benchmarks:<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-07<BR>ratio: 0.6997<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - ME (%, all)<BR>",
          "<BR>task: Object detection<BR>date: 2017-07<BR>ratio: 0.3431666666666667<BR>benchmarks:<BR>  Object Detection: Visual Genome - Object Detection benchmarking - MAP<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other video process<BR>date: 2017-07<BR>ratio: 0.2484<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Other vision process<BR>date: 2017-07<BR>ratio: 0.2633333333333333<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-08<BR>ratio: 0.4921<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Image classification<BR>date: 2017-08<BR>ratio: 0.219<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Object detection<BR>date: 2017-08<BR>ratio: 0.3489<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - Average MAE<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-08<BR>ratio: 0.5913<BR>benchmarks:<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-08<BR>ratio: 0.5744<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-08<BR>ratio: 0.50415<BR>benchmarks:<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Other vision process<BR>date: 2017-08<BR>ratio: 0.17363333333333333<BR>benchmarks:<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2017-09<BR>ratio: 0.0809<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Other image process<BR>date: 2017-09<BR>ratio: 0.8514<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Other vision process<BR>date: 2017-09<BR>ratio: 0.3414<BR>benchmarks:<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Image generation<BR>date: 2017-09<BR>ratio: 0.043050000000000005<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Pancreas Segmentation: TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking - Dice Score<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-09<BR>ratio: 0.0551<BR>benchmarks:<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  3D Human Pose Estimation: Total Capture - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>",
          "<BR>task: Object detection<BR>date: 2017-10<BR>ratio: 0.9976<BR>benchmarks:<BR>  Lane Detection: Caltech Lanes Cordova - Lane Detection benchmarking - F1<BR>  Lane Detection: Caltech Lanes Washington - Lane Detection benchmarking - F1<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: UCF - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>",
          "<BR>task: Image classification<BR>date: 2017-10<BR>ratio: 0.29335<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-10<BR>ratio: 0.656<BR>benchmarks:<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>",
          "<BR>task: Image generation<BR>date: 2017-10<BR>ratio: 0.5297<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Other vision process<BR>date: 2017-10<BR>ratio: 0.7097<BR>benchmarks:<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-10<BR>ratio: 0.23095<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-10<BR>ratio: 0.9847<BR>benchmarks:<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Other image process<BR>date: 2017-10<BR>ratio: 0.5759333333333333<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Object tracking<BR>date: 2017-10<BR>ratio: 0.6429<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-11<BR>ratio: 0.435<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-11<BR>ratio: 0.3768<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>  Pose Estimation: ITOP top-view - Pose Estimation benchmarking - Mean mAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-11<BR>ratio: 0.44705<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CS<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>",
          "<BR>task: Object detection<BR>date: 2017-11<BR>ratio: 0.5154000000000001<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Birds Eye View Object Detection: KITTI Cars Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP50<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>",
          "<BR>task: Image-to-image translation<BR>date: 2017-11<BR>ratio: 0.2407<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>",
          "<BR>task: Other vision process<BR>date: 2017-11<BR>ratio: 0.1634<BR>benchmarks:<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-11<BR>ratio: 0.40068333333333334<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ShapeNet - Semantic Segmentation benchmarking - Mean IoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>",
          "<BR>task: Other 3D task<BR>date: 2017-11<BR>ratio: 0.789<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>",
          "<BR>task: Pose estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2017 - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: J-HMDB - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>",
          "<BR>task: Activity recognition<BR>date: 2017-12<BR>ratio: 0.1885<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>",
          "<BR>task: Other vision process<BR>date: 2017-12<BR>ratio: 0.613<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SYNSIG-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2017-12<BR>ratio: 0.0083<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2017-12<BR>ratio: 0.4283<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>",
          "<BR>task: Activity detection<BR>date: 2017-12<BR>ratio: 0.5028<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>",
          "<BR>task: Pose tracking<BR>date: 2017-12<BR>ratio: 0.1898<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>",
          "<BR>task: Other image process<BR>date: 2017-12<BR>ratio: 0.046<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>",
          "<BR>task: Image generation<BR>date: 2017-12<BR>ratio: 0.5<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - IS<BR>",
          "<BR>task: Activity localization<BR>date: 2017-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2017-12<BR>ratio: 0.0439<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2017-12<BR>ratio: 0.0247<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Other image process<BR>date: 2018-01<BR>ratio: 0.3197<BR>benchmarks:<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-01<BR>ratio: 0.28373333333333334<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-1 (%)<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-5 (%)<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-01<BR>ratio: 0.5359<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-01<BR>ratio: 0.0583<BR>benchmarks:<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-01<BR>ratio: 0.44029999999999997<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Retinal OCT Disease Classification: Srinivasan2014 - Retinal OCT Disease Classification benchmarking - Acc<BR>",
          "<BR>task: Other vision process<BR>date: 2018-01<BR>ratio: 0.3059<BR>benchmarks:<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-02<BR>ratio: 0.9695<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMBD Early Action - Skeleton Based Action Recognition benchmarking - 10%<BR>",
          "<BR>task: Other vision process<BR>date: 2018-02<BR>ratio: 0.3996<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>",
          "<BR>task: Object tracking<BR>date: 2018-02<BR>ratio: 0.7714000000000001<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-02<BR>ratio: 0.628<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D−R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>",
          "<BR>task: Image generation<BR>date: 2018-02<BR>ratio: 0.3205<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-02<BR>ratio: 0.4785333333333333<BR>benchmarks:<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>",
          "<BR>task: Image classification<BR>date: 2018-02<BR>ratio: 0.1678<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>",
          "<BR>task: Object detection<BR>date: 2018-02<BR>ratio: 0.1698<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Pose tracking<BR>date: 2018-02<BR>ratio: 0.2155<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>",
          "<BR>task: Other image process<BR>date: 2018-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma50 - Color Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-03<BR>ratio: 0.1351<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-03<BR>ratio: 0.49684999999999996<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Face Identification: IJB-A - Face Identification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Object tracking<BR>date: 2018-03<BR>ratio: 0.1202<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>",
          "<BR>task: Image classification<BR>date: 2018-03<BR>ratio: 0.1184<BR>benchmarks:<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-03<BR>ratio: 0.24766666666666667<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Object detection<BR>date: 2018-03<BR>ratio: 0.62775<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: iSAID - Object Detection benchmarking - Average Precision<BR>  Weakly Supervised Object Detection: Watercolor2k - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other image process<BR>date: 2018-03<BR>ratio: 0.1099<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-03<BR>ratio: 0.46340000000000003<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - Average Accuracy<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Activity detection<BR>date: 2018-03<BR>ratio: 0.387<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>",
          "<BR>task: Other vision process<BR>date: 2018-03<BR>ratio: 0.1918<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Image generation<BR>date: 2018-03<BR>ratio: 0.3293<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Other video process<BR>date: 2018-04<BR>ratio: 0.6147<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Pose tracking<BR>date: 2018-04<BR>ratio: 0.6801<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>",
          "<BR>task: Object detection<BR>date: 2018-04<BR>ratio: 0.0067<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-04<BR>ratio: 0.21509999999999999<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-04<BR>ratio: 0.5245<BR>benchmarks:<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-04<BR>ratio: 0.211<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>",
          "<BR>task: Gesture recognition<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLearn val - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: Jester test - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>",
          "<BR>task: Action localization<BR>date: 2018-04<BR>ratio: 0.3804<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-04<BR>ratio: 0.27253333333333335<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>",
          "<BR>task: Other vision process<BR>date: 2018-04<BR>ratio: 0.3826<BR>benchmarks:<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Other image process<BR>date: 2018-04<BR>ratio: 0.3237<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-04<BR>ratio: 0.6517<BR>benchmarks:<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Dice Score<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Precision<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Recall<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-05<BR>ratio: 0.028<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-05<BR>ratio: 0.2306<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-05<BR>ratio: 0.857<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Facial Expression Recognition: Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-05<BR>ratio: 0.145<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-05<BR>ratio: 0.6543<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UAV-Human - Skeleton Based Action Recognition benchmarking - Average Accuracy<BR>",
          "<BR>task: Image generation<BR>date: 2018-05<BR>ratio: 0.2865<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Object detection<BR>date: 2018-05<BR>ratio: 0.6951<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other vision process<BR>date: 2018-05<BR>ratio: 0.23425<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Other image process<BR>date: 2018-05<BR>ratio: 0.6039<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Other image process<BR>date: 2018-06<BR>ratio: 0.2047<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Activity localization<BR>date: 2018-06<BR>ratio: 0.7435<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (test)<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>",
          "<BR>task: Other video process<BR>date: 2018-06<BR>ratio: 0.3129<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>",
          "<BR>task: Object tracking<BR>date: 2018-06<BR>ratio: 0.2077<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-06<BR>ratio: 0.1128<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-06<BR>ratio: 0.29605000000000004<BR>benchmarks:<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-06<BR>ratio: 0.9967<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - 3DPCK<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - MJPE<BR>",
          "<BR>task: Emotion recognition<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>",
          "<BR>task: Object detection<BR>date: 2018-06<BR>ratio: 0.5132666666666666<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - F1 score<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Action localization<BR>date: 2018-06<BR>ratio: 0.3894<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>",
          "<BR>task: Other vision process<BR>date: 2018-06<BR>ratio: 0.7323200000000001<BR>benchmarks:<BR>  Formation Energy: Materials Project - Formation Energy benchmarking - MAE<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE log<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - SQ Rel<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Multivariate Time Series Imputation: MuJoCo - Multivariate Time Series Imputation benchmarking - MSE (10^2, 50% missing)<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - mse (10^-3)<BR>  Scene Graph Generation: VRD - Scene Graph Generation benchmarking - Recall-at-50<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Object recognition<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  Traffic Sign Recognition: Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking - MAP<BR>  Traffic Sign Recognition: Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking - MAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-06<BR>ratio: 0.3492333333333333<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: UT-Kinect - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Image generation<BR>date: 2018-07<BR>ratio: 0.7291<BR>benchmarks:<BR>  Image Generation: CAT 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>",
          "<BR>task: Object detection<BR>date: 2018-07<BR>ratio: 0.46819999999999995<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Bare MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Partial MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: ImageNet - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Activity localization<BR>date: 2018-07<BR>ratio: 0.4436<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Image classification<BR>date: 2018-07<BR>ratio: 0.53155<BR>benchmarks:<BR>  Hyperspectral Image Classification: Pavia University - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-07<BR>ratio: 0.5724<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - S-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - max E-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - mean Dice<BR>",
          "<BR>task: Other image process<BR>date: 2018-07<BR>ratio: 0.4427<BR>benchmarks:<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - PSNR (sRGB)<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - SSIM (sRGB)<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Other vision process<BR>date: 2018-07<BR>ratio: 0.2614<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-07<BR>ratio: 0.9928<BR>benchmarks:<BR>  Action Classification: ActivityNet-1.2 - Action Classification benchmarking - mAP<BR>  Action Classification: THUMOS’14 - Action Classification benchmarking - mAP<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>",
          "<BR>task: Other video process<BR>date: 2018-08<BR>ratio: 0.3547<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-08<BR>ratio: 0.6807<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Other vision process<BR>date: 2018-08<BR>ratio: 0.2432<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - Runtime [ms]<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-08<BR>ratio: 0.0244<BR>benchmarks:<BR>  3D Reconstruction: Data3D−R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-08<BR>ratio: 0.6698<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: Freiburg Forest - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ScanNetV2 - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-09<BR>ratio: 0.14682499999999998<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-09<BR>ratio: 0.46865<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Other vision process<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>",
          "<BR>task: Image generation<BR>date: 2018-09<BR>ratio: 0.37644999999999995<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Emotion recognition<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>",
          "<BR>task: Other image process<BR>date: 2018-10<BR>ratio: 0.14425<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-10<BR>ratio: 0.0156<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>",
          "<BR>task: Image classification<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Sequential Image Classification: Sequential CIFAR-10 - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-10<BR>ratio: 0.2615<BR>benchmarks:<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>",
          "<BR>task: Other video process<BR>date: 2018-10<BR>ratio: 0.2609<BR>benchmarks:<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-10<BR>ratio: 0.0079<BR>benchmarks:<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>",
          "<BR>task: Other vision process<BR>date: 2018-11<BR>ratio: 0.4657<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MAE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MSE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - SSIM<BR>",
          "<BR>task: Activity localization<BR>date: 2018-11<BR>ratio: 0.1431<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>",
          "<BR>task: Object tracking<BR>date: 2018-11<BR>ratio: 0.3892<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Pose Estimation: DensePose-COCO - Pose Estimation benchmarking - AP<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Frames Per View<BR>",
          "<BR>task: Object detection<BR>date: 2018-11<BR>ratio: 0.3155<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-11<BR>ratio: 0.4542<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-11<BR>ratio: 0.40796666666666664<BR>benchmarks:<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.5<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2018-11<BR>ratio: 0.0808<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-11<BR>ratio: 0.87635<BR>benchmarks:<BR>  3D Reconstruction: Scan2CAD - 3D Reconstruction benchmarking - Average Accuracy<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>",
          "<BR>task: Emotion recognition<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>",
          "<BR>task: Image generation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Other image process<BR>date: 2018-11<BR>ratio: 0.43694999999999995<BR>benchmarks:<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Object detection<BR>date: 2018-12<BR>ratio: 0.30150000000000005<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2018-12<BR>ratio: 0.4456333333333333<BR>benchmarks:<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>",
          "<BR>task: Activity recognition<BR>date: 2018-12<BR>ratio: 0.500225<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - mAP (Val)<BR>  Action Recognition: AVA v2.2 - Action Recognition benchmarking - mAP<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Object tracking<BR>date: 2018-12<BR>ratio: 0.1675<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>",
          "<BR>task: Other 3D task<BR>date: 2018-12<BR>ratio: 0.027<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Image generation<BR>date: 2018-12<BR>ratio: 0.6087<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - bpd<BR>  Image Generation: CelebA-HQ 1024x1024 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>",
          "<BR>task: Pose estimation<BR>date: 2018-12<BR>ratio: 0.27595000000000003<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>",
          "<BR>task: Other vision process<BR>date: 2018-12<BR>ratio: 0.73865<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - OOB Rate (10^−3)<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Difference<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Length<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Step Change (10^−3)<BR>  Multivariate Time Series Imputation: PEMS-SF - Multivariate Time Series Imputation benchmarking - L2 Loss (10^-4)<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Unsupervised Domain Adaptation: Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Gesture recognition<BR>date: 2018-12<BR>ratio: 0.6789<BR>benchmarks:<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: NVGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2018-12<BR>ratio: 0.30485<BR>benchmarks:<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2018-12<BR>ratio: 0.4718<BR>benchmarks:<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Object detection<BR>date: 2019-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>",
          "<BR>task: Gesture recognition<BR>date: 2019-01<BR>ratio: 0.564<BR>benchmarks:<BR>  Hand Gesture Recognition: Cambridge - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-01<BR>ratio: 0.9117666666666667<BR>benchmarks:<BR>  Action Recognition: ICVL-4 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: IRD - Action Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: LboroHAR - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-01<BR>ratio: 0.16060000000000002<BR>benchmarks:<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Other image process<BR>date: 2019-01<BR>ratio: 0.1323<BR>benchmarks:<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Image classification<BR>date: 2019-01<BR>ratio: 0.3141<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Other 3D task<BR>date: 2019-01<BR>ratio: 0.24609999999999999<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D−R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D−R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>",
          "<BR>task: Other vision process<BR>date: 2019-01<BR>ratio: 0.3127<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-01<BR>ratio: 0.20259999999999997<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-02<BR>ratio: 0.5371333333333334<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (7 emotion)<BR>  Facial Expression Recognition: JAFFE - Facial Expression Recognition benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>  Facial Landmark Detection: AFLW-Full - Facial Landmark Detection benchmarking - Mean NME<BR>",
          "<BR>task: Image classification<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Hyperspectral Image Classification: Indian Pines - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-02<BR>ratio: 0.1<BR>benchmarks:<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>",
          "<BR>task: Pose tracking<BR>date: 2019-02<BR>ratio: 0.0183<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>",
          "<BR>task: Other vision process<BR>date: 2019-02<BR>ratio: 0.22343333333333334<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Other image process<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Retrieval: INRIA Holidays - Image Retrieval benchmarking - Mean mAP<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-02<BR>ratio: 0.0788<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Views<BR>",
          "<BR>task: Action localization<BR>date: 2019-03<BR>ratio: 0.6113999999999999<BR>benchmarks:<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>",
          "<BR>task: Other vision process<BR>date: 2019-03<BR>ratio: 0.5759749999999999<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Generalization: ImageNet-C - Domain Generalization benchmarking - mean Corruption Error (mCE)<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Image generation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 128x128 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-03<BR>ratio: 0.549<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mRec<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-03<BR>ratio: 0.1256<BR>benchmarks:<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Object detection<BR>date: 2019-03<BR>ratio: 0.2573<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - NDS<BR>",
          "<BR>task: Other image process<BR>date: 2019-03<BR>ratio: 0.46475<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: CUB-200-2011 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-04<BR>ratio: 0.355<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>",
          "<BR>task: Other video process<BR>date: 2019-04<BR>ratio: 0.1348<BR>benchmarks:<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>",
          "<BR>task: Other image process<BR>date: 2019-04<BR>ratio: 0.68665<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>",
          "<BR>task: Image classification<BR>date: 2019-04<BR>ratio: 0.3059<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Emotion recognition<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>",
          "<BR>task: Activity detection<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.1<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.2<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-04<BR>ratio: 0.3335<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP50<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP75<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>",
          "<BR>task: Action localization<BR>date: 2019-04<BR>ratio: 0.155<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>",
          "<BR>task: Other 3D task<BR>date: 2019-04<BR>ratio: 0.1081<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>",
          "<BR>task: Image generation<BR>date: 2019-04<BR>ratio: 0.52595<BR>benchmarks:<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - SSIM<BR>",
          "<BR>task: Object recognition<BR>date: 2019-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP at-0.5:0.95<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP-at-0.50<BR>",
          "<BR>task: Object detection<BR>date: 2019-04<BR>ratio: 0.500375<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-04<BR>ratio: 0.279925<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.1<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>",
          "<BR>task: Other vision process<BR>date: 2019-04<BR>ratio: 0.37775000000000003<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-05<BR>ratio: 0.7067<BR>benchmarks:<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>  Facial Expression Recognition: FERPlus - Facial Expression Recognition benchmarking - Accuracy<BR>  Facial Expression Recognition: SFEW - Facial Expression Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-05<BR>ratio: 0.7854<BR>benchmarks:<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-05<BR>ratio: 0.17579999999999998<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>",
          "<BR>task: Image classification<BR>date: 2019-05<BR>ratio: 0.1623<BR>benchmarks:<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>",
          "<BR>task: Pose tracking<BR>date: 2019-05<BR>ratio: 0.0083<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>",
          "<BR>task: Activity localization<BR>date: 2019-05<BR>ratio: 0.7002<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>",
          "<BR>task: Other vision process<BR>date: 2019-05<BR>ratio: 0.7222285714285714<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iMAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iRMSE<BR>  Domain Adaptation: HMDBfull-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Formation Energy: OQMD v1.2 - Formation Energy benchmarking - MAE<BR>  Horizon Line Estimation: Horizon Lines in the Wild - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Video Prediction: CMU Mocap-2 - Video Prediction benchmarking - Test Error<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Action localization<BR>date: 2019-06<BR>ratio: 0.0985<BR>benchmarks:<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-06<BR>ratio: 0.0932<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>",
          "<BR>task: Other video process<BR>date: 2019-06<BR>ratio: 0.6079<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Object detection<BR>date: 2019-06<BR>ratio: 0.45675000000000004<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - mean E-Measure<BR>",
          "<BR>task: Object tracking<BR>date: 2019-06<BR>ratio: 0.2823<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>",
          "<BR>task: Image classification<BR>date: 2019-06<BR>ratio: 0.2305<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 1 Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-06<BR>ratio: 0.1399<BR>benchmarks:<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-06<BR>ratio: 0.1741666666666667<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-5 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-06<BR>ratio: 0.4972200000000001<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  Brain Tumor Segmentation: BRATS-2015 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2017 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - Total Variation of Information<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Merge<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Split<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Other vision process<BR>date: 2019-06<BR>ratio: 0.6436666666666667<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: UCF CC 50 - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Activity localization<BR>date: 2019-06<BR>ratio: 0.0589<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2019-07<BR>ratio: 0.42693333333333333<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Moderate val - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-07<BR>ratio: 0.16535<BR>benchmarks:<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS’14 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>",
          "<BR>task: Activity localization<BR>date: 2019-07<BR>ratio: 0.2415<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>",
          "<BR>task: Image generation<BR>date: 2019-07<BR>ratio: 0.0989<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Image-to-image translation<BR>date: 2019-07<BR>ratio: 0.7593<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>",
          "<BR>task: Action localization<BR>date: 2019-07<BR>ratio: 0.3796<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>",
          "<BR>task: Other vision process<BR>date: 2019-07<BR>ratio: 0.42065<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Object tracking<BR>date: 2019-07<BR>ratio: 0.835<BR>benchmarks:<BR>  Visual Object Tracking: VOT2016 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: VOT2017 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Unseen)<BR>",
          "<BR>task: Other video process<BR>date: 2019-07<BR>ratio: 0.4898<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>",
          "<BR>task: Image generation<BR>date: 2019-08<BR>ratio: 0.3<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-08<BR>ratio: 0.6697500000000001<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 5 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S1)<BR>",
          "<BR>task: Emotion recognition<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-08<BR>ratio: 0.3552333333333333<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>  Unsupervised Facial Landmark Detection: AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>",
          "<BR>task: Image classification<BR>date: 2019-08<BR>ratio: 0.6658<BR>benchmarks:<BR>  Document Image Classification: Noisy Bangla Characters - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: Noisy Bangla Numeral - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: n-MNIST - Document Image Classification benchmarking - Accuracy<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>",
          "<BR>task: Object detection<BR>date: 2019-08<BR>ratio: 0.3188<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-08<BR>ratio: 0.6767666666666666<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2013 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - AUC<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Scene Segmentation: SUN-RGBD - Scene Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>",
          "<BR>task: Activity localization<BR>date: 2019-08<BR>ratio: 0.1982<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>",
          "<BR>task: Other vision process<BR>date: 2019-08<BR>ratio: 0.491<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking - mIoU<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>",
          "<BR>task: Object detection<BR>date: 2019-09<BR>ratio: 0.1053<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>",
          "<BR>task: Other image process<BR>date: 2019-09<BR>ratio: 0.1412<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>",
          "<BR>task: Object tracking<BR>date: 2019-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-09<BR>ratio: 0.2029<BR>benchmarks:<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-09<BR>ratio: 0.41814999999999997<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: PASCAL VOC 2007 - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Other vision process<BR>date: 2019-09<BR>ratio: 0.3091<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: USPS-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-09<BR>ratio: 0.46025000000000005<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>",
          "<BR>task: Emotion recognition<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>",
          "<BR>task: Action localization<BR>date: 2019-09<BR>ratio: 0.26<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS’14 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>",
          "<BR>task: Other vision process<BR>date: 2019-10<BR>ratio: 0.0388<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-10<BR>ratio: 0.0205<BR>benchmarks:<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-10<BR>ratio: 0.5483<BR>benchmarks:<BR>  Face Verification: AgeDB-30 - Face Verification benchmarking - Accuracy<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2019-10<BR>ratio: 0.1955<BR>benchmarks:<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>",
          "<BR>task: Other image process<BR>date: 2019-10<BR>ratio: 0.9715<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-10<BR>ratio: 0.3902<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SkyScapes-Lane - Semantic Segmentation benchmarking - Mean IoU<BR>",
          "<BR>task: Object recognition<BR>date: 2019-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: PA-100K - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: PETA - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: RAP - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2019-10<BR>ratio: 0.12633333333333333<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-11<BR>ratio: 0.39412500000000006<BR>benchmarks:<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes test - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - mIoU<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - oAcc<BR>",
          "<BR>task: Other vision process<BR>date: 2019-11<BR>ratio: 0.48816666666666664<BR>benchmarks:<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Unsupervised Domain Adaptation: PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking - AP-at-0.7<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-11<BR>ratio: 0.0114<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2019-11<BR>ratio: 0.045899999999999996<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>",
          "<BR>task: Activity localization<BR>date: 2019-11<BR>ratio: 0.3356<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>",
          "<BR>task: Image generation<BR>date: 2019-11<BR>ratio: 0.057<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Action localization<BR>date: 2019-11<BR>ratio: 0.0838<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>",
          "<BR>task: Facial recognition and modelling<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Facial Expression Recognition: Oulu-CASIA - Facial Expression Recognition benchmarking - Accuracy (10-fold)<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-11<BR>ratio: 0.9097999999999999<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: LineMOD - 6D Pose Estimation benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>",
          "<BR>task: Image classification<BR>date: 2019-11<BR>ratio: 0.7974<BR>benchmarks:<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>  Satellite Image Classification: SAT-6 - Satellite Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Activity recognition<BR>date: 2019-12<BR>ratio: 0.1018<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>",
          "<BR>task: Image classification<BR>date: 2019-12<BR>ratio: 0.2424<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Params<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>",
          "<BR>task: Other video process<BR>date: 2019-12<BR>ratio: 0.7122<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>",
          "<BR>task: Pose estimation<BR>date: 2019-12<BR>ratio: 0.0932<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2019-12<BR>ratio: 0.15665<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>",
          "<BR>task: Image generation<BR>date: 2019-12<BR>ratio: 0.8135<BR>benchmarks:<BR>  Image Generation: CIFAR-100 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Cat 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Churches 256 x 256 - Image Generation benchmarking - FID<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2020-01<BR>ratio: 0.13385<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>",
          "<BR>task: Other vision process<BR>date: 2020-01<BR>ratio: 0.4159<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>",
          "<BR>task: Image classification<BR>date: 2020-01<BR>ratio: 0.25<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>",
          "<BR>task: Object detection<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD - 3D Object Detection benchmarking - mAP-at-0.25<BR>",
          "<BR>task: Pose estimation<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Pose Estimation: UPenn Action - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>",
          "<BR>task: Other vision process<BR>date: 2020-02<BR>ratio: 0.35829999999999995<BR>benchmarks:<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>",
          "<BR>task: Pose estimation<BR>date: 2020-02<BR>ratio: 0.0125<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>",
          "<BR>task: Activity recognition<BR>date: 2020-02<BR>ratio: 0.4211<BR>benchmarks:<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>",
          "<BR>task: Action localization<BR>date: 2020-03<BR>ratio: 0.6966<BR>benchmarks:<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2020-03<BR>ratio: 0.6336<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  3D Instance Segmentation: SceneNN - 3D Instance Segmentation benchmarking - mAP-at-0.5<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>",
          "<BR>task: Object detection<BR>date: 2020-03<BR>ratio: 0.2605<BR>benchmarks:<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>",
          "<BR>task: Other video process<BR>date: 2020-03<BR>ratio: 0.8021<BR>benchmarks:<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>",
          "<BR>task: Pose estimation<BR>date: 2020-04<BR>ratio: 0.4396<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>",
          "<BR>task: Semantic segmentation<BR>date: 2020-04<BR>ratio: 0.22005000000000002<BR>benchmarks:<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: NYU Depth v2 - Semantic Segmentation benchmarking - Mean IoU<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>"
         ],
         "line": {
          "color": "black",
          "width": 0
         },
         "marker": {
          "color": [
           0.5546,
           0.0189,
           1,
           0.1073,
           1,
           0.0397,
           0.0553,
           0.021,
           0.8122,
           0.289,
           0.4737,
           0.9752,
           0.1446,
           0.2834,
           1,
           0.2406,
           0.2375,
           0.1392,
           0.2513,
           0.0877,
           0.0451,
           0.0134,
           0.6408,
           0.3753,
           0.2327,
           0.0741,
           0.1007,
           0.426,
           0.2878,
           0.24846666666666664,
           0.1675,
           0.8426,
           0.4459,
           0.2457,
           0.4321,
           0.5,
           0.17486666666666664,
           0.3169,
           0.58925,
           0.0331,
           0.1694,
           0.4339,
           0.5016,
           0.47796666666666665,
           1,
           0.7477,
           0.6524,
           1,
           0.0478,
           0.0707,
           0.056,
           1,
           0.2255,
           0.1471,
           0.40365,
           0.3359,
           0.2757,
           0.7322,
           0.21814999999999998,
           0.5532666666666667,
           0.5947,
           0.1069,
           0.1741,
           0.0418,
           0.46724999999999994,
           0.061,
           0.2999,
           0.0741,
           0.31905,
           0.0241,
           0.6356666666666667,
           0.2999,
           0.4643,
           0.1233,
           0.36235,
           0.0251,
           1,
           0.2784,
           0.0093,
           0.0144,
           0.4115,
           0.3082,
           0.0606,
           0.0142,
           0.3571,
           0.674,
           0.3399,
           0.2441,
           0.3954,
           0.096,
           0.27403333333333335,
           0.2492,
           0.0684,
           0.26530000000000004,
           0.77015,
           0.07565,
           0.1001,
           0.5639000000000001,
           0.1288,
           0.1022,
           0.33213333333333334,
           0.1429,
           0.16765000000000002,
           0.0692,
           0.0541,
           0.6995,
           0.9524,
           0.1565,
           0.2235,
           0.1193,
           0.2205,
           0.13935,
           0.1096,
           0.44627500000000003,
           0.5645,
           0.3988,
           0.3612,
           0.5854250000000001,
           0.20765,
           0.2189333333333333,
           0.32234,
           0.103,
           0.8514,
           0.51655,
           0.4855,
           0.1476,
           0.15055000000000002,
           0.2803,
           0.5805666666666667,
           0.53035,
           0.2561,
           0.8352,
           0.2366,
           0.4898,
           0.4844,
           0.6175,
           0.6277,
           0.6641,
           0.7104,
           0.2771333333333333,
           0.2514,
           0.37770000000000004,
           0.0363,
           0.8307,
           0.6997,
           0.3431666666666667,
           0.2484,
           0.2633333333333333,
           0.4921,
           0.219,
           0.3489,
           0.5913,
           0.5744,
           0.50415,
           0.17363333333333333,
           0.0809,
           0.8514,
           0.3414,
           0.043050000000000005,
           1,
           0.0551,
           1,
           0.9976,
           0.29335,
           0.656,
           0.5297,
           0.7097,
           0.23095,
           0.9847,
           0.5759333333333333,
           0.6429,
           0.435,
           0.3768,
           0.44705,
           0.5154000000000001,
           0.2407,
           0.1634,
           0.40068333333333334,
           0.789,
           1,
           0.1885,
           0.613,
           0.0083,
           0.4283,
           0.5028,
           0.1898,
           0.046,
           0.5,
           0.2406,
           0.0439,
           0.0247,
           0.3197,
           0.28373333333333334,
           0.5359,
           0.0583,
           0.44029999999999997,
           0.3059,
           0.2233,
           0.9695,
           0.3996,
           0.7714000000000001,
           0.628,
           0.3205,
           0.4785333333333333,
           0.1678,
           0.1698,
           0.2155,
           1,
           0.1351,
           0.49684999999999996,
           0.1202,
           0.1184,
           0.24766666666666667,
           0.62775,
           0.1099,
           0.46340000000000003,
           0.387,
           0.1918,
           0.3293,
           0.6147,
           0.6801,
           0.0067,
           0.21509999999999999,
           0.5245,
           0.211,
           1,
           0.3804,
           0.27253333333333335,
           0.3826,
           0.3237,
           0.6517,
           0.028,
           0.2306,
           0.857,
           0.145,
           0.6543,
           0.2865,
           0.6951,
           0.23425,
           0.6039,
           0.2047,
           0.7435,
           0.3129,
           0.2077,
           0.1128,
           0.29605000000000004,
           0.9967,
           0.7567,
           0.5132666666666666,
           0.3894,
           0.7323200000000001,
           1,
           0.3492333333333333,
           0.7291,
           0.46819999999999995,
           0.4436,
           0.53155,
           0.5724,
           0.4427,
           0.2614,
           0.9928,
           0.3547,
           0.6807,
           0.2432,
           0.0244,
           0.6698,
           0.14682499999999998,
           0.46865,
           0.1442,
           0.37644999999999995,
           0.2577,
           0.14425,
           0.0156,
           1,
           0.2615,
           0.2609,
           0.0079,
           0.4657,
           0.1431,
           0.3892,
           1,
           0.3155,
           0.4542,
           0.40796666666666664,
           0.0808,
           0.87635,
           0.5661,
           1,
           0.43694999999999995,
           0.30150000000000005,
           0.4456333333333333,
           0.500225,
           0.1675,
           0.027,
           0.6087,
           0.27595000000000003,
           0.73865,
           0.6789,
           0.30485,
           0.4718,
           0.0882,
           0.564,
           0.9117666666666667,
           0.16060000000000002,
           0.1323,
           0.3141,
           0.24609999999999999,
           0.3127,
           0.20259999999999997,
           0.5371333333333334,
           1,
           0.1,
           0.0183,
           0.22343333333333334,
           1,
           0.0788,
           1,
           0.6113999999999999,
           0.5759749999999999,
           1,
           0.549,
           0.1256,
           0.2573,
           0.46475,
           0.355,
           0.1348,
           0.68665,
           0.3059,
           1,
           1,
           0.3335,
           0.155,
           0.1081,
           0.52595,
           0.6667,
           0.500375,
           0.279925,
           0.37775000000000003,
           0.7067,
           0.7854,
           0.17579999999999998,
           0.1623,
           0.0083,
           0.7002,
           0.7222285714285714,
           0.0985,
           0.0932,
           0.6079,
           0.45675000000000004,
           0.2823,
           0.2305,
           0.1399,
           0.1741666666666667,
           0.4972200000000001,
           0.6436666666666667,
           0.0589,
           1,
           0.42693333333333333,
           0.16535,
           0.2415,
           0.0989,
           0.7593,
           0.3796,
           0.42065,
           0.835,
           0.4898,
           0.3,
           0.6697500000000001,
           0.3309,
           0.3552333333333333,
           0.6658,
           0.3188,
           0.6767666666666666,
           0.1982,
           0.491,
           0.1053,
           0.1412,
           0.0378,
           0.2029,
           0.41814999999999997,
           0.3091,
           0.46025000000000005,
           0.046,
           0.26,
           0.0388,
           0.0205,
           0.5483,
           0.1955,
           0.9715,
           0.3902,
           1,
           0.12633333333333333,
           0.39412500000000006,
           0.48816666666666664,
           0.0114,
           0.045899999999999996,
           0.3356,
           0.057,
           0.0838,
           1,
           0.9097999999999999,
           0.7974,
           0.1018,
           0.2424,
           0.7122,
           0.0932,
           0.15665,
           0.8135,
           0.13385,
           0.4159,
           0.25,
           1,
           1,
           0.35829999999999995,
           0.0125,
           0.4211,
           0.6966,
           0.6336,
           0.2605,
           0.8021,
           0.4396,
           0.22005000000000002
          ],
          "colorbar": {
           "len": 500,
           "lenmode": "pixels",
           "thickness": 10,
           "title": {
            "text": "ratio"
           }
          },
          "colorscale": [
           [
            0,
            "rgb(255,255,229)"
           ],
           [
            0.125,
            "rgb(247,252,185)"
           ],
           [
            0.25,
            "rgb(217,240,163)"
           ],
           [
            0.375,
            "rgb(173,221,142)"
           ],
           [
            0.5,
            "rgb(120,198,121)"
           ],
           [
            0.625,
            "rgb(65,171,93)"
           ],
           [
            0.75,
            "rgb(35,132,67)"
           ],
           [
            0.875,
            "rgb(0,104,55)"
           ],
           [
            1,
            "rgb(0,69,41)"
           ]
          ],
          "line": {
           "color": "black",
           "width": 1
          },
          "opacity": 0.7,
          "showscale": true,
          "size": 20,
          "symbol": "circle"
         },
         "mode": "markers",
         "type": "scatter",
         "x": [
          "2012-08",
          "2012-12",
          "2013-02",
          "2013-02",
          "2013-07",
          "2013-11",
          "2013-12",
          "2014-04",
          "2014-06",
          "2014-06",
          "2014-06",
          "2014-09",
          "2014-09",
          "2014-11",
          "2014-12",
          "2014-12",
          "2014-12",
          "2014-12",
          "2014-12",
          "2015-02",
          "2015-02",
          "2015-02",
          "2015-02",
          "2015-03",
          "2015-03",
          "2015-03",
          "2015-04",
          "2015-04",
          "2015-04",
          "2015-05",
          "2015-05",
          "2015-05",
          "2015-06",
          "2015-06",
          "2015-08",
          "2015-09",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-11",
          "2015-12",
          "2015-12",
          "2015-12",
          "2015-12",
          "2016-01",
          "2016-01",
          "2016-01",
          "2016-01",
          "2016-02",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-03",
          "2016-04",
          "2016-04",
          "2016-04",
          "2016-05",
          "2016-05",
          "2016-05",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-06",
          "2016-07",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-08",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-09",
          "2016-10",
          "2016-10",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-11",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2016-12",
          "2017-01",
          "2017-02",
          "2017-02",
          "2017-02",
          "2017-02",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-03",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-04",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-05",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-06",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-07",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-08",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-09",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-10",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-11",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2017-12",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-01",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-02",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-03",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-04",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-05",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-06",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-07",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-08",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-09",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-10",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-11",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2018-12",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-01",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-02",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-03",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-04",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-05",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-06",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-07",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-08",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-09",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-10",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-11",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2019-12",
          "2020-01",
          "2020-01",
          "2020-01",
          "2020-01",
          "2020-01",
          "2020-02",
          "2020-02",
          "2020-02",
          "2020-03",
          "2020-03",
          "2020-03",
          "2020-03",
          "2020-04",
          "2020-04"
         ],
         "y": [
          "Other image process",
          "Image classification",
          "Activity recognition",
          "Image classification",
          "Facial recognition and modelling",
          "Image classification",
          "Image classification",
          "Image classification",
          "Activity recognition",
          "Image classification",
          "Facial recognition and modelling",
          "Other vision process",
          "Image classification",
          "Semantic segmentation",
          "Object detection",
          "Facial recognition and modelling",
          "Activity recognition",
          "Image classification",
          "Semantic segmentation",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Image classification",
          "Other vision process",
          "Activity recognition",
          "Facial recognition and modelling",
          "Semantic segmentation",
          "Other vision process",
          "Other image process",
          "Semantic segmentation",
          "Other vision process",
          "Activity recognition",
          "Semantic segmentation",
          "Image classification",
          "Object detection",
          "Facial recognition and modelling",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Image classification",
          "Semantic segmentation",
          "Pose estimation",
          "Other image process",
          "Object detection",
          "Object detection",
          "Image classification",
          "Activity recognition",
          "Other vision process",
          "Pose estimation",
          "Image generation",
          "Action localization",
          "Activity recognition",
          "Image classification",
          "Activity recognition",
          "Image classification",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Object detection",
          "Other vision process",
          "Pose estimation",
          "Activity recognition",
          "Other image process",
          "Other vision process",
          "Image classification",
          "Other vision process",
          "Semantic segmentation",
          "Image generation",
          "Activity recognition",
          "Other vision process",
          "Semantic segmentation",
          "Object detection",
          "Other vision process",
          "Other vision process",
          "Object detection",
          "Activity recognition",
          "Pose estimation",
          "Other image process",
          "Image classification",
          "Object recognition",
          "Action localization",
          "Pose estimation",
          "Object detection",
          "Other vision process",
          "Activity recognition",
          "Image generation",
          "Image classification",
          "Action localization",
          "Pose estimation",
          "Activity recognition",
          "Other image process",
          "Other vision process",
          "Object tracking",
          "Object detection",
          "Other video process",
          "Image classification",
          "Semantic segmentation",
          "Other 3D task",
          "Object detection",
          "Other image process",
          "Pose estimation",
          "Image generation",
          "Other video process",
          "Semantic segmentation",
          "Activity recognition",
          "Pose estimation",
          "Pose estimation",
          "Image generation",
          "Image classification",
          "Object detection",
          "Object detection",
          "Pose estimation",
          "Facial recognition and modelling",
          "Activity detection",
          "Image generation",
          "Action localization",
          "Semantic segmentation",
          "Other image process",
          "Activity recognition",
          "Other vision process",
          "Other image process",
          "Semantic segmentation",
          "Activity recognition",
          "Other vision process",
          "Object tracking",
          "Other 3D task",
          "Pose estimation",
          "Image classification",
          "Object detection",
          "Facial recognition and modelling",
          "Activity recognition",
          "Other vision process",
          "Pose estimation",
          "Action localization",
          "Gesture recognition",
          "Facial recognition and modelling",
          "Facial recognition and modelling",
          "Other 3D task",
          "Object tracking",
          "Activity recognition",
          "Object detection",
          "Other image process",
          "Semantic segmentation",
          "Image generation",
          "Other vision process",
          "Image classification",
          "Pose estimation",
          "Facial recognition and modelling",
          "Object detection",
          "Other video process",
          "Other vision process",
          "Semantic segmentation",
          "Image classification",
          "Object detection",
          "Activity recognition",
          "Facial recognition and modelling",
          "Pose estimation",
          "Other vision process",
          "Image classification",
          "Other image process",
          "Other vision process",
          "Image generation",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Pose estimation",
          "Object detection",
          "Image classification",
          "Pose estimation",
          "Image generation",
          "Other vision process",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Other image process",
          "Object tracking",
          "Facial recognition and modelling",
          "Pose estimation",
          "Activity recognition",
          "Object detection",
          "Image-to-image translation",
          "Other vision process",
          "Semantic segmentation",
          "Other 3D task",
          "Pose estimation",
          "Activity recognition",
          "Other vision process",
          "Image classification",
          "Object detection",
          "Activity detection",
          "Pose tracking",
          "Other image process",
          "Image generation",
          "Activity localization",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Other image process",
          "Activity recognition",
          "Facial recognition and modelling",
          "Other 3D task",
          "Image classification",
          "Other vision process",
          "Semantic segmentation",
          "Activity recognition",
          "Other vision process",
          "Object tracking",
          "Other 3D task",
          "Image generation",
          "Semantic segmentation",
          "Image classification",
          "Object detection",
          "Pose tracking",
          "Other image process",
          "Other 3D task",
          "Facial recognition and modelling",
          "Object tracking",
          "Image classification",
          "Pose estimation",
          "Object detection",
          "Other image process",
          "Semantic segmentation",
          "Activity detection",
          "Other vision process",
          "Image generation",
          "Other video process",
          "Pose tracking",
          "Object detection",
          "Facial recognition and modelling",
          "Activity recognition",
          "Other 3D task",
          "Gesture recognition",
          "Action localization",
          "Pose estimation",
          "Other vision process",
          "Other image process",
          "Semantic segmentation",
          "Pose estimation",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Image classification",
          "Activity recognition",
          "Image generation",
          "Object detection",
          "Other vision process",
          "Other image process",
          "Other image process",
          "Activity localization",
          "Other video process",
          "Object tracking",
          "Facial recognition and modelling",
          "Semantic segmentation",
          "Pose estimation",
          "Emotion recognition",
          "Object detection",
          "Action localization",
          "Other vision process",
          "Object recognition",
          "Activity recognition",
          "Image generation",
          "Object detection",
          "Activity localization",
          "Image classification",
          "Semantic segmentation",
          "Other image process",
          "Other vision process",
          "Activity recognition",
          "Other video process",
          "Facial recognition and modelling",
          "Other vision process",
          "Other 3D task",
          "Semantic segmentation",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Other vision process",
          "Image generation",
          "Emotion recognition",
          "Other image process",
          "Activity recognition",
          "Image classification",
          "Semantic segmentation",
          "Other video process",
          "Facial recognition and modelling",
          "Other vision process",
          "Activity localization",
          "Object tracking",
          "Pose estimation",
          "Object detection",
          "Semantic segmentation",
          "Activity recognition",
          "Image classification",
          "Other 3D task",
          "Emotion recognition",
          "Image generation",
          "Other image process",
          "Object detection",
          "Semantic segmentation",
          "Activity recognition",
          "Object tracking",
          "Other 3D task",
          "Image generation",
          "Pose estimation",
          "Other vision process",
          "Gesture recognition",
          "Facial recognition and modelling",
          "Other image process",
          "Object detection",
          "Gesture recognition",
          "Activity recognition",
          "Semantic segmentation",
          "Other image process",
          "Image classification",
          "Other 3D task",
          "Other vision process",
          "Pose estimation",
          "Facial recognition and modelling",
          "Image classification",
          "Semantic segmentation",
          "Pose tracking",
          "Other vision process",
          "Other image process",
          "Pose estimation",
          "Pose estimation",
          "Action localization",
          "Other vision process",
          "Image generation",
          "Semantic segmentation",
          "Facial recognition and modelling",
          "Object detection",
          "Other image process",
          "Facial recognition and modelling",
          "Other video process",
          "Other image process",
          "Image classification",
          "Emotion recognition",
          "Activity detection",
          "Semantic segmentation",
          "Action localization",
          "Other 3D task",
          "Image generation",
          "Object recognition",
          "Object detection",
          "Activity recognition",
          "Other vision process",
          "Facial recognition and modelling",
          "Semantic segmentation",
          "Activity recognition",
          "Image classification",
          "Pose tracking",
          "Activity localization",
          "Other vision process",
          "Action localization",
          "Facial recognition and modelling",
          "Other video process",
          "Object detection",
          "Object tracking",
          "Image classification",
          "Pose estimation",
          "Activity recognition",
          "Semantic segmentation",
          "Other vision process",
          "Activity localization",
          "Semantic segmentation",
          "Object detection",
          "Activity recognition",
          "Activity localization",
          "Image generation",
          "Image-to-image translation",
          "Action localization",
          "Other vision process",
          "Object tracking",
          "Other video process",
          "Image generation",
          "Activity recognition",
          "Emotion recognition",
          "Facial recognition and modelling",
          "Image classification",
          "Object detection",
          "Semantic segmentation",
          "Activity localization",
          "Other vision process",
          "Object detection",
          "Other image process",
          "Object tracking",
          "Activity recognition",
          "Semantic segmentation",
          "Other vision process",
          "Pose estimation",
          "Emotion recognition",
          "Action localization",
          "Other vision process",
          "Pose estimation",
          "Facial recognition and modelling",
          "Image classification",
          "Other image process",
          "Semantic segmentation",
          "Object recognition",
          "Object detection",
          "Semantic segmentation",
          "Other vision process",
          "Activity recognition",
          "Object detection",
          "Activity localization",
          "Image generation",
          "Action localization",
          "Facial recognition and modelling",
          "Pose estimation",
          "Image classification",
          "Activity recognition",
          "Image classification",
          "Other video process",
          "Pose estimation",
          "Semantic segmentation",
          "Image generation",
          "Semantic segmentation",
          "Other vision process",
          "Image classification",
          "Object detection",
          "Pose estimation",
          "Other vision process",
          "Pose estimation",
          "Activity recognition",
          "Action localization",
          "Semantic segmentation",
          "Object detection",
          "Other video process",
          "Pose estimation",
          "Semantic segmentation"
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 21
        },
        "height": 1000,
        "legend": {
         "title": {
          "text": "task"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Vision process",
         "y": 0.995
        },
        "width": 1500,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "tickmode": "auto",
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "categoryarray": [
          "Semantic segmentation",
          "Pose tracking",
          "Pose estimation",
          "Other vision process",
          "Other video process",
          "Other image process",
          "Other 3D task",
          "Object tracking",
          "Object recognition",
          "Object detection",
          "Image-to-image translation",
          "Image generation",
          "Image classification",
          "Gesture recognition",
          "Facial recognition and modelling",
          "Emotion recognition",
          "Activity recognition",
          "Activity localization",
          "Activity detection",
          "Action localization"
         ],
         "categoryorder": "array",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightBlue",
         "showgrid": true,
         "side": "left",
         "title": {}
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bdfb3503-22db-42d9-91b3-7fc557b02f47\" class=\"plotly-graph-div\" style=\"height:1000.0px; width:1500px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bdfb3503-22db-42d9-91b3-7fc557b02f47\")) {                    Plotly.newPlot(                        \"bdfb3503-22db-42d9-91b3-7fc557b02f47\",                        [{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Action localization\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Action localization\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-01\",\"2016-09\",\"2016-11\",\"2017-03\",\"2017-05\",\"2018-04\",\"2018-06\",\"2019-03\",\"2019-04\",\"2019-06\",\"2019-07\",\"2019-09\",\"2019-11\",\"2020-03\"],\"xaxis\":\"x\",\"y\":[\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-03\",\"2017-12\",\"2018-03\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Activity detection\",\"Activity detection\",\"Activity detection\",\"Activity detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity localization\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity localization\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-12\",\"2018-06\",\"2018-07\",\"2018-11\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Activity recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Activity recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2013-02\",\"2014-06\",\"2014-12\",\"2015-03\",\"2015-05\",\"2015-12\",\"2016-01\",\"2016-03\",\"2016-04\",\"2016-06\",\"2016-08\",\"2016-09\",\"2016-11\",\"2016-12\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\",\"2017-08\",\"2017-11\",\"2017-12\",\"2018-01\",\"2018-02\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-11\",\"2019-12\",\"2020-02\"],\"xaxis\":\"x\",\"y\":[\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Emotion recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Emotion recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2018-06\",\"2018-10\",\"2018-11\",\"2019-04\",\"2019-08\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Emotion recognition\",\"Emotion recognition\",\"Emotion recognition\",\"Emotion recognition\",\"Emotion recognition\",\"Emotion recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Facial recognition and modelling\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Facial recognition and modelling\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2013-07\",\"2014-06\",\"2014-12\",\"2015-02\",\"2015-03\",\"2015-08\",\"2015-11\",\"2016-03\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\",\"2017-07\",\"2017-08\",\"2017-09\",\"2017-10\",\"2017-11\",\"2017-12\",\"2018-01\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-08\",\"2018-09\",\"2018-10\",\"2018-12\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-08\",\"2019-10\",\"2019-11\"],\"xaxis\":\"x\",\"y\":[\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Gesture recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Gesture recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-05\",\"2018-04\",\"2018-12\",\"2019-01\"],\"xaxis\":\"x\",\"y\":[\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image classification\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image classification\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2012-12\",\"2013-02\",\"2013-11\",\"2013-12\",\"2014-04\",\"2014-06\",\"2014-09\",\"2014-12\",\"2015-02\",\"2015-06\",\"2015-11\",\"2015-12\",\"2016-02\",\"2016-03\",\"2016-05\",\"2016-08\",\"2016-10\",\"2016-11\",\"2017-02\",\"2017-04\",\"2017-07\",\"2017-08\",\"2017-09\",\"2017-10\",\"2017-12\",\"2018-01\",\"2018-02\",\"2018-03\",\"2018-05\",\"2018-07\",\"2018-10\",\"2018-11\",\"2019-01\",\"2019-02\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-08\",\"2019-10\",\"2019-11\",\"2019-12\",\"2020-01\"],\"xaxis\":\"x\",\"y\":[\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image generation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image generation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-01\",\"2016-06\",\"2016-10\",\"2016-12\",\"2017-02\",\"2017-03\",\"2017-06\",\"2017-09\",\"2017-10\",\"2017-12\",\"2018-02\",\"2018-03\",\"2018-05\",\"2018-07\",\"2018-09\",\"2018-11\",\"2018-12\",\"2019-03\",\"2019-04\",\"2019-07\",\"2019-08\",\"2019-11\",\"2019-12\"],\"xaxis\":\"x\",\"y\":[\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Image-to-image translation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Image-to-image translation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-11\",\"2019-07\"],\"xaxis\":\"x\",\"y\":[\"Image-to-image translation\",\"Image-to-image translation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object detection\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object detection\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-12\",\"2015-06\",\"2015-11\",\"2015-12\",\"2016-03\",\"2016-06\",\"2016-08\",\"2016-09\",\"2016-11\",\"2016-12\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-06\",\"2017-07\",\"2017-08\",\"2017-10\",\"2017-11\",\"2017-12\",\"2018-02\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-03\",\"2019-04\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2020-01\",\"2020-03\"],\"xaxis\":\"x\",\"y\":[\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object recognition\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object recognition\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-08\",\"2018-06\",\"2019-04\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Object recognition\",\"Object recognition\",\"Object recognition\",\"Object recognition\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Object tracking\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Object tracking\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-11\",\"2017-04\",\"2017-06\",\"2017-10\",\"2018-02\",\"2018-03\",\"2018-06\",\"2018-11\",\"2018-12\",\"2019-06\",\"2019-07\",\"2019-09\"],\"xaxis\":\"x\",\"y\":[\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other 3D task\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other 3D task\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-12\",\"2017-04\",\"2017-06\",\"2017-11\",\"2018-01\",\"2018-02\",\"2018-03\",\"2018-04\",\"2018-08\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-04\"],\"xaxis\":\"x\",\"y\":[\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other image process\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other image process\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2012-08\",\"2015-04\",\"2015-11\",\"2016-04\",\"2016-08\",\"2016-11\",\"2016-12\",\"2017-03\",\"2017-04\",\"2017-06\",\"2017-09\",\"2017-10\",\"2017-12\",\"2018-01\",\"2018-02\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-09\",\"2019-10\"],\"xaxis\":\"x\",\"y\":[\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other video process\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other video process\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2016-11\",\"2016-12\",\"2017-07\",\"2018-04\",\"2018-06\",\"2018-08\",\"2018-10\",\"2019-04\",\"2019-06\",\"2019-07\",\"2019-12\",\"2020-03\"],\"xaxis\":\"x\",\"y\":[\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Other vision process\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Other vision process\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-09\",\"2015-02\",\"2015-04\",\"2015-05\",\"2015-12\",\"2016-03\",\"2016-04\",\"2016-05\",\"2016-06\",\"2016-07\",\"2016-08\",\"2016-09\",\"2016-11\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\",\"2017-07\",\"2017-08\",\"2017-09\",\"2017-10\",\"2017-11\",\"2017-12\",\"2018-01\",\"2018-02\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2020-01\",\"2020-02\"],\"xaxis\":\"x\",\"y\":[\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose estimation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose estimation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2015-11\",\"2016-01\",\"2016-03\",\"2016-08\",\"2016-09\",\"2016-11\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-07\",\"2017-08\",\"2017-09\",\"2017-10\",\"2017-11\",\"2017-12\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-06\",\"2019-09\",\"2019-10\",\"2019-11\",\"2019-12\",\"2020-01\",\"2020-02\",\"2020-04\"],\"xaxis\":\"x\",\"y\":[\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Pose tracking\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Pose tracking\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2017-12\",\"2018-02\",\"2018-04\",\"2019-02\",\"2019-05\"],\"xaxis\":\"x\",\"y\":[\"Pose tracking\",\"Pose tracking\",\"Pose tracking\",\"Pose tracking\",\"Pose tracking\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"task=%{y}<br>date=%{x}<extra></extra>\",\"legendgroup\":\"Semantic segmentation\",\"line\":{\"color\":\"black\",\"dash\":\"solid\",\"width\":0},\"marker\":{\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"lines\",\"name\":\"Semantic segmentation\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[\"2014-11\",\"2014-12\",\"2015-02\",\"2015-03\",\"2015-04\",\"2015-05\",\"2015-09\",\"2015-11\",\"2016-03\",\"2016-05\",\"2016-06\",\"2016-11\",\"2016-12\",\"2017-03\",\"2017-04\",\"2017-06\",\"2017-08\",\"2017-09\",\"2017-10\",\"2017-11\",\"2017-12\",\"2018-01\",\"2018-02\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2019-11\",\"2019-12\",\"2020-01\",\"2020-03\",\"2020-04\"],\"xaxis\":\"x\",\"y\":[\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\"],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":[\"<BR>task: Action localization<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>\",\"<BR>task: Action localization<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>\",\"<BR>task: Action localization<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: MEXaction2 - Temporal Action Localization benchmarking - mAP<BR>\",\"<BR>task: Action localization<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>\",\"<BR>task: Action localization<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>\",\"<BR>task: Action localization<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Frame-mAP<BR>\",\"<BR>task: Action localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>\",\"<BR>task: Action localization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Video-mAP 0.5<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Video-mAP 0.5<BR>\",\"<BR>task: Action localization<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>\",\"<BR>task: Action localization<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>\",\"<BR>task: Action localization<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@50%<BR>\",\"<BR>task: Action localization<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>\",\"<BR>task: Activity detection<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>\",\"<BR>task: Activity detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>\",\"<BR>task: Activity detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.1<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.2<BR>\",\"<BR>task: Activity detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - mAP<BR>\",\"<BR>task: Activity localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Activity localization<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (test)<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>\",\"<BR>task: Activity localization<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>\",\"<BR>task: Activity localization<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@1000<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@200<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@500<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@50<BR>\",\"<BR>task: Activity localization<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Activity localization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - Mean mAP<BR>\",\"<BR>task: Activity localization<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.5<BR>  Weakly Supervised Action Localization: THUMOS\\u201914 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Activity recognition<BR>date: 2012-07<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2012-10<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: CAD-120 - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Action Recognition: VIRAT Ground 2.0 - Action Recognition benchmarking - Average Accuracy<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UT-Kinect - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>\",\"<BR>task: Activity recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ActivityNet - Action Recognition benchmarking - mAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: SBU - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Volleyball - Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.1<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: ActivityNet-1.2 - Action Classification benchmarking - mAP<BR>  Action Classification: THUMOS\\u201914 - Action Classification benchmarking - mAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CS<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CV1<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CV2<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - mAP (Val)<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ActionNet-VE - Action Recognition benchmarking - F-measure (%)<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: J-HMBD Early Action - Skeleton Based Action Recognition benchmarking - 10%<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 5 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top-1 Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ICVL-4 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: IRD - Action Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-1 (%)<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-5 (%)<BR>  Skeleton Based Action Recognition: UAV-Human - Skeleton Based Action Recognition benchmarking - Average Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: UTD-MHAD - Multimodal Activity Recognition benchmarking - Accuracy (CS)<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-5 Accuracy<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - GFlops<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - Params (M)<BR>  Action Recognition: AVA v2.2 - Action Recognition benchmarking - mAP<BR>  Action Recognition: Diving-48 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: UTD-MHAD - Action Recognition benchmarking - Accuracy<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S1)<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: LboroHAR - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - Speed  (FPS)<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Object Top 5 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Object Top-1 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Verb Top-1 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Verb Top-5 Accuracy<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Video hit-at-5<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: MiniKinetics - Action Classification benchmarking - Top-1 Accuracy<BR>  Skeleton Based Action Recognition: MSR Action3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UPenn Action - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - No. parameters<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: THUMOS'14 - Action Classification benchmarking - mAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking - Train F-measure<BR>\",\"<BR>task: Activity recognition<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: EPIC-KITCHENS-55 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: EgoGesture - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: EgoGesture - Action Recognition benchmarking - Top-5 Accuracy<BR>\",\"<BR>task: Emotion recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Arousal)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>\",\"<BR>task: Emotion recognition<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>\",\"<BR>task: Emotion recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: DailyDialog - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>  Emotion Recognition in Conversation: EmoryNLP - Emotion Recognition in Conversation benchmarking - Weighted Macro-F1<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2013-07<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FER2013 - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW2000 - Face Alignment benchmarking - Error rate<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: Oulu-CASIA - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: JAFFE - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - FR-at-0.1(%, all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - ME (%, all)<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: MMI - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW2000-3D - Face Alignment benchmarking - Mean NME<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Oulu-CASIA - Facial Expression Recognition benchmarking - Accuracy (10-fold)<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: 3DFAW - Face Alignment benchmarking - CVGTCE<BR>  Face Alignment: 3DFAW - Face Alignment benchmarking - GTE<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Cohn-Kanade - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW-Full - Face Alignment benchmarking - Mean NME<BR>  Face Alignment: LS3D-W Balanced - Face Alignment benchmarking - AUC0.07<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>  Unsupervised Facial Landmark Detection: AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Failure private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Face Alignment: 300W - Face Alignment benchmarking - Mean Error Rate private<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: IJB-A - Face Identification benchmarking - Accuracy<BR>  Face Identification: IJB-B - Face Identification benchmarking - Accuracy<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-B - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - Mean Error Rate<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW-LFPA - Face Alignment benchmarking - Mean NME<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.1<BR>  Face Verification: IJB-B - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.001<BR>  Facial Expression Recognition: SFEW - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: COFW - Face Alignment benchmarking - Mean Error Rate<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Facial Landmark Detection: AFLW-Front - Facial Landmark Detection benchmarking - Mean NME<BR>  Facial Landmark Detection: AFLW-Full - Facial Landmark Detection benchmarking - Mean NME<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW - Face Alignment benchmarking - Mean NME<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (7 emotion)<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Real-World Affective Faces - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FERPlus - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: IBUG - Face Alignment benchmarking - Mean Error Rate<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FERG - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IIIT-D Viewed Sketch - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: RAF-DB - Facial Expression Recognition benchmarking - Overall Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: AgeDB-30 - Face Verification benchmarking - Accuracy<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - MOS<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - MS-SSIM<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - PSNR<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - SSIM<BR>\",\"<BR>task: Gesture recognition<BR>date: 2013-03<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Cambridge - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLearn val - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: BUAA - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: MGB - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: SmartWatch - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Jester test - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLean test - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: Jester val - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>  Hand Gesture Recognition: Jester val - Hand Gesture Recognition benchmarking - Top 5 Accuracy<BR>  Hand Gesture Recognition: NVGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Northwestern University - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Gesture recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: DHG-14 - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: DHG-28 - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: SHREC 2017 - Hand Gesture Recognition benchmarking - 14 gestures accuracy<BR>  Hand Gesture Recognition: SHREC 2017 - Hand Gesture Recognition benchmarking - 28 gestures accuracy<BR>  Hand Gesture Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Hand Gesture Recognition benchmarking - 14 gestures accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>\",\"<BR>task: Image classification<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2013-01<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>\",\"<BR>task: Image classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Image classification<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>  Satellite Image Classification: SAT-6 - Satellite Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - # of clusters (k)<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>\",\"<BR>task: Image classification<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Retinal OCT Disease Classification: Srinivasan2014 - Retinal OCT Disease Classification benchmarking - Acc<BR>\",\"<BR>task: Image classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Hyperspectral Image Classification: Indian Pines - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>  Hyperspectral Image Classification: Pavia University - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: iNaturalist 2018 - Image Classification benchmarking - Top-1 Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>\",\"<BR>task: Image classification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: EMNIST-Balanced - Image Classification benchmarking - Accuracy<BR>  Image Classification: MultiMNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: smallNORB - Image Classification benchmarking - Classification Error<BR>\",\"<BR>task: Image classification<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Clothing1M - Image Classification benchmarking - Accuracy<BR>  Image Classification: Food-101N - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: MNIST - Unsupervised Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sequential Image Classification: Sequential CIFAR-10 - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Document Image Classification: Noisy Bangla Characters - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: Noisy Bangla Numeral - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: n-MNIST - Document Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: CIFAR-10 - Unsupervised Image Classification benchmarking - Accuracy<BR>  Unsupervised Image Classification: CIFAR-20 - Unsupervised Image Classification benchmarking - Accuracy<BR>  Unsupervised Image Classification: STL-10 - Unsupervised Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CINIC-10 - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Error<BR>\",\"<BR>task: Image classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Hyperspectral Image Classification: Salinas Scene - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: EMNIST-Letters - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Params<BR>\",\"<BR>task: Image classification<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Flowers-102 - Image Classification benchmarking - Accuracy<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top-1 Accuracy<BR>\",\"<BR>task: Image generation<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - bits/dimension<BR>\",\"<BR>task: Image generation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Image generation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: Binarized MNIST - Image Generation benchmarking - nats<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>\",\"<BR>task: Image generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Image generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Image generation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CAT 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - IS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - SSIM<BR>\",\"<BR>task: Image generation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Bedroom 64 x 64 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Image generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 1024x1024 - Image Generation benchmarking - FID<BR>  Image Generation: CelebA-HQ 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Cat 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Churches 256 x 256 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - LPIPS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - Retrieval Top10 Recall<BR>\",\"<BR>task: Image generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - bpd<BR>\",\"<BR>task: Image generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: ImageNet 128x128 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 128x128 - Image Generation benchmarking - IS<BR>  Image Generation: ImageNet 256x256 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 128x128 - Image Generation benchmarking - FID<BR>  Image Generation: MNIST - Image Generation benchmarking - bits/dimension<BR>\",\"<BR>task: Image generation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Bedroom - Image Generation benchmarking - FID-50k<BR>\",\"<BR>task: Image generation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 64x64 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - DS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - PCKh<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - DS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - IS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - PCKh<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - SSIM<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - mask-IS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - mask-SSIM<BR>\",\"<BR>task: Image generation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: Fashion-MNIST - Image Generation benchmarking - FID<BR>  Image Generation: MNIST - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: Stacked MNIST - Image Generation benchmarking - FID<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: ADE-Indoor - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-100 - Image Generation benchmarking - FID<BR>  Image Generation: Cityscapes-25K 256x512 - Image Generation benchmarking - FID<BR>  Image Generation: Cityscapes-5K 256x512 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Car 512 x 384 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Horse 256 x 256 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image-to-image translation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>\",\"<BR>task: Image-to-image translation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - Kernel Inception Distance<BR>\",\"<BR>task: Object detection<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Detection: Caltech - Pedestrian Detection benchmarking - Reasonable Miss Rate<BR>\",\"<BR>task: Object detection<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Lane Detection: Caltech Lanes Cordova - Lane Detection benchmarking - F1<BR>  Lane Detection: Caltech Lanes Washington - Lane Detection benchmarking - F1<BR>\",\"<BR>task: Object detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>\",\"<BR>task: Object detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Object Detection: Visual Genome - Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>  Weakly Supervised Object Detection: Watercolor2k - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>\",\"<BR>task: Object detection<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>\",\"<BR>task: Object detection<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>\",\"<BR>task: Object detection<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PeopleArt - Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: ISTD - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: UCF - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  Weakly Supervised Object Detection: ImageNet - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP50<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>\",\"<BR>task: Object detection<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Large MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Medium MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Small MR^-2<BR>\",\"<BR>task: Object detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: iSAID - Object Detection benchmarking - Average Precision<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - runtime (ms)<BR>\",\"<BR>task: Object detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - Average MAE<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - mean E-Measure<BR>\",\"<BR>task: Object detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>\",\"<BR>task: Object detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: NYU Depth v2 - 3D Object Detection benchmarking - MAP<BR>  3D Object Detection: SUN-RGBD - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Bare MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Heavy MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Partial MR^-2<BR>\",\"<BR>task: Object detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Lane Detection: CULane - Lane Detection benchmarking - F1 score<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - F1 score<BR>\",\"<BR>task: Object detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Clipart1k - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: Comic2k - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - MAE<BR>\",\"<BR>task: Object detection<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: IconArt - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PeopleArt - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Object detection<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - NDS<BR>\",\"<BR>task: Object detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.5<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-test - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-test - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - F-measure<BR>\",\"<BR>task: Object detection<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP50<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP75<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AR<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARI<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARm<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARs<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP50<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP75<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AR<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARI<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARm<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARs<BR>\",\"<BR>task: Object detection<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cyclists Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Hard - Birds Eye View Object Detection benchmarking - AP<BR>\",\"<BR>task: Object detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - mAP<BR>  Lane Detection: BDD100K - Lane Detection benchmarking - Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: BDD100K - Object Detection benchmarking - mAP-at-0.5<BR>  Object Detection: India Driving Dataset - Object Detection benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Object detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO 2017 - Object Detection benchmarking - Mean mAP<BR>\",\"<BR>task: Object recognition<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: GTSRB - Traffic Sign Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Object recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Backpack<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Gender<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Hat<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCS<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCS<BR>\",\"<BR>task: Object recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: PA-100K - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: PETA - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: RAP - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Object recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking - MAP<BR>  Traffic Sign Recognition: Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking - MAP<BR>\",\"<BR>task: Object recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP at-0.5:0.95<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP-at-0.50<BR>\",\"<BR>task: Object tracking<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>\",\"<BR>task: Object tracking<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>\",\"<BR>task: Object tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>\",\"<BR>task: Object tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Unseen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - O (Average of Measures)<BR>\",\"<BR>task: Object tracking<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: VOT2016 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>\",\"<BR>task: Object tracking<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Unseen)<BR>\",\"<BR>task: Object tracking<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: GOT-10k - Visual Object Tracking benchmarking - Average Overlap<BR>  Visual Object Tracking: GOT-10k - Visual Object Tracking benchmarking - Success Rate 0.5<BR>  Visual Object Tracking: LaSOT - Visual Object Tracking benchmarking - AUC<BR>\",\"<BR>task: Object tracking<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>\",\"<BR>task: Object tracking<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2019 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>\",\"<BR>task: Object tracking<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - Precision<BR>\",\"<BR>task: Other 3D task<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  3D Reconstruction: Scan2CAD - 3D Reconstruction benchmarking - Average Accuracy<BR>\",\"<BR>task: Other 3D task<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: Sydney Urban Objects - 3D Point Cloud Classification benchmarking - F1<BR>  3D Reconstruction: Data3D\\u2212R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>\",\"<BR>task: Other 3D task<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>\",\"<BR>task: Other 3D task<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Mean Accuracy<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>\",\"<BR>task: Other image process<BR>date: 2012-03<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2012-08<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Other image process<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Other image process<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Other image process<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>\",\"<BR>task: Other image process<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - PSNR (sRGB)<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - SSIM (sRGB)<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Other image process<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>\",\"<BR>task: Other image process<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: FRGC - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Clustering: UMist - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>\",\"<BR>task: Other image process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - SSIM<BR>\",\"<BR>task: Other image process<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: BSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Other image process<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Other image process<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: BSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma5 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: FRGC - Image Clustering benchmarking - Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Other image process<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma35 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma75 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma35 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma60 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Other image process<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: ARL Polarimetric Thermal Face Dataset - Image Clustering benchmarking - Accuracy<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Other image process<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: Set12 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Other image process<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: Urban100 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: CUB-200-2011 - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Other image process<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: LetterA-J - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: LetterA-J - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Other image process<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: street2shop - topwear - Image Retrieval benchmarking - Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: INRIA Holidays - Image Retrieval benchmarking - Mean mAP<BR>  Image Retrieval: NUS-WIDE - Image Retrieval benchmarking - MAP<BR>\",\"<BR>task: Other image process<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - HP<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - MMD<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - HP<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - MMD<BR>\",\"<BR>task: Other image process<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: Set12 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Retrieval: DeepFashion - Image Retrieval benchmarking - Recall-at-20<BR>\",\"<BR>task: Other image process<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Clothes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Clothes - Image Reconstruction benchmarking - LPIPS<BR>\",\"<BR>task: Other video process<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Other video process<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>\",\"<BR>task: Other video process<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Other video process<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - Interpolation Error<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>\",\"<BR>task: Other video process<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text Mean Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text Median Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-1<BR>\",\"<BR>task: Other video process<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Other video process<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - tOF<BR>  Video Generation: TrailerFaces - Video Generation benchmarking - FID<BR>\",\"<BR>task: Other video process<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: BAIR Robot Pushing - Video Generation benchmarking - FVD score<BR>  Video Generation: Kinetics-600 12 frames, 128x128 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 12 frames, 64x64 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 12 frames, 64x64 - Video Generation benchmarking - Inception Score<BR>  Video Generation: Kinetics-600 48 frames, 64x64 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 48 frames, 64x64 - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Other video process<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, 64x64, Unconditional - Video Generation benchmarking - Inception Score<BR>\",\"<BR>task: Other video process<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - SSIM<BR>\",\"<BR>task: Other vision process<BR>date: 2010-11<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: Beijing Air Quality - Multivariate Time Series Imputation benchmarking - MAE (PM2.5)<BR>  Multivariate Time Series Imputation: KDD CUP Challenge 2018 - Multivariate Time Series Imputation benchmarking - MSE (10% missing)<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - MAE (10% of data as GT)<BR>  Multivariate Time Series Imputation: UCI localization data - Multivariate Time Series Imputation benchmarking - MAE (10% missing)<BR>\",\"<BR>task: Other vision process<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-Olympic - Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>\",\"<BR>task: Other vision process<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Other vision process<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SYNSIG-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Signs-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>\",\"<BR>task: Other vision process<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>\",\"<BR>task: Other vision process<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>\",\"<BR>task: Other vision process<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Domain Generalization: ImageNet-R - Domain Generalization benchmarking - Top-1 Error Rate<BR>\",\"<BR>task: Other vision process<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Horizon Line Estimation: Eurasian Cities Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: Horizon Lines in the Wild - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: York Urban Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-relRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Other vision process<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: HMDBfull-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - Mean Rank<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Other vision process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: MuJoCo - Multivariate Time Series Imputation benchmarking - MSE (10^2, 50% missing)<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>\",\"<BR>task: Other vision process<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Scene Graph Generation: VRD - Scene Graph Generation benchmarking - Recall-at-50<BR>\",\"<BR>task: Other vision process<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Synth Objects-to-LINEMOD - Domain Adaptation benchmarking - Classification Accuracy<BR>  Domain Adaptation: Synth Objects-to-LINEMOD - Domain Adaptation benchmarking - Mean Angle Error<BR>\",\"<BR>task: Other vision process<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split unsupervised - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE log<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - SQ Rel<BR>\",\"<BR>task: Other vision process<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>\",\"<BR>task: Other vision process<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Other vision process<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>\",\"<BR>task: Other vision process<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Other vision process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - NDCG (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: USPS-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>\",\"<BR>task: Other vision process<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MSE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: UCF CC 50 - Crowd Counting benchmarking - MAE<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>\",\"<BR>task: Other vision process<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - Runtime [ms]<BR>\",\"<BR>task: Other vision process<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - H-Mean<BR>\",\"<BR>task: Other vision process<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>\",\"<BR>task: Other vision process<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MAE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MSE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - SSIM<BR>\",\"<BR>task: Other vision process<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - OOB Rate (10^\\u22123)<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Difference<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Length<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Player Distance<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Step Change (10^\\u22123)<BR>  Multivariate Time Series Imputation: PEMS-SF - Multivariate Time Series Imputation benchmarking - L2 Loss (10^-4)<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Other vision process<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Unsupervised Domain Adaptation: Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Other vision process<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Formation Energy: Materials Project - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - RMSE<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - Sq Rel<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - mse (10^-3)<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - F-Measure<BR>\",\"<BR>task: Other vision process<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech-10 - Domain Adaptation benchmarking - Accuracy (%)<BR>\",\"<BR>task: Other vision process<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: CLEVR - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Other vision process<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: 100 sleep nights of 8 caregivers - Visual Question Answering benchmarking - 14 gestures accuracy<BR>  Visual Question Answering: HowmanyQA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TallyQA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking - mIoU<BR>  Domain Generalization: ImageNet-C - Domain Generalization benchmarking - mean Corruption Error (mCE)<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - TIoU<BR>\",\"<BR>task: Other vision process<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: SIM10K to BDD100K - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Other vision process<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iMAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iRMSE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TDIUC - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - H-Mean<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Other vision process<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: VOID - Depth Completion benchmarking - MAE<BR>  Depth Completion: VOID - Depth Completion benchmarking - RMSE<BR>  Depth Completion: VOID - Depth Completion benchmarking - iMAE<BR>  Depth Completion: VOID - Depth Completion benchmarking - iRMSE<BR>  Formation Energy: OQMD v1.2 - Formation Energy benchmarking - MAE<BR>  Unsupervised Domain Adaptation: PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking - AP-at-0.7<BR>  Video Prediction: CMU Mocap-1 - Video Prediction benchmarking - Test Error<BR>  Video Prediction: CMU Mocap-2 - Video Prediction benchmarking - Test Error<BR>\",\"<BR>task: Other vision process<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: IC19-ReCTs - Scene Text Detection benchmarking - F-Measure<BR>\",\"<BR>task: Other vision process<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - ATV<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - AUC<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - MSE<BR>  Visual Question Answering: GQA test-dev - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - number<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - other<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - unanswerable<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - yes/no<BR>\",\"<BR>task: Other vision process<BR>date: 2020-02<BR>Anchor.<BR>benchmarks:<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - mean Recall @20<BR>\",\"<BR>task: Pose estimation<BR>date: 2010-03<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: HumanEva-I - 3D Human Pose Estimation benchmarking - Mean Reconstruction Error (mm)<BR>\",\"<BR>task: Pose estimation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Human3.6M - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>\",\"<BR>task: Pose estimation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>\",\"<BR>task: Pose estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Pose estimation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: BIWI - Head Pose Estimation benchmarking - MAE (trained with other data)<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Total Capture - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>  Pose Estimation: FLIC Elbows - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: FLIC Wrists - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: J-HMDB - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>  Pose Estimation: ITOP top-view - Pose Estimation benchmarking - Mean mAP<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: BJUT-3D - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: Pointing'04 - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - FPS<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: UAV-Human - Pose Estimation benchmarking - mAP<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Frames Per View<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Views<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Hand Pose Estimation: MSRA Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - MPJPE (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - MPJPE (CS)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CS)<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - 3DPCK<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - MJPE<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2017 - Hand Pose Estimation benchmarking - Average 3D Error<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: BIWI - Head Pose Estimation benchmarking - MAE (trained with BIWI data)<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean IoU<BR>  6D Pose Estimation using RGB: OCCLUSION - 6D Pose Estimation using RGB benchmarking - MAP<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean IoU<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - IoU-2D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - IoU-3D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - VSS-2D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - VSS-3D<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - PA-MPJPE<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - acceleration error<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>  Pose Estimation: UPenn Action - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: DensePose-COCO - Pose Estimation benchmarking - AP<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADI<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADI<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: CHALL H80K - 3D Human Pose Estimation benchmarking - MPJPE<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean AUC<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 10cm<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 5cm<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-25<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-50<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 5, 5cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 10cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 5cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-25<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-50<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 5, 5cm<BR>  6D Pose Estimation: LineMOD - 6D Pose Estimation benchmarking - Accuracy (ADD)<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP<BR>  Pose Estimation: COCO minival - Pose Estimation benchmarking - AP<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: T-LESS - 6D Pose Estimation using RGB benchmarking - Mean Recall<BR>  6D Pose Estimation using RGBD: T-LESS - 6D Pose Estimation using RGBD benchmarking - Mean Recall<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking - MPJAE<BR>  3D Human Pose Estimation: 3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking - MPJPE<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: T-LESS - 6D Pose Estimation using RGB benchmarking - Recall (VSD)<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - FPS<BR>  Hand Pose Estimation: K2HPD - Hand Pose Estimation benchmarking - PDJ@5mm<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - FPS<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPVPE<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - 5\\u00b05 cm<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - IOU25<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - Rerr<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - Terr<BR>\",\"<BR>task: Pose estimation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2019 - Hand Pose Estimation benchmarking - Average 3D Error<BR>\",\"<BR>task: Pose estimation<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - PCK3D<BR>\",\"<BR>task: Pose tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: Multi-Person PoseTrack - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: Multi-Person PoseTrack - Pose Tracking benchmarking - MOTP<BR>\",\"<BR>task: Pose tracking<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>\",\"<BR>task: Pose tracking<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2018 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2018 - Pose Tracking benchmarking - mAP<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Dice<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SkyScapes-Lane - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Scene Segmentation: SUN-RGBD - Scene Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2013 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2013 leaderboard - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Medical Image Segmentation: CVC-ClinicDB - Medical Image Segmentation benchmarking - mean Dice<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - Warping Error<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - Average MAE<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - S-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - max E-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - mean Dice<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Dice<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Jaccard Index<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Dice Score<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Precision<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Recall<BR>  Pancreas Segmentation: TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking - Dice Score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: Kvasir-Instrument - Semantic Segmentation benchmarking - DSC<BR>  Semantic Segmentation: Kvasir-Instrument - Semantic Segmentation benchmarking - mIoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Global Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2015 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: ISLES-2015 - Lesion Segmentation benchmarking - Dice Score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: NYU Depth v2 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2011 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - Average Accuracy<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Lesion Segmentation: ISIC 2017 - Lesion Segmentation benchmarking - Mean IoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: Cityscapes test - Panoptic Segmentation benchmarking - PQ<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: ShapeNet - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2007 - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2014 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2017 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Instance Segmentation: ScanNetV1 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.25<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Instance Segmentation: NYU Depth v2 - Instance Segmentation benchmarking - mAP-at-0.5<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - oAcc<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mAcc<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mIoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQst<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: iSAID - Instance Segmentation benchmarking - Average Precision<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Medical Image Segmentation: iSEG 2017 Challenge - Medical Image Segmentation benchmarking - Dice Score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Retinal Vessel Segmentation: HRF - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: HRF - Retinal Vessel Segmentation benchmarking - F1 score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Medical Image Segmentation: 2018 Data Science Bowl - Medical Image Segmentation benchmarking - Dice<BR>  Medical Image Segmentation: 2018 Data Science Bowl - Medical Image Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Freiburg Forest - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SUN-RGBD - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SYNTHIA-CVPR\\u201916 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ScanNetV2 - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: BUS 2017 Dataset B - Lesion Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: MHP v2.0 - Human Part Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: KITTI Semantic Segmentation - Semantic Segmentation benchmarking - Mean IoU (class)<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: COCO panoptic - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - Pixel Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mRec<BR>  3D Instance Segmentation: ScanNet - 3D Instance Segmentation benchmarking - mAP<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - Accuracy<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - VInfo<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - VRand<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: SceneNN - 3D Instance Segmentation benchmarking - mAP-at-0.5<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - AVD<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Dice Score<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Precision<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Recall<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - VS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP50<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP75<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - 3DIoU<BR>  Semantic Segmentation: ParisLille3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - MSD<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - VS<BR>  Brain Tumor Segmentation: BRATS 2018 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - Total Variation of Information<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Merge<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Split<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - MSD<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - VS<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - MSD<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - VS<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Segmentation: S3DIS - 3D Semantic Segmentation benchmarking - mAcc<BR>  3D Semantic Segmentation: S3DIS - 3D Semantic Segmentation benchmarking - mIoU<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - mIoU<BR>  Lung Nodule Segmentation: Montgomery County - Lung Nodule Segmentation benchmarking - Accuracy<BR>  Lung Nodule Segmentation: Montgomery County - Lung Nodule Segmentation benchmarking - mIoU<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - F1-Score<BR>  Lung Nodule Segmentation: LIDC-IDRI - Lung Nodule Segmentation benchmarking - Dice<BR>  Lung Nodule Segmentation: LIDC-IDRI - Lung Nodule Segmentation benchmarking - IoU<BR>  Lung Nodule Segmentation: Lung Nodule  - Lung Nodule Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: DRIVE - Medical Image Segmentation benchmarking - F1 score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Skin Cancer Segmentation: PH2 - Skin Cancer Segmentation benchmarking - IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mCov<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mWCov<BR>  Instance Segmentation: LVIS v1.0 - Instance Segmentation benchmarking - mask AP<BR>  Medical Image Segmentation: Cell - Medical Image Segmentation benchmarking - IoU<BR>  Medical Image Segmentation: EM - Medical Image Segmentation benchmarking - IoU<BR>  Semantic Segmentation: BDD - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Category mIoU<BR>  Semantic Segmentation: GTAV-to-Cityscapes Labels - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - Frame (fps)<BR>\"],\"marker\":{\"line\":{\"width\":1,\"color\":\"black\"},\"size\":21,\"symbol\":42},\"mode\":\"markers\",\"x\":[\"2015-06\",\"2015-11\",\"2016-01\",\"2016-02\",\"2016-04\",\"2016-09\",\"2017-03\",\"2017-05\",\"2018-04\",\"2018-06\",\"2019-03\",\"2020-01\",\"2015-07\",\"2016-12\",\"2017-03\",\"2019-04\",\"2017-03\",\"2017-07\",\"2017-12\",\"2018-06\",\"2018-07\",\"2019-08\",\"2019-11\",\"2012-07\",\"2012-10\",\"2012-12\",\"2013-06\",\"2014-03\",\"2014-06\",\"2014-11\",\"2015-11\",\"2015-12\",\"2016-04\",\"2016-06\",\"2016-08\",\"2016-11\",\"2016-12\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\",\"2017-08\",\"2017-10\",\"2017-11\",\"2017-12\",\"2018-01\",\"2018-02\",\"2018-06\",\"2018-07\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2020-04\",\"2017-07\",\"2019-03\",\"2019-09\",\"2013-07\",\"2014-04\",\"2014-06\",\"2014-08\",\"2014-12\",\"2015-03\",\"2015-05\",\"2015-06\",\"2015-07\",\"2015-09\",\"2015-11\",\"2016-04\",\"2016-07\",\"2016-09\",\"2016-10\",\"2017-01\",\"2017-03\",\"2017-05\",\"2017-06\",\"2017-08\",\"2017-09\",\"2017-10\",\"2018-02\",\"2018-03\",\"2018-04\",\"2018-05\",\"2018-08\",\"2018-12\",\"2019-02\",\"2019-03\",\"2019-05\",\"2019-07\",\"2019-08\",\"2013-03\",\"2014-06\",\"2017-01\",\"2017-05\",\"2017-07\",\"2017-11\",\"2018-04\",\"2019-01\",\"2019-07\",\"2012-02\",\"2012-12\",\"2013-01\",\"2013-06\",\"2013-12\",\"2015-02\",\"2015-04\",\"2015-09\",\"2015-11\",\"2015-12\",\"2016-03\",\"2016-12\",\"2017-07\",\"2017-08\",\"2017-10\",\"2017-11\",\"2018-02\",\"2018-03\",\"2018-05\",\"2018-06\",\"2018-07\",\"2018-10\",\"2019-01\",\"2019-02\",\"2019-04\",\"2019-06\",\"2019-10\",\"2019-12\",\"2014-10\",\"2015-11\",\"2016-01\",\"2016-06\",\"2016-10\",\"2017-03\",\"2017-05\",\"2017-06\",\"2017-09\",\"2017-10\",\"2017-12\",\"2018-02\",\"2018-07\",\"2018-09\",\"2018-11\",\"2018-12\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-10\",\"2019-11\",\"2019-12\",\"2016-11\",\"2017-11\",\"2014-03\",\"2014-06\",\"2014-07\",\"2014-11\",\"2015-04\",\"2015-05\",\"2015-06\",\"2015-11\",\"2015-12\",\"2016-03\",\"2016-08\",\"2016-09\",\"2016-10\",\"2016-11\",\"2016-12\",\"2017-02\",\"2017-03\",\"2017-07\",\"2017-08\",\"2017-11\",\"2017-12\",\"2018-03\",\"2018-06\",\"2018-10\",\"2018-12\",\"2019-03\",\"2019-04\",\"2019-05\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-12\",\"2012-02\",\"2015-12\",\"2017-09\",\"2018-06\",\"2019-04\",\"2015-04\",\"2015-12\",\"2016-06\",\"2016-11\",\"2017-04\",\"2017-06\",\"2018-11\",\"2019-01\",\"2019-07\",\"2019-09\",\"2016-03\",\"2016-04\",\"2016-10\",\"2016-12\",\"2018-03\",\"2012-03\",\"2012-08\",\"2013-12\",\"2014-12\",\"2015-04\",\"2015-08\",\"2015-11\",\"2015-12\",\"2016-04\",\"2016-06\",\"2016-08\",\"2016-11\",\"2017-03\",\"2017-04\",\"2017-06\",\"2017-10\",\"2018-04\",\"2018-05\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-08\",\"2019-10\",\"2015-06\",\"2016-09\",\"2016-10\",\"2017-08\",\"2018-06\",\"2018-08\",\"2019-04\",\"2019-07\",\"2019-12\",\"2020-03\",\"2010-11\",\"2012-12\",\"2014-06\",\"2014-09\",\"2014-11\",\"2014-12\",\"2015-02\",\"2015-05\",\"2015-06\",\"2015-08\",\"2015-11\",\"2015-12\",\"2016-04\",\"2016-05\",\"2016-06\",\"2016-07\",\"2016-08\",\"2016-09\",\"2016-10\",\"2016-12\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\",\"2017-07\",\"2017-08\",\"2017-09\",\"2017-11\",\"2017-12\",\"2018-01\",\"2018-03\",\"2018-06\",\"2018-07\",\"2018-08\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-02\",\"2019-04\",\"2019-05\",\"2019-06\",\"2019-07\",\"2019-08\",\"2020-02\",\"2010-03\",\"2013-12\",\"2014-07\",\"2014-11\",\"2015-11\",\"2016-01\",\"2016-03\",\"2016-05\",\"2016-11\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-08\",\"2017-10\",\"2017-11\",\"2017-12\",\"2018-02\",\"2018-03\",\"2018-09\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-10\",\"2020-01\",\"2020-04\",\"2016-11\",\"2017-10\",\"2018-04\",\"2014-07\",\"2014-11\",\"2014-12\",\"2015-05\",\"2015-11\",\"2015-12\",\"2016-03\",\"2016-04\",\"2016-05\",\"2016-06\",\"2016-11\",\"2016-12\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-06\",\"2017-07\",\"2017-09\",\"2017-11\",\"2018-01\",\"2018-03\",\"2018-04\",\"2018-06\",\"2018-07\",\"2018-08\",\"2018-09\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-01\",\"2019-02\",\"2019-03\",\"2019-04\",\"2019-06\",\"2019-07\",\"2019-08\",\"2019-09\",\"2019-11\",\"2019-12\",\"2020-01\"],\"y\":[\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Action localization\",\"Activity detection\",\"Activity detection\",\"Activity detection\",\"Activity detection\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity localization\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Activity recognition\",\"Emotion recognition\",\"Emotion recognition\",\"Emotion recognition\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Gesture recognition\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image classification\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image generation\",\"Image-to-image translation\",\"Image-to-image translation\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object detection\",\"Object recognition\",\"Object recognition\",\"Object recognition\",\"Object recognition\",\"Object recognition\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Object tracking\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other 3D task\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other image process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other video process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Other vision process\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose estimation\",\"Pose tracking\",\"Pose tracking\",\"Pose tracking\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\",\"Semantic segmentation\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}},{\"hovertemplate\":[\"<BR>task: Other image process<BR>date: 2012-08<BR>ratio: 0.5546<BR>benchmarks:<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Image classification<BR>date: 2012-12<BR>ratio: 0.0189<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Activity recognition<BR>date: 2013-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Skeleton Based Action Recognition: CAD-120 - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2013-02<BR>ratio: 0.1073<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2013-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Facial Expression Recognition: FER2013 - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2013-11<BR>ratio: 0.0397<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2013-12<BR>ratio: 0.0553<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2014-04<BR>ratio: 0.021<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>\",\"<BR>task: Activity recognition<BR>date: 2014-06<BR>ratio: 0.8122<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2014-06<BR>ratio: 0.289<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2014-06<BR>ratio: 0.4737<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2014-09<BR>ratio: 0.9752<BR>benchmarks:<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-Olympic - Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2014-09<BR>ratio: 0.1446<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2014-11<BR>ratio: 0.2834<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Object detection<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Detection: Caltech - Pedestrian Detection benchmarking - Reasonable Miss Rate<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2014-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2014-12<BR>ratio: 0.2375<BR>benchmarks:<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>\",\"<BR>task: Image classification<BR>date: 2014-12<BR>ratio: 0.1392<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2014-12<BR>ratio: 0.2513<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-02<BR>ratio: 0.0877<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-02<BR>ratio: 0.0451<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2015-02<BR>ratio: 0.0134<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2015-02<BR>ratio: 0.6408<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2015-03<BR>ratio: 0.3753<BR>benchmarks:<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-03<BR>ratio: 0.2327<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-03<BR>ratio: 0.0741<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Other vision process<BR>date: 2015-04<BR>ratio: 0.1007<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Other image process<BR>date: 2015-04<BR>ratio: 0.426<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-04<BR>ratio: 0.2878<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Other vision process<BR>date: 2015-05<BR>ratio: 0.24846666666666664<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2015-05<BR>ratio: 0.1675<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-05<BR>ratio: 0.8426<BR>benchmarks:<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>\",\"<BR>task: Image classification<BR>date: 2015-06<BR>ratio: 0.4459<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Object detection<BR>date: 2015-06<BR>ratio: 0.2457<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-08<BR>ratio: 0.4321<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-09<BR>ratio: 0.5<BR>benchmarks:<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2015-11<BR>ratio: 0.17486666666666664<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Image classification<BR>date: 2015-11<BR>ratio: 0.3169<BR>benchmarks:<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2015-11<BR>ratio: 0.58925<BR>benchmarks:<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Jaccard Index<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Pose estimation<BR>date: 2015-11<BR>ratio: 0.0331<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Other image process<BR>date: 2015-11<BR>ratio: 0.1694<BR>benchmarks:<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>\",\"<BR>task: Object detection<BR>date: 2015-11<BR>ratio: 0.4339<BR>benchmarks:<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Object detection<BR>date: 2015-12<BR>ratio: 0.5016<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>\",\"<BR>task: Image classification<BR>date: 2015-12<BR>ratio: 0.47796666666666665<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2015-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Action Recognition: ActivityNet - Action Recognition benchmarking - mAP<BR>\",\"<BR>task: Other vision process<BR>date: 2015-12<BR>ratio: 0.7477<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-01<BR>ratio: 0.6524<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Image generation<BR>date: 2016-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: Binarized MNIST - Image Generation benchmarking - nats<BR>\",\"<BR>task: Action localization<BR>date: 2016-01<BR>ratio: 0.0478<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-01<BR>ratio: 0.0707<BR>benchmarks:<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Image classification<BR>date: 2016-02<BR>ratio: 0.056<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Multimodal Activity Recognition: MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2016-03<BR>ratio: 0.2255<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-03<BR>ratio: 0.1471<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2016-03<BR>ratio: 0.40365<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Object detection<BR>date: 2016-03<BR>ratio: 0.3359<BR>benchmarks:<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other vision process<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-03<BR>ratio: 0.7322<BR>benchmarks:<BR>  Pose Estimation: FLIC Elbows - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: FLIC Wrists - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-04<BR>ratio: 0.21814999999999998<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>\",\"<BR>task: Other image process<BR>date: 2016-04<BR>ratio: 0.5532666666666667<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - NMI<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>\",\"<BR>task: Other vision process<BR>date: 2016-04<BR>ratio: 0.5947<BR>benchmarks:<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-relRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Image classification<BR>date: 2016-05<BR>ratio: 0.1069<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Other vision process<BR>date: 2016-05<BR>ratio: 0.1741<BR>benchmarks:<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-05<BR>ratio: 0.0418<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Image generation<BR>date: 2016-06<BR>ratio: 0.46724999999999994<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-06<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-06<BR>ratio: 0.0741<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Object detection<BR>date: 2016-06<BR>ratio: 0.31905<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>\",\"<BR>task: Other vision process<BR>date: 2016-07<BR>ratio: 0.0241<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>\",\"<BR>task: Other vision process<BR>date: 2016-08<BR>ratio: 0.6356666666666667<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Signs-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>  Horizon Line Estimation: Eurasian Cities Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: York Urban Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>\",\"<BR>task: Object detection<BR>date: 2016-08<BR>ratio: 0.2999<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-08<BR>ratio: 0.4643<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-08<BR>ratio: 0.1233<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Other image process<BR>date: 2016-08<BR>ratio: 0.36235<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Image classification<BR>date: 2016-08<BR>ratio: 0.0251<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Object recognition<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Backpack<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Gender<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Hat<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCS<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCS<BR>\",\"<BR>task: Action localization<BR>date: 2016-09<BR>ratio: 0.2784<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-09<BR>ratio: 0.0093<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>\",\"<BR>task: Object detection<BR>date: 2016-09<BR>ratio: 0.0144<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other vision process<BR>date: 2016-09<BR>ratio: 0.4115<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-09<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>\",\"<BR>task: Image generation<BR>date: 2016-10<BR>ratio: 0.0606<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Image classification<BR>date: 2016-10<BR>ratio: 0.0142<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Action localization<BR>date: 2016-11<BR>ratio: 0.3571<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-11<BR>ratio: 0.674<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-11<BR>ratio: 0.3399<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>\",\"<BR>task: Other image process<BR>date: 2016-11<BR>ratio: 0.2441<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>\",\"<BR>task: Other vision process<BR>date: 2016-11<BR>ratio: 0.3954<BR>benchmarks:<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>\",\"<BR>task: Object tracking<BR>date: 2016-11<BR>ratio: 0.096<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>\",\"<BR>task: Object detection<BR>date: 2016-11<BR>ratio: 0.27403333333333335<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other video process<BR>date: 2016-11<BR>ratio: 0.2492<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>\",\"<BR>task: Image classification<BR>date: 2016-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-11<BR>ratio: 0.26530000000000004<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Global Accuracy<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Other 3D task<BR>date: 2016-12<BR>ratio: 0.77015<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Reconstruction: Data3D\\u2212R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>\",\"<BR>task: Object detection<BR>date: 2016-12<BR>ratio: 0.07565<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>\",\"<BR>task: Other image process<BR>date: 2016-12<BR>ratio: 0.1001<BR>benchmarks:<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>\",\"<BR>task: Pose estimation<BR>date: 2016-12<BR>ratio: 0.5639000000000001<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>\",\"<BR>task: Image generation<BR>date: 2016-12<BR>ratio: 0.1288<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Other video process<BR>date: 2016-12<BR>ratio: 0.1022<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2016-12<BR>ratio: 0.33213333333333334<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Activity recognition<BR>date: 2016-12<BR>ratio: 0.1429<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-01<BR>ratio: 0.16765000000000002<BR>benchmarks:<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-02<BR>ratio: 0.0692<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Image generation<BR>date: 2017-02<BR>ratio: 0.0541<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Image classification<BR>date: 2017-02<BR>ratio: 0.6995<BR>benchmarks:<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>\",\"<BR>task: Object detection<BR>date: 2017-02<BR>ratio: 0.9524<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Medium MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Small MR^-2<BR>\",\"<BR>task: Object detection<BR>date: 2017-03<BR>ratio: 0.1565<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-03<BR>ratio: 0.2235<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-03<BR>ratio: 0.1193<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Activity detection<BR>date: 2017-03<BR>ratio: 0.2205<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>\",\"<BR>task: Image generation<BR>date: 2017-03<BR>ratio: 0.13935<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>\",\"<BR>task: Action localization<BR>date: 2017-03<BR>ratio: 0.1096<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-03<BR>ratio: 0.44627500000000003<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Other image process<BR>date: 2017-03<BR>ratio: 0.5645<BR>benchmarks:<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-03<BR>ratio: 0.3988<BR>benchmarks:<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: SBU - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2017-03<BR>ratio: 0.3612<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Other image process<BR>date: 2017-04<BR>ratio: 0.5854250000000001<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Color Image Denoising: BSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: FRGC - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-04<BR>ratio: 0.20765<BR>benchmarks:<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-04<BR>ratio: 0.2189333333333333<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>\",\"<BR>task: Other vision process<BR>date: 2017-04<BR>ratio: 0.32234<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Object tracking<BR>date: 2017-04<BR>ratio: 0.103<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>\",\"<BR>task: Other 3D task<BR>date: 2017-04<BR>ratio: 0.8514<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: Sydney Urban Objects - 3D Point Cloud Classification benchmarking - F1<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-04<BR>ratio: 0.51655<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>\",\"<BR>task: Image classification<BR>date: 2017-04<BR>ratio: 0.4855<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2017-04<BR>ratio: 0.1476<BR>benchmarks:<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-04<BR>ratio: 0.15055000000000002<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-05<BR>ratio: 0.2803<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>\",\"<BR>task: Other vision process<BR>date: 2017-05<BR>ratio: 0.5805666666666667<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-05<BR>ratio: 0.53035<BR>benchmarks:<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CS)<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>\",\"<BR>task: Action localization<BR>date: 2017-05<BR>ratio: 0.2561<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Frame-mAP<BR>\",\"<BR>task: Gesture recognition<BR>date: 2017-05<BR>ratio: 0.8352<BR>benchmarks:<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-05<BR>ratio: 0.2366<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-06<BR>ratio: 0.4898<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Other 3D task<BR>date: 2017-06<BR>ratio: 0.4844<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Object tracking<BR>date: 2017-06<BR>ratio: 0.6175<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Seen)<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-06<BR>ratio: 0.6277<BR>benchmarks:<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2017-06<BR>ratio: 0.6641<BR>benchmarks:<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other image process<BR>date: 2017-06<BR>ratio: 0.7104<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-06<BR>ratio: 0.2771333333333333<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>\",\"<BR>task: Image generation<BR>date: 2017-06<BR>ratio: 0.2514<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Other vision process<BR>date: 2017-06<BR>ratio: 0.37770000000000004<BR>benchmarks:<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>\",\"<BR>task: Image classification<BR>date: 2017-07<BR>ratio: 0.0363<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-07<BR>ratio: 0.8307<BR>benchmarks:<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-07<BR>ratio: 0.6997<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - ME (%, all)<BR>\",\"<BR>task: Object detection<BR>date: 2017-07<BR>ratio: 0.3431666666666667<BR>benchmarks:<BR>  Object Detection: Visual Genome - Object Detection benchmarking - MAP<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other video process<BR>date: 2017-07<BR>ratio: 0.2484<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Other vision process<BR>date: 2017-07<BR>ratio: 0.2633333333333333<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-08<BR>ratio: 0.4921<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Image classification<BR>date: 2017-08<BR>ratio: 0.219<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Object detection<BR>date: 2017-08<BR>ratio: 0.3489<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - Average MAE<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-08<BR>ratio: 0.5913<BR>benchmarks:<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-08<BR>ratio: 0.5744<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-08<BR>ratio: 0.50415<BR>benchmarks:<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Other vision process<BR>date: 2017-08<BR>ratio: 0.17363333333333333<BR>benchmarks:<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2017-09<BR>ratio: 0.0809<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Other image process<BR>date: 2017-09<BR>ratio: 0.8514<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Other vision process<BR>date: 2017-09<BR>ratio: 0.3414<BR>benchmarks:<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Image generation<BR>date: 2017-09<BR>ratio: 0.043050000000000005<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Pancreas Segmentation: TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking - Dice Score<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-09<BR>ratio: 0.0551<BR>benchmarks:<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  3D Human Pose Estimation: Total Capture - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>\",\"<BR>task: Object detection<BR>date: 2017-10<BR>ratio: 0.9976<BR>benchmarks:<BR>  Lane Detection: Caltech Lanes Cordova - Lane Detection benchmarking - F1<BR>  Lane Detection: Caltech Lanes Washington - Lane Detection benchmarking - F1<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: UCF - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>\",\"<BR>task: Image classification<BR>date: 2017-10<BR>ratio: 0.29335<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-10<BR>ratio: 0.656<BR>benchmarks:<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>\",\"<BR>task: Image generation<BR>date: 2017-10<BR>ratio: 0.5297<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Other vision process<BR>date: 2017-10<BR>ratio: 0.7097<BR>benchmarks:<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-10<BR>ratio: 0.23095<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-10<BR>ratio: 0.9847<BR>benchmarks:<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Other image process<BR>date: 2017-10<BR>ratio: 0.5759333333333333<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Object tracking<BR>date: 2017-10<BR>ratio: 0.6429<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-11<BR>ratio: 0.435<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-11<BR>ratio: 0.3768<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>  Pose Estimation: ITOP top-view - Pose Estimation benchmarking - Mean mAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-11<BR>ratio: 0.44705<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CS<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>\",\"<BR>task: Object detection<BR>date: 2017-11<BR>ratio: 0.5154000000000001<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Birds Eye View Object Detection: KITTI Cars Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP50<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>\",\"<BR>task: Image-to-image translation<BR>date: 2017-11<BR>ratio: 0.2407<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>\",\"<BR>task: Other vision process<BR>date: 2017-11<BR>ratio: 0.1634<BR>benchmarks:<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-11<BR>ratio: 0.40068333333333334<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ShapeNet - Semantic Segmentation benchmarking - Mean IoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>\",\"<BR>task: Other 3D task<BR>date: 2017-11<BR>ratio: 0.789<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>\",\"<BR>task: Pose estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2017 - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: J-HMDB - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>\",\"<BR>task: Activity recognition<BR>date: 2017-12<BR>ratio: 0.1885<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>\",\"<BR>task: Other vision process<BR>date: 2017-12<BR>ratio: 0.613<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SYNSIG-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2017-12<BR>ratio: 0.0083<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2017-12<BR>ratio: 0.4283<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>\",\"<BR>task: Activity detection<BR>date: 2017-12<BR>ratio: 0.5028<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>\",\"<BR>task: Pose tracking<BR>date: 2017-12<BR>ratio: 0.1898<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>\",\"<BR>task: Other image process<BR>date: 2017-12<BR>ratio: 0.046<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>\",\"<BR>task: Image generation<BR>date: 2017-12<BR>ratio: 0.5<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - IS<BR>\",\"<BR>task: Activity localization<BR>date: 2017-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2017-12<BR>ratio: 0.0439<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2017-12<BR>ratio: 0.0247<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Other image process<BR>date: 2018-01<BR>ratio: 0.3197<BR>benchmarks:<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-01<BR>ratio: 0.28373333333333334<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-1 (%)<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-5 (%)<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-01<BR>ratio: 0.5359<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-01<BR>ratio: 0.0583<BR>benchmarks:<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-01<BR>ratio: 0.44029999999999997<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Retinal OCT Disease Classification: Srinivasan2014 - Retinal OCT Disease Classification benchmarking - Acc<BR>\",\"<BR>task: Other vision process<BR>date: 2018-01<BR>ratio: 0.3059<BR>benchmarks:<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-02<BR>ratio: 0.9695<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMBD Early Action - Skeleton Based Action Recognition benchmarking - 10%<BR>\",\"<BR>task: Other vision process<BR>date: 2018-02<BR>ratio: 0.3996<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>\",\"<BR>task: Object tracking<BR>date: 2018-02<BR>ratio: 0.7714000000000001<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-02<BR>ratio: 0.628<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>\",\"<BR>task: Image generation<BR>date: 2018-02<BR>ratio: 0.3205<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-02<BR>ratio: 0.4785333333333333<BR>benchmarks:<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>\",\"<BR>task: Image classification<BR>date: 2018-02<BR>ratio: 0.1678<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>\",\"<BR>task: Object detection<BR>date: 2018-02<BR>ratio: 0.1698<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Pose tracking<BR>date: 2018-02<BR>ratio: 0.2155<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>\",\"<BR>task: Other image process<BR>date: 2018-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma50 - Color Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-03<BR>ratio: 0.1351<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-03<BR>ratio: 0.49684999999999996<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Face Identification: IJB-A - Face Identification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Object tracking<BR>date: 2018-03<BR>ratio: 0.1202<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>\",\"<BR>task: Image classification<BR>date: 2018-03<BR>ratio: 0.1184<BR>benchmarks:<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-03<BR>ratio: 0.24766666666666667<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Object detection<BR>date: 2018-03<BR>ratio: 0.62775<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: iSAID - Object Detection benchmarking - Average Precision<BR>  Weakly Supervised Object Detection: Watercolor2k - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other image process<BR>date: 2018-03<BR>ratio: 0.1099<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-03<BR>ratio: 0.46340000000000003<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - Average Accuracy<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Activity detection<BR>date: 2018-03<BR>ratio: 0.387<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>\",\"<BR>task: Other vision process<BR>date: 2018-03<BR>ratio: 0.1918<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Image generation<BR>date: 2018-03<BR>ratio: 0.3293<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Other video process<BR>date: 2018-04<BR>ratio: 0.6147<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Pose tracking<BR>date: 2018-04<BR>ratio: 0.6801<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>\",\"<BR>task: Object detection<BR>date: 2018-04<BR>ratio: 0.0067<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-04<BR>ratio: 0.21509999999999999<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-04<BR>ratio: 0.5245<BR>benchmarks:<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-04<BR>ratio: 0.211<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>\",\"<BR>task: Gesture recognition<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLearn val - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: Jester test - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>\",\"<BR>task: Action localization<BR>date: 2018-04<BR>ratio: 0.3804<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-04<BR>ratio: 0.27253333333333335<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>\",\"<BR>task: Other vision process<BR>date: 2018-04<BR>ratio: 0.3826<BR>benchmarks:<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Other image process<BR>date: 2018-04<BR>ratio: 0.3237<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-04<BR>ratio: 0.6517<BR>benchmarks:<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Dice Score<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Precision<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Recall<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-05<BR>ratio: 0.028<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-05<BR>ratio: 0.2306<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-05<BR>ratio: 0.857<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Facial Expression Recognition: Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-05<BR>ratio: 0.145<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-05<BR>ratio: 0.6543<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UAV-Human - Skeleton Based Action Recognition benchmarking - Average Accuracy<BR>\",\"<BR>task: Image generation<BR>date: 2018-05<BR>ratio: 0.2865<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Object detection<BR>date: 2018-05<BR>ratio: 0.6951<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other vision process<BR>date: 2018-05<BR>ratio: 0.23425<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Other image process<BR>date: 2018-05<BR>ratio: 0.6039<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Other image process<BR>date: 2018-06<BR>ratio: 0.2047<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Activity localization<BR>date: 2018-06<BR>ratio: 0.7435<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (test)<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>\",\"<BR>task: Other video process<BR>date: 2018-06<BR>ratio: 0.3129<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>\",\"<BR>task: Object tracking<BR>date: 2018-06<BR>ratio: 0.2077<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-06<BR>ratio: 0.1128<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-06<BR>ratio: 0.29605000000000004<BR>benchmarks:<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-06<BR>ratio: 0.9967<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - 3DPCK<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - MJPE<BR>\",\"<BR>task: Emotion recognition<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>\",\"<BR>task: Object detection<BR>date: 2018-06<BR>ratio: 0.5132666666666666<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - F1 score<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Action localization<BR>date: 2018-06<BR>ratio: 0.3894<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>\",\"<BR>task: Other vision process<BR>date: 2018-06<BR>ratio: 0.7323200000000001<BR>benchmarks:<BR>  Formation Energy: Materials Project - Formation Energy benchmarking - MAE<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE log<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - SQ Rel<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Multivariate Time Series Imputation: MuJoCo - Multivariate Time Series Imputation benchmarking - MSE (10^2, 50% missing)<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - mse (10^-3)<BR>  Scene Graph Generation: VRD - Scene Graph Generation benchmarking - Recall-at-50<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Object recognition<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  Traffic Sign Recognition: Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking - MAP<BR>  Traffic Sign Recognition: Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking - MAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-06<BR>ratio: 0.3492333333333333<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: UT-Kinect - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Image generation<BR>date: 2018-07<BR>ratio: 0.7291<BR>benchmarks:<BR>  Image Generation: CAT 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>\",\"<BR>task: Object detection<BR>date: 2018-07<BR>ratio: 0.46819999999999995<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Bare MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Partial MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: ImageNet - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Activity localization<BR>date: 2018-07<BR>ratio: 0.4436<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Image classification<BR>date: 2018-07<BR>ratio: 0.53155<BR>benchmarks:<BR>  Hyperspectral Image Classification: Pavia University - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-07<BR>ratio: 0.5724<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - S-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - max E-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - mean Dice<BR>\",\"<BR>task: Other image process<BR>date: 2018-07<BR>ratio: 0.4427<BR>benchmarks:<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - PSNR (sRGB)<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - SSIM (sRGB)<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Other vision process<BR>date: 2018-07<BR>ratio: 0.2614<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-07<BR>ratio: 0.9928<BR>benchmarks:<BR>  Action Classification: ActivityNet-1.2 - Action Classification benchmarking - mAP<BR>  Action Classification: THUMOS\\u201914 - Action Classification benchmarking - mAP<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>\",\"<BR>task: Other video process<BR>date: 2018-08<BR>ratio: 0.3547<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-08<BR>ratio: 0.6807<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Other vision process<BR>date: 2018-08<BR>ratio: 0.2432<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - Runtime [ms]<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-08<BR>ratio: 0.0244<BR>benchmarks:<BR>  3D Reconstruction: Data3D\\u2212R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-08<BR>ratio: 0.6698<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: Freiburg Forest - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ScanNetV2 - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-09<BR>ratio: 0.14682499999999998<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-09<BR>ratio: 0.46865<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Other vision process<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>\",\"<BR>task: Image generation<BR>date: 2018-09<BR>ratio: 0.37644999999999995<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Emotion recognition<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>\",\"<BR>task: Other image process<BR>date: 2018-10<BR>ratio: 0.14425<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-10<BR>ratio: 0.0156<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>\",\"<BR>task: Image classification<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Sequential Image Classification: Sequential CIFAR-10 - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-10<BR>ratio: 0.2615<BR>benchmarks:<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>\",\"<BR>task: Other video process<BR>date: 2018-10<BR>ratio: 0.2609<BR>benchmarks:<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-10<BR>ratio: 0.0079<BR>benchmarks:<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>\",\"<BR>task: Other vision process<BR>date: 2018-11<BR>ratio: 0.4657<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MAE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MSE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - SSIM<BR>\",\"<BR>task: Activity localization<BR>date: 2018-11<BR>ratio: 0.1431<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>\",\"<BR>task: Object tracking<BR>date: 2018-11<BR>ratio: 0.3892<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Pose Estimation: DensePose-COCO - Pose Estimation benchmarking - AP<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Frames Per View<BR>\",\"<BR>task: Object detection<BR>date: 2018-11<BR>ratio: 0.3155<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-11<BR>ratio: 0.4542<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-11<BR>ratio: 0.40796666666666664<BR>benchmarks:<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2018-11<BR>ratio: 0.0808<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-11<BR>ratio: 0.87635<BR>benchmarks:<BR>  3D Reconstruction: Scan2CAD - 3D Reconstruction benchmarking - Average Accuracy<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>\",\"<BR>task: Emotion recognition<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>\",\"<BR>task: Image generation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Other image process<BR>date: 2018-11<BR>ratio: 0.43694999999999995<BR>benchmarks:<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Object detection<BR>date: 2018-12<BR>ratio: 0.30150000000000005<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2018-12<BR>ratio: 0.4456333333333333<BR>benchmarks:<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>\",\"<BR>task: Activity recognition<BR>date: 2018-12<BR>ratio: 0.500225<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - mAP (Val)<BR>  Action Recognition: AVA v2.2 - Action Recognition benchmarking - mAP<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Object tracking<BR>date: 2018-12<BR>ratio: 0.1675<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>\",\"<BR>task: Other 3D task<BR>date: 2018-12<BR>ratio: 0.027<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Image generation<BR>date: 2018-12<BR>ratio: 0.6087<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - bpd<BR>  Image Generation: CelebA-HQ 1024x1024 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>\",\"<BR>task: Pose estimation<BR>date: 2018-12<BR>ratio: 0.27595000000000003<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>\",\"<BR>task: Other vision process<BR>date: 2018-12<BR>ratio: 0.73865<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - OOB Rate (10^\\u22123)<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Difference<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Length<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Step Change (10^\\u22123)<BR>  Multivariate Time Series Imputation: PEMS-SF - Multivariate Time Series Imputation benchmarking - L2 Loss (10^-4)<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Unsupervised Domain Adaptation: Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Gesture recognition<BR>date: 2018-12<BR>ratio: 0.6789<BR>benchmarks:<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: NVGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2018-12<BR>ratio: 0.30485<BR>benchmarks:<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2018-12<BR>ratio: 0.4718<BR>benchmarks:<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Object detection<BR>date: 2019-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>\",\"<BR>task: Gesture recognition<BR>date: 2019-01<BR>ratio: 0.564<BR>benchmarks:<BR>  Hand Gesture Recognition: Cambridge - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-01<BR>ratio: 0.9117666666666667<BR>benchmarks:<BR>  Action Recognition: ICVL-4 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: IRD - Action Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: LboroHAR - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-01<BR>ratio: 0.16060000000000002<BR>benchmarks:<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Other image process<BR>date: 2019-01<BR>ratio: 0.1323<BR>benchmarks:<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Image classification<BR>date: 2019-01<BR>ratio: 0.3141<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Other 3D task<BR>date: 2019-01<BR>ratio: 0.24609999999999999<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D\\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>\",\"<BR>task: Other vision process<BR>date: 2019-01<BR>ratio: 0.3127<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-01<BR>ratio: 0.20259999999999997<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-02<BR>ratio: 0.5371333333333334<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (7 emotion)<BR>  Facial Expression Recognition: JAFFE - Facial Expression Recognition benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>  Facial Landmark Detection: AFLW-Full - Facial Landmark Detection benchmarking - Mean NME<BR>\",\"<BR>task: Image classification<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Hyperspectral Image Classification: Indian Pines - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-02<BR>ratio: 0.1<BR>benchmarks:<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>\",\"<BR>task: Pose tracking<BR>date: 2019-02<BR>ratio: 0.0183<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>\",\"<BR>task: Other vision process<BR>date: 2019-02<BR>ratio: 0.22343333333333334<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Other image process<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Retrieval: INRIA Holidays - Image Retrieval benchmarking - Mean mAP<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-02<BR>ratio: 0.0788<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Views<BR>\",\"<BR>task: Action localization<BR>date: 2019-03<BR>ratio: 0.6113999999999999<BR>benchmarks:<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>\",\"<BR>task: Other vision process<BR>date: 2019-03<BR>ratio: 0.5759749999999999<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Generalization: ImageNet-C - Domain Generalization benchmarking - mean Corruption Error (mCE)<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Image generation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 128x128 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-03<BR>ratio: 0.549<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mRec<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-03<BR>ratio: 0.1256<BR>benchmarks:<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Object detection<BR>date: 2019-03<BR>ratio: 0.2573<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - NDS<BR>\",\"<BR>task: Other image process<BR>date: 2019-03<BR>ratio: 0.46475<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: CUB-200-2011 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-04<BR>ratio: 0.355<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>\",\"<BR>task: Other video process<BR>date: 2019-04<BR>ratio: 0.1348<BR>benchmarks:<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>\",\"<BR>task: Other image process<BR>date: 2019-04<BR>ratio: 0.68665<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>\",\"<BR>task: Image classification<BR>date: 2019-04<BR>ratio: 0.3059<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Emotion recognition<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>\",\"<BR>task: Activity detection<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.1<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.2<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-04<BR>ratio: 0.3335<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP50<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP75<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>\",\"<BR>task: Action localization<BR>date: 2019-04<BR>ratio: 0.155<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>\",\"<BR>task: Other 3D task<BR>date: 2019-04<BR>ratio: 0.1081<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>\",\"<BR>task: Image generation<BR>date: 2019-04<BR>ratio: 0.52595<BR>benchmarks:<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - SSIM<BR>\",\"<BR>task: Object recognition<BR>date: 2019-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP at-0.5:0.95<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP-at-0.50<BR>\",\"<BR>task: Object detection<BR>date: 2019-04<BR>ratio: 0.500375<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-04<BR>ratio: 0.279925<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.1<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>\",\"<BR>task: Other vision process<BR>date: 2019-04<BR>ratio: 0.37775000000000003<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-05<BR>ratio: 0.7067<BR>benchmarks:<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>  Facial Expression Recognition: FERPlus - Facial Expression Recognition benchmarking - Accuracy<BR>  Facial Expression Recognition: SFEW - Facial Expression Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-05<BR>ratio: 0.7854<BR>benchmarks:<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-05<BR>ratio: 0.17579999999999998<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>\",\"<BR>task: Image classification<BR>date: 2019-05<BR>ratio: 0.1623<BR>benchmarks:<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>\",\"<BR>task: Pose tracking<BR>date: 2019-05<BR>ratio: 0.0083<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>\",\"<BR>task: Activity localization<BR>date: 2019-05<BR>ratio: 0.7002<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>\",\"<BR>task: Other vision process<BR>date: 2019-05<BR>ratio: 0.7222285714285714<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iMAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iRMSE<BR>  Domain Adaptation: HMDBfull-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Formation Energy: OQMD v1.2 - Formation Energy benchmarking - MAE<BR>  Horizon Line Estimation: Horizon Lines in the Wild - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Video Prediction: CMU Mocap-2 - Video Prediction benchmarking - Test Error<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Action localization<BR>date: 2019-06<BR>ratio: 0.0985<BR>benchmarks:<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-06<BR>ratio: 0.0932<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>\",\"<BR>task: Other video process<BR>date: 2019-06<BR>ratio: 0.6079<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Object detection<BR>date: 2019-06<BR>ratio: 0.45675000000000004<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - mean E-Measure<BR>\",\"<BR>task: Object tracking<BR>date: 2019-06<BR>ratio: 0.2823<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>\",\"<BR>task: Image classification<BR>date: 2019-06<BR>ratio: 0.2305<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 1 Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-06<BR>ratio: 0.1399<BR>benchmarks:<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-06<BR>ratio: 0.1741666666666667<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-5 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-06<BR>ratio: 0.4972200000000001<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  Brain Tumor Segmentation: BRATS-2015 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2017 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - Total Variation of Information<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Merge<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Split<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Other vision process<BR>date: 2019-06<BR>ratio: 0.6436666666666667<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: UCF CC 50 - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Activity localization<BR>date: 2019-06<BR>ratio: 0.0589<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2019-07<BR>ratio: 0.42693333333333333<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Moderate val - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-07<BR>ratio: 0.16535<BR>benchmarks:<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>\",\"<BR>task: Activity localization<BR>date: 2019-07<BR>ratio: 0.2415<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>\",\"<BR>task: Image generation<BR>date: 2019-07<BR>ratio: 0.0989<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Image-to-image translation<BR>date: 2019-07<BR>ratio: 0.7593<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>\",\"<BR>task: Action localization<BR>date: 2019-07<BR>ratio: 0.3796<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>\",\"<BR>task: Other vision process<BR>date: 2019-07<BR>ratio: 0.42065<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Object tracking<BR>date: 2019-07<BR>ratio: 0.835<BR>benchmarks:<BR>  Visual Object Tracking: VOT2016 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: VOT2017 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Unseen)<BR>\",\"<BR>task: Other video process<BR>date: 2019-07<BR>ratio: 0.4898<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>\",\"<BR>task: Image generation<BR>date: 2019-08<BR>ratio: 0.3<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-08<BR>ratio: 0.6697500000000001<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 5 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S1)<BR>\",\"<BR>task: Emotion recognition<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-08<BR>ratio: 0.3552333333333333<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>  Unsupervised Facial Landmark Detection: AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>\",\"<BR>task: Image classification<BR>date: 2019-08<BR>ratio: 0.6658<BR>benchmarks:<BR>  Document Image Classification: Noisy Bangla Characters - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: Noisy Bangla Numeral - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: n-MNIST - Document Image Classification benchmarking - Accuracy<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>\",\"<BR>task: Object detection<BR>date: 2019-08<BR>ratio: 0.3188<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-08<BR>ratio: 0.6767666666666666<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2013 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - AUC<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Scene Segmentation: SUN-RGBD - Scene Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>\",\"<BR>task: Activity localization<BR>date: 2019-08<BR>ratio: 0.1982<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>\",\"<BR>task: Other vision process<BR>date: 2019-08<BR>ratio: 0.491<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking - mIoU<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>\",\"<BR>task: Object detection<BR>date: 2019-09<BR>ratio: 0.1053<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>\",\"<BR>task: Other image process<BR>date: 2019-09<BR>ratio: 0.1412<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>\",\"<BR>task: Object tracking<BR>date: 2019-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-09<BR>ratio: 0.2029<BR>benchmarks:<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-09<BR>ratio: 0.41814999999999997<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: PASCAL VOC 2007 - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Other vision process<BR>date: 2019-09<BR>ratio: 0.3091<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: USPS-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-09<BR>ratio: 0.46025000000000005<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>\",\"<BR>task: Emotion recognition<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>\",\"<BR>task: Action localization<BR>date: 2019-09<BR>ratio: 0.26<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>\",\"<BR>task: Other vision process<BR>date: 2019-10<BR>ratio: 0.0388<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-10<BR>ratio: 0.0205<BR>benchmarks:<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-10<BR>ratio: 0.5483<BR>benchmarks:<BR>  Face Verification: AgeDB-30 - Face Verification benchmarking - Accuracy<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2019-10<BR>ratio: 0.1955<BR>benchmarks:<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>\",\"<BR>task: Other image process<BR>date: 2019-10<BR>ratio: 0.9715<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-10<BR>ratio: 0.3902<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SkyScapes-Lane - Semantic Segmentation benchmarking - Mean IoU<BR>\",\"<BR>task: Object recognition<BR>date: 2019-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: PA-100K - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: PETA - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: RAP - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2019-10<BR>ratio: 0.12633333333333333<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-11<BR>ratio: 0.39412500000000006<BR>benchmarks:<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes test - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - mIoU<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - oAcc<BR>\",\"<BR>task: Other vision process<BR>date: 2019-11<BR>ratio: 0.48816666666666664<BR>benchmarks:<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Unsupervised Domain Adaptation: PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking - AP-at-0.7<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-11<BR>ratio: 0.0114<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2019-11<BR>ratio: 0.045899999999999996<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>\",\"<BR>task: Activity localization<BR>date: 2019-11<BR>ratio: 0.3356<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>\",\"<BR>task: Image generation<BR>date: 2019-11<BR>ratio: 0.057<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Action localization<BR>date: 2019-11<BR>ratio: 0.0838<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>\",\"<BR>task: Facial recognition and modelling<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Facial Expression Recognition: Oulu-CASIA - Facial Expression Recognition benchmarking - Accuracy (10-fold)<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-11<BR>ratio: 0.9097999999999999<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: LineMOD - 6D Pose Estimation benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>\",\"<BR>task: Image classification<BR>date: 2019-11<BR>ratio: 0.7974<BR>benchmarks:<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>  Satellite Image Classification: SAT-6 - Satellite Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Activity recognition<BR>date: 2019-12<BR>ratio: 0.1018<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>\",\"<BR>task: Image classification<BR>date: 2019-12<BR>ratio: 0.2424<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Params<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>\",\"<BR>task: Other video process<BR>date: 2019-12<BR>ratio: 0.7122<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>\",\"<BR>task: Pose estimation<BR>date: 2019-12<BR>ratio: 0.0932<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2019-12<BR>ratio: 0.15665<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>\",\"<BR>task: Image generation<BR>date: 2019-12<BR>ratio: 0.8135<BR>benchmarks:<BR>  Image Generation: CIFAR-100 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Cat 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Churches 256 x 256 - Image Generation benchmarking - FID<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2020-01<BR>ratio: 0.13385<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>\",\"<BR>task: Other vision process<BR>date: 2020-01<BR>ratio: 0.4159<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>\",\"<BR>task: Image classification<BR>date: 2020-01<BR>ratio: 0.25<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>\",\"<BR>task: Object detection<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD - 3D Object Detection benchmarking - mAP-at-0.25<BR>\",\"<BR>task: Pose estimation<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Pose Estimation: UPenn Action - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>\",\"<BR>task: Other vision process<BR>date: 2020-02<BR>ratio: 0.35829999999999995<BR>benchmarks:<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>\",\"<BR>task: Pose estimation<BR>date: 2020-02<BR>ratio: 0.0125<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>\",\"<BR>task: Activity recognition<BR>date: 2020-02<BR>ratio: 0.4211<BR>benchmarks:<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>\",\"<BR>task: Action localization<BR>date: 2020-03<BR>ratio: 0.6966<BR>benchmarks:<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2020-03<BR>ratio: 0.6336<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  3D Instance Segmentation: SceneNN - 3D Instance Segmentation benchmarking - mAP-at-0.5<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>\",\"<BR>task: Object detection<BR>date: 2020-03<BR>ratio: 0.2605<BR>benchmarks:<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>\",\"<BR>task: Other video process<BR>date: 2020-03<BR>ratio: 0.8021<BR>benchmarks:<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>\",\"<BR>task: Pose estimation<BR>date: 2020-04<BR>ratio: 0.4396<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>\",\"<BR>task: Semantic segmentation<BR>date: 2020-04<BR>ratio: 0.22005000000000002<BR>benchmarks:<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: NYU Depth v2 - Semantic Segmentation benchmarking - Mean IoU<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>\"],\"marker\":{\"color\":[0.5546,0.0189,1.0,0.1073,1.0,0.0397,0.0553,0.021,0.8122,0.289,0.4737,0.9752,0.1446,0.2834,1.0,0.2406,0.2375,0.1392,0.2513,0.0877,0.0451,0.0134,0.6408,0.3753,0.2327,0.0741,0.1007,0.426,0.2878,0.24846666666666664,0.1675,0.8426,0.4459,0.2457,0.4321,0.5,0.17486666666666664,0.3169,0.58925,0.0331,0.1694,0.4339,0.5016,0.47796666666666665,1.0,0.7477,0.6524,1.0,0.0478,0.0707,0.056,1.0,0.2255,0.1471,0.40365,0.3359,0.2757,0.7322,0.21814999999999998,0.5532666666666667,0.5947,0.1069,0.1741,0.0418,0.46724999999999994,0.061,0.2999,0.0741,0.31905,0.0241,0.6356666666666667,0.2999,0.4643,0.1233,0.36235,0.0251,1.0,0.2784,0.0093,0.0144,0.4115,0.3082,0.0606,0.0142,0.3571,0.674,0.3399,0.2441,0.3954,0.096,0.27403333333333335,0.2492,0.0684,0.26530000000000004,0.77015,0.07565,0.1001,0.5639000000000001,0.1288,0.1022,0.33213333333333334,0.1429,0.16765000000000002,0.0692,0.0541,0.6995,0.9524,0.1565,0.2235,0.1193,0.2205,0.13935,0.1096,0.44627500000000003,0.5645,0.3988,0.3612,0.5854250000000001,0.20765,0.2189333333333333,0.32234,0.103,0.8514,0.51655,0.4855,0.1476,0.15055000000000002,0.2803,0.5805666666666667,0.53035,0.2561,0.8352,0.2366,0.4898,0.4844,0.6175,0.6277,0.6641,0.7104,0.2771333333333333,0.2514,0.37770000000000004,0.0363,0.8307,0.6997,0.3431666666666667,0.2484,0.2633333333333333,0.4921,0.219,0.3489,0.5913,0.5744,0.50415,0.17363333333333333,0.0809,0.8514,0.3414,0.043050000000000005,1.0,0.0551,1.0,0.9976,0.29335,0.656,0.5297,0.7097,0.23095,0.9847,0.5759333333333333,0.6429,0.435,0.3768,0.44705,0.5154000000000001,0.2407,0.1634,0.40068333333333334,0.789,1.0,0.1885,0.613,0.0083,0.4283,0.5028,0.1898,0.046,0.5,0.2406,0.0439,0.0247,0.3197,0.28373333333333334,0.5359,0.0583,0.44029999999999997,0.3059,0.2233,0.9695,0.3996,0.7714000000000001,0.628,0.3205,0.4785333333333333,0.1678,0.1698,0.2155,1.0,0.1351,0.49684999999999996,0.1202,0.1184,0.24766666666666667,0.62775,0.1099,0.46340000000000003,0.387,0.1918,0.3293,0.6147,0.6801,0.0067,0.21509999999999999,0.5245,0.211,1.0,0.3804,0.27253333333333335,0.3826,0.3237,0.6517,0.028,0.2306,0.857,0.145,0.6543,0.2865,0.6951,0.23425,0.6039,0.2047,0.7435,0.3129,0.2077,0.1128,0.29605000000000004,0.9967,0.7567,0.5132666666666666,0.3894,0.7323200000000001,1.0,0.3492333333333333,0.7291,0.46819999999999995,0.4436,0.53155,0.5724,0.4427,0.2614,0.9928,0.3547,0.6807,0.2432,0.0244,0.6698,0.14682499999999998,0.46865,0.1442,0.37644999999999995,0.2577,0.14425,0.0156,1.0,0.2615,0.2609,0.0079,0.4657,0.1431,0.3892,1.0,0.3155,0.4542,0.40796666666666664,0.0808,0.87635,0.5661,1.0,0.43694999999999995,0.30150000000000005,0.4456333333333333,0.500225,0.1675,0.027,0.6087,0.27595000000000003,0.73865,0.6789,0.30485,0.4718,0.0882,0.564,0.9117666666666667,0.16060000000000002,0.1323,0.3141,0.24609999999999999,0.3127,0.20259999999999997,0.5371333333333334,1.0,0.1,0.0183,0.22343333333333334,1.0,0.0788,1.0,0.6113999999999999,0.5759749999999999,1.0,0.549,0.1256,0.2573,0.46475,0.355,0.1348,0.68665,0.3059,1.0,1.0,0.3335,0.155,0.1081,0.52595,0.6667,0.500375,0.279925,0.37775000000000003,0.7067,0.7854,0.17579999999999998,0.1623,0.0083,0.7002,0.7222285714285714,0.0985,0.0932,0.6079,0.45675000000000004,0.2823,0.2305,0.1399,0.1741666666666667,0.4972200000000001,0.6436666666666667,0.0589,1.0,0.42693333333333333,0.16535,0.2415,0.0989,0.7593,0.3796,0.42065,0.835,0.4898,0.3,0.6697500000000001,0.3309,0.3552333333333333,0.6658,0.3188,0.6767666666666666,0.1982,0.491,0.1053,0.1412,0.0378,0.2029,0.41814999999999997,0.3091,0.46025000000000005,0.046,0.26,0.0388,0.0205,0.5483,0.1955,0.9715,0.3902,1.0,0.12633333333333333,0.39412500000000006,0.48816666666666664,0.0114,0.045899999999999996,0.3356,0.057,0.0838,1.0,0.9097999999999999,0.7974,0.1018,0.2424,0.7122,0.0932,0.15665,0.8135,0.13385,0.4159,0.25,1.0,1.0,0.35829999999999995,0.0125,0.4211,0.6966,0.6336,0.2605,0.8021,0.4396,0.22005000000000002],\"colorbar\":{\"len\":500,\"lenmode\":\"pixels\",\"thickness\":10,\"title\":{\"text\":\"ratio\"}},\"colorscale\":[[0.0,\"rgb(255,255,229)\"],[0.125,\"rgb(247,252,185)\"],[0.25,\"rgb(217,240,163)\"],[0.375,\"rgb(173,221,142)\"],[0.5,\"rgb(120,198,121)\"],[0.625,\"rgb(65,171,93)\"],[0.75,\"rgb(35,132,67)\"],[0.875,\"rgb(0,104,55)\"],[1.0,\"rgb(0,69,41)\"]],\"opacity\":0.7,\"showscale\":true,\"size\":20,\"symbol\":\"circle\",\"line\":{\"color\":\"black\",\"width\":1}},\"mode\":\"markers\",\"x\":[\"2012-08\",\"2012-12\",\"2013-02\",\"2013-02\",\"2013-07\",\"2013-11\",\"2013-12\",\"2014-04\",\"2014-06\",\"2014-06\",\"2014-06\",\"2014-09\",\"2014-09\",\"2014-11\",\"2014-12\",\"2014-12\",\"2014-12\",\"2014-12\",\"2014-12\",\"2015-02\",\"2015-02\",\"2015-02\",\"2015-02\",\"2015-03\",\"2015-03\",\"2015-03\",\"2015-04\",\"2015-04\",\"2015-04\",\"2015-05\",\"2015-05\",\"2015-05\",\"2015-06\",\"2015-06\",\"2015-08\",\"2015-09\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-11\",\"2015-12\",\"2015-12\",\"2015-12\",\"2015-12\",\"2016-01\",\"2016-01\",\"2016-01\",\"2016-01\",\"2016-02\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-03\",\"2016-04\",\"2016-04\",\"2016-04\",\"2016-05\",\"2016-05\",\"2016-05\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-06\",\"2016-07\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-08\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-09\",\"2016-10\",\"2016-10\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-11\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-02\",\"2017-02\",\"2017-02\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-03\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-04\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-05\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-06\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-07\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-08\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-09\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-10\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-11\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2017-12\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-01\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-02\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-03\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-04\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-05\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-06\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-07\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-08\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-09\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-10\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-11\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2018-12\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-01\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-02\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-03\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-04\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-05\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-06\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-07\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-08\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-09\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-10\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-11\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2019-12\",\"2020-01\",\"2020-01\",\"2020-01\",\"2020-01\",\"2020-01\",\"2020-02\",\"2020-02\",\"2020-02\",\"2020-03\",\"2020-03\",\"2020-03\",\"2020-03\",\"2020-04\",\"2020-04\"],\"y\":[\"Other image process\",\"Image classification\",\"Activity recognition\",\"Image classification\",\"Facial recognition and modelling\",\"Image classification\",\"Image classification\",\"Image classification\",\"Activity recognition\",\"Image classification\",\"Facial recognition and modelling\",\"Other vision process\",\"Image classification\",\"Semantic segmentation\",\"Object detection\",\"Facial recognition and modelling\",\"Activity recognition\",\"Image classification\",\"Semantic segmentation\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Image classification\",\"Other vision process\",\"Activity recognition\",\"Facial recognition and modelling\",\"Semantic segmentation\",\"Other vision process\",\"Other image process\",\"Semantic segmentation\",\"Other vision process\",\"Activity recognition\",\"Semantic segmentation\",\"Image classification\",\"Object detection\",\"Facial recognition and modelling\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Image classification\",\"Semantic segmentation\",\"Pose estimation\",\"Other image process\",\"Object detection\",\"Object detection\",\"Image classification\",\"Activity recognition\",\"Other vision process\",\"Pose estimation\",\"Image generation\",\"Action localization\",\"Activity recognition\",\"Image classification\",\"Activity recognition\",\"Image classification\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Object detection\",\"Other vision process\",\"Pose estimation\",\"Activity recognition\",\"Other image process\",\"Other vision process\",\"Image classification\",\"Other vision process\",\"Semantic segmentation\",\"Image generation\",\"Activity recognition\",\"Other vision process\",\"Semantic segmentation\",\"Object detection\",\"Other vision process\",\"Other vision process\",\"Object detection\",\"Activity recognition\",\"Pose estimation\",\"Other image process\",\"Image classification\",\"Object recognition\",\"Action localization\",\"Pose estimation\",\"Object detection\",\"Other vision process\",\"Activity recognition\",\"Image generation\",\"Image classification\",\"Action localization\",\"Pose estimation\",\"Activity recognition\",\"Other image process\",\"Other vision process\",\"Object tracking\",\"Object detection\",\"Other video process\",\"Image classification\",\"Semantic segmentation\",\"Other 3D task\",\"Object detection\",\"Other image process\",\"Pose estimation\",\"Image generation\",\"Other video process\",\"Semantic segmentation\",\"Activity recognition\",\"Pose estimation\",\"Pose estimation\",\"Image generation\",\"Image classification\",\"Object detection\",\"Object detection\",\"Pose estimation\",\"Facial recognition and modelling\",\"Activity detection\",\"Image generation\",\"Action localization\",\"Semantic segmentation\",\"Other image process\",\"Activity recognition\",\"Other vision process\",\"Other image process\",\"Semantic segmentation\",\"Activity recognition\",\"Other vision process\",\"Object tracking\",\"Other 3D task\",\"Pose estimation\",\"Image classification\",\"Object detection\",\"Facial recognition and modelling\",\"Activity recognition\",\"Other vision process\",\"Pose estimation\",\"Action localization\",\"Gesture recognition\",\"Facial recognition and modelling\",\"Facial recognition and modelling\",\"Other 3D task\",\"Object tracking\",\"Activity recognition\",\"Object detection\",\"Other image process\",\"Semantic segmentation\",\"Image generation\",\"Other vision process\",\"Image classification\",\"Pose estimation\",\"Facial recognition and modelling\",\"Object detection\",\"Other video process\",\"Other vision process\",\"Semantic segmentation\",\"Image classification\",\"Object detection\",\"Activity recognition\",\"Facial recognition and modelling\",\"Pose estimation\",\"Other vision process\",\"Image classification\",\"Other image process\",\"Other vision process\",\"Image generation\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Pose estimation\",\"Object detection\",\"Image classification\",\"Pose estimation\",\"Image generation\",\"Other vision process\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Other image process\",\"Object tracking\",\"Facial recognition and modelling\",\"Pose estimation\",\"Activity recognition\",\"Object detection\",\"Image-to-image translation\",\"Other vision process\",\"Semantic segmentation\",\"Other 3D task\",\"Pose estimation\",\"Activity recognition\",\"Other vision process\",\"Image classification\",\"Object detection\",\"Activity detection\",\"Pose tracking\",\"Other image process\",\"Image generation\",\"Activity localization\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Other image process\",\"Activity recognition\",\"Facial recognition and modelling\",\"Other 3D task\",\"Image classification\",\"Other vision process\",\"Semantic segmentation\",\"Activity recognition\",\"Other vision process\",\"Object tracking\",\"Other 3D task\",\"Image generation\",\"Semantic segmentation\",\"Image classification\",\"Object detection\",\"Pose tracking\",\"Other image process\",\"Other 3D task\",\"Facial recognition and modelling\",\"Object tracking\",\"Image classification\",\"Pose estimation\",\"Object detection\",\"Other image process\",\"Semantic segmentation\",\"Activity detection\",\"Other vision process\",\"Image generation\",\"Other video process\",\"Pose tracking\",\"Object detection\",\"Facial recognition and modelling\",\"Activity recognition\",\"Other 3D task\",\"Gesture recognition\",\"Action localization\",\"Pose estimation\",\"Other vision process\",\"Other image process\",\"Semantic segmentation\",\"Pose estimation\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Image classification\",\"Activity recognition\",\"Image generation\",\"Object detection\",\"Other vision process\",\"Other image process\",\"Other image process\",\"Activity localization\",\"Other video process\",\"Object tracking\",\"Facial recognition and modelling\",\"Semantic segmentation\",\"Pose estimation\",\"Emotion recognition\",\"Object detection\",\"Action localization\",\"Other vision process\",\"Object recognition\",\"Activity recognition\",\"Image generation\",\"Object detection\",\"Activity localization\",\"Image classification\",\"Semantic segmentation\",\"Other image process\",\"Other vision process\",\"Activity recognition\",\"Other video process\",\"Facial recognition and modelling\",\"Other vision process\",\"Other 3D task\",\"Semantic segmentation\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Other vision process\",\"Image generation\",\"Emotion recognition\",\"Other image process\",\"Activity recognition\",\"Image classification\",\"Semantic segmentation\",\"Other video process\",\"Facial recognition and modelling\",\"Other vision process\",\"Activity localization\",\"Object tracking\",\"Pose estimation\",\"Object detection\",\"Semantic segmentation\",\"Activity recognition\",\"Image classification\",\"Other 3D task\",\"Emotion recognition\",\"Image generation\",\"Other image process\",\"Object detection\",\"Semantic segmentation\",\"Activity recognition\",\"Object tracking\",\"Other 3D task\",\"Image generation\",\"Pose estimation\",\"Other vision process\",\"Gesture recognition\",\"Facial recognition and modelling\",\"Other image process\",\"Object detection\",\"Gesture recognition\",\"Activity recognition\",\"Semantic segmentation\",\"Other image process\",\"Image classification\",\"Other 3D task\",\"Other vision process\",\"Pose estimation\",\"Facial recognition and modelling\",\"Image classification\",\"Semantic segmentation\",\"Pose tracking\",\"Other vision process\",\"Other image process\",\"Pose estimation\",\"Pose estimation\",\"Action localization\",\"Other vision process\",\"Image generation\",\"Semantic segmentation\",\"Facial recognition and modelling\",\"Object detection\",\"Other image process\",\"Facial recognition and modelling\",\"Other video process\",\"Other image process\",\"Image classification\",\"Emotion recognition\",\"Activity detection\",\"Semantic segmentation\",\"Action localization\",\"Other 3D task\",\"Image generation\",\"Object recognition\",\"Object detection\",\"Activity recognition\",\"Other vision process\",\"Facial recognition and modelling\",\"Semantic segmentation\",\"Activity recognition\",\"Image classification\",\"Pose tracking\",\"Activity localization\",\"Other vision process\",\"Action localization\",\"Facial recognition and modelling\",\"Other video process\",\"Object detection\",\"Object tracking\",\"Image classification\",\"Pose estimation\",\"Activity recognition\",\"Semantic segmentation\",\"Other vision process\",\"Activity localization\",\"Semantic segmentation\",\"Object detection\",\"Activity recognition\",\"Activity localization\",\"Image generation\",\"Image-to-image translation\",\"Action localization\",\"Other vision process\",\"Object tracking\",\"Other video process\",\"Image generation\",\"Activity recognition\",\"Emotion recognition\",\"Facial recognition and modelling\",\"Image classification\",\"Object detection\",\"Semantic segmentation\",\"Activity localization\",\"Other vision process\",\"Object detection\",\"Other image process\",\"Object tracking\",\"Activity recognition\",\"Semantic segmentation\",\"Other vision process\",\"Pose estimation\",\"Emotion recognition\",\"Action localization\",\"Other vision process\",\"Pose estimation\",\"Facial recognition and modelling\",\"Image classification\",\"Other image process\",\"Semantic segmentation\",\"Object recognition\",\"Object detection\",\"Semantic segmentation\",\"Other vision process\",\"Activity recognition\",\"Object detection\",\"Activity localization\",\"Image generation\",\"Action localization\",\"Facial recognition and modelling\",\"Pose estimation\",\"Image classification\",\"Activity recognition\",\"Image classification\",\"Other video process\",\"Pose estimation\",\"Semantic segmentation\",\"Image generation\",\"Semantic segmentation\",\"Other vision process\",\"Image classification\",\"Object detection\",\"Pose estimation\",\"Other vision process\",\"Pose estimation\",\"Activity recognition\",\"Action localization\",\"Semantic segmentation\",\"Object detection\",\"Other video process\",\"Pose estimation\",\"Semantic segmentation\"],\"type\":\"scatter\",\"line\":{\"color\":\"black\",\"width\":0}}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Year\"},\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"tickmode\":\"auto\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{},\"categoryorder\":\"array\",\"categoryarray\":[\"Semantic segmentation\",\"Pose tracking\",\"Pose estimation\",\"Other vision process\",\"Other video process\",\"Other image process\",\"Other 3D task\",\"Object tracking\",\"Object recognition\",\"Object detection\",\"Image-to-image translation\",\"Image generation\",\"Image classification\",\"Gesture recognition\",\"Facial recognition and modelling\",\"Emotion recognition\",\"Activity recognition\",\"Activity localization\",\"Activity detection\",\"Action localization\"],\"showgrid\":true,\"gridcolor\":\"lightBlue\",\"side\":\"left\"},\"legend\":{\"title\":{\"text\":\"task\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Vision process\",\"y\":0.995},\"font\":{\"size\":21},\"showlegend\":false,\"plot_bgcolor\":\"white\",\"height\":1000.0,\"width\":1500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bdfb3503-22db-42d9-91b3-7fc557b02f47');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Computer Vision Process, showing anchors and complete trajectories (single arros removed).\n",
    "traj_df=plot_group_trajectory(\"ITO_00101\", \"Vision process\", anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b39iupUx5D3P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Reviewer: Summary Stats - Interactive Global Maps for CVP and NLP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
